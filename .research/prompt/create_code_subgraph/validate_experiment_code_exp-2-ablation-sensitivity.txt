
Input:
You are an AI code reviewer validating production-ready experiment code for research papers.

Your task is to compare the derived experiment_code with the original base_code to ensure that:
1. No important functionality has been omitted or truncated
2. All placeholders have been completely replaced with working implementations (no TODO, PLACEHOLDER, pass, or ... allowed)
3. The code is immediately executable and ready for research paper experiments
4. The derived code maintains the quality and completeness of the base foundation

# Instructions

## Core Validation Criteria
Check if the derived experiment code meets ALL of the following requirements:

1. **Complete Implementation Preservation**:
   - All functionality from base_code is preserved or properly enhanced
   - No code sections have been omitted or significantly shortened
   - Core algorithms and logic remain intact and functional
   - No reduction in code quality or completeness

2. **Complete Placeholder Replacement and Variation Implementation**:
   - All `DATASET_PLACEHOLDER` entries replaced with complete, working Hugging Face dataset loading
   - All `MODEL_PLACEHOLDER` entries replaced with complete, working model architectures
   - All `SPECIFIC_CONFIG_PLACEHOLDER` entries replaced with actual parameters
   - All run_variations are defined in both `config/smoke_test.yaml` and `config/full_experiment.yaml`
   - All run_variations are implemented in `src/model.py`
   - `config/smoke_test.yaml` contains ALL run variations in lightweight form
   - No TODO, PLACEHOLDER, pass, ..., or any incomplete implementations remain

3. **Functional Enhancement**:
   - Dataset-specific preprocessing is properly implemented
   - Model-specific configurations are correctly applied
   - Evaluation metrics are adapted for the specific experimental setup
   - All external resources are properly integrated

4. **Code Completeness**:
   - No truncated functions or incomplete implementations
   - All imports and dependencies are properly specified
   - Configuration files contain real experimental parameters
   - No "[UNCHANGED]" markers or similar placeholders remain

5. **Consistency with Base Code**:
   - Same file structure and organization
   - Consistent coding style and patterns
   - Proper error handling and logging maintained
   - All base functionality enhanced, not removed

## Detection of Common Issues
Flag the following problems if found:

- **Truncation**: Code sections that are significantly shorter than base_code equivalents
- **Omission**: Missing functions, classes, or important code blocks from base_code
- **Incomplete Replacement**: TODO, PLACEHOLDER, pass, ..., or any placeholder patterns that haven't been fully replaced with working code
- **Quality Degradation**: Simplified logic that reduces functionality
- **Structural Changes**: Unexpected modifications to the core architecture
- **Not Executable**: Code that cannot be run immediately due to missing implementations

## Output Format
Respond with a JSON object containing:
- `is_experiment_code_ready`: boolean - true if ALL criteria are met, false otherwise
- `experiment_code_issue`: string - specific issues found if any criteria are not met

# Current Research Method
{
    "Open Problems": "1. (Scope) Closed–form proximal updates in ProxTTA are limited to frozen Σ taken from the source domain. In practice, the Hessian/Fisher of the test distribution may drift, making the pre-conditioner sub-optimal or even harmful when the shift is large or non-stationary.\n2. (Expressiveness) Restricting adaptation to BN affine parameters fails when the source model uses other normalisers (LN, GN, RMSNorm) or when shifts mainly affect early convolutional filters or input statistics.\n3. (Safety) Even per-parameter trust-region steps can overshoot on extremely hard samples; a cheap on-line certificate of improvement is missing.\n4. (Latency) Skipping whole batches when the time budget is tight wastes potentially useful statistics; we need finer control that degrades gracefully instead of all-or-nothing.",
    "Methods": "We propose AdaNPC – Adaptive Natural-gradient & Probabilistically-Certified Test-time Adaptation.\n\nKey pieces:\nA. Streaming Fisher approximation  Σ̂_t  \na) maintain an exponential moving average of squared gradients g_t ⊙ g_t.  Σ̂_t = β Σ̂_{t-1}+(1-β)(g_t^2+ϵ) (diagonal)  (β=0.99).\nThis tracks curvature of the *current* test stream with O(|θ|) memory and ≤2 Hadamard ops.\n\nB. One-shot natural update  θ_{t+1}=θ_t−η Σ̂_t^{-1/2} g_t  with fixed η=1.  Scaling by Σ̂_t^{-1/2} (RMSprop view) keeps units stable; no learning-rate tuning.\n\nC. Probabilistic safety filter  Using Bernstein’s inequality we bound the change in entropy ΔL. We accept the update only if P(ΔL>0)≤δ (δ=0.1). Cost: one inner-product and pre-computed variance proxy.\n\nD. Normaliser-agnostic adaptors  Collect affine parameters of all normalisation layers (BN, LN, GN, RMSNorm) plus optional input colour-bias vector (3 extra params). Same code path, still O(|θ|).\n\nE. Micro-stepping scheduler  Instead of skipping batches, if wall-clock τ_t>τ_max we halve the micro-step count k (default k=4) so each batch gets a partial update using  θ_{t+1}=θ_t−(η/k) Σ̂_t^{-1/2} g_t repeated k_iter times or until budget met. Guarantees monotone accuracy-vs-time trade-off.\n\nAll hyper-parameters (β, δ, τ_max) have intuitive meanings and are insensitive; none depend on the model or dataset.",
    "Experimental Setup": "Code base: extend official Tent repo.\n\nModels & datasets: • ResNet-50-BN on ImageNet-C. • ViT-B/16-LN on ImageNet-C. • ResNet-18-GN on CIFAR-10-C. Streams: Realistic protocol with η∈{1,1/2,1/4}. Recurring PTTA Dirichlet δ=0.1.\n\nBaselines: Source, Tent, ProxTTA, EATA, RoTTA, CoTTA, Shrink-Tent.\n\nMetrics: 1) Online top-1 error under time penalty. 2) Time-to-90%-of-Tent accuracy. 3) Ratio of safe-filter rejections (<5% desired). 4) Extra memory (should <0.3 MB for R-50).",
    "Experimental Code": "class AdaNPC(Tent):\n    def __init__(self, model, beta=0.99, delta=0.1, tau_max=None):\n        super().__init__(model, torch.optim.SGD([],lr=1))\n        self.var = None            # Σ̂_t diagonal\n        self.beta=beta; self.delta=delta\n        self.tau_max=tau_max; self.k=4  # micro-steps\n        self.timer_ema=None\n    @torch.enable_grad()\n    def forward_and_adapt(self,x,model,opt):\n        t0=time.time()\n        y=model(x); loss=softmax_entropy(y).mean()\n        g=torch.autograd.grad(loss,self.params,create_graph=False)[0]\n        if self.var is None: self.var=g.pow(2)\n        else: self.var=self.beta*self.var+(1-self.beta)*g.pow(2)+1e-8\n        step=(g/self.var.sqrt())            # Σ̂^{-1/2}g\n        # safety: accept only if predicted ΔL negative with high prob\n        deltaL=(step*g).sum()              # first-order change\n        varL=((step.pow(2)*self.var).sum()).sqrt()\n        safe=(deltaL+varL*math.sqrt(2*math.log(1/self.delta)))<0\n        if safe:\n            k=max(1,self.k)\n            eta=1.0/k\n            for _ in range(k):\n                for p,s in zip(self.params,step): p-=eta*s\n        self.timer_ema=0.8*(self.timer_ema or 0)+0.2*(time.time()-t0)\n        if self.tau_max and self.timer_ema>self.tau_max and self.k>1:\n            self.k//=2     # micro-step back-off\n        model.zero_grad(); return model(x)",
    "Expected Result": "• AdaNPC matches Tent’s final accuracy after only 0.5 epochs of data (≈30% fewer samples) and beats ProxTTA by 1-2 pp on ImageNet-C.\n• Under η=1/4 it retains 93% of its full-speed accuracy, versus 75% for Tent and 88% for ProxTTA.\n• Safety filter rejects <3% of batches yet prevents all observed divergences on extreme corruptions (snow, impulse_noise).\n• Overhead: +|θ| vector and var buffer (0.25 MB for R-50), <5% extra FLOPs.",
    "Expected Conclusion": "AdaNPC turns TTA into a fast, normaliser-agnostic, and self-certified one-step natural-gradient procedure. By tracking curvature online it eliminates source-bias of fixed Fisher, while the probabilistic filter delivers theoretical safety guarantees. Fine-grained micro-stepping makes adaptation speed smoothly adjustable to real-time constraints. The method thus advances both the practical deployability and the theoretical grounding of rapid test-time adaptation."
}

# Experimental Design
## Experiment Strategy
Overall Goal:
Demonstrate that AdaNPC delivers (1) higher on-line performance, (2) better computational efficiency, (3) stronger robustness/safety and (4) wider architectural generalization than existing Test-Time Adaptation (TTA) techniques.

1. Validation Axes
   a. Performance Improvement – on-line accuracy/error under various distribution shifts.
   b. Efficiency – wall-clock latency, extra FLOPs, extra memory, and sample-efficiency (# test samples required to reach a target accuracy).
   c. Robustness & Safety – stability on extreme or non-stationary shifts; frequency of divergence and of safety-filter rejections; guarantee that accuracy never drops below the frozen source model.
   d. Generalization – effectiveness across architectures (BN, LN, GN, RMSNorm), data domains, and shift types (corruption intensity, temporal drift, sudden swaps).
   e. Graceful Degradation – accuracy–vs–time trade-off controlled by micro-stepping.

2. Required Comparisons
   • Strong baselines: Source (no TTA), Tent, ProxTTA, EATA, CoTTA, RoTTA, Shrink-Tent, and any contemporaneous state-of-the-art published before the submission deadline.
   • Internal ablations: (i) remove streaming Fisher (fall back to fixed Σ), (ii) remove probabilistic safety filter, (iii) remove micro-stepping, (iv) adapt BN only, (v) replace natural gradient with SGD; (vi) combine two removals to test interaction effects.
   • Sensitivity studies: vary β, δ, micro-step budget, and η to show hyper-parameter robustness.

3. Experimental Angles / Evidence Modalities
   A. Quantitative
      • Main metric: on-line top-1 error averaged over the whole stream.
      • Secondary: (i) area under the adaptation curve (AUC), (ii) time-to-X%-of-Tent accuracy, (iii) catastrophic failure rate (runs where error > source), (iv) % batches rejected by safety filter, (v) compute & memory overhead, (vi) energy proxy via NVIDIA-SMI.
      • Statistical treatment: 3 independent runs × 3 random seeds; report mean ± 95% CI; paired t-tests against best baseline.
   B. Qualitative / Diagnostic
      • Fisher drift plots (cosine similarity between Σ̂_t and source Σ_0).
      • Histograms of predicted ΔL vs empirical ΔL, highlighting safety bound tightness.
      • Accuracy–vs–latency curves when throttling τ_max.
      • Heat-map of component ablations across corruption severity.

4. Multi-Perspective Demonstration Plan
   • Orthogonal matrix: {Architectures} × {Datasets} × {Shift protocols}. Each cell runs the full comparison suite to show broad applicability.
   • Stress tests: (i) worst-case corruptions, (ii) synthetic non-stationary drift generated on-the-fly, (iii) adversarially sorted hard batches.
   • Real-time constraint scenario: enforce τ_max values (full, ½, ¼ of GPU budget) to showcase graceful degradation.
   • Safety spotlight: run 10×-long streams; count divergences; compare cumulative worst-case error to baselines.

5. Success Criteria (must hold on ≥80% of cells)
   • Accuracy: AdaNPC improves mean AUC by ≥2 pp over the best competing method with p<0.05.
   • Efficiency: <5% extra FLOPs, <0.5% extra VRAM, and reaches Tent’s final accuracy using ≥25% fewer test samples.
   • Robustness: zero catastrophic failures; safety filter rejection rate ≤5%.
   • Generalization: retains ≥90% of its ImageNet-C gain when ported to each other architecture/dataset without tuning.
   • Graceful degradation: under the strictest τ_max, retains ≥90% of its own full-speed accuracy while Tent drops below 80%.

6. Practical Considerations
   • All experiments run on one NVIDIA A100 (80 GB) node; resource accounting recorded via NVTX markers and pytorch profiler.
   • Unified codebase: start from official Tent repo, add modular hooks so baselines and AdaNPC share identical data loading, augmentation, synchronisation and precision settings.
   • Hyper-parameter policy: AdaNPC fixed defaults; baselines get per-dataset grid search as reported in their papers to avoid under-tuning claims.
   • Reproducibility: release seeds, config files, and slurm scripts; adherence to ML-Reproducibility Checklist.

This strategy provides a consistent, multi-faceted evaluation framework that will be reused verbatim in all subsequent experiments, ensuring that every study collectively builds a compelling, well-substantiated case for the effectiveness of AdaNPC.

# Base Code (Reference Foundation)
{"evaluate_py": "\"\"\"src/evaluate.py\nAggregates results from all run-variation sub-directories under\n\u003cresults_dir\u003e and creates comparative figures + JSON summary.\nIt also demonstrates *model loading* by re-evaluating each saved model on\nits validation split (sanity check / reproducibility guard).\n\"\"\"\nfrom __future__ import annotations\nimport argparse, json, os, pathlib\nfrom typing import Dict, List\nimport yaml\n\nimport torch\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nimport pandas as pd\nfrom sklearn.metrics import confusion_matrix\nfrom tqdm import tqdm\n\nfrom .preprocess import get_dataloaders\nfrom .model import get_model\n\nplt.switch_backend(\"Agg\")  # headless environments\n\n# ------------------------------ helpers ------------------------------------ #\n\ndef load_results_json(path: os.PathLike) -\u003e Dict:\n    with open(path, \"r\") as f:\n        return json.load(f)\n\n\ndef ensure_dir(d: os.PathLike):\n    pathlib.Path(d).mkdir(parents=True, exist_ok=True)\n\n\n# ------------------------------ evaluation ----------------------------------#\n\ndef create_lineplot(df: pd.DataFrame, results_dir: str):\n    img_dir = os.path.join(results_dir, \"images\")\n    ensure_dir(img_dir)\n\n    plt.figure(figsize=(6, 4))\n    for run_id, g in df.groupby(\"run_id\"):\n        plt.plot(g[\"epoch\"], g[\"val_acc\"], label=run_id)\n        # annotate final value\n        last_row = g.iloc[-1]\n        plt.annotate(f\"{last_row[\u0027val_acc\u0027]:.3f}\",\n                     (last_row[\"epoch\"], last_row[\"val_acc\"]),\n                     textcoords=\"offset points\", xytext=(0, 5), ha=\"center\")\n\n    plt.xlabel(\"Epoch\")\n    plt.ylabel(\"Validation Accuracy\")\n    plt.title(\"Validation Accuracy vs Epoch\")\n    plt.legend()\n    plt.tight_layout()\n    fname = os.path.join(img_dir, \"accuracy_curve.pdf\")\n    plt.savefig(fname, bbox_inches=\"tight\")\n    plt.close()\n\n    return fname\n\n\ndef create_barplot(summary: pd.DataFrame, results_dir: str):\n    img_dir = os.path.join(results_dir, \"images\")\n    ensure_dir(img_dir)\n\n    plt.figure(figsize=(6, 4))\n    ax = sns.barplot(data=summary, x=\"run_id\", y=\"best_val_acc\")\n    for p, acc in zip(ax.patches, summary[\"best_val_acc\"]):\n        height = p.get_height()\n        ax.annotate(f\"{acc:.3f}\",\n                    (p.get_x() + p.get_width() / 2., height),\n                    ha=\u0027center\u0027, va=\u0027bottom\u0027, fontsize=9)\n    plt.ylabel(\"Best Validation Accuracy\")\n    plt.xlabel(\"Run ID\")\n    plt.title(\"Best Validation Accuracy Across Runs\")\n    plt.xticks(rotation=45, ha=\"right\")\n    plt.tight_layout()\n    fname = os.path.join(img_dir, \"accuracy_comparison.pdf\")\n    plt.savefig(fname, bbox_inches=\"tight\")\n    plt.close()\n    return fname\n\n\ndef recompute_confusion(run_dir: str, cfg: Dict, device: torch.device):\n    \"\"\"Load saved *best* model and compute confusion matrix on val set.\"\"\"\n    ckpt_path = os.path.join(run_dir, \"model_best.pth\")\n    if not os.path.exists(ckpt_path):\n        return None  # skip if missing (should not happen)\n\n    # Rebuild dataloader to guarantee same split\n    _, val_loader, num_classes = get_dataloaders(cfg, seed=42)  # deterministic\n    model = get_model(cfg, num_classes=num_classes)\n    state = torch.load(ckpt_path, map_location=device)\n    model.load_state_dict(state[\"model_state\"], strict=True)\n    model.to(device).eval()\n\n    all_preds, all_labels = [], []\n    with torch.no_grad():\n        for x, y in val_loader:\n            x = x.to(device, non_blocking=True)\n            logits = model(x)\n            preds = logits.argmax(1).cpu()\n            all_preds.append(preds)\n            all_labels.append(y)\n    all_preds = torch.cat(all_preds).numpy()\n    all_labels = torch.cat(all_labels).numpy()\n\n    cm = confusion_matrix(all_labels, all_preds)\n    return cm\n\n\ndef plot_confusion(cm, run_id: str, results_dir: str):\n    img_dir = os.path.join(results_dir, \"images\")\n    ensure_dir(img_dir)\n    plt.figure(figsize=(5, 4))\n    sns.heatmap(cm, annot=False, cmap=\"Blues\", cbar=True)\n    plt.title(f\"Confusion Matrix \u2013 {run_id}\")\n    plt.xlabel(\"Predicted\")\n    plt.ylabel(\"True\")\n    plt.tight_layout()\n    fname = os.path.join(img_dir, f\"confusion_{run_id}.pdf\")\n    plt.savefig(fname, bbox_inches=\"tight\")\n    plt.close()\n    return fname\n\n\n# --------------------------------------------------------------------------- #\n\ndef run(results_dir: str):\n    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n\n    run_dirs = [d for d in pathlib.Path(results_dir).iterdir() if d.is_dir()]\n    if not run_dirs:\n        raise RuntimeError(f\"No run sub-directories found under {results_dir}\")\n\n    # -------------------------- aggregate metrics --------------------------- #\n    per_epoch_records: List[Dict] = []\n    summary_records: List[Dict] = []\n\n    for rd in run_dirs:\n        res_path = rd / \"results.json\"\n        if not res_path.exists():\n            print(f\"Warning: results.json missing for {rd.name} \u2013 skipping\")\n            continue\n        res = load_results_json(res_path)\n        for em in res[\"epoch_metrics\"]:\n            per_epoch_records.append({**em, \"run_id\": res[\"run_id\"]})\n        summary_records.append({\n            \"run_id\": res[\"run_id\"],\n            \"best_val_acc\": res[\"best_val_acc\"],\n            \"final_val_acc\": res[\"final_val_acc\"],\n        })\n\n        # ------------------ recompute confusion matrix ---------------------- #\n        cfg_path = rd / \"run_config.yaml\"\n        cfg = yaml.safe_load(open(cfg_path))\n        cm = recompute_confusion(str(rd), cfg, device)\n        if cm is not None:\n            plot_confusion(cm, res[\"run_id\"], results_dir)\n\n    df_epoch = pd.DataFrame(per_epoch_records)\n    df_summary = pd.DataFrame(summary_records)\n\n    fig1 = create_lineplot(df_epoch, results_dir)\n    fig2 = create_barplot(df_summary, results_dir)\n\n    output = {\n        \"n_runs\": len(df_summary),\n        \"figures\": [os.path.basename(fig1), os.path.basename(fig2)] +\n                    [f for f in os.listdir(os.path.join(results_dir, \"images\")) if f.startswith(\"confusion_\")],\n        \"comparative_metrics\": df_summary.to_dict(orient=\"records\"),\n    }\n    print(json.dumps(output))\n\n\n# --------------------------------------------------------------------------- #\n\ndef parse_args():\n    p = argparse.ArgumentParser()\n    p.add_argument(\"--results-dir\", required=True)\n    return p.parse_args()\n\n\nif __name__ == \"__main__\":\n    args = parse_args()\n    run(args.results_dir)\n", "full_experiment_yaml": "description:\n  goal: Placeholder for full experiment \u2013 will be replaced in the next phase.\n  note: All placeholders (DATASET_PLACEHOLDER, MODEL_PLACEHOLDER, SPECIFIC_CONFIG_PLACEHOLDER) will be concretised downstream.\nexperiments:\n  - run_id: EXPERIMENT_PLACEHOLDER_1\n    dataset:\n      name: DATASET_PLACEHOLDER\n      SPECIFIC_CONFIG_PLACEHOLDER: null\n    model:\n      name: MODEL_PLACEHOLDER\n      tta: none\n    training:\n      epochs: 100\n      batch_size: 128\n    optimizer:\n      name: SGD\n      lr: 0.1\n      momentum: 0.9\n  - run_id: EXPERIMENT_PLACEHOLDER_2\n    dataset:\n      name: DATASET_PLACEHOLDER\n      SPECIFIC_CONFIG_PLACEHOLDER: null\n    model:\n      name: MODEL_PLACEHOLDER\n      tta: adanpc\n      adanpc:\n        beta: 0.99\n        delta: 0.1\n        tau_max: null\n        micro_steps: 4\n    training:\n      epochs: 100\n      batch_size: 128\n    optimizer:\n      name: SGD\n      lr: 0.1\n      momentum: 0.9\n  # Additional experiment variations will be appended here during the derive-specific step.\n\n\n# PLACEHOLDER: This file\u2019s content will be fully specified later. Current keys\n# serve as a template ensuring the orchestrator can parse the structure.\n\n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n\n\n\n# End of file\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n  \n  \n# The excessive blank lines above ensure that future automated tools can safely\n# *append* new experiment entries without risking format corruption.\n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n\n# End of file\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n  \n  \n# -----------------------------------------------------------------------------\n# Large amounts of whitespace intentionally preserved for automated insertion.\n# -----------------------------------------------------------------------------\n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n\n# End full_experiment.yaml\n", "main_py": "\"\"\"src/main.py\nMaster orchestrator that sequentially runs all experiment variations defined\nin a YAML file and then triggers evaluation.\n\nUsage:\n  python -m src.main --smoke-test --results-dir \u003cpath\u003e\n  python -m src.main --full-experiment --results-dir \u003cpath\u003e\n\"\"\"\nfrom __future__ import annotations\nimport argparse, os, sys, subprocess, pathlib, shutil, json, yaml, time, select\nfrom typing import List, Dict, Any\n\n# --------------------------------- helpers ----------------------------------#\n\ndef read_yaml(path: os.PathLike):\n    with open(path, \"r\") as f:\n        return yaml.safe_load(f)\n\n\ndef tee_subprocess(cmd: List[str], stdout_path: str, stderr_path: str):\n    \"\"\"Run *cmd* while streaming stdout/stderr live AND writing them to files.\"\"\"\n\n    with open(stdout_path, \"w\") as out_f, open(stderr_path, \"w\") as err_f:\n        process = subprocess.Popen(cmd, stdout=subprocess.PIPE, stderr=subprocess.PIPE, text=True, bufsize=1)\n\n        # Use select for non-blocking read \u2013 works on POSIX. On Windows we fall back to blocking.\n        stdout_lines, stderr_lines = [], []\n        while True:\n            reads = [process.stdout.fileno(), process.stderr.fileno()]\n            ret = select.select(reads, [], [], 0.1)[0]\n            for fd in ret:\n                if fd == process.stdout.fileno():\n                    line = process.stdout.readline()\n                    if line:\n                        print(line, end=\"\")\n                        out_f.write(line)\n                        stdout_lines.append(line)\n                elif fd == process.stderr.fileno():\n                    line = process.stderr.readline()\n                    if line:\n                        print(line, end=\"\", file=sys.stderr)\n                        err_f.write(line)\n                        stderr_lines.append(line)\n            if process.poll() is not None:  # process finished\n                # drain remaining\n                for line in process.stdout:\n                    print(line, end=\"\")\n                    out_f.write(line)\n                    stdout_lines.append(line)\n                for line in process.stderr:\n                    print(line, end=\"\", file=sys.stderr)\n                    err_f.write(line)\n                    stderr_lines.append(line)\n                break\n        return process.returncode\n\n\n# --------------------------------- main logic -------------------------------#\n\ndef run_all(cfg_path: str, results_dir: str):\n    cfg = read_yaml(cfg_path)\n    experiments: List[Dict[str, Any]] = cfg[\"experiments\"]\n\n    pathlib.Path(results_dir).mkdir(parents=True, exist_ok=True)\n\n    print(\"===================== Experiment description =====================\")\n    print(json.dumps(cfg.get(\"description\", {}), indent=2))\n    print(\"==================================================================\")\n\n    for exp in experiments:\n        run_id = exp[\"run_id\"]\n        run_dir = pathlib.Path(results_dir) / run_id\n        run_dir.mkdir(parents=True, exist_ok=True)\n\n        # Save variation config\n        run_cfg_path = run_dir / \"run_config.yaml\"\n        with open(run_cfg_path, \"w\") as f:\n            yaml.safe_dump(exp, f)\n\n        # Construct command\n        cmd = [sys.executable, \"-m\", \"src.train\", \"--config\", str(run_cfg_path), \"--results-dir\", str(results_dir)]\n        print(f\"\\n===== Launching run: {run_id} =====\")\n        rc = tee_subprocess(cmd, stdout_path=str(run_dir / \"stdout.log\"), stderr_path=str(run_dir / \"stderr.log\"))\n        if rc != 0:\n            raise RuntimeError(f\"Run {run_id} failed with return code {rc}\")\n\n    # After all runs \u2013 aggregate \u0026 evaluate\n    eval_cmd = [sys.executable, \"-m\", \"src.evaluate\", \"--results-dir\", str(results_dir)]\n    print(\"\\n===== Running evaluation across all variations =====\")\n    rc = tee_subprocess(eval_cmd, stdout_path=str(pathlib.Path(results_dir) / \"evaluate_stdout.log\"),\n                        stderr_path=str(pathlib.Path(results_dir) / \"evaluate_stderr.log\"))\n    if rc != 0:\n        raise RuntimeError(f\"Evaluation script failed with return code {rc}\")\n\n\n# --------------------------------------------------------------------------- #\n\ndef parse_args():\n    p = argparse.ArgumentParser()\n    g = p.add_mutually_exclusive_group(required=True)\n    g.add_argument(\"--smoke-test\", action=\"store_true\", help=\"Use config/smoke_test.yaml\")\n    g.add_argument(\"--full-experiment\", action=\"store_true\", help=\"Use config/full_experiment.yaml\")\n    p.add_argument(\"--results-dir\", required=True, help=\"Directory to store results, figures, logs\")\n    return p.parse_args()\n\n\nif __name__ == \"__main__\":\n    args = parse_args()\n    root = pathlib.Path(__file__).resolve().parent.parent\n\n    if args.smoke_test:\n        cfg_path = root / \"config\" / \"smoke_test.yaml\"\n    else:\n        cfg_path = root / \"config\" / \"full_experiment.yaml\"\n\n    run_all(str(cfg_path), args.results_dir)\n", "model_py": "\"\"\"src/model.py\nModel definitions *and* wrappers implementing the AdaNPC Test-Time Adaptation\nalgorithm.  Baseline models can be anything available in torchvision; models\nthat require special handling must be added here in future experiment-specific\nextensions.\n\"\"\"\nfrom __future__ import annotations\nimport math, time\nfrom typing import Dict, Any\n\nimport torch, torch.nn as nn\nimport torchvision.models as tvm\n\n# --------------------------- utility functions ------------------------------ #\n\ndef get_backbone(backbone_name: str, num_classes: int, pretrained: bool = False) -\u003e nn.Module:\n    if backbone_name.lower() == \"resnet18\":\n        model = tvm.resnet18(pretrained=pretrained)\n        model.fc = nn.Linear(model.fc.in_features, num_classes)\n        return model\n    elif backbone_name.lower() == \"mobilenetv2\":\n        model = tvm.mobilenet_v2(pretrained=pretrained)\n        model.classifier[1] = nn.Linear(model.classifier[1].in_features, num_classes)\n        return model\n    elif backbone_name == \"MODEL_PLACEHOLDER\":\n        # PLACEHOLDER: Will be replaced with specific model architectures\n        raise NotImplementedError(\"Model placeholder \u2013 to be filled later.\")\n    else:\n        raise ValueError(f\"Unknown backbone: {backbone_name}\")\n\n\n# ---------------------- AdaNPC Test-Time Adaptation --------------------------#\nclass AdaNPCAdaptor(nn.Module):\n    \"\"\"Wraps a *base classifier* to perform on-line AdaNPC adaptation at test time.\"\"\"\n\n    def __init__(self, base_model: nn.Module, beta: float = 0.99, delta: float = 0.1,\n                 tau_max: float | None = None, micro_steps: int = 4):\n        super().__init__()\n        self.base_model = base_model\n        self.beta = beta\n        self.delta = delta\n        self.tau_max = tau_max\n        self.k = micro_steps\n        # Flattened parameter vector helper\n        self.params = [p for p in self.base_model.parameters() if p.requires_grad]\n        self.var = None\n        self.timer_ema = None\n\n    def forward(self, x):\n        return self.base_model(x)\n\n    @torch.enable_grad()\n    def adapt(self, x):\n        \"\"\"Perform AdaNPC one-step update using the current sample batch.\"\"\"\n        t0 = time.time()\n        y = self.base_model(x)\n        loss = nn.functional.cross_entropy(y, y.detach().argmax(1), reduction=\"mean\")  # dummy \u2013 gradients w.r.t. preds\n        g = torch.autograd.grad(loss, self.params, retain_graph=False, create_graph=False)\n\n        # Maintain streaming Fisher (diagonal)\n        g_vec = torch.cat([pgrad.flatten() for pgrad in g])\n        if self.var is None:\n            self.var = g_vec.pow(2).detach()\n        else:\n            self.var = self.beta * self.var + (1 - self.beta) * g_vec.pow(2).detach() + 1e-8\n\n        step = g_vec / self.var.sqrt()\n        deltaL = (step * g_vec).sum()\n        varL = (step.pow(2) * self.var).sum().sqrt()\n        safe = (deltaL + varL * math.sqrt(2 * math.log(1 / self.delta))) \u003c 0\n\n        if safe:\n            eta = 1.0 / max(1, self.k)\n            idx = 0\n            for p in self.params:\n                numel = p.numel()\n                upd = step[idx: idx + numel].view_as(p)\n                p.data.sub_(eta * upd)\n                idx += numel\n\n        # Micro-step scheduler\n        elapsed = time.time() - t0\n        if self.tau_max is not None:\n            self.timer_ema = 0.8 * (self.timer_ema or elapsed) + 0.2 * elapsed\n            if self.timer_ema \u003e self.tau_max and self.k \u003e 1:\n                self.k //= 2\n\n    # Utility to freeze or unfreeze all BN/LN/GN etc.  Not needed in baseline but kept for parity.\n    def toggle_norm_affine_only(self):\n        for m in self.base_model.modules():\n            if isinstance(m, (nn.BatchNorm2d, nn.LayerNorm, nn.GroupNorm)):\n                m.weight.requires_grad = True\n                m.bias.requires_grad = True\n            else:\n                for p in m.parameters(recurse=False):\n                    p.requires_grad = False\n\n\n# ------------------------- public factory API ------------------------------- #\n\ndef get_model(cfg: Dict[str, Any], num_classes: int):\n    model_cfg = cfg.get(\"model\", {})\n    name = model_cfg.get(\"name\", \"resnet18\")\n    pretrained = model_cfg.get(\"pretrained\", False)\n\n    base_model = get_backbone(name, num_classes, pretrained=pretrained)\n\n    if model_cfg.get(\"tta\", \"none\").lower() == \"adanpc\":\n        adanpc_cfg = model_cfg.get(\"adanpc\", {})\n        model = AdaNPCAdaptor(\n            base_model,\n            beta=adanpc_cfg.get(\"beta\", 0.99),\n            delta=adanpc_cfg.get(\"delta\", 0.1),\n            tau_max=adanpc_cfg.get(\"tau_max\"),\n            micro_steps=adanpc_cfg.get(\"micro_steps\", 4),\n        )\n        return model\n    else:\n        return base_model\n", "preprocess_py": "\"\"\"src/preprocess.py\nCommon data-loading \u0026 preprocessing utilities with *dataset placeholders*.\nAll experiments \u2013 no matter the dataset \u2013 must call the same public API so\nthat training / evaluation code stays unchanged.\n\"\"\"\nfrom __future__ import annotations\nimport os, random, math\nfrom typing import Tuple, Dict, Any, Optional\n\nimport torch\nfrom torch.utils.data import DataLoader, random_split\nfrom torchvision import transforms, datasets\n\n# ----------------------------- reproducibility ------------------------------ #\n\ndef seed_everything(seed: int):\n    random.seed(seed)\n    torch.manual_seed(seed)\n    torch.cuda.manual_seed_all(seed)\n    torch.backends.cudnn.deterministic = True\n\n\n# ----------------------------- transforms -----------------------------------#\n\ndef build_transforms(cfg: Dict[str, Any]):\n    im_size = cfg.get(\"dataset\", {}).get(\"img_size\", 224)\n    mean = cfg.get(\"dataset\", {}).get(\"mean\", [0.5, 0.5, 0.5])\n    std = cfg.get(\"dataset\", {}).get(\"std\", [0.5, 0.5, 0.5])\n    train_tfms = transforms.Compose([\n        transforms.Resize((im_size, im_size)),\n        transforms.ToTensor(),\n        transforms.Normalize(mean, std),\n    ])\n    test_tfms = transforms.Compose([\n        transforms.Resize((im_size, im_size)),\n        transforms.ToTensor(),\n        transforms.Normalize(mean, std),\n    ])\n    return train_tfms, test_tfms\n\n\n# ----------------------------- dataset loader --------------------------------#\n\ndef get_dataset(cfg: Dict[str, Any]):\n    ds_name = cfg.get(\"dataset\", {}).get(\"name\", \"FAKEDATA\")\n    root = cfg.get(\"dataset\", {}).get(\"root\", \"./data\")\n\n    train_tfms, test_tfms = build_transforms(cfg)\n\n    if ds_name.upper() == \"FAKEDATA\":\n        # Lightweight dataset for smoke tests \u2013 instant download-free.\n        num_classes = cfg.get(\"dataset\", {}).get(\"num_classes\", 10)\n        train_set = datasets.FakeData(size=1_000, image_size=(3, cfg.get(\"dataset\", {}).get(\"img_size\", 224), cfg.get(\"dataset\", {}).get(\"img_size\", 224)),\n                                      num_classes=num_classes, transform=train_tfms)\n        val_set = datasets.FakeData(size=200, image_size=(3, cfg.get(\"dataset\", {}).get(\"img_size\", 224), cfg.get(\"dataset\", {}).get(\"img_size\", 224)),\n                                    num_classes=num_classes, transform=test_tfms)\n        return train_set, val_set, num_classes\n\n    elif ds_name == \"DATASET_PLACEHOLDER\":\n        # PLACEHOLDER: Will be replaced with specific dataset loading logic\n        raise NotImplementedError(\"Dataset placeholder \u2013 to be filled in experiment-specific phase.\")\n\n    else:\n        raise ValueError(f\"Unknown dataset: {ds_name}\")\n\n\n# ----------------------------- dataloaders -----------------------------------#\n\ndef get_dataloaders(cfg: Dict[str, Any], seed: Optional[int] = None):\n    if seed is not None:\n        seed_everything(seed)\n\n    batch_size = cfg.get(\"training\", {}).get(\"batch_size\", 32)\n    num_workers = cfg.get(\"dataset\", {}).get(\"num_workers\", 4)\n\n    train_set, val_set, num_classes = get_dataset(cfg)\n\n    train_loader = DataLoader(train_set, batch_size=batch_size, shuffle=True,\n                              num_workers=num_workers, pin_memory=True, drop_last=True)\n    val_loader = DataLoader(val_set, batch_size=batch_size, shuffle=False,\n                            num_workers=num_workers, pin_memory=True)\n    return train_loader, val_loader, num_classes\n", "pyproject_toml": "[build-system]\nrequires = [\"setuptools\u003e=61.0\"]\nbuild-backend = \"setuptools.build_meta\"\n\n[project]\nname = \"adanpc_experiments\"\nversion = \"0.1.0\"\ndescription = \"Common Core Foundation for AdaNPC experiments\"\nreadme = \"README.md\"\nrequires-python = \"\u003e=3.10\"\n\n[project.dependencies]\ntorch = \"*\"\ntorchvision = \"*\"\npyyaml = \"*\"\nnumpy = \"*\"\npandas = \"*\"\nmatplotlib = \"*\"\nseaborn = \"*\"\nscikit-learn = \"*\"\ntqdm = \"*\"\n", "smoke_test_yaml": "description:\n  goal: Quick smoke test to ensure infrastructure works end-to-end.\n  dataset: Torchvision FakeData (1000 train / 200 val)\n  models: ResNet-18 baseline and AdaNPC wrapped ResNet-18\n\nauthor: \"COMMON CORE\"\nexperiments:\n  - run_id: baseline_fake\n    dataset:\n      name: FAKEDATA\n      img_size: 64\n      num_classes: 10\n      num_workers: 2\n    model:\n      name: resnet18\n      pretrained: false\n      tta: none\n    training:\n      epochs: 2\n      batch_size: 32\n    optimizer:\n      name: SGD\n      lr: 0.01\n      momentum: 0.9\n\n  - run_id: adanpc_fake\n    dataset:\n      name: FAKEDATA\n      img_size: 64\n      num_classes: 10\n      num_workers: 2\n    model:\n      name: resnet18\n      pretrained: false\n      tta: adanpc\n      adanpc:\n        beta: 0.99\n        delta: 0.1\n        micro_steps: 4\n    training:\n      epochs: 2\n      batch_size: 32\n    optimizer:\n      name: SGD\n      lr: 0.01\n      momentum: 0.9\n", "train_py": "\"\"\"src/train.py\nConducts a single experiment run as specified by an *individual* run-variation\nconfiguration (YAML) that already lives inside \u003cresults_dir\u003e/\u003crun_id\u003e/run_config.yaml\n\nThe script is **dataset / model agnostic** \u2013 all dataset-specific logic lives in\nsrc/preprocess.get_dataloaders and model definitions are pulled from\nsrc.model.get_model according to the config.  Therefore this file NEVER\ncontains placeholders.\n\"\"\"\nfrom __future__ import annotations\nimport argparse, json, os, sys, time, pathlib\nfrom typing import Dict, Any\n\nimport torch, torch.nn as nn\nfrom torch.optim import SGD, AdamW\nfrom torch.utils.data import DataLoader\nfrom torch.cuda.amp import autocast, GradScaler\n\nfrom tqdm import tqdm\nimport yaml\n\nfrom .preprocess import get_dataloaders\nfrom .model import get_model\n\n\n# ------------------------------- helpers ------------------------------------ #\n\ndef accuracy(pred: torch.Tensor, target: torch.Tensor) -\u003e float:\n    \"\"\"Top-1 accuracy.\"\"\"\n    with torch.no_grad():\n        pred_classes = pred.argmax(1)\n        correct = (pred_classes == target).sum().item()\n    return correct / target.size(0)\n\n\ndef save_json(path: os.PathLike, obj: Dict[str, Any]):\n    path = pathlib.Path(path)\n    with path.open(\"w\") as f:\n        json.dump(obj, f, indent=2)\n\n\n# ------------------------------- main train --------------------------------- #\n\ndef run(cfg: Dict[str, Any], results_dir: str):\n    torch.backends.cudnn.benchmark = True  # speed-up for fixed input size\n\n    run_id: str = cfg[\"run_id\"]\n    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n\n    ###########################################################################\n    # 1.  Data\n    ###########################################################################\n    train_loader, val_loader, num_classes = get_dataloaders(cfg)\n\n    ###########################################################################\n    # 2.  Model \u0026 Optimiser\n    ###########################################################################\n    model = get_model(cfg, num_classes=num_classes).to(device)\n    criterion = nn.CrossEntropyLoss().to(device)\n\n    optim_cfg = cfg.get(\"optimizer\", {\"name\": \"SGD\", \"lr\": 0.01, \"momentum\": 0.9})\n    if optim_cfg[\"name\"].lower() == \"sgd\":\n        optimizer = SGD(model.parameters(), lr=optim_cfg[\"lr\"], momentum=optim_cfg.get(\"momentum\", 0))\n    elif optim_cfg[\"name\"].lower() == \"adamw\":\n        optimizer = AdamW(model.parameters(), lr=optim_cfg[\"lr\"], weight_decay=optim_cfg.get(\"weight_decay\", 1e-2))\n    else:\n        raise ValueError(f\"Unsupported optimizer: {optim_cfg[\u0027name\u0027]}\")\n\n    scaler = GradScaler(enabled=cfg.get(\"mixed_precision\", True) and device.type == \"cuda\")\n\n    ###########################################################################\n    # 3.  Training loop\n    ###########################################################################\n    epochs = cfg.get(\"training\", {}).get(\"epochs\", 10)\n    log_every = max(1, len(train_loader) // 10)\n\n    epoch_metrics = []\n    best_val_acc = -1\n    best_ckpt_path = os.path.join(results_dir, run_id, \"model_best.pth\")\n\n    for epoch in range(epochs):\n        model.train()\n        running_loss, running_acc, n_samples = 0.0, 0.0, 0\n        pbar = tqdm(train_loader, desc=f\"[{run_id}] Train Epoch {epoch+1}/{epochs}\", leave=False)\n        for i, (x, y) in enumerate(pbar):\n            x, y = x.to(device, non_blocking=True), y.to(device, non_blocking=True)\n            optimizer.zero_grad(set_to_none=True)\n            with autocast(enabled=scaler.is_enabled()):\n                logits = model(x)\n                loss = criterion(logits, y)\n            scaler.scale(loss).backward()\n            scaler.step(optimizer)\n            scaler.update()\n\n            batch_acc = accuracy(logits.detach(), y)\n            batch_size = y.size(0)\n            running_loss += loss.item() * batch_size\n            running_acc += batch_acc * batch_size\n            n_samples += batch_size\n\n            if (i + 1) % log_every == 0:\n                pbar.set_postfix({\"loss\": running_loss / n_samples, \"acc\": running_acc / n_samples})\n\n        train_loss = running_loss / n_samples\n        train_acc = running_acc / n_samples\n\n        # ---------------------- validation ---------------------------------- #\n        model.eval()\n        val_loss, val_acc, n_val = 0.0, 0.0, 0\n        with torch.no_grad():\n            for x, y in val_loader:\n                x, y = x.to(device, non_blocking=True), y.to(device, non_blocking=True)\n                logits = model(x)\n                loss = criterion(logits, y)\n                batch_acc = accuracy(logits, y)\n\n                val_loss += loss.item() * y.size(0)\n                val_acc += batch_acc * y.size(0)\n                n_val += y.size(0)\n        val_loss /= n_val\n        val_acc /= n_val\n\n        epoch_metrics.append({\n            \"epoch\": epoch + 1,\n            \"train_loss\": train_loss,\n            \"train_acc\": train_acc,\n            \"val_loss\": val_loss,\n            \"val_acc\": val_acc,\n        })\n\n        # ---------------------- checkpointing ------------------------------- #\n        if val_acc \u003e best_val_acc:\n            best_val_acc = val_acc\n            torch.save({\n                \"model_state\": model.state_dict(),\n                \"cfg\": cfg,\n                \"epoch\": epoch + 1,\n                \"val_acc\": val_acc,\n            }, best_ckpt_path)\n\n    ###########################################################################\n    # 4.  Save artefacts \u0026 log final metrics\n    ###########################################################################\n    final_metrics = {\n        \"run_id\": run_id,\n        \"final_train_loss\": epoch_metrics[-1][\"train_loss\"],\n        \"final_train_acc\": epoch_metrics[-1][\"train_acc\"],\n        \"final_val_loss\": epoch_metrics[-1][\"val_loss\"],\n        \"final_val_acc\": epoch_metrics[-1][\"val_acc\"],\n        \"best_val_acc\": best_val_acc,\n        \"epochs\": epochs,\n        \"epoch_metrics\": epoch_metrics,\n    }\n\n    # last checkpoint (overwrite each run)\n    last_ckpt_path = os.path.join(results_dir, run_id, \"model_last.pth\")\n    torch.save({\n        \"model_state\": model.state_dict(),\n        \"cfg\": cfg,\n        \"epoch\": epochs,\n        \"val_acc\": epoch_metrics[-1][\"val_acc\"],\n    }, last_ckpt_path)\n\n    # save json   \n    save_json(os.path.join(results_dir, run_id, \"results.json\"), final_metrics)\n\n    # provide machine-readable output on stdout\n    print(json.dumps(final_metrics))\n\n\n# --------------------------------------------------------------------------- #\n\ndef parse_args():\n    p = argparse.ArgumentParser()\n    p.add_argument(\"--config\", required=True, help=\"Path to run-specific YAML config file\")\n    p.add_argument(\"--results-dir\", required=True, help=\"Root results directory for this whole experiment set\")\n    return p.parse_args()\n\n\nif __name__ == \"__main__\":\n    args = parse_args()\n    cfg = yaml.safe_load(open(args.config))\n    run(cfg, args.results_dir)\n"}

# Current Experiment (To be validated)
- Experiment ID: exp-2-ablation-sensitivity
- Description: Objective / Hypothesis:
Identify which algorithmic components of AdaNPC contribute most to performance, safety and efficiency, and verify that the method is stable over a wide hyper-parameter range.

Model / Dataset:
• ResNet-50 (BN) on ImageNet-C (severity 3 default stream).
Ablations are applied only to AdaNPC to keep comparison focused.

Component Definitions:
• fixed-Fisher – replace online Σ̂_t by frozen source Σ.
• no-safety-filter – skip Bernstein test, always step.
• no-micro-stepping – k=1, disables adaptive τ.
• SGD-adapter – use plain SGD (same η) instead of natural gradient.

Hyper-parameter Grids:
• β ∈ {0.9,0.95,0.99,0.999}
• δ ∈ {0.01,0.1,0.3}
• k_init ∈ {1,2,4,8}
A Latin-hyper-cube sample of 20 combos is evaluated for AdaNPC-full to produce sensitivity heat-maps.

Data Splitting & Seeds:
1 fixed corruption order × 3 random seeds.

Evaluation Metrics:
Primary – AUC; Secondary – # rejections, worst-case error spike (max over stream), FLOPs/sample.
Selection criterion for plotting – last checkpoint (no early stop).

Robustness Analyses:
• Noise-injection: Add 5 % i.i.d. impulse noise to every incoming batch online.
• OOD Fisher Drift: Interleave severity levels 1→5 every 200 batches.

Diagnostics Collected:
• Cosine similarity between Σ̂_t and Σ_0.
• Histogram of predicted vs empirical ΔL.

Example Code Snippet:
```bash
python run_ablation.py \
  --variant $VAR \
  --beta $BETA --delta $DELTA --k_init $K \
  --seed $SEED > logs/$VAR_$BETA_$DELTA_$K_$SEED.txt
```
- Run Variations: ['AdaNPC-full', 'fixed-Fisher', 'no-safety-filter', 'no-micro-stepping', 'SGD-adapter']

# Derived Experiment Code (To be validated)

{"evaluate_py": "\"\"\"src/evaluate.py\nAggregates results from all run-variation sub-directories under\n\u003cresults_dir\u003e and creates comparative figures + JSON summary.\nIt also demonstrates *model loading* by re-evaluating each saved model on\nits validation split (sanity check / reproducibility guard).\n\"\"\"\nfrom __future__ import annotations\nimport argparse, json, os, pathlib, math\nfrom typing import Dict, List\nimport yaml\n\nimport torch\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nimport pandas as pd\nfrom sklearn.metrics import confusion_matrix\nfrom tqdm import tqdm\n\nfrom .preprocess import get_dataloaders\nfrom .model import get_model, AdaNPCAdaptor  # exported for isinstance check\n\nplt.switch_backend(\"Agg\")  # headless environments\n\n# ------------------------------ helpers ------------------------------------ #\n\ndef load_results_json(path: os.PathLike) -\u003e Dict:\n    with open(path, \"r\") as f:\n        return json.load(f)\n\n\ndef ensure_dir(d: os.PathLike):\n    pathlib.Path(d).mkdir(parents=True, exist_ok=True)\n\n\n# ------------------------------ evaluation ----------------------------------#\n\ndef create_lineplot(df: pd.DataFrame, results_dir: str):\n    img_dir = os.path.join(results_dir, \"images\")\n    ensure_dir(img_dir)\n\n    plt.figure(figsize=(6, 4))\n    for run_id, g in df.groupby(\"run_id\"):\n        plt.plot(g[\"epoch\"], g[\"val_acc\"], label=run_id)\n        # annotate final value\n        last_row = g.iloc[-1]\n        plt.annotate(f\"{last_row[\u0027val_acc\u0027]:.3f}\",\n                     (last_row[\"epoch\"], last_row[\"val_acc\"]),\n                     textcoords=\"offset points\", xytext=(0, 5), ha=\"center\")\n\n    plt.xlabel(\"Epoch\")\n    plt.ylabel(\"Validation Accuracy\")\n    plt.title(\"Validation Accuracy vs Epoch\")\n    plt.legend()\n    plt.tight_layout()\n    fname = os.path.join(img_dir, \"accuracy_curve.pdf\")\n    plt.savefig(fname, bbox_inches=\"tight\")\n    plt.close()\n\n    return fname\n\n\ndef create_barplot(summary: pd.DataFrame, results_dir: str):\n    img_dir = os.path.join(results_dir, \"images\")\n    ensure_dir(img_dir)\n\n    plt.figure(figsize=(6, 4))\n    ax = sns.barplot(data=summary, x=\"run_id\", y=\"best_val_acc\")\n    for p, acc in zip(ax.patches, summary[\"best_val_acc\"]):\n        height = p.get_height()\n        ax.annotate(f\"{acc:.3f}\",\n                    (p.get_x() + p.get_width() / 2., height),\n                    ha=\u0027center\u0027, va=\u0027bottom\u0027, fontsize=9)\n    plt.ylabel(\"Best Validation Accuracy\")\n    plt.xlabel(\"Run ID\")\n    plt.title(\"Best Validation Accuracy Across Runs\")\n    plt.xticks(rotation=45, ha=\"right\")\n    plt.tight_layout()\n    fname = os.path.join(img_dir, \"accuracy_comparison.pdf\")\n    plt.savefig(fname, bbox_inches=\"tight\")\n    plt.close()\n    return fname\n\n\ndef recompute_confusion(run_dir: str, cfg: Dict, device: torch.device):\n    \"\"\"Load saved *best* model and compute confusion matrix on val set.\n    If the model supports test-time adaptation (AdaNPCAdaptor), we perform\n    on-line adaptation *per batch* during this re-evaluation to faithfully\n    reproduce TTA behaviour.\n    \"\"\"\n    ckpt_path = os.path.join(run_dir, \"model_best.pth\")\n    if not os.path.exists(ckpt_path):\n        return None  # skip if missing (should not happen)\n\n    # Rebuild dataloader to guarantee same split (deterministic seed)\n    _, val_loader, num_classes = get_dataloaders(cfg, seed=42)\n    model = get_model(cfg, num_classes=num_classes)\n    state = torch.load(ckpt_path, map_location=device)\n    model.load_state_dict(state[\"model_state\"], strict=True)\n    model.to(device).eval()\n\n    all_preds, all_labels = [], []\n    for x, y in tqdm(val_loader, desc=\"Eval\", leave=False):\n        x = x.to(device, non_blocking=True)\n        # ---------------------- AdaNPC adaptation ------------------------- #\n        if isinstance(model, AdaNPCAdaptor):\n            model.adapt(x)  # performs forward + parameter update internally\n        logits = model(x)\n        preds = logits.argmax(1).cpu()\n        all_preds.append(preds)\n        all_labels.append(y)\n    all_preds = torch.cat(all_preds).numpy()\n    all_labels = torch.cat(all_labels).numpy()\n\n    cm = confusion_matrix(all_labels, all_preds, labels=list(range(num_classes)))\n    return cm\n\n\ndef plot_confusion(cm, run_id: str, results_dir: str):\n    img_dir = os.path.join(results_dir, \"images\")\n    ensure_dir(img_dir)\n    plt.figure(figsize=(5, 4))\n    sns.heatmap(cm, annot=False, cmap=\"Blues\", cbar=True)\n    plt.title(f\"Confusion Matrix \u2013 {run_id}\")\n    plt.xlabel(\"Predicted\")\n    plt.ylabel(\"True\")\n    plt.tight_layout()\n    fname = os.path.join(img_dir, f\"confusion_{run_id}.pdf\")\n    plt.savefig(fname, bbox_inches=\"tight\")\n    plt.close()\n    return fname\n\n\n# --------------------------------------------------------------------------- #\n\ndef run(results_dir: str):\n    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n\n    run_dirs = [d for d in pathlib.Path(results_dir).iterdir() if d.is_dir()]\n    if not run_dirs:\n        raise RuntimeError(f\"No run sub-directories found under {results_dir}\")\n\n    # -------------------------- aggregate metrics --------------------------- #\n    per_epoch_records: List[Dict] = []\n    summary_records: List[Dict] = []\n\n    for rd in run_dirs:\n        res_path = rd / \"results.json\"\n        if not res_path.exists():\n            print(f\"Warning: results.json missing for {rd.name} \u2013 skipping\")\n            continue\n        res = load_results_json(res_path)\n        for em in res[\"epoch_metrics\"]:\n            per_epoch_records.append({**em, \"run_id\": res[\"run_id\"]})\n        summary_records.append({\n            \"run_id\": res[\"run_id\"],\n            \"best_val_acc\": res[\"best_val_acc\"],\n            \"final_val_acc\": res[\"final_val_acc\"],\n        })\n\n        # ------------------ recompute confusion matrix ---------------------- #\n        cfg_path = rd / \"run_config.yaml\"\n        cfg = yaml.safe_load(open(cfg_path))\n        cm = recompute_confusion(str(rd), cfg, device)\n        if cm is not None:\n            plot_confusion(cm, res[\"run_id\"], results_dir)\n\n    df_epoch = pd.DataFrame(per_epoch_records)\n    df_summary = pd.DataFrame(summary_records)\n\n    fig1 = create_lineplot(df_epoch, results_dir)\n    fig2 = create_barplot(df_summary, results_dir)\n\n    output = {\n        \"n_runs\": len(df_summary),\n        \"figures\": [os.path.basename(fig1), os.path.basename(fig2)] +\n                    [f for f in os.listdir(os.path.join(results_dir, \"images\")) if f.startswith(\"confusion_\")],\n        \"comparative_metrics\": df_summary.to_dict(orient=\"records\"),\n    }\n    print(json.dumps(output))\n\n\n# --------------------------------------------------------------------------- #\n\ndef parse_args():\n    p = argparse.ArgumentParser()\n    p.add_argument(\"--results-dir\", required=True)\n    return p.parse_args()\n\n\nif __name__ == \"__main__\":\n    args = parse_args()\n    run(args.results_dir)\n", "full_experiment_yaml": "description:\n  goal: Ablation \u0026 sensitivity study for AdaNPC components on corrupted\n    Mini-ImageNet-C (severity 3).\n  dataset: niuniandaji/mini-imagenet-c (100 classes)\n  runs: [\u0027AdaNPC-full\u0027, \u0027fixed-Fisher\u0027, \u0027no-safety-filter\u0027,\n         \u0027no-micro-stepping\u0027, \u0027SGD-adapter\u0027]\n  note: Each run is trained from scratch on the same data; differences only\n    manifest during *test-time* via the AdaNPC adaptor settings.\n\nauthor: \"ADA-NPC RESEARCH TEAM\"\nexperiments:\n  # ------------------------------------------------------------------------ #\n  - run_id: AdaNPC-full\n    dataset:\n      name: MINI_IMAGENET_C\n      severity: 3\n      img_size: 224\n      num_workers: 8\n    model:\n      name: resnet50\n      pretrained: true          # start from ImageNet-pretrained weights\n      tta: adanpc\n      adanpc:\n        beta: 0.99\n        delta: 0.1\n        micro_steps: 4\n        tau_max: null           # no time budget limit\n        online_fisher: true\n        safety_filter: true\n        micro_step_adapt: true\n        natural_gradient: true\n    training:\n      epochs: 5                 # keeping modest for run-time considerations\n      batch_size: 64\n    optimizer:\n      name: SGD\n      lr: 0.01\n      momentum: 0.9\n\n  # ------------------------- Fixed Fisher ablation ------------------------ #\n  - run_id: fixed-Fisher\n    dataset:\n      name: MINI_IMAGENET_C\n      severity: 3\n      img_size: 224\n      num_workers: 8\n    model:\n      name: resnet50\n      pretrained: true\n      tta: adanpc\n      adanpc:\n        beta: 0.99\n        delta: 0.1\n        micro_steps: 4\n        tau_max: null\n        online_fisher: false    # freeze \u03a3 after first batch\n        safety_filter: true\n        micro_step_adapt: true\n        natural_gradient: true\n    training:\n      epochs: 5\n      batch_size: 64\n    optimizer:\n      name: SGD\n      lr: 0.01\n      momentum: 0.9\n\n  # ------------------------- No safety filter ----------------------------- #\n  - run_id: no-safety-filter\n    dataset:\n      name: MINI_IMAGENET_C\n      severity: 3\n      img_size: 224\n      num_workers: 8\n    model:\n      name: resnet50\n      pretrained: true\n      tta: adanpc\n      adanpc:\n        beta: 0.99\n        delta: 0.1\n        micro_steps: 4\n        tau_max: null\n        online_fisher: true\n        safety_filter: false    # remove Bernstein guard\n        micro_step_adapt: true\n        natural_gradient: true\n    training:\n      epochs: 5\n      batch_size: 64\n    optimizer:\n      name: SGD\n      lr: 0.01\n      momentum: 0.9\n\n  # ------------------------- No micro-stepping ---------------------------- #\n  - run_id: no-micro-stepping\n    dataset:\n      name: MINI_IMAGENET_C\n      severity: 3\n      img_size: 224\n      num_workers: 8\n    model:\n      name: resnet50\n      pretrained: true\n      tta: adanpc\n      adanpc:\n        beta: 0.99\n        delta: 0.1\n        micro_steps: 1          # single shot\n        tau_max: null\n        online_fisher: true\n        safety_filter: true\n        micro_step_adapt: false # scheduler disabled\n        natural_gradient: true\n    training:\n      epochs: 5\n      batch_size: 64\n    optimizer:\n      name: SGD\n      lr: 0.01\n      momentum: 0.9\n\n  # ------------------------- SGD direction ablation ----------------------- #\n  - run_id: SGD-adapter\n    dataset:\n      name: MINI_IMAGENET_C\n      severity: 3\n      img_size: 224\n      num_workers: 8\n    model:\n      name: resnet50\n      pretrained: true\n      tta: adanpc\n      adanpc:\n        beta: 0.99\n        delta: 0.1\n        micro_steps: 4\n        tau_max: null\n        online_fisher: true\n        safety_filter: true\n        micro_step_adapt: true\n        natural_gradient: false # use raw gradients (SGD)\n    training:\n      epochs: 5\n      batch_size: 64\n    optimizer:\n      name: SGD\n      lr: 0.01\n      momentum: 0.9\n\n# End of full_experiment.yaml\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n", "main_py": "\"\"\"src/main.py\nMaster orchestrator that sequentially runs all experiment variations defined\nin a YAML file and then triggers evaluation.\n\nUsage:\n  python -m src.main --smoke-test --results-dir \u003cpath\u003e\n  python -m src.main --full-experiment --results-dir \u003cpath\u003e\n\"\"\"\nfrom __future__ import annotations\nimport argparse, os, sys, subprocess, pathlib, shutil, json, yaml, time, select\nfrom typing import List, Dict, Any\n\n# --------------------------------- helpers ----------------------------------#\n\ndef read_yaml(path: os.PathLike):\n    with open(path, \"r\") as f:\n        return yaml.safe_load(f)\n\n\ndef tee_subprocess(cmd: List[str], stdout_path: str, stderr_path: str):\n    \"\"\"Run *cmd* while streaming stdout/stderr live AND writing them to files.\"\"\"\n\n    with open(stdout_path, \"w\") as out_f, open(stderr_path, \"w\") as err_f:\n        process = subprocess.Popen(cmd, stdout=subprocess.PIPE, stderr=subprocess.PIPE, text=True, bufsize=1)\n\n        # Use select for non-blocking read \u2013 works on POSIX. On Windows we fall back to blocking.\n        stdout_lines, stderr_lines = [], []\n        while True:\n            reads = [process.stdout.fileno(), process.stderr.fileno()]\n            ret = select.select(reads, [], [], 0.1)[0]\n            for fd in ret:\n                if fd == process.stdout.fileno():\n                    line = process.stdout.readline()\n                    if line:\n                        print(line, end=\"\")\n                        out_f.write(line)\n                        stdout_lines.append(line)\n                elif fd == process.stderr.fileno():\n                    line = process.stderr.readline()\n                    if line:\n                        print(line, end=\"\", file=sys.stderr)\n                        err_f.write(line)\n                        stderr_lines.append(line)\n            if process.poll() is not None:  # process finished\n                # drain remaining\n                for line in process.stdout:\n                    print(line, end=\"\")\n                    out_f.write(line)\n                    stdout_lines.append(line)\n                for line in process.stderr:\n                    print(line, end=\"\", file=sys.stderr)\n                    err_f.write(line)\n                    stderr_lines.append(line)\n                break\n        return process.returncode\n\n\n# --------------------------------- main logic -------------------------------#\n\ndef run_all(cfg_path: str, results_dir: str):\n    cfg = read_yaml(cfg_path)\n    experiments: List[Dict[str, Any]] = cfg[\"experiments\"]\n\n    pathlib.Path(results_dir).mkdir(parents=True, exist_ok=True)\n\n    print(\"===================== Experiment description =====================\")\n    print(json.dumps(cfg.get(\"description\", {}), indent=2))\n    print(\"==================================================================\")\n\n    for exp in experiments:\n        run_id = exp[\"run_id\"]\n        run_dir = pathlib.Path(results_dir) / run_id\n        run_dir.mkdir(parents=True, exist_ok=True)\n\n        # Save variation config\n        run_cfg_path = run_dir / \"run_config.yaml\"\n        with open(run_cfg_path, \"w\") as f:\n            yaml.safe_dump(exp, f)\n\n        # Construct command\n        cmd = [sys.executable, \"-m\", \"src.train\", \"--config\", str(run_cfg_path), \"--results-dir\", str(results_dir)]\n        print(f\"\\n===== Launching run: {run_id} =====\")\n        rc = tee_subprocess(cmd, stdout_path=str(run_dir / \"stdout.log\"), stderr_path=str(run_dir / \"stderr.log\"))\n        if rc != 0:\n            raise RuntimeError(f\"Run {run_id} failed with return code {rc}\")\n\n    # After all runs \u2013 aggregate \u0026 evaluate\n    eval_cmd = [sys.executable, \"-m\", \"src.evaluate\", \"--results-dir\", str(results_dir)]\n    print(\"\\n===== Running evaluation across all variations =====\")\n    rc = tee_subprocess(eval_cmd, stdout_path=str(pathlib.Path(results_dir) / \"evaluate_stdout.log\"),\n                        stderr_path=str(pathlib.Path(results_dir) / \"evaluate_stderr.log\"))\n    if rc != 0:\n        raise RuntimeError(f\"Evaluation script failed with return code {rc}\")\n\n\n# --------------------------------------------------------------------------- #\n\ndef parse_args():\n    p = argparse.ArgumentParser()\n    g = p.add_mutually_exclusive_group(required=True)\n    g.add_argument(\"--smoke-test\", action=\"store_true\", help=\"Use config/smoke_test.yaml\")\n    g.add_argument(\"--full-experiment\", action=\"store_true\", help=\"Use config/full_experiment.yaml\")\n    p.add_argument(\"--results-dir\", required=True, help=\"Directory to store results, figures, logs\")\n    return p.parse_args()\n\n\nif __name__ == \"__main__\":\n    args = parse_args()\n    root = pathlib.Path(__file__).resolve().parent.parent\n\n    if args.smoke_test:\n        cfg_path = root / \"config\" / \"smoke_test.yaml\"\n    else:\n        cfg_path = root / \"config\" / \"full_experiment.yaml\"\n\n    run_all(str(cfg_path), args.results_dir)\n", "model_py": "\"\"\"src/model.py\nModel definitions and the AdaNPC Test-Time Adaptation wrapper implementing all\nablation variants required for *exp-2-ablation-sensitivity*.\n\"\"\"\nfrom __future__ import annotations\nimport math, time, warnings\nfrom typing import Dict, Any, List\n\nimport torch, torch.nn as nn\nimport torchvision.models as tvm\ntry:\n    import timm  # type: ignore\nexcept ImportError as e:  # pragma: no cover \u2013 timm is declared dependency\n    timm = None\n    warnings.warn(\"timm not installed \u2013 ResNet-50 backbone will not be available.\")\n\n# --------------------------- utility functions ------------------------------ #\n\ndef softmax_entropy(logits: torch.Tensor) -\u003e torch.Tensor:\n    \"\"\"Per-sample softmax entropy (returns **mean** over batch).\"\"\"\n    p = torch.softmax(logits, dim=1)\n    return -(p * torch.log_softmax(logits, dim=1)).sum(1).mean()\n\n\ndef _collect_affine_norm_params(model: nn.Module) -\u003e List[nn.Parameter]:\n    params: List[nn.Parameter] = []\n    for m in model.modules():\n        if isinstance(m, (nn.BatchNorm2d, nn.BatchNorm1d, nn.LayerNorm, nn.GroupNorm)):\n            if getattr(m, \"weight\", None) is not None:\n                m.weight.requires_grad_(True)\n                params.append(m.weight)\n            if getattr(m, \"bias\", None) is not None:\n                m.bias.requires_grad_(True)\n                params.append(m.bias)\n    return params\n\n\ndef get_backbone(backbone_name: str, num_classes: int, pretrained: bool = False) -\u003e nn.Module:\n    name = backbone_name.lower()\n    if name == \"resnet18\":\n        model = tvm.resnet18(pretrained=pretrained)\n        model.fc = nn.Linear(model.fc.in_features, num_classes)\n        return model\n    if name == \"resnet50\":\n        if timm is None:\n            raise RuntimeError(\"Requested ResNet-50 backbone but timm is missing.\")\n        model = timm.create_model(\"resnet50.a1_in1k\", pretrained=pretrained, num_classes=num_classes)\n        return model\n    if name == \"mobilenetv2\":\n        model = tvm.mobilenet_v2(pretrained=pretrained)\n        model.classifier[1] = nn.Linear(model.classifier[1].in_features, num_classes)\n        return model\n    raise ValueError(f\"Unknown backbone: {backbone_name}\")\n\n\n# ---------------------- AdaNPC Test-Time Adaptation --------------------------#\nclass AdaNPCAdaptor(nn.Module):\n    \"\"\"Wraps a *base classifier* to perform on-line AdaNPC adaptation at test\n    time.  Behaviour is controlled via boolean flags enabling ablation\n    variants (fixed Fisher, no safety filter, etc.).\"\"\"\n\n    def __init__(self, base_model: nn.Module,\n                 beta: float = 0.99,\n                 delta: float = 0.1,\n                 tau_max: float | None = None,\n                 micro_steps: int = 4,\n                 online_fisher: bool = True,\n                 safety_filter: bool = True,\n                 micro_step_adapt: bool = True,\n                 natural_gradient: bool = True):\n        super().__init__()\n        self.base_model = base_model\n        self.beta = beta\n        self.delta = delta\n        self.tau_max = tau_max\n        self.k = max(1, int(micro_steps))\n\n        # Flags for ablations\n        self.online_fisher = online_fisher\n        self.safety_filter = safety_filter\n        self.micro_step_adapt = micro_step_adapt\n        self.natural_gradient = natural_gradient\n\n        # Collect parameters to adapt (affine norms)\n        self.params: List[torch.nn.Parameter] = _collect_affine_norm_params(self.base_model)\n        if not self.params:\n            warnings.warn(\"AdaNPCAdaptor found no affine norm parameters \u2013 defaulting to all parameters.\")\n            self.params = [p for p in self.base_model.parameters() if p.requires_grad]\n\n        self.var: torch.Tensor | None = None  # diagonal Fisher / variance proxy\n        self.timer_ema: float | None = None\n\n    # --------------------------- forward pass ------------------------------ #\n    def forward(self, x):\n        return self.base_model(x)\n\n    # --------------------------- adaptation logic -------------------------- #\n    @torch.enable_grad()\n    def adapt(self, x: torch.Tensor):\n        \"\"\"Performs one AdaNPC update using the current *unlabelled* batch.\"\"\"\n        t0 = time.time()\n        logits = self.base_model(x)\n        loss = softmax_entropy(logits)\n        # gradients wrt *selected* parameters\n        grads = torch.autograd.grad(loss, self.params, retain_graph=False, create_graph=False)\n        g_vec = torch.cat([g.detach().flatten() for g in grads])\n\n        # -------------------- Fisher / variance update --------------------- #\n        if self.var is None:\n            self.var = g_vec.pow(2) + 1e-8\n        elif self.online_fisher:\n            self.var = self.beta * self.var + (1 - self.beta) * g_vec.pow(2) + 1e-8\n        # else: keep var frozen at initial estimate\n\n        # -------------------- compute update step -------------------------- #\n        if self.natural_gradient:\n            step_vec = g_vec / self.var.sqrt()\n        else:  # SGD direction\n            step_vec = g_vec\n\n        # -------------------- safety filter ------------------------------- #\n        if self.safety_filter:\n            deltaL = (step_vec * g_vec).sum()\n            varL = (step_vec.pow(2) * self.var).sum().sqrt()\n            bound = deltaL + varL * math.sqrt(2 * math.log(1 / self.delta))\n            safe = bound \u003c 0\n        else:\n            safe = True\n\n        if safe:\n            eta = 1.0 / self.k\n            idx = 0\n            for p in self.params:\n                numel = p.numel()\n                upd = step_vec[idx: idx + numel].view_as(p)\n                p.data.sub_(eta * upd)\n                idx += numel\n\n        # -------------------- micro-step scheduler ------------------------- #\n        elapsed = time.time() - t0\n        if self.tau_max is not None and self.micro_step_adapt:\n            self.timer_ema = 0.8 * (self.timer_ema or elapsed) + 0.2 * elapsed\n            if self.timer_ema \u003e self.tau_max and self.k \u003e 1:\n                self.k //= 2\n\n    # Convenience property for callers -------------------------------------- #\n    @property\n    def is_adaptive(self):\n        return True\n\n\n# ------------------------- public factory API ------------------------------- #\n\ndef get_model(cfg: Dict[str, Any], num_classes: int):\n    model_cfg = cfg.get(\"model\", {})\n    name = model_cfg.get(\"name\", \"resnet18\")\n    pretrained = model_cfg.get(\"pretrained\", False)\n\n    base_model = get_backbone(name, num_classes, pretrained=pretrained)\n\n    if model_cfg.get(\"tta\", \"none\").lower() == \"adanpc\":\n        ad_cfg = model_cfg.get(\"adanpc\", {})\n        model = AdaNPCAdaptor(\n            base_model,\n            beta=float(ad_cfg.get(\"beta\", 0.99)),\n            delta=float(ad_cfg.get(\"delta\", 0.1)),\n            tau_max=ad_cfg.get(\"tau_max\"),\n            micro_steps=int(ad_cfg.get(\"micro_steps\", 4)),\n            online_fisher=bool(ad_cfg.get(\"online_fisher\", True)),\n            safety_filter=bool(ad_cfg.get(\"safety_filter\", True)),\n            micro_step_adapt=bool(ad_cfg.get(\"micro_step_adapt\", True)),\n            natural_gradient=bool(ad_cfg.get(\"natural_gradient\", True)),\n        )\n        return model\n    else:\n        return base_model\n", "preprocess_py": "\"\"\"src/preprocess.py\nCommon data-loading \u0026 preprocessing utilities with *dataset placeholders*.\nAll experiments \u2013 no matter the dataset \u2013 must call the same public API so\nthat training / evaluation code stays unchanged.\n\"\"\"\nfrom __future__ import annotations\nimport os, random, math\nfrom typing import Tuple, Dict, Any, Optional\n\nimport torch\nfrom torch.utils.data import DataLoader, random_split\nfrom torchvision import transforms, datasets\n\n# ----------------------------- reproducibility ------------------------------ #\n\ndef seed_everything(seed: int):\n    random.seed(seed)\n    torch.manual_seed(seed)\n    torch.cuda.manual_seed_all(seed)\n    torch.backends.cudnn.deterministic = True\n\n\n# ----------------------------- transforms -----------------------------------#\n\ndef build_transforms(cfg: Dict[str, Any]):\n    im_size = cfg.get(\"dataset\", {}).get(\"img_size\", 224)\n    mean = cfg.get(\"dataset\", {}).get(\"mean\", [0.5, 0.5, 0.5])\n    std = cfg.get(\"dataset\", {}).get(\"std\", [0.5, 0.5, 0.5])\n    train_tfms = transforms.Compose([\n        transforms.Resize((im_size, im_size)),\n        transforms.ToTensor(),\n        transforms.Normalize(mean, std),\n    ])\n    test_tfms = transforms.Compose([\n        transforms.Resize((im_size, im_size)),\n        transforms.ToTensor(),\n        transforms.Normalize(mean, std),\n    ])\n    return train_tfms, test_tfms\n\n\n# ----------------------------- dataset loader --------------------------------#\n\ndef get_dataset(cfg: Dict[str, Any]):\n    ds_name = cfg.get(\"dataset\", {}).get(\"name\", \"FAKEDATA\")\n    root = cfg.get(\"dataset\", {}).get(\"root\", \"./data\")\n\n    train_tfms, test_tfms = build_transforms(cfg)\n\n    if ds_name.upper() == \"FAKEDATA\":\n        # Lightweight dataset for smoke tests \u2013 instant download-free.\n        num_classes = cfg.get(\"dataset\", {}).get(\"num_classes\", 10)\n        train_set = datasets.FakeData(size=1_000, image_size=(3, cfg.get(\"dataset\", {}).get(\"img_size\", 224), cfg.get(\"dataset\", {}).get(\"img_size\", 224)),\n                                      num_classes=num_classes, transform=train_tfms)\n        val_set = datasets.FakeData(size=200, image_size=(3, cfg.get(\"dataset\", {}).get(\"img_size\", 224), cfg.get(\"dataset\", {}).get(\"img_size\", 224)),\n                                    num_classes=num_classes, transform=test_tfms)\n        return train_set, val_set, num_classes\n\n    elif ds_name == \"DATASET_PLACEHOLDER\":\n        # PLACEHOLDER: Will be replaced with specific dataset loading logic\n        raise NotImplementedError(\"Dataset placeholder \u2013 to be filled in experiment-specific phase.\")\n\n    else:\n        raise ValueError(f\"Unknown dataset: {ds_name}\")\n\n\n# ----------------------------- dataloaders -----------------------------------#\n\ndef get_dataloaders(cfg: Dict[str, Any], seed: Optional[int] = None):\n    if seed is not None:\n        seed_everything(seed)\n\n    batch_size = cfg.get(\"training\", {}).get(\"batch_size\", 32)\n    num_workers = cfg.get(\"dataset\", {}).get(\"num_workers\", 4)\n\n    train_set, val_set, num_classes = get_dataset(cfg)\n\n    train_loader = DataLoader(train_set, batch_size=batch_size, shuffle=True,\n                              num_workers=num_workers, pin_memory=True, drop_last=True)\n    val_loader = DataLoader(val_set, batch_size=batch_size, shuffle=False,\n                            num_workers=num_workers, pin_memory=True)\n    return train_loader, val_loader, num_classes\n", "pyproject_toml": "[build-system]\nrequires = [\"setuptools\u003e=61.0\"]\nbuild-backend = \"setuptools.build_meta\"\n\n[project]\nname = \"adanpc_experiments\"\nversion = \"0.1.0\"\ndescription = \"Common Core Foundation for AdaNPC experiments\"\nreadme = \"README.md\"\nrequires-python = \"\u003e=3.10\"\n\n[project.dependencies]\ntorch = \"*\"\ntorchvision = \"*\"\npyyaml = \"*\"\nnumpy = \"*\"\npandas = \"*\"\nmatplotlib = \"*\"\nseaborn = \"*\"\nscikit-learn = \"*\"\ntqdm = \"*\"\ntimm = \"*\"\ndatasets = \"*\"\n", "smoke_test_yaml": "description:\n  goal: Quick smoke test to ensure infrastructure works end-to-end with all\n    AdaNPC ablation variants.\n  dataset: Torchvision FakeData (1000 train / 200 val)\n  models: ResNet-18 baseline and 5 AdaNPC variants (full + ablations)\n\nauthor: \"COMMON CORE\"\nexperiments:\n  # ------------------------------------------------------------------------ #\n  - run_id: baseline_fake\n    dataset:\n      name: FAKEDATA\n      img_size: 64\n      num_classes: 10\n      num_workers: 2\n    model:\n      name: resnet18\n      pretrained: false\n      tta: none\n    training:\n      epochs: 2\n      batch_size: 32\n    optimizer:\n      name: SGD\n      lr: 0.01\n      momentum: 0.9\n\n  # ------------------------- AdaNPC full variant --------------------------- #\n  - run_id: AdaNPC-full_fake\n    dataset:\n      name: FAKEDATA\n      img_size: 64\n      num_classes: 10\n      num_workers: 2\n    model:\n      name: resnet18\n      pretrained: false\n      tta: adanpc\n      adanpc:\n        beta: 0.99\n        delta: 0.1\n        micro_steps: 4\n        online_fisher: true\n        safety_filter: true\n        micro_step_adapt: true\n        natural_gradient: true\n    training:\n      epochs: 2\n      batch_size: 32\n    optimizer:\n      name: SGD\n      lr: 0.01\n      momentum: 0.9\n\n  # ------------------------- Fixed Fisher (no online) ---------------------- #\n  - run_id: fixed-Fisher_fake\n    dataset:\n      name: FAKEDATA\n      img_size: 64\n      num_classes: 10\n      num_workers: 2\n    model:\n      name: resnet18\n      pretrained: false\n      tta: adanpc\n      adanpc:\n        beta: 0.99\n        delta: 0.1\n        micro_steps: 4\n        online_fisher: false   # freeze after first batch\n        safety_filter: true\n        micro_step_adapt: true\n        natural_gradient: true\n    training:\n      epochs: 2\n      batch_size: 32\n    optimizer:\n      name: SGD\n      lr: 0.01\n      momentum: 0.9\n\n  # ------------------------- No safety filter ------------------------------ #\n  - run_id: no-safety-filter_fake\n    dataset:\n      name: FAKEDATA\n      img_size: 64\n      num_classes: 10\n      num_workers: 2\n    model:\n      name: resnet18\n      pretrained: false\n      tta: adanpc\n      adanpc:\n        beta: 0.99\n        delta: 0.1\n        micro_steps: 4\n        online_fisher: true\n        safety_filter: false   # remove safety\n        micro_step_adapt: true\n        natural_gradient: true\n    training:\n      epochs: 2\n      batch_size: 32\n    optimizer:\n      name: SGD\n      lr: 0.01\n      momentum: 0.9\n\n  # ------------------------- No micro-stepping ----------------------------- #\n  - run_id: no-micro-stepping_fake\n    dataset:\n      name: FAKEDATA\n      img_size: 64\n      num_classes: 10\n      num_workers: 2\n    model:\n      name: resnet18\n      pretrained: false\n      tta: adanpc\n      adanpc:\n        beta: 0.99\n        delta: 0.1\n        micro_steps: 1          # single step\n        online_fisher: true\n        safety_filter: true\n        micro_step_adapt: false # disable scheduler\n        natural_gradient: true\n    training:\n      epochs: 2\n      batch_size: 32\n    optimizer:\n      name: SGD\n      lr: 0.01\n      momentum: 0.9\n\n  # ------------------------- SGD adapter ----------------------------------- #\n  - run_id: SGD-adapter_fake\n    dataset:\n      name: FAKEDATA\n      img_size: 64\n      num_classes: 10\n      num_workers: 2\n    model:\n      name: resnet18\n      pretrained: false\n      tta: adanpc\n      adanpc:\n        beta: 0.99\n        delta: 0.1\n        micro_steps: 4\n        online_fisher: true\n        safety_filter: true\n        micro_step_adapt: true\n        natural_gradient: false  # plain SGD direction\n    training:\n      epochs: 2\n      batch_size: 32\n    optimizer:\n      name: SGD\n      lr: 0.01\n      momentum: 0.9\n", "train_py": "\"\"\"src/train.py\nConducts a single experiment run as specified by an *individual* run-variation\nconfiguration (YAML) that already lives inside \u003cresults_dir\u003e/\u003crun_id\u003e/run_config.yaml\n\nThe script is **dataset / model agnostic** \u2013 all dataset-specific logic lives in\nsrc/preprocess.get_dataloaders and model definitions are pulled from\nsrc.model.get_model according to the config.  Therefore this file NEVER\ncontains placeholders.\n\"\"\"\nfrom __future__ import annotations\nimport argparse, json, os, sys, time, pathlib\nfrom typing import Dict, Any\n\nimport torch, torch.nn as nn\nfrom torch.optim import SGD, AdamW\nfrom torch.utils.data import DataLoader\nfrom torch.cuda.amp import autocast, GradScaler\n\nfrom tqdm import tqdm\nimport yaml\n\nfrom .preprocess import get_dataloaders\nfrom .model import get_model\n\n\n# ------------------------------- helpers ------------------------------------ #\n\ndef accuracy(pred: torch.Tensor, target: torch.Tensor) -\u003e float:\n    \"\"\"Top-1 accuracy.\"\"\"\n    with torch.no_grad():\n        pred_classes = pred.argmax(1)\n        correct = (pred_classes == target).sum().item()\n    return correct / target.size(0)\n\n\ndef save_json(path: os.PathLike, obj: Dict[str, Any]):\n    path = pathlib.Path(path)\n    with path.open(\"w\") as f:\n        json.dump(obj, f, indent=2)\n\n\n# ------------------------------- main train --------------------------------- #\n\ndef run(cfg: Dict[str, Any], results_dir: str):\n    torch.backends.cudnn.benchmark = True  # speed-up for fixed input size\n\n    run_id: str = cfg[\"run_id\"]\n    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n\n    ###########################################################################\n    # 1.  Data\n    ###########################################################################\n    train_loader, val_loader, num_classes = get_dataloaders(cfg)\n\n    ###########################################################################\n    # 2.  Model \u0026 Optimiser\n    ###########################################################################\n    model = get_model(cfg, num_classes=num_classes).to(device)\n    criterion = nn.CrossEntropyLoss().to(device)\n\n    optim_cfg = cfg.get(\"optimizer\", {\"name\": \"SGD\", \"lr\": 0.01, \"momentum\": 0.9})\n    if optim_cfg[\"name\"].lower() == \"sgd\":\n        optimizer = SGD(model.parameters(), lr=optim_cfg[\"lr\"], momentum=optim_cfg.get(\"momentum\", 0))\n    elif optim_cfg[\"name\"].lower() == \"adamw\":\n        optimizer = AdamW(model.parameters(), lr=optim_cfg[\"lr\"], weight_decay=optim_cfg.get(\"weight_decay\", 1e-2))\n    else:\n        raise ValueError(f\"Unsupported optimizer: {optim_cfg[\u0027name\u0027]}\")\n\n    scaler = GradScaler(enabled=cfg.get(\"mixed_precision\", True) and device.type == \"cuda\")\n\n    ###########################################################################\n    # 3.  Training loop\n    ###########################################################################\n    epochs = cfg.get(\"training\", {}).get(\"epochs\", 10)\n    log_every = max(1, len(train_loader) // 10)\n\n    epoch_metrics = []\n    best_val_acc = -1\n    best_ckpt_path = os.path.join(results_dir, run_id, \"model_best.pth\")\n\n    for epoch in range(epochs):\n        model.train()\n        running_loss, running_acc, n_samples = 0.0, 0.0, 0\n        pbar = tqdm(train_loader, desc=f\"[{run_id}] Train Epoch {epoch+1}/{epochs}\", leave=False)\n        for i, (x, y) in enumerate(pbar):\n            x, y = x.to(device, non_blocking=True), y.to(device, non_blocking=True)\n            optimizer.zero_grad(set_to_none=True)\n            with autocast(enabled=scaler.is_enabled()):\n                logits = model(x)\n                loss = criterion(logits, y)\n            scaler.scale(loss).backward()\n            scaler.step(optimizer)\n            scaler.update()\n\n            batch_acc = accuracy(logits.detach(), y)\n            batch_size = y.size(0)\n            running_loss += loss.item() * batch_size\n            running_acc += batch_acc * batch_size\n            n_samples += batch_size\n\n            if (i + 1) % log_every == 0:\n                pbar.set_postfix({\"loss\": running_loss / n_samples, \"acc\": running_acc / n_samples})\n\n        train_loss = running_loss / n_samples\n        train_acc = running_acc / n_samples\n\n        # ---------------------- validation ---------------------------------- #\n        model.eval()\n        val_loss, val_acc, n_val = 0.0, 0.0, 0\n        with torch.no_grad():\n            for x, y in val_loader:\n                x, y = x.to(device, non_blocking=True), y.to(device, non_blocking=True)\n                logits = model(x)\n                loss = criterion(logits, y)\n                batch_acc = accuracy(logits, y)\n\n                val_loss += loss.item() * y.size(0)\n                val_acc += batch_acc * y.size(0)\n                n_val += y.size(0)\n        val_loss /= n_val\n        val_acc /= n_val\n\n        epoch_metrics.append({\n            \"epoch\": epoch + 1,\n            \"train_loss\": train_loss,\n            \"train_acc\": train_acc,\n            \"val_loss\": val_loss,\n            \"val_acc\": val_acc,\n        })\n\n        # ---------------------- checkpointing ------------------------------- #\n        if val_acc \u003e best_val_acc:\n            best_val_acc = val_acc\n            torch.save({\n                \"model_state\": model.state_dict(),\n                \"cfg\": cfg,\n                \"epoch\": epoch + 1,\n                \"val_acc\": val_acc,\n            }, best_ckpt_path)\n\n    ###########################################################################\n    # 4.  Save artefacts \u0026 log final metrics\n    ###########################################################################\n    final_metrics = {\n        \"run_id\": run_id,\n        \"final_train_loss\": epoch_metrics[-1][\"train_loss\"],\n        \"final_train_acc\": epoch_metrics[-1][\"train_acc\"],\n        \"final_val_loss\": epoch_metrics[-1][\"val_loss\"],\n        \"final_val_acc\": epoch_metrics[-1][\"val_acc\"],\n        \"best_val_acc\": best_val_acc,\n        \"epochs\": epochs,\n        \"epoch_metrics\": epoch_metrics,\n    }\n\n    # last checkpoint (overwrite each run)\n    last_ckpt_path = os.path.join(results_dir, run_id, \"model_last.pth\")\n    torch.save({\n        \"model_state\": model.state_dict(),\n        \"cfg\": cfg,\n        \"epoch\": epochs,\n        \"val_acc\": epoch_metrics[-1][\"val_acc\"],\n    }, last_ckpt_path)\n\n    # save json   \n    save_json(os.path.join(results_dir, run_id, \"results.json\"), final_metrics)\n\n    # provide machine-readable output on stdout\n    print(json.dumps(final_metrics))\n\n\n# --------------------------------------------------------------------------- #\n\ndef parse_args():\n    p = argparse.ArgumentParser()\n    p.add_argument(\"--config\", required=True, help=\"Path to run-specific YAML config file\")\n    p.add_argument(\"--results-dir\", required=True, help=\"Root results directory for this whole experiment set\")\n    return p.parse_args()\n\n\nif __name__ == \"__main__\":\n    args = parse_args()\n    cfg = yaml.safe_load(open(args.config))\n    run(cfg, args.results_dir)\n"}


Compare the Base Code with the Derived Experiment Code for this specific experiment thoroughly. Ensure the derived code maintains all the quality, completeness, and functionality of the base code while properly replacing placeholders with specific implementations.
Output:
{
    "is_experiment_code_ready": false,
    "experiment_code_issue": "Dataset placeholder not replaced: get_dataset only supports FAKEDATA and DATASET_PLACEHOLDER raises NotImplementedError; new full_experiment.yaml uses \"MINI_IMAGENET_C\" which is unsupported – code will crash. Therefore experiment not executable. Incomplete placeholder removal in preprocess.py. All criteria not met."
}
