
Input:
You are an AI code reviewer specializing in validating base experiment foundations.

Analyze the provided CORE experiment code (which contains placeholders for datasets/models) and determine if it implements a solid foundation that follows the base code generation requirements.

# Instructions

## Core Validation Criteria
Check if the generated base code meets ALL of the following requirements:

1. **Complete Core Logic Implementation**:
   - Training loops are fully implemented (no placeholders in base training logic)
   - Evaluation framework is complete with proper metrics calculation
   - Model saving/loading mechanisms are implemented
   - Result visualization and figure generation is complete

2. **Proper Placeholder Strategy**:
   - Uses clear, descriptive placeholders like `DATASET_PLACEHOLDER`, `MODEL_PLACEHOLDER`
   - Placeholders are ONLY used for dataset-specific and model-specific components
   - Core algorithm logic has NO placeholders
   - Includes comments explaining what each placeholder will be replaced with

3. **8-File Structure Compliance**:
   - Contains EXACTLY these 8 required files:
     * `src/train.py`
     * `src/evaluate.py`
     * `src/preprocess.py`
     * `src/model.py`
     * `src/main.py`
     * `pyproject.toml`
     * `config/smoke_test.yaml`
     * `config/full_experiment.yaml`
   - No additional utility files, helper modules, or separate components
   - All functionality is contained within the specified 8 files only

4. **Command Line Interface & Module Structure**:
   - main.py properly supports `--smoke-test` and `--full-experiment` flags with `--results-dir <path>` argument
   - main.py reads configuration YAML files and launches train.py for each run variation sequentially
   - main.py executes run variations one at a time in sequential order
   - main.py redirects each subprocess stdout/stderr to `{results_dir}/{run_id}/stdout.log` and `stderr.log` while forwarding to main stdout/stderr
   - train.py outputs JSON-formatted metrics with `run_id` field using `print(json.dumps({...}))`
   - evaluate.py outputs JSON-formatted comparison results to stdout
   - Configuration YAML structure is ready to accept run variations (specific values will be added in derive_specific step)
   - Import statements are compatible with `uv run python -m src.main` execution

5. **Publication-Ready Infrastructure**:
   - Figure generation with proper formatting (PDF output to `{results_dir}/images/` directory, legends, annotations)
   - Consistent result formatting and comparison logic
   - Proper experimental description output

6. **PyTorch Framework Usage**:
   - Uses PyTorch exclusively for deep learning components
   - Proper model definition and training patterns
   - Appropriate use of existing Python libraries

7. **No Premature Specialization**:
   - Does NOT assume specific datasets or models (uses placeholders appropriately)
   - Does NOT contain real dataset loading code (should be placeholder)
   - Focuses on base algorithm and evaluation framework
   - Does NOT validate specific run_variation names (they will be provided later in derive_specific_experiments step)

## Output Format
Respond with a JSON object containing:
- `is_base_code_ready`: boolean - true if ALL base criteria are met, false otherwise
- `base_code_issue`: string - specific issues found if any criteria are not met, focusing on base foundation quality

# Current Research Method
{
    "Open Problems": "1. (Scope) Closed–form proximal updates in ProxTTA are limited to frozen Σ taken from the source domain. In practice, the Hessian/Fisher of the test distribution may drift, making the pre-conditioner sub-optimal or even harmful when the shift is large or non-stationary.\n2. (Expressiveness) Restricting adaptation to BN affine parameters fails when the source model uses other normalisers (LN, GN, RMSNorm) or when shifts mainly affect early convolutional filters or input statistics.\n3. (Safety) Even per-parameter trust-region steps can overshoot on extremely hard samples; a cheap on-line certificate of improvement is missing.\n4. (Latency) Skipping whole batches when the time budget is tight wastes potentially useful statistics; we need finer control that degrades gracefully instead of all-or-nothing.",
    "Methods": "We propose AdaNPC – Adaptive Natural-gradient & Probabilistically-Certified Test-time Adaptation.\n\nKey pieces:\nA. Streaming Fisher approximation  Σ̂_t  \na) maintain an exponential moving average of squared gradients g_t ⊙ g_t.  Σ̂_t = β Σ̂_{t-1}+(1-β)(g_t^2+ϵ) (diagonal)  (β=0.99).\nThis tracks curvature of the *current* test stream with O(|θ|) memory and ≤2 Hadamard ops.\n\nB. One-shot natural update  θ_{t+1}=θ_t−η Σ̂_t^{-1/2} g_t  with fixed η=1.  Scaling by Σ̂_t^{-1/2} (RMSprop view) keeps units stable; no learning-rate tuning.\n\nC. Probabilistic safety filter  Using Bernstein’s inequality we bound the change in entropy ΔL. We accept the update only if P(ΔL>0)≤δ (δ=0.1). Cost: one inner-product and pre-computed variance proxy.\n\nD. Normaliser-agnostic adaptors  Collect affine parameters of all normalisation layers (BN, LN, GN, RMSNorm) plus optional input colour-bias vector (3 extra params). Same code path, still O(|θ|).\n\nE. Micro-stepping scheduler  Instead of skipping batches, if wall-clock τ_t>τ_max we halve the micro-step count k (default k=4) so each batch gets a partial update using  θ_{t+1}=θ_t−(η/k) Σ̂_t^{-1/2} g_t repeated k_iter times or until budget met. Guarantees monotone accuracy-vs-time trade-off.\n\nAll hyper-parameters (β, δ, τ_max) have intuitive meanings and are insensitive; none depend on the model or dataset.",
    "Experimental Setup": "Code base: extend official Tent repo.\n\nModels & datasets: • ResNet-50-BN on ImageNet-C. • ViT-B/16-LN on ImageNet-C. • ResNet-18-GN on CIFAR-10-C. Streams: Realistic protocol with η∈{1,1/2,1/4}. Recurring PTTA Dirichlet δ=0.1.\n\nBaselines: Source, Tent, ProxTTA, EATA, RoTTA, CoTTA, Shrink-Tent.\n\nMetrics: 1) Online top-1 error under time penalty. 2) Time-to-90%-of-Tent accuracy. 3) Ratio of safe-filter rejections (<5% desired). 4) Extra memory (should <0.3 MB for R-50).",
    "Experimental Code": "class AdaNPC(Tent):\n    def __init__(self, model, beta=0.99, delta=0.1, tau_max=None):\n        super().__init__(model, torch.optim.SGD([],lr=1))\n        self.var = None            # Σ̂_t diagonal\n        self.beta=beta; self.delta=delta\n        self.tau_max=tau_max; self.k=4  # micro-steps\n        self.timer_ema=None\n    @torch.enable_grad()\n    def forward_and_adapt(self,x,model,opt):\n        t0=time.time()\n        y=model(x); loss=softmax_entropy(y).mean()\n        g=torch.autograd.grad(loss,self.params,create_graph=False)[0]\n        if self.var is None: self.var=g.pow(2)\n        else: self.var=self.beta*self.var+(1-self.beta)*g.pow(2)+1e-8\n        step=(g/self.var.sqrt())            # Σ̂^{-1/2}g\n        # safety: accept only if predicted ΔL negative with high prob\n        deltaL=(step*g).sum()              # first-order change\n        varL=((step.pow(2)*self.var).sum()).sqrt()\n        safe=(deltaL+varL*math.sqrt(2*math.log(1/self.delta)))<0\n        if safe:\n            k=max(1,self.k)\n            eta=1.0/k\n            for _ in range(k):\n                for p,s in zip(self.params,step): p-=eta*s\n        self.timer_ema=0.8*(self.timer_ema or 0)+0.2*(time.time()-t0)\n        if self.tau_max and self.timer_ema>self.tau_max and self.k>1:\n            self.k//=2     # micro-step back-off\n        model.zero_grad(); return model(x)",
    "Expected Result": "• AdaNPC matches Tent’s final accuracy after only 0.5 epochs of data (≈30% fewer samples) and beats ProxTTA by 1-2 pp on ImageNet-C.\n• Under η=1/4 it retains 93% of its full-speed accuracy, versus 75% for Tent and 88% for ProxTTA.\n• Safety filter rejects <3% of batches yet prevents all observed divergences on extreme corruptions (snow, impulse_noise).\n• Overhead: +|θ| vector and var buffer (0.25 MB for R-50), <5% extra FLOPs.",
    "Expected Conclusion": "AdaNPC turns TTA into a fast, normaliser-agnostic, and self-certified one-step natural-gradient procedure. By tracking curvature online it eliminates source-bias of fixed Fisher, while the probabilistic filter delivers theoretical safety guarantees. Fine-grained micro-stepping makes adaptation speed smoothly adjustable to real-time constraints. The method thus advances both the practical deployability and the theoretical grounding of rapid test-time adaptation."
}

# Experimental Design
## Experiment Strategy
Overall Goal:
Demonstrate that AdaNPC delivers (1) higher on-line performance, (2) better computational efficiency, (3) stronger robustness/safety and (4) wider architectural generalization than existing Test-Time Adaptation (TTA) techniques.

1. Validation Axes
   a. Performance Improvement – on-line accuracy/error under various distribution shifts.
   b. Efficiency – wall-clock latency, extra FLOPs, extra memory, and sample-efficiency (# test samples required to reach a target accuracy).
   c. Robustness & Safety – stability on extreme or non-stationary shifts; frequency of divergence and of safety-filter rejections; guarantee that accuracy never drops below the frozen source model.
   d. Generalization – effectiveness across architectures (BN, LN, GN, RMSNorm), data domains, and shift types (corruption intensity, temporal drift, sudden swaps).
   e. Graceful Degradation – accuracy–vs–time trade-off controlled by micro-stepping.

2. Required Comparisons
   • Strong baselines: Source (no TTA), Tent, ProxTTA, EATA, CoTTA, RoTTA, Shrink-Tent, and any contemporaneous state-of-the-art published before the submission deadline.
   • Internal ablations: (i) remove streaming Fisher (fall back to fixed Σ), (ii) remove probabilistic safety filter, (iii) remove micro-stepping, (iv) adapt BN only, (v) replace natural gradient with SGD; (vi) combine two removals to test interaction effects.
   • Sensitivity studies: vary β, δ, micro-step budget, and η to show hyper-parameter robustness.

3. Experimental Angles / Evidence Modalities
   A. Quantitative
      • Main metric: on-line top-1 error averaged over the whole stream.
      • Secondary: (i) area under the adaptation curve (AUC), (ii) time-to-X%-of-Tent accuracy, (iii) catastrophic failure rate (runs where error > source), (iv) % batches rejected by safety filter, (v) compute & memory overhead, (vi) energy proxy via NVIDIA-SMI.
      • Statistical treatment: 3 independent runs × 3 random seeds; report mean ± 95% CI; paired t-tests against best baseline.
   B. Qualitative / Diagnostic
      • Fisher drift plots (cosine similarity between Σ̂_t and source Σ_0).
      • Histograms of predicted ΔL vs empirical ΔL, highlighting safety bound tightness.
      • Accuracy–vs–latency curves when throttling τ_max.
      • Heat-map of component ablations across corruption severity.

4. Multi-Perspective Demonstration Plan
   • Orthogonal matrix: {Architectures} × {Datasets} × {Shift protocols}. Each cell runs the full comparison suite to show broad applicability.
   • Stress tests: (i) worst-case corruptions, (ii) synthetic non-stationary drift generated on-the-fly, (iii) adversarially sorted hard batches.
   • Real-time constraint scenario: enforce τ_max values (full, ½, ¼ of GPU budget) to showcase graceful degradation.
   • Safety spotlight: run 10×-long streams; count divergences; compare cumulative worst-case error to baselines.

5. Success Criteria (must hold on ≥80% of cells)
   • Accuracy: AdaNPC improves mean AUC by ≥2 pp over the best competing method with p<0.05.
   • Efficiency: <5% extra FLOPs, <0.5% extra VRAM, and reaches Tent’s final accuracy using ≥25% fewer test samples.
   • Robustness: zero catastrophic failures; safety filter rejection rate ≤5%.
   • Generalization: retains ≥90% of its ImageNet-C gain when ported to each other architecture/dataset without tuning.
   • Graceful degradation: under the strictest τ_max, retains ≥90% of its own full-speed accuracy while Tent drops below 80%.

6. Practical Considerations
   • All experiments run on one NVIDIA A100 (80 GB) node; resource accounting recorded via NVTX markers and pytorch profiler.
   • Unified codebase: start from official Tent repo, add modular hooks so baselines and AdaNPC share identical data loading, augmentation, synchronisation and precision settings.
   • Hyper-parameter policy: AdaNPC fixed defaults; baselines get per-dataset grid search as reported in their papers to avoid under-tuning claims.
   • Reproducibility: release seeds, config files, and slurm scripts; adherence to ML-Reproducibility Checklist.

This strategy provides a consistent, multi-faceted evaluation framework that will be reused verbatim in all subsequent experiments, ensuring that every study collectively builds a compelling, well-substantiated case for the effectiveness of AdaNPC.

# Generated Base Code Files
{"evaluate_py": "import argparse\nimport json\nimport os\nfrom pathlib import Path\nfrom typing import Dict, List\n\nimport matplotlib.pyplot as plt\nimport pandas as pd\nimport seaborn as sns\n\n\nFIGURE_TOPIC_BAR = \"final_accuracy\"\nFIGURE_TOPIC_LINE = \"val_accuracy_curves\"\n\n\ndef save_figure(fig, results_dir: str, filename: str):\n    images_dir = os.path.join(results_dir, \"images\")\n    os.makedirs(images_dir, exist_ok=True)\n    fig.savefig(os.path.join(images_dir, filename), bbox_inches=\"tight\")\n    plt.close(fig)\n\n\ndef parse_args():\n    p = argparse.ArgumentParser(description=\"Aggregate \u0026 visualise experiment results.\")\n    p.add_argument(\"--results-dir\", type=str, required=True, help=\"Root directory containing run result subfolders.\")\n    return p.parse_args()\n\n\ndef load_all_results(results_dir: str) -\u003e List[Dict]:\n    results = []\n    for run_dir in sorted(Path(results_dir).iterdir()):\n        if not run_dir.is_dir():\n            continue\n        result_file = run_dir / \"results.json\"\n        if result_file.exists():\n            with open(result_file, \"r\") as f:\n                results.append(json.load(f))\n    return results\n\n\ndef make_bar_plot(df, results_dir):\n    fig, ax = plt.subplots(figsize=(6, 4))\n    sns.barplot(x=\"run_id\", y=\"final_val_acc\", data=df, ax=ax)\n    for idx, row in df.iterrows():\n        ax.annotate(f\"{row.final_val_acc:.3f}\", (idx, row.final_val_acc), ha=\"center\", va=\"bottom\")\n    ax.set_xlabel(\"Run ID\")\n    ax.set_ylabel(\"Final Validation Accuracy\")\n    ax.set_title(\"Final Validation Accuracy Across Runs\")\n    plt.tight_layout()\n    save_figure(fig, results_dir, f\"{FIGURE_TOPIC_BAR}.pdf\")\n\n\ndef make_line_plot(df, results_dir):\n    fig, ax = plt.subplots(figsize=(6, 4))\n    for _, row in df.iterrows():\n        epochs = row.history[\"epoch\"]\n        vals = row.history[\"val_acc\"]\n        ax.plot(epochs, vals, label=row.run_id)\n        ax.annotate(f\"{vals[-1]:.3f}\", (epochs[-1], vals[-1]))\n    ax.set_xlabel(\"Epoch\")\n    ax.set_ylabel(\"Validation Accuracy\")\n    ax.set_title(\"Validation Accuracy Curves\")\n    ax.legend()\n    ax.grid(True)\n    plt.tight_layout()\n    save_figure(fig, results_dir, f\"{FIGURE_TOPIC_LINE}.pdf\")\n\n\ndef main():\n    args = parse_args()\n    results = load_all_results(args.results_dir)\n    if len(results) == 0:\n        print(\"No result.json files found in\", args.results_dir)\n        return\n\n    df = pd.DataFrame(results)\n    # Flatten nested history for easier plotting\n    make_bar_plot(df, args.results_dir)\n    make_line_plot(df, args.results_dir)\n\n    comparison = (\n        df[[\"run_id\", \"final_val_acc\", \"best_val_acc\", \"total_time_sec\"]]\n        .set_index(\"run_id\")\n        .to_dict(orient=\"index\")\n    )\n\n    print(json.dumps(comparison, indent=2))\n\n\nif __name__ == \"__main__\":\n    main()\n", "full_experiment_yaml": "# Placeholder full experiment configuration \u2013 to be populated in the next phase.\n# Replace DATASET_PLACEHOLDER, MODEL_PLACEHOLDER, and other placeholders with\n# concrete values when deriving specific experiments.\n\nruns:\n  - name: MODEL_PLACEHOLDER_on_DATASET_PLACEHOLDER_baseline\n    model: MODEL_PLACEHOLDER  # e.g., BASELINE, TENT, etc.\n    dataset: DATASET_PLACEHOLDER # e.g., ImageNet-C\n    epochs: SPECIFIC_CONFIG_PLACEHOLDER\n    batch_size: SPECIFIC_CONFIG_PLACEHOLDER\n    lr: SPECIFIC_CONFIG_PLACEHOLDER\n  # Add additional run variations here following the same schema.\n", "main_py": "\"\"\"Main orchestrator: executes all run variations defined in a YAML config file.\"\"\"\n\nimport argparse\nimport json\nimport os\nimport subprocess\nimport sys\nimport tempfile\nfrom pathlib import Path\nfrom threading import Thread\n\nimport yaml\n\n\ndef tee_stream(stream, log_file):\n    \"\"\"Copy subprocess stream to both console and file.\"\"\"\n\n    def _forward():\n        for line in iter(stream.readline, b\"\"):\n            decoded = line.decode()\n            sys.stdout.write(decoded)\n            log_file.write(decoded)\n            log_file.flush()\n        stream.close()\n\n    t = Thread(target=_forward)\n    t.daemon = True\n    t.start()\n\n\ndef run_subprocess(cmd, stdout_path, stderr_path):\n    with open(stdout_path, \"w\") as out_f, open(stderr_path, \"w\") as err_f:\n        proc = subprocess.Popen(cmd, stdout=subprocess.PIPE, stderr=subprocess.PIPE)\n        tee_stream(proc.stdout, out_f)\n        tee_stream(proc.stderr, err_f)\n        proc.wait()\n        if proc.returncode != 0:\n            raise RuntimeError(f\"Subprocess {cmd} exited with code {proc.returncode}\")\n\n\ndef parse_args():\n    p = argparse.ArgumentParser(description=\"Run all experiments defined in a YAML file.\")\n    group = p.add_mutually_exclusive_group(required=True)\n    group.add_argument(\"--smoke-test\", action=\"store_true\", help=\"Run smoke_test.yaml config.\")\n    group.add_argument(\"--full-experiment\", action=\"store_true\", help=\"Run full_experiment.yaml config.\")\n    p.add_argument(\"--results-dir\", type=str, required=True, help=\"Where to store all outputs.\")\n    return p.parse_args()\n\n\ndef main():\n    args = parse_args()\n    root_dir = Path(__file__).resolve().parent.parent  # project root\n    cfg_path = (\n        root_dir / \"config\" / (\"smoke_test.yaml\" if args.smoke_test else \"full_experiment.yaml\")\n    )\n    with open(cfg_path, \"r\") as f:\n        all_cfg = yaml.safe_load(f)\n\n    runs = all_cfg.get(\"runs\", [])\n    if len(runs) == 0:\n        print(\"No runs defined in config.\")\n        sys.exit(1)\n\n    results_root = Path(args.results_dir).resolve()\n    results_root.mkdir(parents=True, exist_ok=True)\n\n    for run_cfg in runs:\n        run_id = run_cfg[\"name\"]\n        run_dir = results_root / run_id\n        run_dir.mkdir(parents=True, exist_ok=True)\n\n        # Write run-specific config to temp file (YAML) inside run dir\n        run_cfg_path = run_dir / \"config.yaml\"\n        with open(run_cfg_path, \"w\") as f:\n            yaml.safe_dump(run_cfg, f)\n\n        stdout_path = run_dir / \"stdout.log\"\n        stderr_path = run_dir / \"stderr.log\"\n\n        cmd = [\n            sys.executable,\n            \"-m\",\n            \"src.train\",\n            \"--config\",\n            str(run_cfg_path),\n            \"--run-id\",\n            run_id,\n            \"--results-dir\",\n            str(results_root),\n        ]\n\n        print(f\"\\n===== Launching run: {run_id} =====\")\n        run_subprocess(cmd, stdout_path, stderr_path)\n        print(f\"===== Completed run: {run_id} =====\\n\")\n\n    # After all runs, invoke evaluator\n    eval_cmd = [\n        sys.executable,\n        \"-m\",\n        \"src.evaluate\",\n        \"--results-dir\",\n        str(results_root),\n    ]\n    print(\"\\n===== Running aggregated evaluation =====\")\n    subprocess.check_call(eval_cmd)\n\n\nif __name__ == \"__main__\":\n    main()\n", "model_py": "\"\"\"Model architectures \u0026 adaptation algorithms (shared across experiments).\"\"\"\n\nimport math\nfrom collections import OrderedDict\nfrom typing import Dict, List\n\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\n# -----------------------------------------------------------------------------\n# Utility functions\n# -----------------------------------------------------------------------------\n\ndef softmax_entropy(x: torch.Tensor) -\u003e torch.Tensor:\n    \"\"\"Returns the entropy of softmax distributions for each sample.\"\"\"\n    return -(F.softmax(x, dim=1) * F.log_softmax(x, dim=1)).sum(1)\n\n\ndef accuracy(output: torch.Tensor, target: torch.Tensor) -\u003e float:\n    return (output.argmax(1) == target).float().mean().item()\n\n\n# -----------------------------------------------------------------------------\n# Simple CNN backbone (suitable for synthetic \u0026 CIFAR-like datasets)\n# -----------------------------------------------------------------------------\n\n\nclass SmallConvNet(nn.Module):\n    def __init__(self, num_classes: int = 10):\n        super().__init__()\n        self.features = nn.Sequential(\n            nn.Conv2d(3, 32, 3, padding=1),\n            nn.BatchNorm2d(32),\n            nn.ReLU(inplace=True),\n            nn.MaxPool2d(2),\n            nn.Conv2d(32, 64, 3, padding=1),\n            nn.BatchNorm2d(64),\n            nn.ReLU(inplace=True),\n            nn.MaxPool2d(2),\n            nn.Conv2d(64, 128, 3, padding=1),\n            nn.BatchNorm2d(128),\n            nn.ReLU(inplace=True),\n            nn.AdaptiveAvgPool2d((1, 1)),\n        )\n        self.classifier = nn.Linear(128, num_classes)\n\n    def forward(self, x):\n        x = self.features(x)\n        x = torch.flatten(x, 1)\n        return self.classifier(x)\n\n\n# -----------------------------------------------------------------------------\n# AdaNPC adaptation wrapper (core algorithm)\n# -----------------------------------------------------------------------------\n\n\nclass AdaNPC(nn.Module):\n    \"\"\"\n    Wrapper that equips a backbone with AdaNPC test-time adaptation.\n\n    Adaptation is only triggered when `adapt=True` is passed to forward().\n    \"\"\"\n\n    def __init__(\n        self,\n        backbone: nn.Module,\n        beta: float = 0.99,\n        delta: float = 0.1,\n        tau_max: float = None,\n        k: int = 4,\n    ):\n        super().__init__()\n        self.backbone = backbone\n        self.beta = beta\n        self.delta = delta\n        self.tau_max = tau_max\n        self.k_init = k\n        self.k = k  # Mutable during runtime\n        self.register_buffer(\"var_diag\", torch.tensor([]))  # \u03a3\u0302_t diagonal\n        self.timer_ema = None\n        self._collect_adapt_params()\n\n    # ---------------------------------------------------------------------\n    def _collect_adapt_params(self):\n        \"\"\"Collect affine parameters of all normalisation layers.\"\"\"\n        params = []\n        for m in self.backbone.modules():\n            if isinstance(m, (nn.BatchNorm2d, nn.LayerNorm, nn.GroupNorm)):  # extend as needed\n                if m.weight is not None:\n                    params.append(m.weight)\n                if m.bias is not None:\n                    params.append(m.bias)\n        self.adapt_params: List[torch.nn.Parameter] = params\n\n    # ---------------------------------------------------------------------\n    def _natural_gradient_step(self, grads: List[torch.Tensor]):\n        if self.var_diag.numel() == 0:\n            self.var_diag = torch.cat([g.detach().pow(2) + 1e-8 for g in grads])\n        else:\n            flat_g2 = torch.cat([g.detach().pow(2) for g in grads]) + 1e-8\n            self.var_diag = self.beta * self.var_diag + (1 - self.beta) * flat_g2\n\n        flat_g = torch.cat([g.detach() for g in grads])\n        step = flat_g / self.var_diag.sqrt()\n        return step\n\n    # ---------------------------------------------------------------------\n    def _safety_filter(self, step: torch.Tensor, grads: List[torch.Tensor]) -\u003e bool:\n        flat_g = torch.cat([g.detach() for g in grads])\n        delta_L = (step * flat_g).sum()\n        var_L = (step.pow(2) * self.var_diag).sum().sqrt()\n        bound = var_L * math.sqrt(2 * math.log(1 / self.delta))\n        return (delta_L + bound) \u003c 0  # Safe if true\n\n    # ---------------------------------------------------------------------\n    def _apply_step(self, step: torch.Tensor, eta: float):\n        offset = 0\n        for p in self.adapt_params:\n            numel = p.numel()\n            direction = step[offset : offset + numel].view_as(p)\n            p.data.sub_(eta * direction)\n            offset += numel\n\n    # ---------------------------------------------------------------------\n    def adapt(self, loss):\n        grads = torch.autograd.grad(loss, self.adapt_params, create_graph=False, retain_graph=False)\n        step = self._natural_gradient_step(list(grads))\n        safe = self._safety_filter(step, list(grads))\n        if safe:\n            # micro-stepping\n            k = max(1, self.k)\n            eta = 1.0 / k\n            for _ in range(k):\n                self._apply_step(step, eta)\n\n    # ---------------------------------------------------------------------\n    def forward(self, x: torch.Tensor, adapt: bool = False):\n        if adapt:\n            x.requires_grad = False  # Ensure no grads for input\n            y = self.backbone(x)\n            loss = softmax_entropy(y).mean()\n            self.adapt(loss)\n            # Forward again with updated params for fresh prediction\n            y = self.backbone(x)\n            return y\n        else:\n            return self.backbone(x)\n\n\n# -----------------------------------------------------------------------------\n# Model registry for easy instantiation from config\n# -----------------------------------------------------------------------------\n\n\ndef _baseline_model(num_classes: int, **kwargs):\n    return SmallConvNet(num_classes)\n\n\ndef _adanpc_model(num_classes: int, cfg: Dict):\n    backbone = SmallConvNet(num_classes)\n    return AdaNPC(\n        backbone=backbone,\n        beta=cfg.get(\"beta\", 0.99),\n        delta=cfg.get(\"delta\", 0.1),\n        tau_max=cfg.get(\"tau_max\"),\n        k=cfg.get(\"k\", 4),\n    )\n\n\nMODEL_REGISTRY = {\n    \"BASELINE\": _baseline_model,\n    \"ADANPC\": _adanpc_model,\n    # PLACEHOLDER: additional model variants (ablation, etc.) will be added in the next phase.\n}\n", "preprocess_py": "import random\nimport os\nfrom typing import Tuple, List\n\nimport numpy as np\nimport torch\nfrom torch.utils.data import Dataset, DataLoader, random_split\n\n\nclass SyntheticClassificationDataset(Dataset):\n    \"\"\"A lightweight synthetic dataset useful for smoke tests.\"\"\"\n\n    def __init__(\n        self,\n        num_samples: int = 1024,\n        num_classes: int = 10,\n        img_shape: Tuple[int, int, int] = (3, 32, 32),\n        seed: int = 42,\n    ):\n        super().__init__()\n        rng = np.random.RandomState(seed)\n        self.data = rng.randn(num_samples, *img_shape).astype(np.float32)\n        self.targets = rng.randint(0, num_classes, size=(num_samples,)).astype(np.int64)\n        self.num_classes = num_classes\n\n    def __len__(self):\n        return len(self.targets)\n\n    def __getitem__(self, idx):\n        img = torch.tensor(self.data[idx])\n        label = torch.tensor(self.targets[idx])\n        return img, label\n\n\n# === Generic Data Module =====================================================\n\n\nclass DataModule:\n    \"\"\"Unified data handling that is dataset-agnostic except for the loading step.\"\"\"\n\n    def __init__(self, cfg: dict):\n        self.cfg = cfg\n        self.dataset_name = cfg.get(\"dataset\", \"DATASET_PLACEHOLDER\").upper()\n        self.batch_size = int(cfg.get(\"batch_size\", 64))\n        self.num_workers = int(cfg.get(\"num_workers\", 4))\n        self.val_split = float(cfg.get(\"val_split\", 0.1))\n        self.seed = int(cfg.get(\"seed\", 42))\n\n        # === Dataset loading (placeholder friendly) ===\n        if self.dataset_name == \"SYNTHETIC\":\n            self.dataset = SyntheticClassificationDataset(\n                num_samples=cfg.get(\"synthetic_num_samples\", 1024),\n                num_classes=cfg.get(\"num_classes\", 10),\n                img_shape=tuple(cfg.get(\"input_shape\", (3, 32, 32))),\n                seed=self.seed,\n            )\n        else:\n            # PLACEHOLDER: Will be replaced with specific dataset loading logic\n            raise NotImplementedError(\n                f\"Dataset \u0027{self.dataset_name}\u0027 is not implemented in the common foundation yet.\"\n            )\n\n        self.num_classes = getattr(self.dataset, \"num_classes\", cfg.get(\"num_classes\", 10))\n        self._prepare_splits()\n\n    # -------------------------------------------------------------------------\n    def _prepare_splits(self):\n        val_size = int(len(self.dataset) * self.val_split)\n        train_size = len(self.dataset) - val_size\n        generator = torch.Generator().manual_seed(self.seed)\n        self.train_set, self.val_set = random_split(self.dataset, [train_size, val_size], generator=generator)\n\n    # -------------------------------------------------------------------------\n    def train_loader(self):\n        return DataLoader(\n            self.train_set,\n            batch_size=self.batch_size,\n            shuffle=True,\n            num_workers=self.num_workers,\n            pin_memory=True,\n        )\n\n    def val_loader(self):\n        return DataLoader(\n            self.val_set,\n            batch_size=self.batch_size,\n            shuffle=False,\n            num_workers=self.num_workers,\n            pin_memory=True,\n        )\n", "pyproject_toml": "[build-system]\nrequires = [\"setuptools\u003e=42\", \"wheel\"]\n\n[project]\nname = \"adanpc_experiments\"\nversion = \"0.1.0\"\ndescription = \"Common core foundation for AdaNPC experiments\"\nrequires-python = \"\u003e=3.9\"\n\n[tool.poetry.dependencies]\npython = \"\u003e=3.9,\u003c3.12\"\ntorch = \"*\"\ntorchvision = \"*\"\nmatplotlib = \"*\"\nseaborn = \"*\"\npandas = \"*\"\npyyaml = \"*\"\ntqdm = \"*\"\n", "smoke_test_yaml": "# Smoke test configuration with minimal synthetic dataset (fast execution)\n\nruns:\n  - name: baseline_synthetic\n    model: BASELINE\n    dataset: SYNTHETIC\n    epochs: 2\n    batch_size: 32\n    lr: 0.01\n    seed: 1\n  - name: adanpc_synthetic\n    model: ADANPC\n    dataset: SYNTHETIC\n    epochs: 2\n    batch_size: 32\n    lr: 0.01\n    beta: 0.9\n    delta: 0.1\n    k: 2\n    seed: 2\n", "train_py": "import argparse\nimport json\nimport os\nimport sys\nimport time\nfrom pathlib import Path\n\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport torch\nimport torch.nn.functional as F\nfrom torch import optim\nfrom tqdm import tqdm\n\nfrom . import preprocess as prep\nfrom .model import MODEL_REGISTRY, accuracy\n\n\ndef set_seed(seed: int = 42):\n    \"\"\"Utility to make experiments deterministic (where possible).\"\"\"\n    import random\n\n    torch.manual_seed(seed)\n    np.random.seed(seed)\n    random.seed(seed)\n    torch.backends.cudnn.deterministic = True\n    torch.backends.cudnn.benchmark = False\n\n\ndef save_figure(fig, results_dir: str, filename: str):\n    images_dir = os.path.join(results_dir, \"images\")\n    os.makedirs(images_dir, exist_ok=True)\n    fig.savefig(os.path.join(images_dir, filename), bbox_inches=\"tight\")\n    plt.close(fig)\n\n\ndef make_line_plot(x, ys, labels, xlabel, ylabel, title, annotate_final=True):\n    fig, ax = plt.subplots(figsize=(6, 4))\n    for y, label in zip(ys, labels):\n        ax.plot(x, y, label=label)\n        if annotate_final:\n            ax.annotate(f\"{y[-1]:.3f}\", (x[-1], y[-1]))\n    ax.set_xlabel(xlabel)\n    ax.set_ylabel(ylabel)\n    ax.set_title(title)\n    ax.legend()\n    ax.grid(True)\n    plt.tight_layout()\n    return fig\n\n\ndef parse_args():\n    parser = argparse.ArgumentParser(description=\"Run a single experiment variation.\")\n    parser.add_argument(\"--config\", type=str, required=True, help=\"Path to run-specific YAML config file.\")\n    parser.add_argument(\"--run-id\", type=str, required=True, help=\"Unique run identifier (used for logging \u0026 saving).\")\n    parser.add_argument(\"--results-dir\", type=str, required=True, help=\"Root directory to store results.\")\n    return parser.parse_args()\n\n\ndef main():\n    import yaml  # Local import to avoid adding global dependency if not used elsewhere\n\n    args = parse_args()\n    with open(args.config, \"r\") as f:\n        cfg = yaml.safe_load(f)\n\n    run_id = args.run_id\n    results_dir = os.path.join(args.results_dir, run_id)\n    os.makedirs(results_dir, exist_ok=True)\n    os.makedirs(os.path.join(results_dir, \"images\"), exist_ok=True)\n\n    # 1. Reproducibility\n    set_seed(cfg.get(\"seed\", 42))\n\n    # 2. Data\n    data_module = prep.DataModule(cfg)\n    train_loader = data_module.train_loader()\n    val_loader = data_module.val_loader()\n\n    num_classes = data_module.num_classes\n\n    # 3. Model\n    model_name = cfg.get(\"model\").upper()\n    if model_name not in MODEL_REGISTRY:\n        raise ValueError(f\"Unknown model {model_name}. Registered models: {list(MODEL_REGISTRY.keys())}\")\n    model = MODEL_REGISTRY[model_name](num_classes=num_classes, cfg=cfg)\n    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n    model.to(device)\n\n    # 4. Optimizer \u0026 LR scheduler (core logic \u2013 will remain unchanged across datasets)\n    optimizer = optim.SGD(model.parameters(), lr=cfg.get(\"lr\", 0.01), momentum=0.9, weight_decay=5e-4)\n    scheduler = optim.lr_scheduler.MultiStepLR(optimizer, milestones=cfg.get(\"lr_milestones\", [100, 150]), gamma=0.1)\n\n    # 5. Training Loop\n    epochs = int(cfg.get(\"epochs\", 10))\n    history = {\n        \"epoch\": [],\n        \"train_loss\": [],\n        \"train_acc\": [],\n        \"val_loss\": [],\n        \"val_acc\": [],\n    }\n\n    start_time = time.time()\n    for epoch in range(1, epochs + 1):\n        model.train()\n        running_loss, correct, total = 0.0, 0, 0\n        pbar = tqdm(train_loader, desc=f\"[{run_id}] Epoch {epoch}/{epochs}\", leave=False)\n        for inputs, targets in pbar:\n            inputs, targets = inputs.to(device), targets.to(device)\n\n            optimizer.zero_grad()\n            outputs = model(inputs)\n            loss = F.cross_entropy(outputs, targets)\n            loss.backward()\n            optimizer.step()\n\n            running_loss += loss.item() * inputs.size(0)\n            correct += (outputs.argmax(1) == targets).sum().item()\n            total += targets.size(0)\n            pbar.set_postfix(loss=running_loss / total, acc=correct / total)\n\n        train_loss = running_loss / total\n        train_acc = correct / total\n\n        # Validation\n        model.eval()\n        val_loss, val_correct, val_total = 0.0, 0, 0\n        with torch.no_grad():\n            for inputs, targets in val_loader:\n                inputs, targets = inputs.to(device), targets.to(device)\n                outputs = model(inputs)\n                loss = F.cross_entropy(outputs, targets)\n                val_loss += loss.item() * inputs.size(0)\n                val_correct += (outputs.argmax(1) == targets).sum().item()\n                val_total += targets.size(0)\n        val_loss /= val_total\n        val_acc = val_correct / val_total\n\n        scheduler.step()\n\n        # Logging\n        history[\"epoch\"].append(epoch)\n        history[\"train_loss\"].append(train_loss)\n        history[\"train_acc\"].append(train_acc)\n        history[\"val_loss\"].append(val_loss)\n        history[\"val_acc\"].append(val_acc)\n\n        tqdm.write(\n            json.dumps({\n                \"run_id\": run_id,\n                \"epoch\": epoch,\n                \"train_loss\": train_loss,\n                \"train_acc\": train_acc,\n                \"val_loss\": val_loss,\n                \"val_acc\": val_acc,\n            })\n        )\n\n    total_time = time.time() - start_time\n\n    # 6. Save metrics to JSON\n    metrics_path = os.path.join(results_dir, \"results.json\")\n    with open(metrics_path, \"w\") as f:\n        json.dump({\n            \"run_id\": run_id,\n            \"config\": cfg,\n            \"history\": history,\n            \"total_time_sec\": total_time,\n        }, f, indent=2)\n\n    # 7. Generate Figures\n    epochs_range = history[\"epoch\"]\n    # Loss plot\n    loss_fig = make_line_plot(\n        epochs_range,\n        [history[\"train_loss\"], history[\"val_loss\"]],\n        labels=[\"Train\", \"Validation\"],\n        xlabel=\"Epoch\",\n        ylabel=\"Cross-Entropy Loss\",\n        title=f\"Training \u0026 Validation Loss \u2013 {run_id}\",\n    )\n    save_figure(loss_fig, results_dir, \"training_loss.pdf\")\n\n    # Accuracy plot\n    acc_fig = make_line_plot(\n        epochs_range,\n        [history[\"train_acc\"], history[\"val_acc\"]],\n        labels=[\"Train\", \"Validation\"],\n        xlabel=\"Epoch\",\n        ylabel=\"Accuracy\",\n        title=f\"Training \u0026 Validation Accuracy \u2013 {run_id}\",\n    )\n    save_figure(acc_fig, results_dir, \"accuracy.pdf\")\n\n    # 8. Final console output summarising experiment\n    summary = {\n        \"run_id\": run_id,\n        \"final_train_acc\": history[\"train_acc\"][-1],\n        \"final_val_acc\": history[\"val_acc\"][-1],\n        \"best_val_acc\": max(history[\"val_acc\"]),\n        \"total_time_sec\": total_time,\n        \"result_files\": {\n            \"metrics\": metrics_path,\n            \"figures\": [\n                \"training_loss.pdf\",\n                \"accuracy.pdf\",\n            ],\n        },\n    }\n    print(\"\\n======== Experiment Description ========\")\n    print(json.dumps(cfg, indent=2))\n    print(\"\\n======== Numerical Results ============\")\n    print(json.dumps(summary, indent=2))\n\n\nif __name__ == \"__main__\":\n    main()\n"}

Analyze the Base code thoroughly, focusing on whether it provides a solid, consistent foundation for ALL future experimental variations while properly using placeholders for dataset/model-specific components.
Output:
{
    "is_base_code_ready": false,
    "base_code_issue": "Requirement 1 not fully met – train.py does not save the trained model (e.g. *.pth file) and evaluate.py has no logic to load and use saved weights; therefore model saving/loading mechanisms are missing. All other core criteria appear satisfied but this missing functionality breaks the “complete core logic” mandate."
}
