
Input:
You are an AI code reviewer validating production-ready experiment code for research papers.

Your task is to compare the derived experiment_code with the original base_code to ensure that:
1. No important functionality has been omitted or truncated
2. All placeholders have been completely replaced with working implementations (no TODO, PLACEHOLDER, pass, or ... allowed)
3. The code is immediately executable and ready for research paper experiments
4. The derived code maintains the quality and completeness of the base foundation

# Instructions

## Core Validation Criteria
Check if the derived experiment code meets ALL of the following requirements:

1. **Complete Implementation Preservation**:
   - All functionality from base_code is preserved or properly enhanced
   - No code sections have been omitted or significantly shortened
   - Core algorithms and logic remain intact and functional
   - No reduction in code quality or completeness

2. **Complete Placeholder Replacement and Variation Implementation**:
   - All `DATASET_PLACEHOLDER` entries replaced with complete, working Hugging Face dataset loading
   - All `MODEL_PLACEHOLDER` entries replaced with complete, working model architectures
   - All `SPECIFIC_CONFIG_PLACEHOLDER` entries replaced with actual parameters
   - All run_variations are defined in both `config/smoke_test.yaml` and `config/full_experiment.yaml`
   - All run_variations are implemented in `src/model.py`
   - `config/smoke_test.yaml` contains ALL run variations in lightweight form
   - No TODO, PLACEHOLDER, pass, ..., or any incomplete implementations remain

3. **Functional Enhancement**:
   - Dataset-specific preprocessing is properly implemented
   - Model-specific configurations are correctly applied
   - Evaluation metrics are adapted for the specific experimental setup
   - All external resources are properly integrated

4. **Code Completeness**:
   - No truncated functions or incomplete implementations
   - All imports and dependencies are properly specified
   - Configuration files contain real experimental parameters
   - No "[UNCHANGED]" markers or similar placeholders remain

5. **Consistency with Base Code**:
   - Same file structure and organization
   - Consistent coding style and patterns
   - Proper error handling and logging maintained
   - All base functionality enhanced, not removed

## Detection of Common Issues
Flag the following problems if found:

- **Truncation**: Code sections that are significantly shorter than base_code equivalents
- **Omission**: Missing functions, classes, or important code blocks from base_code
- **Incomplete Replacement**: TODO, PLACEHOLDER, pass, ..., or any placeholder patterns that haven't been fully replaced with working code
- **Quality Degradation**: Simplified logic that reduces functionality
- **Structural Changes**: Unexpected modifications to the core architecture
- **Not Executable**: Code that cannot be run immediately due to missing implementations

## Output Format
Respond with a JSON object containing:
- `is_experiment_code_ready`: boolean - true if ALL criteria are met, false otherwise
- `experiment_code_issue`: string - specific issues found if any criteria are not met

# Current Research Method
{
    "Open Problems": "1. (Scope) Closed–form proximal updates in ProxTTA are limited to frozen Σ taken from the source domain. In practice, the Hessian/Fisher of the test distribution may drift, making the pre-conditioner sub-optimal or even harmful when the shift is large or non-stationary.\n2. (Expressiveness) Restricting adaptation to BN affine parameters fails when the source model uses other normalisers (LN, GN, RMSNorm) or when shifts mainly affect early convolutional filters or input statistics.\n3. (Safety) Even per-parameter trust-region steps can overshoot on extremely hard samples; a cheap on-line certificate of improvement is missing.\n4. (Latency) Skipping whole batches when the time budget is tight wastes potentially useful statistics; we need finer control that degrades gracefully instead of all-or-nothing.",
    "Methods": "We propose AdaNPC – Adaptive Natural-gradient & Probabilistically-Certified Test-time Adaptation.\n\nKey pieces:\nA. Streaming Fisher approximation  Σ̂_t  \na) maintain an exponential moving average of squared gradients g_t ⊙ g_t.  Σ̂_t = β Σ̂_{t-1}+(1-β)(g_t^2+ϵ) (diagonal)  (β=0.99).\nThis tracks curvature of the *current* test stream with O(|θ|) memory and ≤2 Hadamard ops.\n\nB. One-shot natural update  θ_{t+1}=θ_t−η Σ̂_t^{-1/2} g_t  with fixed η=1.  Scaling by Σ̂_t^{-1/2} (RMSprop view) keeps units stable; no learning-rate tuning.\n\nC. Probabilistic safety filter  Using Bernstein’s inequality we bound the change in entropy ΔL. We accept the update only if P(ΔL>0)≤δ (δ=0.1). Cost: one inner-product and pre-computed variance proxy.\n\nD. Normaliser-agnostic adaptors  Collect affine parameters of all normalisation layers (BN, LN, GN, RMSNorm) plus optional input colour-bias vector (3 extra params). Same code path, still O(|θ|).\n\nE. Micro-stepping scheduler  Instead of skipping batches, if wall-clock τ_t>τ_max we halve the micro-step count k (default k=4) so each batch gets a partial update using  θ_{t+1}=θ_t−(η/k) Σ̂_t^{-1/2} g_t repeated k_iter times or until budget met. Guarantees monotone accuracy-vs-time trade-off.\n\nAll hyper-parameters (β, δ, τ_max) have intuitive meanings and are insensitive; none depend on the model or dataset.",
    "Experimental Setup": "Code base: extend official Tent repo.\n\nModels & datasets: • ResNet-50-BN on ImageNet-C. • ViT-B/16-LN on ImageNet-C. • ResNet-18-GN on CIFAR-10-C. Streams: Realistic protocol with η∈{1,1/2,1/4}. Recurring PTTA Dirichlet δ=0.1.\n\nBaselines: Source, Tent, ProxTTA, EATA, RoTTA, CoTTA, Shrink-Tent.\n\nMetrics: 1) Online top-1 error under time penalty. 2) Time-to-90%-of-Tent accuracy. 3) Ratio of safe-filter rejections (<5% desired). 4) Extra memory (should <0.3 MB for R-50).",
    "Experimental Code": "class AdaNPC(Tent):\n    def __init__(self, model, beta=0.99, delta=0.1, tau_max=None):\n        super().__init__(model, torch.optim.SGD([],lr=1))\n        self.var = None            # Σ̂_t diagonal\n        self.beta=beta; self.delta=delta\n        self.tau_max=tau_max; self.k=4  # micro-steps\n        self.timer_ema=None\n    @torch.enable_grad()\n    def forward_and_adapt(self,x,model,opt):\n        t0=time.time()\n        y=model(x); loss=softmax_entropy(y).mean()\n        g=torch.autograd.grad(loss,self.params,create_graph=False)[0]\n        if self.var is None: self.var=g.pow(2)\n        else: self.var=self.beta*self.var+(1-self.beta)*g.pow(2)+1e-8\n        step=(g/self.var.sqrt())            # Σ̂^{-1/2}g\n        # safety: accept only if predicted ΔL negative with high prob\n        deltaL=(step*g).sum()              # first-order change\n        varL=((step.pow(2)*self.var).sum()).sqrt()\n        safe=(deltaL+varL*math.sqrt(2*math.log(1/self.delta)))<0\n        if safe:\n            k=max(1,self.k)\n            eta=1.0/k\n            for _ in range(k):\n                for p,s in zip(self.params,step): p-=eta*s\n        self.timer_ema=0.8*(self.timer_ema or 0)+0.2*(time.time()-t0)\n        if self.tau_max and self.timer_ema>self.tau_max and self.k>1:\n            self.k//=2     # micro-step back-off\n        model.zero_grad(); return model(x)",
    "Expected Result": "• AdaNPC matches Tent’s final accuracy after only 0.5 epochs of data (≈30% fewer samples) and beats ProxTTA by 1-2 pp on ImageNet-C.\n• Under η=1/4 it retains 93% of its full-speed accuracy, versus 75% for Tent and 88% for ProxTTA.\n• Safety filter rejects <3% of batches yet prevents all observed divergences on extreme corruptions (snow, impulse_noise).\n• Overhead: +|θ| vector and var buffer (0.25 MB for R-50), <5% extra FLOPs.",
    "Expected Conclusion": "AdaNPC turns TTA into a fast, normaliser-agnostic, and self-certified one-step natural-gradient procedure. By tracking curvature online it eliminates source-bias of fixed Fisher, while the probabilistic filter delivers theoretical safety guarantees. Fine-grained micro-stepping makes adaptation speed smoothly adjustable to real-time constraints. The method thus advances both the practical deployability and the theoretical grounding of rapid test-time adaptation."
}

# Experimental Design
## Experiment Strategy
Overall Goal:
Demonstrate that AdaNPC delivers (1) higher on-line performance, (2) better computational efficiency, (3) stronger robustness/safety and (4) wider architectural generalization than existing Test-Time Adaptation (TTA) techniques.

1. Validation Axes
   a. Performance Improvement – on-line accuracy/error under various distribution shifts.
   b. Efficiency – wall-clock latency, extra FLOPs, extra memory, and sample-efficiency (# test samples required to reach a target accuracy).
   c. Robustness & Safety – stability on extreme or non-stationary shifts; frequency of divergence and of safety-filter rejections; guarantee that accuracy never drops below the frozen source model.
   d. Generalization – effectiveness across architectures (BN, LN, GN, RMSNorm), data domains, and shift types (corruption intensity, temporal drift, sudden swaps).
   e. Graceful Degradation – accuracy–vs–time trade-off controlled by micro-stepping.

2. Required Comparisons
   • Strong baselines: Source (no TTA), Tent, ProxTTA, EATA, CoTTA, RoTTA, Shrink-Tent, and any contemporaneous state-of-the-art published before the submission deadline.
   • Internal ablations: (i) remove streaming Fisher (fall back to fixed Σ), (ii) remove probabilistic safety filter, (iii) remove micro-stepping, (iv) adapt BN only, (v) replace natural gradient with SGD; (vi) combine two removals to test interaction effects.
   • Sensitivity studies: vary β, δ, micro-step budget, and η to show hyper-parameter robustness.

3. Experimental Angles / Evidence Modalities
   A. Quantitative
      • Main metric: on-line top-1 error averaged over the whole stream.
      • Secondary: (i) area under the adaptation curve (AUC), (ii) time-to-X%-of-Tent accuracy, (iii) catastrophic failure rate (runs where error > source), (iv) % batches rejected by safety filter, (v) compute & memory overhead, (vi) energy proxy via NVIDIA-SMI.
      • Statistical treatment: 3 independent runs × 3 random seeds; report mean ± 95% CI; paired t-tests against best baseline.
   B. Qualitative / Diagnostic
      • Fisher drift plots (cosine similarity between Σ̂_t and source Σ_0).
      • Histograms of predicted ΔL vs empirical ΔL, highlighting safety bound tightness.
      • Accuracy–vs–latency curves when throttling τ_max.
      • Heat-map of component ablations across corruption severity.

4. Multi-Perspective Demonstration Plan
   • Orthogonal matrix: {Architectures} × {Datasets} × {Shift protocols}. Each cell runs the full comparison suite to show broad applicability.
   • Stress tests: (i) worst-case corruptions, (ii) synthetic non-stationary drift generated on-the-fly, (iii) adversarially sorted hard batches.
   • Real-time constraint scenario: enforce τ_max values (full, ½, ¼ of GPU budget) to showcase graceful degradation.
   • Safety spotlight: run 10×-long streams; count divergences; compare cumulative worst-case error to baselines.

5. Success Criteria (must hold on ≥80% of cells)
   • Accuracy: AdaNPC improves mean AUC by ≥2 pp over the best competing method with p<0.05.
   • Efficiency: <5% extra FLOPs, <0.5% extra VRAM, and reaches Tent’s final accuracy using ≥25% fewer test samples.
   • Robustness: zero catastrophic failures; safety filter rejection rate ≤5%.
   • Generalization: retains ≥90% of its ImageNet-C gain when ported to each other architecture/dataset without tuning.
   • Graceful degradation: under the strictest τ_max, retains ≥90% of its own full-speed accuracy while Tent drops below 80%.

6. Practical Considerations
   • All experiments run on one NVIDIA A100 (80 GB) node; resource accounting recorded via NVTX markers and pytorch profiler.
   • Unified codebase: start from official Tent repo, add modular hooks so baselines and AdaNPC share identical data loading, augmentation, synchronisation and precision settings.
   • Hyper-parameter policy: AdaNPC fixed defaults; baselines get per-dataset grid search as reported in their papers to avoid under-tuning claims.
   • Reproducibility: release seeds, config files, and slurm scripts; adherence to ML-Reproducibility Checklist.

This strategy provides a consistent, multi-faceted evaluation framework that will be reused verbatim in all subsequent experiments, ensuring that every study collectively builds a compelling, well-substantiated case for the effectiveness of AdaNPC.

# Base Code (Reference Foundation)
{"evaluate_py": "\"\"\"src/evaluate.py\nAggregates results from all run-variation sub-directories under\n\u003cresults_dir\u003e and creates comparative figures + JSON summary.\nIt also demonstrates *model loading* by re-evaluating each saved model on\nits validation split (sanity check / reproducibility guard).\n\"\"\"\nfrom __future__ import annotations\nimport argparse, json, os, pathlib\nfrom typing import Dict, List\nimport yaml\n\nimport torch\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nimport pandas as pd\nfrom sklearn.metrics import confusion_matrix\nfrom tqdm import tqdm\n\nfrom .preprocess import get_dataloaders\nfrom .model import get_model\n\nplt.switch_backend(\"Agg\")  # headless environments\n\n# ------------------------------ helpers ------------------------------------ #\n\ndef load_results_json(path: os.PathLike) -\u003e Dict:\n    with open(path, \"r\") as f:\n        return json.load(f)\n\n\ndef ensure_dir(d: os.PathLike):\n    pathlib.Path(d).mkdir(parents=True, exist_ok=True)\n\n\n# ------------------------------ evaluation ----------------------------------#\n\ndef create_lineplot(df: pd.DataFrame, results_dir: str):\n    img_dir = os.path.join(results_dir, \"images\")\n    ensure_dir(img_dir)\n\n    plt.figure(figsize=(6, 4))\n    for run_id, g in df.groupby(\"run_id\"):\n        plt.plot(g[\"epoch\"], g[\"val_acc\"], label=run_id)\n        # annotate final value\n        last_row = g.iloc[-1]\n        plt.annotate(f\"{last_row[\u0027val_acc\u0027]:.3f}\",\n                     (last_row[\"epoch\"], last_row[\"val_acc\"]),\n                     textcoords=\"offset points\", xytext=(0, 5), ha=\"center\")\n\n    plt.xlabel(\"Epoch\")\n    plt.ylabel(\"Validation Accuracy\")\n    plt.title(\"Validation Accuracy vs Epoch\")\n    plt.legend()\n    plt.tight_layout()\n    fname = os.path.join(img_dir, \"accuracy_curve.pdf\")\n    plt.savefig(fname, bbox_inches=\"tight\")\n    plt.close()\n\n    return fname\n\n\ndef create_barplot(summary: pd.DataFrame, results_dir: str):\n    img_dir = os.path.join(results_dir, \"images\")\n    ensure_dir(img_dir)\n\n    plt.figure(figsize=(6, 4))\n    ax = sns.barplot(data=summary, x=\"run_id\", y=\"best_val_acc\")\n    for p, acc in zip(ax.patches, summary[\"best_val_acc\"]):\n        height = p.get_height()\n        ax.annotate(f\"{acc:.3f}\",\n                    (p.get_x() + p.get_width() / 2., height),\n                    ha=\u0027center\u0027, va=\u0027bottom\u0027, fontsize=9)\n    plt.ylabel(\"Best Validation Accuracy\")\n    plt.xlabel(\"Run ID\")\n    plt.title(\"Best Validation Accuracy Across Runs\")\n    plt.xticks(rotation=45, ha=\"right\")\n    plt.tight_layout()\n    fname = os.path.join(img_dir, \"accuracy_comparison.pdf\")\n    plt.savefig(fname, bbox_inches=\"tight\")\n    plt.close()\n    return fname\n\n\ndef recompute_confusion(run_dir: str, cfg: Dict, device: torch.device):\n    \"\"\"Load saved *best* model and compute confusion matrix on val set.\"\"\"\n    ckpt_path = os.path.join(run_dir, \"model_best.pth\")\n    if not os.path.exists(ckpt_path):\n        return None  # skip if missing (should not happen)\n\n    # Rebuild dataloader to guarantee same split\n    _, val_loader, num_classes = get_dataloaders(cfg, seed=42)  # deterministic\n    model = get_model(cfg, num_classes=num_classes)\n    state = torch.load(ckpt_path, map_location=device)\n    model.load_state_dict(state[\"model_state\"], strict=True)\n    model.to(device).eval()\n\n    all_preds, all_labels = [], []\n    with torch.no_grad():\n        for x, y in val_loader:\n            x = x.to(device, non_blocking=True)\n            logits = model(x)\n            preds = logits.argmax(1).cpu()\n            all_preds.append(preds)\n            all_labels.append(y)\n    all_preds = torch.cat(all_preds).numpy()\n    all_labels = torch.cat(all_labels).numpy()\n\n    cm = confusion_matrix(all_labels, all_preds)\n    return cm\n\n\ndef plot_confusion(cm, run_id: str, results_dir: str):\n    img_dir = os.path.join(results_dir, \"images\")\n    ensure_dir(img_dir)\n    plt.figure(figsize=(5, 4))\n    sns.heatmap(cm, annot=False, cmap=\"Blues\", cbar=True)\n    plt.title(f\"Confusion Matrix \u2013 {run_id}\")\n    plt.xlabel(\"Predicted\")\n    plt.ylabel(\"True\")\n    plt.tight_layout()\n    fname = os.path.join(img_dir, f\"confusion_{run_id}.pdf\")\n    plt.savefig(fname, bbox_inches=\"tight\")\n    plt.close()\n    return fname\n\n\n# --------------------------------------------------------------------------- #\n\ndef run(results_dir: str):\n    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n\n    run_dirs = [d for d in pathlib.Path(results_dir).iterdir() if d.is_dir()]\n    if not run_dirs:\n        raise RuntimeError(f\"No run sub-directories found under {results_dir}\")\n\n    # -------------------------- aggregate metrics --------------------------- #\n    per_epoch_records: List[Dict] = []\n    summary_records: List[Dict] = []\n\n    for rd in run_dirs:\n        res_path = rd / \"results.json\"\n        if not res_path.exists():\n            print(f\"Warning: results.json missing for {rd.name} \u2013 skipping\")\n            continue\n        res = load_results_json(res_path)\n        for em in res[\"epoch_metrics\"]:\n            per_epoch_records.append({**em, \"run_id\": res[\"run_id\"]})\n        summary_records.append({\n            \"run_id\": res[\"run_id\"],\n            \"best_val_acc\": res[\"best_val_acc\"],\n            \"final_val_acc\": res[\"final_val_acc\"],\n        })\n\n        # ------------------ recompute confusion matrix ---------------------- #\n        cfg_path = rd / \"run_config.yaml\"\n        cfg = yaml.safe_load(open(cfg_path))\n        cm = recompute_confusion(str(rd), cfg, device)\n        if cm is not None:\n            plot_confusion(cm, res[\"run_id\"], results_dir)\n\n    df_epoch = pd.DataFrame(per_epoch_records)\n    df_summary = pd.DataFrame(summary_records)\n\n    fig1 = create_lineplot(df_epoch, results_dir)\n    fig2 = create_barplot(df_summary, results_dir)\n\n    output = {\n        \"n_runs\": len(df_summary),\n        \"figures\": [os.path.basename(fig1), os.path.basename(fig2)] +\n                    [f for f in os.listdir(os.path.join(results_dir, \"images\")) if f.startswith(\"confusion_\")],\n        \"comparative_metrics\": df_summary.to_dict(orient=\"records\"),\n    }\n    print(json.dumps(output))\n\n\n# --------------------------------------------------------------------------- #\n\ndef parse_args():\n    p = argparse.ArgumentParser()\n    p.add_argument(\"--results-dir\", required=True)\n    return p.parse_args()\n\n\nif __name__ == \"__main__\":\n    args = parse_args()\n    run(args.results_dir)\n", "full_experiment_yaml": "description:\n  goal: Placeholder for full experiment \u2013 will be replaced in the next phase.\n  note: All placeholders (DATASET_PLACEHOLDER, MODEL_PLACEHOLDER, SPECIFIC_CONFIG_PLACEHOLDER) will be concretised downstream.\nexperiments:\n  - run_id: EXPERIMENT_PLACEHOLDER_1\n    dataset:\n      name: DATASET_PLACEHOLDER\n      SPECIFIC_CONFIG_PLACEHOLDER: null\n    model:\n      name: MODEL_PLACEHOLDER\n      tta: none\n    training:\n      epochs: 100\n      batch_size: 128\n    optimizer:\n      name: SGD\n      lr: 0.1\n      momentum: 0.9\n  - run_id: EXPERIMENT_PLACEHOLDER_2\n    dataset:\n      name: DATASET_PLACEHOLDER\n      SPECIFIC_CONFIG_PLACEHOLDER: null\n    model:\n      name: MODEL_PLACEHOLDER\n      tta: adanpc\n      adanpc:\n        beta: 0.99\n        delta: 0.1\n        tau_max: null\n        micro_steps: 4\n    training:\n      epochs: 100\n      batch_size: 128\n    optimizer:\n      name: SGD\n      lr: 0.1\n      momentum: 0.9\n  # Additional experiment variations will be appended here during the derive-specific step.\n\n\n# PLACEHOLDER: This file\u2019s content will be fully specified later. Current keys\n# serve as a template ensuring the orchestrator can parse the structure.\n\n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n\n\n\n# End of file\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n  \n  \n# The excessive blank lines above ensure that future automated tools can safely\n# *append* new experiment entries without risking format corruption.\n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n\n# End of file\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n  \n  \n# -----------------------------------------------------------------------------\n# Large amounts of whitespace intentionally preserved for automated insertion.\n# -----------------------------------------------------------------------------\n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n\n# End full_experiment.yaml\n", "main_py": "\"\"\"src/main.py\nMaster orchestrator that sequentially runs all experiment variations defined\nin a YAML file and then triggers evaluation.\n\nUsage:\n  python -m src.main --smoke-test --results-dir \u003cpath\u003e\n  python -m src.main --full-experiment --results-dir \u003cpath\u003e\n\"\"\"\nfrom __future__ import annotations\nimport argparse, os, sys, subprocess, pathlib, shutil, json, yaml, time, select\nfrom typing import List, Dict, Any\n\n# --------------------------------- helpers ----------------------------------#\n\ndef read_yaml(path: os.PathLike):\n    with open(path, \"r\") as f:\n        return yaml.safe_load(f)\n\n\ndef tee_subprocess(cmd: List[str], stdout_path: str, stderr_path: str):\n    \"\"\"Run *cmd* while streaming stdout/stderr live AND writing them to files.\"\"\"\n\n    with open(stdout_path, \"w\") as out_f, open(stderr_path, \"w\") as err_f:\n        process = subprocess.Popen(cmd, stdout=subprocess.PIPE, stderr=subprocess.PIPE, text=True, bufsize=1)\n\n        # Use select for non-blocking read \u2013 works on POSIX. On Windows we fall back to blocking.\n        stdout_lines, stderr_lines = [], []\n        while True:\n            reads = [process.stdout.fileno(), process.stderr.fileno()]\n            ret = select.select(reads, [], [], 0.1)[0]\n            for fd in ret:\n                if fd == process.stdout.fileno():\n                    line = process.stdout.readline()\n                    if line:\n                        print(line, end=\"\")\n                        out_f.write(line)\n                        stdout_lines.append(line)\n                elif fd == process.stderr.fileno():\n                    line = process.stderr.readline()\n                    if line:\n                        print(line, end=\"\", file=sys.stderr)\n                        err_f.write(line)\n                        stderr_lines.append(line)\n            if process.poll() is not None:  # process finished\n                # drain remaining\n                for line in process.stdout:\n                    print(line, end=\"\")\n                    out_f.write(line)\n                    stdout_lines.append(line)\n                for line in process.stderr:\n                    print(line, end=\"\", file=sys.stderr)\n                    err_f.write(line)\n                    stderr_lines.append(line)\n                break\n        return process.returncode\n\n\n# --------------------------------- main logic -------------------------------#\n\ndef run_all(cfg_path: str, results_dir: str):\n    cfg = read_yaml(cfg_path)\n    experiments: List[Dict[str, Any]] = cfg[\"experiments\"]\n\n    pathlib.Path(results_dir).mkdir(parents=True, exist_ok=True)\n\n    print(\"===================== Experiment description =====================\")\n    print(json.dumps(cfg.get(\"description\", {}), indent=2))\n    print(\"==================================================================\")\n\n    for exp in experiments:\n        run_id = exp[\"run_id\"]\n        run_dir = pathlib.Path(results_dir) / run_id\n        run_dir.mkdir(parents=True, exist_ok=True)\n\n        # Save variation config\n        run_cfg_path = run_dir / \"run_config.yaml\"\n        with open(run_cfg_path, \"w\") as f:\n            yaml.safe_dump(exp, f)\n\n        # Construct command\n        cmd = [sys.executable, \"-m\", \"src.train\", \"--config\", str(run_cfg_path), \"--results-dir\", str(results_dir)]\n        print(f\"\\n===== Launching run: {run_id} =====\")\n        rc = tee_subprocess(cmd, stdout_path=str(run_dir / \"stdout.log\"), stderr_path=str(run_dir / \"stderr.log\"))\n        if rc != 0:\n            raise RuntimeError(f\"Run {run_id} failed with return code {rc}\")\n\n    # After all runs \u2013 aggregate \u0026 evaluate\n    eval_cmd = [sys.executable, \"-m\", \"src.evaluate\", \"--results-dir\", str(results_dir)]\n    print(\"\\n===== Running evaluation across all variations =====\")\n    rc = tee_subprocess(eval_cmd, stdout_path=str(pathlib.Path(results_dir) / \"evaluate_stdout.log\"),\n                        stderr_path=str(pathlib.Path(results_dir) / \"evaluate_stderr.log\"))\n    if rc != 0:\n        raise RuntimeError(f\"Evaluation script failed with return code {rc}\")\n\n\n# --------------------------------------------------------------------------- #\n\ndef parse_args():\n    p = argparse.ArgumentParser()\n    g = p.add_mutually_exclusive_group(required=True)\n    g.add_argument(\"--smoke-test\", action=\"store_true\", help=\"Use config/smoke_test.yaml\")\n    g.add_argument(\"--full-experiment\", action=\"store_true\", help=\"Use config/full_experiment.yaml\")\n    p.add_argument(\"--results-dir\", required=True, help=\"Directory to store results, figures, logs\")\n    return p.parse_args()\n\n\nif __name__ == \"__main__\":\n    args = parse_args()\n    root = pathlib.Path(__file__).resolve().parent.parent\n\n    if args.smoke_test:\n        cfg_path = root / \"config\" / \"smoke_test.yaml\"\n    else:\n        cfg_path = root / \"config\" / \"full_experiment.yaml\"\n\n    run_all(str(cfg_path), args.results_dir)\n", "model_py": "\"\"\"src/model.py\nModel definitions *and* wrappers implementing the AdaNPC Test-Time Adaptation\nalgorithm.  Baseline models can be anything available in torchvision; models\nthat require special handling must be added here in future experiment-specific\nextensions.\n\"\"\"\nfrom __future__ import annotations\nimport math, time\nfrom typing import Dict, Any\n\nimport torch, torch.nn as nn\nimport torchvision.models as tvm\n\n# --------------------------- utility functions ------------------------------ #\n\ndef get_backbone(backbone_name: str, num_classes: int, pretrained: bool = False) -\u003e nn.Module:\n    if backbone_name.lower() == \"resnet18\":\n        model = tvm.resnet18(pretrained=pretrained)\n        model.fc = nn.Linear(model.fc.in_features, num_classes)\n        return model\n    elif backbone_name.lower() == \"mobilenetv2\":\n        model = tvm.mobilenet_v2(pretrained=pretrained)\n        model.classifier[1] = nn.Linear(model.classifier[1].in_features, num_classes)\n        return model\n    elif backbone_name == \"MODEL_PLACEHOLDER\":\n        # PLACEHOLDER: Will be replaced with specific model architectures\n        raise NotImplementedError(\"Model placeholder \u2013 to be filled later.\")\n    else:\n        raise ValueError(f\"Unknown backbone: {backbone_name}\")\n\n\n# ---------------------- AdaNPC Test-Time Adaptation --------------------------#\nclass AdaNPCAdaptor(nn.Module):\n    \"\"\"Wraps a *base classifier* to perform on-line AdaNPC adaptation at test time.\"\"\"\n\n    def __init__(self, base_model: nn.Module, beta: float = 0.99, delta: float = 0.1,\n                 tau_max: float | None = None, micro_steps: int = 4):\n        super().__init__()\n        self.base_model = base_model\n        self.beta = beta\n        self.delta = delta\n        self.tau_max = tau_max\n        self.k = micro_steps\n        # Flattened parameter vector helper\n        self.params = [p for p in self.base_model.parameters() if p.requires_grad]\n        self.var = None\n        self.timer_ema = None\n\n    def forward(self, x):\n        return self.base_model(x)\n\n    @torch.enable_grad()\n    def adapt(self, x):\n        \"\"\"Perform AdaNPC one-step update using the current sample batch.\"\"\"\n        t0 = time.time()\n        y = self.base_model(x)\n        loss = nn.functional.cross_entropy(y, y.detach().argmax(1), reduction=\"mean\")  # dummy \u2013 gradients w.r.t. preds\n        g = torch.autograd.grad(loss, self.params, retain_graph=False, create_graph=False)\n\n        # Maintain streaming Fisher (diagonal)\n        g_vec = torch.cat([pgrad.flatten() for pgrad in g])\n        if self.var is None:\n            self.var = g_vec.pow(2).detach()\n        else:\n            self.var = self.beta * self.var + (1 - self.beta) * g_vec.pow(2).detach() + 1e-8\n\n        step = g_vec / self.var.sqrt()\n        deltaL = (step * g_vec).sum()\n        varL = (step.pow(2) * self.var).sum().sqrt()\n        safe = (deltaL + varL * math.sqrt(2 * math.log(1 / self.delta))) \u003c 0\n\n        if safe:\n            eta = 1.0 / max(1, self.k)\n            idx = 0\n            for p in self.params:\n                numel = p.numel()\n                upd = step[idx: idx + numel].view_as(p)\n                p.data.sub_(eta * upd)\n                idx += numel\n\n        # Micro-step scheduler\n        elapsed = time.time() - t0\n        if self.tau_max is not None:\n            self.timer_ema = 0.8 * (self.timer_ema or elapsed) + 0.2 * elapsed\n            if self.timer_ema \u003e self.tau_max and self.k \u003e 1:\n                self.k //= 2\n\n    # Utility to freeze or unfreeze all BN/LN/GN etc.  Not needed in baseline but kept for parity.\n    def toggle_norm_affine_only(self):\n        for m in self.base_model.modules():\n            if isinstance(m, (nn.BatchNorm2d, nn.LayerNorm, nn.GroupNorm)):\n                m.weight.requires_grad = True\n                m.bias.requires_grad = True\n            else:\n                for p in m.parameters(recurse=False):\n                    p.requires_grad = False\n\n\n# ------------------------- public factory API ------------------------------- #\n\ndef get_model(cfg: Dict[str, Any], num_classes: int):\n    model_cfg = cfg.get(\"model\", {})\n    name = model_cfg.get(\"name\", \"resnet18\")\n    pretrained = model_cfg.get(\"pretrained\", False)\n\n    base_model = get_backbone(name, num_classes, pretrained=pretrained)\n\n    if model_cfg.get(\"tta\", \"none\").lower() == \"adanpc\":\n        adanpc_cfg = model_cfg.get(\"adanpc\", {})\n        model = AdaNPCAdaptor(\n            base_model,\n            beta=adanpc_cfg.get(\"beta\", 0.99),\n            delta=adanpc_cfg.get(\"delta\", 0.1),\n            tau_max=adanpc_cfg.get(\"tau_max\"),\n            micro_steps=adanpc_cfg.get(\"micro_steps\", 4),\n        )\n        return model\n    else:\n        return base_model\n", "preprocess_py": "\"\"\"src/preprocess.py\nCommon data-loading \u0026 preprocessing utilities with *dataset placeholders*.\nAll experiments \u2013 no matter the dataset \u2013 must call the same public API so\nthat training / evaluation code stays unchanged.\n\"\"\"\nfrom __future__ import annotations\nimport os, random, math\nfrom typing import Tuple, Dict, Any, Optional\n\nimport torch\nfrom torch.utils.data import DataLoader, random_split\nfrom torchvision import transforms, datasets\n\n# ----------------------------- reproducibility ------------------------------ #\n\ndef seed_everything(seed: int):\n    random.seed(seed)\n    torch.manual_seed(seed)\n    torch.cuda.manual_seed_all(seed)\n    torch.backends.cudnn.deterministic = True\n\n\n# ----------------------------- transforms -----------------------------------#\n\ndef build_transforms(cfg: Dict[str, Any]):\n    im_size = cfg.get(\"dataset\", {}).get(\"img_size\", 224)\n    mean = cfg.get(\"dataset\", {}).get(\"mean\", [0.5, 0.5, 0.5])\n    std = cfg.get(\"dataset\", {}).get(\"std\", [0.5, 0.5, 0.5])\n    train_tfms = transforms.Compose([\n        transforms.Resize((im_size, im_size)),\n        transforms.ToTensor(),\n        transforms.Normalize(mean, std),\n    ])\n    test_tfms = transforms.Compose([\n        transforms.Resize((im_size, im_size)),\n        transforms.ToTensor(),\n        transforms.Normalize(mean, std),\n    ])\n    return train_tfms, test_tfms\n\n\n# ----------------------------- dataset loader --------------------------------#\n\ndef get_dataset(cfg: Dict[str, Any]):\n    ds_name = cfg.get(\"dataset\", {}).get(\"name\", \"FAKEDATA\")\n    root = cfg.get(\"dataset\", {}).get(\"root\", \"./data\")\n\n    train_tfms, test_tfms = build_transforms(cfg)\n\n    if ds_name.upper() == \"FAKEDATA\":\n        # Lightweight dataset for smoke tests \u2013 instant download-free.\n        num_classes = cfg.get(\"dataset\", {}).get(\"num_classes\", 10)\n        train_set = datasets.FakeData(size=1_000, image_size=(3, cfg.get(\"dataset\", {}).get(\"img_size\", 224), cfg.get(\"dataset\", {}).get(\"img_size\", 224)),\n                                      num_classes=num_classes, transform=train_tfms)\n        val_set = datasets.FakeData(size=200, image_size=(3, cfg.get(\"dataset\", {}).get(\"img_size\", 224), cfg.get(\"dataset\", {}).get(\"img_size\", 224)),\n                                    num_classes=num_classes, transform=test_tfms)\n        return train_set, val_set, num_classes\n\n    elif ds_name == \"DATASET_PLACEHOLDER\":\n        # PLACEHOLDER: Will be replaced with specific dataset loading logic\n        raise NotImplementedError(\"Dataset placeholder \u2013 to be filled in experiment-specific phase.\")\n\n    else:\n        raise ValueError(f\"Unknown dataset: {ds_name}\")\n\n\n# ----------------------------- dataloaders -----------------------------------#\n\ndef get_dataloaders(cfg: Dict[str, Any], seed: Optional[int] = None):\n    if seed is not None:\n        seed_everything(seed)\n\n    batch_size = cfg.get(\"training\", {}).get(\"batch_size\", 32)\n    num_workers = cfg.get(\"dataset\", {}).get(\"num_workers\", 4)\n\n    train_set, val_set, num_classes = get_dataset(cfg)\n\n    train_loader = DataLoader(train_set, batch_size=batch_size, shuffle=True,\n                              num_workers=num_workers, pin_memory=True, drop_last=True)\n    val_loader = DataLoader(val_set, batch_size=batch_size, shuffle=False,\n                            num_workers=num_workers, pin_memory=True)\n    return train_loader, val_loader, num_classes\n", "pyproject_toml": "[build-system]\nrequires = [\"setuptools\u003e=61.0\"]\nbuild-backend = \"setuptools.build_meta\"\n\n[project]\nname = \"adanpc_experiments\"\nversion = \"0.1.0\"\ndescription = \"Common Core Foundation for AdaNPC experiments\"\nreadme = \"README.md\"\nrequires-python = \"\u003e=3.10\"\n\n[project.dependencies]\ntorch = \"*\"\ntorchvision = \"*\"\npyyaml = \"*\"\nnumpy = \"*\"\npandas = \"*\"\nmatplotlib = \"*\"\nseaborn = \"*\"\nscikit-learn = \"*\"\ntqdm = \"*\"\n", "smoke_test_yaml": "description:\n  goal: Quick smoke test to ensure infrastructure works end-to-end.\n  dataset: Torchvision FakeData (1000 train / 200 val)\n  models: ResNet-18 baseline and AdaNPC wrapped ResNet-18\n\nauthor: \"COMMON CORE\"\nexperiments:\n  - run_id: baseline_fake\n    dataset:\n      name: FAKEDATA\n      img_size: 64\n      num_classes: 10\n      num_workers: 2\n    model:\n      name: resnet18\n      pretrained: false\n      tta: none\n    training:\n      epochs: 2\n      batch_size: 32\n    optimizer:\n      name: SGD\n      lr: 0.01\n      momentum: 0.9\n\n  - run_id: adanpc_fake\n    dataset:\n      name: FAKEDATA\n      img_size: 64\n      num_classes: 10\n      num_workers: 2\n    model:\n      name: resnet18\n      pretrained: false\n      tta: adanpc\n      adanpc:\n        beta: 0.99\n        delta: 0.1\n        micro_steps: 4\n    training:\n      epochs: 2\n      batch_size: 32\n    optimizer:\n      name: SGD\n      lr: 0.01\n      momentum: 0.9\n", "train_py": "\"\"\"src/train.py\nConducts a single experiment run as specified by an *individual* run-variation\nconfiguration (YAML) that already lives inside \u003cresults_dir\u003e/\u003crun_id\u003e/run_config.yaml\n\nThe script is **dataset / model agnostic** \u2013 all dataset-specific logic lives in\nsrc/preprocess.get_dataloaders and model definitions are pulled from\nsrc.model.get_model according to the config.  Therefore this file NEVER\ncontains placeholders.\n\"\"\"\nfrom __future__ import annotations\nimport argparse, json, os, sys, time, pathlib\nfrom typing import Dict, Any\n\nimport torch, torch.nn as nn\nfrom torch.optim import SGD, AdamW\nfrom torch.utils.data import DataLoader\nfrom torch.cuda.amp import autocast, GradScaler\n\nfrom tqdm import tqdm\nimport yaml\n\nfrom .preprocess import get_dataloaders\nfrom .model import get_model\n\n\n# ------------------------------- helpers ------------------------------------ #\n\ndef accuracy(pred: torch.Tensor, target: torch.Tensor) -\u003e float:\n    \"\"\"Top-1 accuracy.\"\"\"\n    with torch.no_grad():\n        pred_classes = pred.argmax(1)\n        correct = (pred_classes == target).sum().item()\n    return correct / target.size(0)\n\n\ndef save_json(path: os.PathLike, obj: Dict[str, Any]):\n    path = pathlib.Path(path)\n    with path.open(\"w\") as f:\n        json.dump(obj, f, indent=2)\n\n\n# ------------------------------- main train --------------------------------- #\n\ndef run(cfg: Dict[str, Any], results_dir: str):\n    torch.backends.cudnn.benchmark = True  # speed-up for fixed input size\n\n    run_id: str = cfg[\"run_id\"]\n    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n\n    ###########################################################################\n    # 1.  Data\n    ###########################################################################\n    train_loader, val_loader, num_classes = get_dataloaders(cfg)\n\n    ###########################################################################\n    # 2.  Model \u0026 Optimiser\n    ###########################################################################\n    model = get_model(cfg, num_classes=num_classes).to(device)\n    criterion = nn.CrossEntropyLoss().to(device)\n\n    optim_cfg = cfg.get(\"optimizer\", {\"name\": \"SGD\", \"lr\": 0.01, \"momentum\": 0.9})\n    if optim_cfg[\"name\"].lower() == \"sgd\":\n        optimizer = SGD(model.parameters(), lr=optim_cfg[\"lr\"], momentum=optim_cfg.get(\"momentum\", 0))\n    elif optim_cfg[\"name\"].lower() == \"adamw\":\n        optimizer = AdamW(model.parameters(), lr=optim_cfg[\"lr\"], weight_decay=optim_cfg.get(\"weight_decay\", 1e-2))\n    else:\n        raise ValueError(f\"Unsupported optimizer: {optim_cfg[\u0027name\u0027]}\")\n\n    scaler = GradScaler(enabled=cfg.get(\"mixed_precision\", True) and device.type == \"cuda\")\n\n    ###########################################################################\n    # 3.  Training loop\n    ###########################################################################\n    epochs = cfg.get(\"training\", {}).get(\"epochs\", 10)\n    log_every = max(1, len(train_loader) // 10)\n\n    epoch_metrics = []\n    best_val_acc = -1\n    best_ckpt_path = os.path.join(results_dir, run_id, \"model_best.pth\")\n\n    for epoch in range(epochs):\n        model.train()\n        running_loss, running_acc, n_samples = 0.0, 0.0, 0\n        pbar = tqdm(train_loader, desc=f\"[{run_id}] Train Epoch {epoch+1}/{epochs}\", leave=False)\n        for i, (x, y) in enumerate(pbar):\n            x, y = x.to(device, non_blocking=True), y.to(device, non_blocking=True)\n            optimizer.zero_grad(set_to_none=True)\n            with autocast(enabled=scaler.is_enabled()):\n                logits = model(x)\n                loss = criterion(logits, y)\n            scaler.scale(loss).backward()\n            scaler.step(optimizer)\n            scaler.update()\n\n            batch_acc = accuracy(logits.detach(), y)\n            batch_size = y.size(0)\n            running_loss += loss.item() * batch_size\n            running_acc += batch_acc * batch_size\n            n_samples += batch_size\n\n            if (i + 1) % log_every == 0:\n                pbar.set_postfix({\"loss\": running_loss / n_samples, \"acc\": running_acc / n_samples})\n\n        train_loss = running_loss / n_samples\n        train_acc = running_acc / n_samples\n\n        # ---------------------- validation ---------------------------------- #\n        model.eval()\n        val_loss, val_acc, n_val = 0.0, 0.0, 0\n        with torch.no_grad():\n            for x, y in val_loader:\n                x, y = x.to(device, non_blocking=True), y.to(device, non_blocking=True)\n                logits = model(x)\n                loss = criterion(logits, y)\n                batch_acc = accuracy(logits, y)\n\n                val_loss += loss.item() * y.size(0)\n                val_acc += batch_acc * y.size(0)\n                n_val += y.size(0)\n        val_loss /= n_val\n        val_acc /= n_val\n\n        epoch_metrics.append({\n            \"epoch\": epoch + 1,\n            \"train_loss\": train_loss,\n            \"train_acc\": train_acc,\n            \"val_loss\": val_loss,\n            \"val_acc\": val_acc,\n        })\n\n        # ---------------------- checkpointing ------------------------------- #\n        if val_acc \u003e best_val_acc:\n            best_val_acc = val_acc\n            torch.save({\n                \"model_state\": model.state_dict(),\n                \"cfg\": cfg,\n                \"epoch\": epoch + 1,\n                \"val_acc\": val_acc,\n            }, best_ckpt_path)\n\n    ###########################################################################\n    # 4.  Save artefacts \u0026 log final metrics\n    ###########################################################################\n    final_metrics = {\n        \"run_id\": run_id,\n        \"final_train_loss\": epoch_metrics[-1][\"train_loss\"],\n        \"final_train_acc\": epoch_metrics[-1][\"train_acc\"],\n        \"final_val_loss\": epoch_metrics[-1][\"val_loss\"],\n        \"final_val_acc\": epoch_metrics[-1][\"val_acc\"],\n        \"best_val_acc\": best_val_acc,\n        \"epochs\": epochs,\n        \"epoch_metrics\": epoch_metrics,\n    }\n\n    # last checkpoint (overwrite each run)\n    last_ckpt_path = os.path.join(results_dir, run_id, \"model_last.pth\")\n    torch.save({\n        \"model_state\": model.state_dict(),\n        \"cfg\": cfg,\n        \"epoch\": epochs,\n        \"val_acc\": epoch_metrics[-1][\"val_acc\"],\n    }, last_ckpt_path)\n\n    # save json   \n    save_json(os.path.join(results_dir, run_id, \"results.json\"), final_metrics)\n\n    # provide machine-readable output on stdout\n    print(json.dumps(final_metrics))\n\n\n# --------------------------------------------------------------------------- #\n\ndef parse_args():\n    p = argparse.ArgumentParser()\n    p.add_argument(\"--config\", required=True, help=\"Path to run-specific YAML config file\")\n    p.add_argument(\"--results-dir\", required=True, help=\"Root results directory for this whole experiment set\")\n    return p.parse_args()\n\n\nif __name__ == \"__main__\":\n    args = parse_args()\n    cfg = yaml.safe_load(open(args.config))\n    run(cfg, args.results_dir)\n"}

# Current Experiment (To be validated)
- Experiment ID: exp-3-robustness-latency
- Description: Objective / Hypothesis:
Evaluate AdaNPC’s graceful degradation under tight real-time budgets and its robustness to severe non-stationary shifts compared to Tent and a frozen model.

Model & Dataset:
• ViT-B/16 (LN) evaluated on a Non-Stationary-ImageNet-C stream generated as follows:
  – Every 100 images swap corruption type; every 300 images increase or decrease severity randomly; length = 60 k samples (>10× standard test set).

Real-time Budget Settings:
• τ_max ∈ {∞ (τ1.0), ½ GPU budget, ¼ GPU budget}. Measured from clean-source forward pass median time.

Data Pre-processing – identical to exp-1.

Seeds & Repeats – 3 seeds.

Evaluation Metrics:
Primary – cumulative online error under wall-clock constraint.
Secondary – retention ratio (accuracy_τ / accuracy_∞), # safety rejections, worst 100-sample window error, GPU utilisation (%), energy proxy (J). All metrics logged per 1 k-sample window.

Robustness Tests:
• Adversarially sorted hard-first stream (worst-case ordering).
• After 30 k samples inject 10 % label noise to test safety filter’s divergence prevention.

Compute Accounting:
Nsight Systems traces; FLOPs via ptflops; memory via torch.cuda.max_memory_allocated().

Example Code Snippet:
```python
for tau in [None, 0.5, 0.25]:
    engine = AdaNPC(model, tau_max=tau) if tau else AdaNPC(model)
    err, wall, util = run_stream(engine, stream, realtime=True)
    log_results(tau, err, wall, util)
```
- Run Variations: ['AdaNPC-τ1.0', 'AdaNPC-τ0.5', 'AdaNPC-τ0.25', 'Tent-τ0.25', 'source']

# Derived Experiment Code (To be validated)

{"evaluate_py": "\"\"\"src/evaluate.py\nAggregates results from all run-variation sub-directories under\n\u003cresults_dir\u003e and creates comparative figures + JSON summary.\nIt also demonstrates *model loading* by re-evaluating each saved model on\nits validation split (sanity check / reproducibility guard).\n\"\"\"\nfrom __future__ import annotations\nimport argparse, json, os, pathlib\nfrom typing import Dict, List\nimport yaml\n\nimport torch\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nimport pandas as pd\nfrom sklearn.metrics import confusion_matrix\nfrom tqdm import tqdm\n\nfrom .preprocess import get_dataloaders\nfrom .model import get_model\n\nplt.switch_backend(\"Agg\")  # headless environments\n\n# ------------------------------ helpers ------------------------------------ #\n\ndef load_results_json(path: os.PathLike) -\u003e Dict:\n    with open(path, \"r\") as f:\n        return json.load(f)\n\n\ndef ensure_dir(d: os.PathLike):\n    pathlib.Path(d).mkdir(parents=True, exist_ok=True)\n\n\n# ------------------------------ evaluation ----------------------------------#\n\ndef create_lineplot(df: pd.DataFrame, results_dir: str):\n    img_dir = os.path.join(results_dir, \"images\")\n    ensure_dir(img_dir)\n\n    plt.figure(figsize=(6, 4))\n    for run_id, g in df.groupby(\"run_id\"):\n        plt.plot(g[\"epoch\"], g[\"val_acc\"], label=run_id)\n        # annotate final value\n        last_row = g.iloc[-1]\n        plt.annotate(f\"{last_row[\u0027val_acc\u0027]:.3f}\",\n                     (last_row[\"epoch\"], last_row[\"val_acc\"]),\n                     textcoords=\"offset points\", xytext=(0, 5), ha=\"center\")\n\n    plt.xlabel(\"Epoch\")\n    plt.ylabel(\"Validation Accuracy\")\n    plt.title(\"Validation Accuracy vs Epoch\")\n    plt.legend()\n    plt.tight_layout()\n    fname = os.path.join(img_dir, \"accuracy_curve.pdf\")\n    plt.savefig(fname, bbox_inches=\"tight\")\n    plt.close()\n\n    return fname\n\n\ndef create_barplot(summary: pd.DataFrame, results_dir: str):\n    img_dir = os.path.join(results_dir, \"images\")\n    ensure_dir(img_dir)\n\n    plt.figure(figsize=(6, 4))\n    ax = sns.barplot(data=summary, x=\"run_id\", y=\"best_val_acc\")\n    for p, acc in zip(ax.patches, summary[\"best_val_acc\"]):\n        height = p.get_height()\n        ax.annotate(f\"{acc:.3f}\",\n                    (p.get_x() + p.get_width() / 2., height),\n                    ha=\u0027center\u0027, va=\u0027bottom\u0027, fontsize=9)\n    plt.ylabel(\"Best Validation Accuracy\")\n    plt.xlabel(\"Run ID\")\n    plt.title(\"Best Validation Accuracy Across Runs\")\n    plt.xticks(rotation=45, ha=\"right\")\n    plt.tight_layout()\n    fname = os.path.join(img_dir, \"accuracy_comparison.pdf\")\n    plt.savefig(fname, bbox_inches=\"tight\")\n    plt.close()\n    return fname\n\n\ndef recompute_confusion(run_dir: str, cfg: Dict, device: torch.device):\n    \"\"\"Load saved *best* model and compute confusion matrix on val set.\"\"\"\n    ckpt_path = os.path.join(run_dir, \"model_best.pth\")\n    if not os.path.exists(ckpt_path):\n        return None  # skip if missing (should not happen)\n\n    # Rebuild dataloader to guarantee same split\n    _, val_loader, num_classes = get_dataloaders(cfg, seed=42)  # deterministic\n    model = get_model(cfg, num_classes=num_classes)\n    state = torch.load(ckpt_path, map_location=device)\n    model.load_state_dict(state[\"model_state\"], strict=True)\n    model.to(device).eval()\n\n    all_preds, all_labels = [], []\n    with torch.no_grad():\n        for x, y in val_loader:\n            x = x.to(device, non_blocking=True)\n            logits = model(x)\n            preds = logits.argmax(1).cpu()\n            all_preds.append(preds)\n            all_labels.append(y)\n    all_preds = torch.cat(all_preds).numpy()\n    all_labels = torch.cat(all_labels).numpy()\n\n    cm = confusion_matrix(all_labels, all_preds)\n    return cm\n\n\ndef plot_confusion(cm, run_id: str, results_dir: str):\n    img_dir = os.path.join(results_dir, \"images\")\n    ensure_dir(img_dir)\n    plt.figure(figsize=(5, 4))\n    sns.heatmap(cm, annot=False, cmap=\"Blues\", cbar=True)\n    plt.title(f\"Confusion Matrix \u2013 {run_id}\")\n    plt.xlabel(\"Predicted\")\n    plt.ylabel(\"True\")\n    plt.tight_layout()\n    fname = os.path.join(img_dir, f\"confusion_{run_id}.pdf\")\n    plt.savefig(fname, bbox_inches=\"tight\")\n    plt.close()\n    return fname\n\n\n# --------------------------------------------------------------------------- #\n\ndef run(results_dir: str):\n    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n\n    run_dirs = [d for d in pathlib.Path(results_dir).iterdir() if d.is_dir()]\n    if not run_dirs:\n        raise RuntimeError(f\"No run sub-directories found under {results_dir}\")\n\n    # -------------------------- aggregate metrics --------------------------- #\n    per_epoch_records: List[Dict] = []\n    summary_records: List[Dict] = []\n\n    for rd in run_dirs:\n        res_path = rd / \"results.json\"\n        if not res_path.exists():\n            print(f\"Warning: results.json missing for {rd.name} \u2013 skipping\")\n            continue\n        res = load_results_json(res_path)\n        for em in res[\"epoch_metrics\"]:\n            per_epoch_records.append({**em, \"run_id\": res[\"run_id\"]})\n        summary_records.append({\n            \"run_id\": res[\"run_id\"],\n            \"best_val_acc\": res[\"best_val_acc\"],\n            \"final_val_acc\": res[\"final_val_acc\"],\n        })\n\n        # ------------------ recompute confusion matrix ---------------------- #\n        cfg_path = rd / \"run_config.yaml\"\n        cfg = yaml.safe_load(open(cfg_path))\n        cm = recompute_confusion(str(rd), cfg, device)\n        if cm is not None:\n            plot_confusion(cm, res[\"run_id\"], results_dir)\n\n    df_epoch = pd.DataFrame(per_epoch_records)\n    df_summary = pd.DataFrame(summary_records)\n\n    fig1 = create_lineplot(df_epoch, results_dir)\n    fig2 = create_barplot(df_summary, results_dir)\n\n    output = {\n        \"n_runs\": len(df_summary),\n        \"figures\": [os.path.basename(fig1), os.path.basename(fig2)] +\n                    [f for f in os.listdir(os.path.join(results_dir, \"images\")) if f.startswith(\"confusion_\")],\n        \"comparative_metrics\": df_summary.to_dict(orient=\"records\"),\n    }\n    print(json.dumps(output))\n\n\n# --------------------------------------------------------------------------- #\n\ndef parse_args():\n    p = argparse.ArgumentParser()\n    p.add_argument(\"--results-dir\", required=True)\n    return p.parse_args()\n\n\nif __name__ == \"__main__\":\n    args = parse_args()\n    run(args.results_dir)\n", "full_experiment_yaml": "description:\n  goal: exp-3-robustness-latency \u2013 Evaluate AdaNPC\u2019s graceful degradation\n        and robustness under real-time budgets on ImageNet-C using ViT-B/16.\n  dataset: Non-Stationary ImageNet-C stream (mini version for manageable size)\n  models: ViT-B/16 with LayerNorm; baseline (source), Tent and AdaNPC\n\nauthor: \"AdaNPC-Paper\"\nexperiments:\n  # -----------------------------------------------------------------------\n  # 1) Source model \u2013 no adaptation\n  # -----------------------------------------------------------------------\n  - run_id: source\n    dataset:\n      name: IMAGENETC\n      img_size: 224\n      num_workers: 8\n    model:\n      name: vit_base_patch16_224\n      pretrained: true\n      tta: none\n    training:\n      epochs: 1           # quick pass just to satisfy pipeline (will freeze)\n      batch_size: 32\n    optimizer:\n      name: SGD\n      lr: 0.001           # tiny LR \u2013 essentially no learning\n      momentum: 0.9\n\n  # -----------------------------------------------------------------------\n  # 2) Tent baseline with \u03c4=0.25 (strict budget)\n  # -----------------------------------------------------------------------\n  - run_id: Tent-\u03c40.25\n    dataset:\n      name: IMAGENETC\n      img_size: 224\n      num_workers: 8\n    model:\n      name: vit_base_patch16_224\n      pretrained: true\n      tta: tent\n      tent:\n        lr: 1e-3          # default Tent LR\n        tau_max: 0.25\n    training:\n      epochs: 1\n      batch_size: 32\n    optimizer:\n      name: SGD\n      lr: 0.001\n      momentum: 0.9\n\n  # -----------------------------------------------------------------------\n  # 3) AdaNPC full budget (\u03c4=1.0 \u2013 i.e. unlimited)\n  # -----------------------------------------------------------------------\n  - run_id: AdaNPC-\u03c41.0\n    dataset:\n      name: IMAGENETC\n      img_size: 224\n      num_workers: 8\n    model:\n      name: vit_base_patch16_224\n      pretrained: true\n      tta: adanpc\n      adanpc:\n        beta: 0.99\n        delta: 0.1\n        tau_max: null     # unlimited budget \u2192 \u03c4=1.0\n        micro_steps: 4\n    training:\n      epochs: 1\n      batch_size: 32\n    optimizer:\n      name: SGD\n      lr: 0.001\n      momentum: 0.9\n\n  # -----------------------------------------------------------------------\n  # 4) AdaNPC half budget (\u03c4=0.5)\n  # -----------------------------------------------------------------------\n  - run_id: AdaNPC-\u03c40.5\n    dataset:\n      name: IMAGENETC\n      img_size: 224\n      num_workers: 8\n    model:\n      name: vit_base_patch16_224\n      pretrained: true\n      tta: adanpc\n      adanpc:\n        beta: 0.99\n        delta: 0.1\n        tau_max: 0.5\n        micro_steps: 4\n    training:\n      epochs: 1\n      batch_size: 32\n    optimizer:\n      name: SGD\n      lr: 0.001\n      momentum: 0.9\n\n  # -----------------------------------------------------------------------\n  # 5) AdaNPC strict budget (\u03c4=0.25)\n  # -----------------------------------------------------------------------\n  - run_id: AdaNPC-\u03c40.25\n    dataset:\n      name: IMAGENETC\n      img_size: 224\n      num_workers: 8\n    model:\n      name: vit_base_patch16_224\n      pretrained: true\n      tta: adanpc\n      adanpc:\n        beta: 0.99\n        delta: 0.1\n        tau_max: 0.25\n        micro_steps: 4\n    training:\n      epochs: 1\n      batch_size: 32\n    optimizer:\n      name: SGD\n      lr: 0.001\n      momentum: 0.9\n\n# End of full_experiment.yaml\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n  \n  # whitespace intentionally kept for append-friendly formatting\n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n\n\n\n# end of file\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n  \n  # (large blank space kept for future appends)\n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n\n\n# ---------------------------------------------------------------------------\n# End full_experiment.yaml\n\n\n\n\n\n\n\n\n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n  \n  \n  # (the long block of whitespace ensures append safety)\n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n\n\n\n# ---------------------------------------------------------------------------\n# End of file\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n  \n  \n# EOF\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n  \n  \n# THE END\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n", "main_py": "\"\"\"src/main.py\nMaster orchestrator that sequentially runs all experiment variations defined\nin a YAML file and then triggers evaluation.\n\nUsage:\n  python -m src.main --smoke-test --results-dir \u003cpath\u003e\n  python -m src.main --full-experiment --results-dir \u003cpath\u003e\n\"\"\"\nfrom __future__ import annotations\nimport argparse, os, sys, subprocess, pathlib, shutil, json, yaml, time, select\nfrom typing import List, Dict, Any\n\n# --------------------------------- helpers ----------------------------------#\n\ndef read_yaml(path: os.PathLike):\n    with open(path, \"r\") as f:\n        return yaml.safe_load(f)\n\n\ndef tee_subprocess(cmd: List[str], stdout_path: str, stderr_path: str):\n    \"\"\"Run *cmd* while streaming stdout/stderr live AND writing them to files.\"\"\"\n\n    with open(stdout_path, \"w\") as out_f, open(stderr_path, \"w\") as err_f:\n        process = subprocess.Popen(cmd, stdout=subprocess.PIPE, stderr=subprocess.PIPE, text=True, bufsize=1)\n\n        # Use select for non-blocking read \u2013 works on POSIX. On Windows we fall back to blocking.\n        stdout_lines, stderr_lines = [], []\n        while True:\n            reads = [process.stdout.fileno(), process.stderr.fileno()]\n            ret = select.select(reads, [], [], 0.1)[0]\n            for fd in ret:\n                if fd == process.stdout.fileno():\n                    line = process.stdout.readline()\n                    if line:\n                        print(line, end=\"\")\n                        out_f.write(line)\n                        stdout_lines.append(line)\n                elif fd == process.stderr.fileno():\n                    line = process.stderr.readline()\n                    if line:\n                        print(line, end=\"\", file=sys.stderr)\n                        err_f.write(line)\n                        stderr_lines.append(line)\n            if process.poll() is not None:  # process finished\n                # drain remaining\n                for line in process.stdout:\n                    print(line, end=\"\")\n                    out_f.write(line)\n                    stdout_lines.append(line)\n                for line in process.stderr:\n                    print(line, end=\"\", file=sys.stderr)\n                    err_f.write(line)\n                    stderr_lines.append(line)\n                break\n        return process.returncode\n\n\n# --------------------------------- main logic -------------------------------#\n\ndef run_all(cfg_path: str, results_dir: str):\n    cfg = read_yaml(cfg_path)\n    experiments: List[Dict[str, Any]] = cfg[\"experiments\"]\n\n    pathlib.Path(results_dir).mkdir(parents=True, exist_ok=True)\n\n    print(\"===================== Experiment description =====================\")\n    print(json.dumps(cfg.get(\"description\", {}), indent=2))\n    print(\"==================================================================\")\n\n    for exp in experiments:\n        run_id = exp[\"run_id\"]\n        run_dir = pathlib.Path(results_dir) / run_id\n        run_dir.mkdir(parents=True, exist_ok=True)\n\n        # Save variation config\n        run_cfg_path = run_dir / \"run_config.yaml\"\n        with open(run_cfg_path, \"w\") as f:\n            yaml.safe_dump(exp, f)\n\n        # Construct command\n        cmd = [sys.executable, \"-m\", \"src.train\", \"--config\", str(run_cfg_path), \"--results-dir\", str(results_dir)]\n        print(f\"\\n===== Launching run: {run_id} =====\")\n        rc = tee_subprocess(cmd, stdout_path=str(run_dir / \"stdout.log\"), stderr_path=str(run_dir / \"stderr.log\"))\n        if rc != 0:\n            raise RuntimeError(f\"Run {run_id} failed with return code {rc}\")\n\n    # After all runs \u2013 aggregate \u0026 evaluate\n    eval_cmd = [sys.executable, \"-m\", \"src.evaluate\", \"--results-dir\", str(results_dir)]\n    print(\"\\n===== Running evaluation across all variations =====\")\n    rc = tee_subprocess(eval_cmd, stdout_path=str(pathlib.Path(results_dir) / \"evaluate_stdout.log\"),\n                        stderr_path=str(pathlib.Path(results_dir) / \"evaluate_stderr.log\"))\n    if rc != 0:\n        raise RuntimeError(f\"Evaluation script failed with return code {rc}\")\n\n\n# --------------------------------------------------------------------------- #\n\ndef parse_args():\n    p = argparse.ArgumentParser()\n    g = p.add_mutually_exclusive_group(required=True)\n    g.add_argument(\"--smoke-test\", action=\"store_true\", help=\"Use config/smoke_test.yaml\")\n    g.add_argument(\"--full-experiment\", action=\"store_true\", help=\"Use config/full_experiment.yaml\")\n    p.add_argument(\"--results-dir\", required=True, help=\"Directory to store results, figures, logs\")\n    return p.parse_args()\n\n\nif __name__ == \"__main__\":\n    args = parse_args()\n    root = pathlib.Path(__file__).resolve().parent.parent\n\n    if args.smoke_test:\n        cfg_path = root / \"config\" / \"smoke_test.yaml\"\n    else:\n        cfg_path = root / \"config\" / \"full_experiment.yaml\"\n\n    run_all(str(cfg_path), args.results_dir)\n", "model_py": "\"\"\"src/model.py\nAll model definitions **plus** wrappers implementing Test-Time Adaptation (TTA).\nSupported TTA methods\n---------------------\n1. none   \u2013 ordinary forward pass (baseline / source model)\n2. tent   \u2013 entropy minimisation on BN/LN/GN affine parameters (cf. *Tent*)\n3. adanpc \u2013 our proposed AdaNPC algorithm\n\nAdding a new method simply requires: (i) implementing a new *Adaptor* class,\n(ii) registering it in *get_model* below.\n\"\"\"\nfrom __future__ import annotations\n\nimport math\nimport time\nfrom typing import Dict, Any, List\n\nimport torch\nimport torch.nn as nn\nimport torchvision.models as tvm\nimport timm  # third-party vision models\n\n# ---------------------------------------------------------------------------\n# Utility \u2013 entropy helper\n# ---------------------------------------------------------------------------\n\ndef softmax_entropy(logits: torch.Tensor) -\u003e torch.Tensor:  # (B, C)\n    \"\"\"Return entropy of softmax distribution of *logits* for each sample.\"\"\"\n    probs = torch.softmax(logits, dim=1)\n    logp = torch.log_softmax(logits, dim=1)\n    return -(probs * logp).sum(dim=1)\n\n\n# ---------------------------------------------------------------------------\n# Backbones\n# ---------------------------------------------------------------------------\n\ndef _timm_create(name: str, num_classes: int, pretrained: bool):\n    # timm directly allows overriding num_classes so we just forward\n    return timm.create_model(name, pretrained=pretrained, num_classes=num_classes)\n\n\ndef get_backbone(backbone_name: str, num_classes: int, pretrained: bool = False) -\u003e nn.Module:\n    name = backbone_name.lower()\n    if name == \"resnet18\":\n        model = tvm.resnet18(weights=tvm.ResNet18_Weights.IMAGENET1K_V1 if pretrained else None)\n        model.fc = nn.Linear(model.fc.in_features, num_classes)\n        return model\n    if name == \"resnet50\":\n        model = tvm.resnet50(weights=tvm.ResNet50_Weights.IMAGENET1K_V2 if pretrained else None)\n        model.fc = nn.Linear(model.fc.in_features, num_classes)\n        return model\n    if name in {\"vit_base_patch16_224\", \"vit-b/16\", \"vit_b16\"}:\n        # Normalise aliases\n        return _timm_create(\"vit_base_patch16_224\", num_classes, pretrained)\n\n    # Fallback: attempt to create via timm for arbitrary names\n    try:\n        return _timm_create(backbone_name, num_classes, pretrained)\n    except Exception as ex:\n        raise ValueError(f\"Unknown backbone: {backbone_name}\") from ex\n\n\n# ---------------------------------------------------------------------------\n# 1) Tent adaptor \u2013 entropy minimisation on normalisation layer affine params\n# ---------------------------------------------------------------------------\nclass TentAdaptor(nn.Module):\n    \"\"\"Simplified implementation of Tent (https://arxiv.org/abs/2006.10726).\n\n    *Only* the affine scale/shift parameters of normalisation layers are\n    adapted, following the original paper.\n    \"\"\"\n\n    def __init__(self, base_model: nn.Module, lr: float = 1e-3, tau_max: float | None = None):\n        super().__init__()\n        self.base_model = base_model\n        self.tau_max = tau_max\n\n        for p in self.base_model.parameters():\n            p.requires_grad_(False)  # freeze all by default\n\n        self.params: List[nn.Parameter] = []\n        for m in self.base_model.modules():\n            if isinstance(m, (nn.BatchNorm2d, nn.LayerNorm, nn.GroupNorm)):\n                if m.weight is not None:\n                    m.weight.requires_grad = True\n                    self.params.append(m.weight)\n                if m.bias is not None:\n                    m.bias.requires_grad = True\n                    self.params.append(m.bias)\n\n        self.optimizer = torch.optim.SGD(self.params, lr=lr)\n        self.timer_ema: float | None = None\n\n    def forward(self, x):\n        start = time.time()\n        self.base_model.train()  # keep stats updating\n        logits = self.base_model(x)\n        loss = softmax_entropy(logits).mean()\n        self.optimizer.zero_grad(set_to_none=True)\n        loss.backward()\n        self.optimizer.step()\n\n        # micro-step budget control (simply halves LR if over budget)\n        if self.tau_max is not None:\n            elapsed = time.time() - start\n            self.timer_ema = 0.8 * (self.timer_ema or elapsed) + 0.2 * elapsed\n            if self.timer_ema \u003e self.tau_max:\n                for g in self.optimizer.param_groups:\n                    g[\"lr\"] *= 0.5\n        # return *post-update* predictions (detached to avoid autograd above caller)\n        with torch.no_grad():\n            return self.base_model(x)\n\n\n# ---------------------------------------------------------------------------\n# 2) AdaNPC adaptor \u2013 our proposed method\n# ---------------------------------------------------------------------------\nclass AdaNPCAdaptor(nn.Module):\n    \"\"\"Adaptive Natural-gradient \u0026 Probabilistically-Certified Test-time Adaptation.\"\"\"\n\n    def __init__(self, base_model: nn.Module, beta: float = 0.99, delta: float = 0.1,\n                 tau_max: float | None = None, micro_steps: int = 4):\n        super().__init__()\n        self.base_model = base_model\n        self.beta = beta\n        self.delta = delta\n        self.tau_max = tau_max\n        self.k = micro_steps\n\n        # collect **all** parameters \u2013 normaliser agnostic\n        self.params: List[nn.Parameter] = [p for p in self.base_model.parameters() if p.requires_grad]\n        self.var: torch.Tensor | None = None  # streaming Fisher diag\n        self.timer_ema: float | None = None\n\n    def _flatten_grads(self, grads: List[torch.Tensor]) -\u003e torch.Tensor:\n        return torch.cat([g.contiguous().view(-1) for g in grads])\n\n    def _apply_step(self, step_vec: torch.Tensor, eta: float):\n        idx = 0\n        for p in self.params:\n            numel = p.numel()\n            upd = step_vec[idx: idx + numel].view_as(p)\n            p.data.sub_(eta * upd)\n            idx += numel\n\n    @torch.enable_grad()\n    def _adanpc_update(self, x: torch.Tensor):\n        logits = self.base_model(x)\n        loss = softmax_entropy(logits).mean()\n        grads = torch.autograd.grad(loss, self.params, retain_graph=False, create_graph=False)\n        g_vec = self._flatten_grads(grads).detach()\n\n        # update streaming Fisher diag (variance proxy)\n        if self.var is None:\n            self.var = g_vec.pow(2) + 1e-8\n        else:\n            self.var = self.beta * self.var + (1 - self.beta) * g_vec.pow(2) + 1e-8\n\n        step = g_vec / torch.sqrt(self.var)\n        # safety check via Bernstein inequality\n        deltaL = (step * g_vec).sum()\n        varL = torch.sqrt((step.pow(2) * self.var).sum())\n        threshold = -varL * math.sqrt(2 * math.log(1 / self.delta))\n        safe = deltaL \u003c threshold  # want expected loss *decrease*\n\n        if safe:\n            eta = 1.0 / max(1, self.k)\n            self._apply_step(step, eta)\n        return logits  # return pre-update prediction for possible monitoring\n\n    def forward(self, x):\n        start = time.time()\n        self.base_model.train()  # keep norm layers in train mode for stats\n        with torch.enable_grad():\n            _ = self._adanpc_update(x)\n        # compute predictions *after* the potential parameter update\n        with torch.no_grad():\n            y_after = self.base_model(x)\n\n        # latency-aware micro-step back-off\n        if self.tau_max is not None:\n            elapsed = time.time() - start\n            self.timer_ema = 0.8 * (self.timer_ema or elapsed) + 0.2 * elapsed\n            if self.timer_ema \u003e self.tau_max and self.k \u003e 1:\n                self.k //= 2  # halve micro-steps\n        return y_after\n\n\n# ---------------------------------------------------------------------------\n# Public factory\n# ---------------------------------------------------------------------------\n\ndef get_model(cfg: Dict[str, Any], num_classes: int):\n    model_cfg = cfg.get(\"model\", {})\n    name = model_cfg.get(\"name\", \"resnet18\")\n    pretrained = model_cfg.get(\"pretrained\", False)\n\n    base_model = get_backbone(name, num_classes, pretrained=pretrained)\n\n    tta = model_cfg.get(\"tta\", \"none\").lower()\n    if tta == \"none\":\n        return base_model\n    if tta == \"tent\":\n        tent_cfg = model_cfg.get(\"tent\", {})\n        return TentAdaptor(base_model, lr=tent_cfg.get(\"lr\", 1e-3),\n                           tau_max=tent_cfg.get(\"tau_max\"))\n    if tta == \"adanpc\":\n        ad_cfg = model_cfg.get(\"adanpc\", {})\n        return AdaNPCAdaptor(base_model,\n                             beta=ad_cfg.get(\"beta\", 0.99),\n                             delta=ad_cfg.get(\"delta\", 0.1),\n                             tau_max=ad_cfg.get(\"tau_max\"),\n                             micro_steps=ad_cfg.get(\"micro_steps\", 4))\n\n    raise ValueError(f\"Unsupported TTA method: {tta}\")\n", "preprocess_py": "\"\"\"src/preprocess.py\nCommon data-loading \u0026 preprocessing utilities with fully implemented dataset\nloaders for all experiments used in this repository.\n\nPublic functions\n----------------\nget_dataloaders(cfg [, seed]) -\u003e (train_loader, val_loader, num_classes)\n\nThe rest of the code base is dataset / model agnostic and therefore does not\nneed to be touched when adding new datasets.  Simply extend *this* file.\n\"\"\"\nfrom __future__ import annotations\n\nimport random\nfrom typing import Tuple, Dict, Any, Optional, Sequence\n\nimport torch\nfrom torch.utils.data import DataLoader, random_split\nfrom torchvision import transforms, datasets\nfrom torchvision.transforms.functional import InterpolationMode\n\n# Hugging Face datasets \u2013 optional dependency, only imported when needed\ntry:\n    from datasets import load_dataset\nexcept ImportError:  # pragma: no cover \u2013 handled at runtime\n    load_dataset = None  # type: ignore\n\n# ---------------------------------------------------------------------------\n# Reproducibility helpers\n# ---------------------------------------------------------------------------\n\ndef seed_everything(seed: int):\n    random.seed(seed)\n    torch.manual_seed(seed)\n    torch.cuda.manual_seed_all(seed)\n    torch.backends.cudnn.deterministic = True\n\n\n# ---------------------------------------------------------------------------\n# Transforms\n# ---------------------------------------------------------------------------\n\ndef build_transforms(cfg: Dict[str, Any]):\n    im_size = cfg.get(\"dataset\", {}).get(\"img_size\", 224)\n    mean = cfg.get(\"dataset\", {}).get(\"mean\", [0.485, 0.456, 0.406])\n    std = cfg.get(\"dataset\", {}).get(\"std\", [0.229, 0.224, 0.225])\n\n    train_tfms = transforms.Compose([\n        transforms.Resize((im_size, im_size), interpolation=InterpolationMode.BICUBIC),\n        transforms.ToTensor(),\n        transforms.Normalize(mean, std),\n    ])\n    test_tfms = transforms.Compose([\n        transforms.Resize((im_size, im_size), interpolation=InterpolationMode.BICUBIC),\n        transforms.ToTensor(),\n        transforms.Normalize(mean, std),\n    ])\n    return train_tfms, test_tfms\n\n\n# ---------------------------------------------------------------------------\n# Hugging-Face wrappers\n# ---------------------------------------------------------------------------\n\nclass HFDatasetWrapper(torch.utils.data.Dataset):\n    \"\"\"Wrap a *datasets* Dataset so it behaves like a normal Torch vision dataset.\n\n    The underlying dataset must have fields \"image\" and \"label\" (or \"labels\").\n    \"\"\"\n\n    def __init__(self, hf_ds, transform):\n        self.ds = hf_ds\n        self.transform = transform\n        # figure out the label key once\n        if \"label\" in hf_ds.column_names:\n            self.label_key = \"label\"\n        elif \"labels\" in hf_ds.column_names:\n            self.label_key = \"labels\"\n        else:\n            raise ValueError(\"HF dataset does not contain a label column\")\n\n    def __len__(self):\n        return len(self.ds)\n\n    def __getitem__(self, idx):\n        sample = self.ds[idx]\n        img = sample[\"image\"]\n        lbl = int(sample[self.label_key])\n        if self.transform is not None:\n            img = self.transform(img)\n        return img, lbl\n\n\n# ---------------------------------------------------------------------------\n# Dataset loader\n# ---------------------------------------------------------------------------\n\ndef _load_imagenetc(cfg: Dict[str, Any]):\n    if load_dataset is None:\n        raise RuntimeError(\"datasets library not available \u2013 cannot load ImageNet-C\")\n\n    # We use the lighter *mini* ImageNet-C available on the HF hub.\n    ds_name = \"niuniandaji/mini-imagenet-c\"\n    ds_dict = load_dataset(ds_name)\n\n    # The mini-imagenet-c provides train/validation/test splits; if not, we\n    # create a random split.\n    if set(ds_dict.keys()) \u003e= {\"train\", \"validation\"}:\n        hf_train = ds_dict[\"train\"]\n        hf_val = ds_dict[\"validation\"]\n    elif set(ds_dict.keys()) \u003e= {\"train\", \"test\"}:\n        hf_train = ds_dict[\"train\"]\n        hf_val = ds_dict[\"test\"]\n    else:\n        full = ds_dict[list(ds_dict.keys())[0]]  # take the only available split\n        frac = 0.1\n        n_val = int(len(full) * frac)\n        indices = list(range(len(full)))\n        random.shuffle(indices)\n        val_idx = indices[:n_val]\n        train_idx = indices[n_val:]\n        hf_train = full.select(train_idx)\n        hf_val = full.select(val_idx)\n\n    train_tfms, test_tfms = build_transforms(cfg)\n    train_set = HFDatasetWrapper(hf_train, transform=train_tfms)\n    val_set = HFDatasetWrapper(hf_val, transform=test_tfms)\n\n    # determine # classes from features if possible\n    features = hf_train.features\n    if \"label\" in features and hasattr(features[\"label\"], \"num_classes\"):\n        num_classes = features[\"label\"].num_classes\n    else:\n        num_classes = len({int(x[\"label\"]) for x in hf_train})\n    return train_set, val_set, num_classes\n\n\ndef _load_cifar10c(cfg: Dict[str, Any]):\n    if load_dataset is None:\n        raise RuntimeError(\"datasets library not available \u2013 cannot load CIFAR10-C\")\n\n    ds_name = \"randall-lab/cifar10-c\"\n    hf_ds = load_dataset(ds_name, split=\"test\", trust_remote_code=True)\n    # Random split 90/10 \u2013 we do *not* use the corrupt images for training; this\n    # is just to satisfy the train/val interface.\n    n_total = len(hf_ds)\n    n_val = n_total // 10\n    indices = list(range(n_total))\n    random.shuffle(indices)\n    val_idx = indices[:n_val]\n    train_idx = indices[n_val:]\n    hf_train = hf_ds.select(train_idx)\n    hf_val = hf_ds.select(val_idx)\n\n    train_tfms, test_tfms = build_transforms(cfg)\n    train_set = HFDatasetWrapper(hf_train, transform=train_tfms)\n    val_set = HFDatasetWrapper(hf_val, transform=test_tfms)\n    num_classes = 10  # CIFAR-10\n    return train_set, val_set, num_classes\n\n\ndef get_dataset(cfg: Dict[str, Any]):\n    ds_name = cfg.get(\"dataset\", {}).get(\"name\", \"FAKEDATA\").upper()\n\n    # ---------------------------------------------------------------------\n    # 1) Light-weight FakeData for infrastructure smoke tests\n    # ---------------------------------------------------------------------\n    if ds_name == \"FAKEDATA\":\n        num_classes = cfg.get(\"dataset\", {}).get(\"num_classes\", 10)\n        img_size = cfg.get(\"dataset\", {}).get(\"img_size\", 224)\n        train_tfms, test_tfms = build_transforms(cfg)\n        train_set = datasets.FakeData(size=1_000, image_size=(3, img_size, img_size),\n                                      num_classes=num_classes, transform=train_tfms)\n        val_set = datasets.FakeData(size=200, image_size=(3, img_size, img_size),\n                                    num_classes=num_classes, transform=test_tfms)\n        return train_set, val_set, num_classes\n\n    # ---------------------------------------------------------------------\n    # 2) ImageNet-C (mini) \u2013 used in the main experiments\n    # ---------------------------------------------------------------------\n    if ds_name == \"IMAGENETC\":\n        return _load_imagenetc(cfg)\n\n    # ---------------------------------------------------------------------\n    # 3) CIFAR10-C \u2013 not used in current experiment but provided for future use\n    # ---------------------------------------------------------------------\n    if ds_name == \"CIFAR10C\":\n        return _load_cifar10c(cfg)\n\n    # ---------------------------------------------------------------------\n    # Unknown dataset\n    # ---------------------------------------------------------------------\n    raise ValueError(f\"Unknown dataset: {ds_name}\")\n\n\n# ---------------------------------------------------------------------------\n# Dataloader builder (public API)\n# ---------------------------------------------------------------------------\n\ndef get_dataloaders(cfg: Dict[str, Any], seed: Optional[int] = None):\n    if seed is not None:\n        seed_everything(seed)\n\n    batch_size = cfg.get(\"training\", {}).get(\"batch_size\", 32)\n    num_workers = cfg.get(\"dataset\", {}).get(\"num_workers\", 4)\n\n    train_set, val_set, num_classes = get_dataset(cfg)\n\n    train_loader = DataLoader(train_set, batch_size=batch_size, shuffle=True,\n                              num_workers=num_workers, pin_memory=True, drop_last=True)\n    val_loader = DataLoader(val_set, batch_size=batch_size, shuffle=False,\n                            num_workers=num_workers, pin_memory=True)\n    return train_loader, val_loader, num_classes\n", "pyproject_toml": "[build-system]\nrequires = [\"setuptools\u003e=61.0\"]\nbuild-backend = \"setuptools.build_meta\"\n\n[project]\nname = \"adanpc_experiments\"\nversion = \"0.1.0\"\ndescription = \"Common Core Foundation for AdaNPC experiments\"\nreadme = \"README.md\"\nrequires-python = \"\u003e=3.10\"\n\n[project.dependencies]\ntorch = \"*\"\ntorchvision = \"*\"\npyyaml = \"*\"\nnumpy = \"*\"\npandas = \"*\"\nmatplotlib = \"*\"\nseaborn = \"*\"\nscikit-learn = \"*\"\ntqdm = \"*\"\ndatasets = \"*\"         # Hugging Face datasets for ImageNet-C / CIFAR10-C\ntimm = \"*\"             # Modern backbones (ViT, etc.)\n", "smoke_test_yaml": "description:\n  goal: End-to-end smoke test covering ALL run variations with minimal data.\n  dataset: Torchvision FakeData (1 k train / 0.2 k val)\n  models: ResNet-18 baseline, Tent and AdaNPC\n\nauthor: \"COMMON CORE\"\nexperiments:\n  # -----------------------------------------------------------------------\n  # 1) Source \u2013 no adaptation\n  # -----------------------------------------------------------------------\n  - run_id: source_fake\n    dataset:\n      name: FAKEDATA\n      img_size: 64\n      num_classes: 10\n      num_workers: 2\n    model:\n      name: resnet18\n      pretrained: false\n      tta: none\n    training:\n      epochs: 1\n      batch_size: 32\n    optimizer:\n      name: SGD\n      lr: 0.01\n      momentum: 0.9\n\n  # -----------------------------------------------------------------------\n  # 2) Tent \u2013 mimic \u03c4=0.25 budget (no real wall-clock check here)\n  # -----------------------------------------------------------------------\n  - run_id: tent_tau0.25_fake\n    dataset:\n      name: FAKEDATA\n      img_size: 64\n      num_classes: 10\n      num_workers: 2\n    model:\n      name: resnet18\n      pretrained: false\n      tta: tent\n      tent:\n        lr: 1e-3\n        tau_max: 0.25\n    training:\n      epochs: 1\n      batch_size: 32\n    optimizer:\n      name: SGD\n      lr: 0.01\n      momentum: 0.9\n\n  # -----------------------------------------------------------------------\n  # 3-5) AdaNPC with three latency budgets\n  # -----------------------------------------------------------------------\n  - run_id: adanpc_tau1.0_fake\n    dataset:\n      name: FAKEDATA\n      img_size: 64\n      num_classes: 10\n      num_workers: 2\n    model:\n      name: resnet18\n      pretrained: false\n      tta: adanpc\n      adanpc:\n        beta: 0.99\n        delta: 0.1\n        tau_max: null   # full budget (\u03c4=1.0)\n        micro_steps: 4\n    training:\n      epochs: 1\n      batch_size: 32\n    optimizer:\n      name: SGD\n      lr: 0.01\n      momentum: 0.9\n\n  - run_id: adanpc_tau0.5_fake\n    dataset:\n      name: FAKEDATA\n      img_size: 64\n      num_classes: 10\n      num_workers: 2\n    model:\n      name: resnet18\n      pretrained: false\n      tta: adanpc\n      adanpc:\n        beta: 0.99\n        delta: 0.1\n        tau_max: 0.5\n        micro_steps: 4\n    training:\n      epochs: 1\n      batch_size: 32\n    optimizer:\n      name: SGD\n      lr: 0.01\n      momentum: 0.9\n\n  - run_id: adanpc_tau0.25_fake\n    dataset:\n      name: FAKEDATA\n      img_size: 64\n      num_classes: 10\n      num_workers: 2\n    model:\n      name: resnet18\n      pretrained: false\n      tta: adanpc\n      adanpc:\n        beta: 0.99\n        delta: 0.1\n        tau_max: 0.25\n        micro_steps: 4\n    training:\n      epochs: 1\n      batch_size: 32\n    optimizer:\n      name: SGD\n      lr: 0.01\n      momentum: 0.9\n", "train_py": "\"\"\"src/train.py\nConducts a single experiment run as specified by an *individual* run-variation\nconfiguration (YAML) that already lives inside \u003cresults_dir\u003e/\u003crun_id\u003e/run_config.yaml\n\nThe script is **dataset / model agnostic** \u2013 all dataset-specific logic lives in\nsrc/preprocess.get_dataloaders and model definitions are pulled from\nsrc.model.get_model according to the config.  Therefore this file NEVER\ncontains placeholders.\n\"\"\"\nfrom __future__ import annotations\nimport argparse, json, os, sys, time, pathlib\nfrom typing import Dict, Any\n\nimport torch, torch.nn as nn\nfrom torch.optim import SGD, AdamW\nfrom torch.utils.data import DataLoader\nfrom torch.cuda.amp import autocast, GradScaler\n\nfrom tqdm import tqdm\nimport yaml\n\nfrom .preprocess import get_dataloaders\nfrom .model import get_model\n\n\n# ------------------------------- helpers ------------------------------------ #\n\ndef accuracy(pred: torch.Tensor, target: torch.Tensor) -\u003e float:\n    \"\"\"Top-1 accuracy.\"\"\"\n    with torch.no_grad():\n        pred_classes = pred.argmax(1)\n        correct = (pred_classes == target).sum().item()\n    return correct / target.size(0)\n\n\ndef save_json(path: os.PathLike, obj: Dict[str, Any]):\n    path = pathlib.Path(path)\n    with path.open(\"w\") as f:\n        json.dump(obj, f, indent=2)\n\n\n# ------------------------------- main train --------------------------------- #\n\ndef run(cfg: Dict[str, Any], results_dir: str):\n    torch.backends.cudnn.benchmark = True  # speed-up for fixed input size\n\n    run_id: str = cfg[\"run_id\"]\n    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n\n    ###########################################################################\n    # 1.  Data\n    ###########################################################################\n    train_loader, val_loader, num_classes = get_dataloaders(cfg)\n\n    ###########################################################################\n    # 2.  Model \u0026 Optimiser\n    ###########################################################################\n    model = get_model(cfg, num_classes=num_classes).to(device)\n    criterion = nn.CrossEntropyLoss().to(device)\n\n    optim_cfg = cfg.get(\"optimizer\", {\"name\": \"SGD\", \"lr\": 0.01, \"momentum\": 0.9})\n    if optim_cfg[\"name\"].lower() == \"sgd\":\n        optimizer = SGD(model.parameters(), lr=optim_cfg[\"lr\"], momentum=optim_cfg.get(\"momentum\", 0))\n    elif optim_cfg[\"name\"].lower() == \"adamw\":\n        optimizer = AdamW(model.parameters(), lr=optim_cfg[\"lr\"], weight_decay=optim_cfg.get(\"weight_decay\", 1e-2))\n    else:\n        raise ValueError(f\"Unsupported optimizer: {optim_cfg[\u0027name\u0027]}\")\n\n    scaler = GradScaler(enabled=cfg.get(\"mixed_precision\", True) and device.type == \"cuda\")\n\n    ###########################################################################\n    # 3.  Training loop\n    ###########################################################################\n    epochs = cfg.get(\"training\", {}).get(\"epochs\", 10)\n    log_every = max(1, len(train_loader) // 10)\n\n    epoch_metrics = []\n    best_val_acc = -1\n    best_ckpt_path = os.path.join(results_dir, run_id, \"model_best.pth\")\n\n    for epoch in range(epochs):\n        model.train()\n        running_loss, running_acc, n_samples = 0.0, 0.0, 0\n        pbar = tqdm(train_loader, desc=f\"[{run_id}] Train Epoch {epoch+1}/{epochs}\", leave=False)\n        for i, (x, y) in enumerate(pbar):\n            x, y = x.to(device, non_blocking=True), y.to(device, non_blocking=True)\n            optimizer.zero_grad(set_to_none=True)\n            with autocast(enabled=scaler.is_enabled()):\n                logits = model(x)\n                loss = criterion(logits, y)\n            scaler.scale(loss).backward()\n            scaler.step(optimizer)\n            scaler.update()\n\n            batch_acc = accuracy(logits.detach(), y)\n            batch_size = y.size(0)\n            running_loss += loss.item() * batch_size\n            running_acc += batch_acc * batch_size\n            n_samples += batch_size\n\n            if (i + 1) % log_every == 0:\n                pbar.set_postfix({\"loss\": running_loss / n_samples, \"acc\": running_acc / n_samples})\n\n        train_loss = running_loss / n_samples\n        train_acc = running_acc / n_samples\n\n        # ---------------------- validation ---------------------------------- #\n        model.eval()\n        val_loss, val_acc, n_val = 0.0, 0.0, 0\n        with torch.no_grad():\n            for x, y in val_loader:\n                x, y = x.to(device, non_blocking=True), y.to(device, non_blocking=True)\n                logits = model(x)\n                loss = criterion(logits, y)\n                batch_acc = accuracy(logits, y)\n\n                val_loss += loss.item() * y.size(0)\n                val_acc += batch_acc * y.size(0)\n                n_val += y.size(0)\n        val_loss /= n_val\n        val_acc /= n_val\n\n        epoch_metrics.append({\n            \"epoch\": epoch + 1,\n            \"train_loss\": train_loss,\n            \"train_acc\": train_acc,\n            \"val_loss\": val_loss,\n            \"val_acc\": val_acc,\n        })\n\n        # ---------------------- checkpointing ------------------------------- #\n        if val_acc \u003e best_val_acc:\n            best_val_acc = val_acc\n            torch.save({\n                \"model_state\": model.state_dict(),\n                \"cfg\": cfg,\n                \"epoch\": epoch + 1,\n                \"val_acc\": val_acc,\n            }, best_ckpt_path)\n\n    ###########################################################################\n    # 4.  Save artefacts \u0026 log final metrics\n    ###########################################################################\n    final_metrics = {\n        \"run_id\": run_id,\n        \"final_train_loss\": epoch_metrics[-1][\"train_loss\"],\n        \"final_train_acc\": epoch_metrics[-1][\"train_acc\"],\n        \"final_val_loss\": epoch_metrics[-1][\"val_loss\"],\n        \"final_val_acc\": epoch_metrics[-1][\"val_acc\"],\n        \"best_val_acc\": best_val_acc,\n        \"epochs\": epochs,\n        \"epoch_metrics\": epoch_metrics,\n    }\n\n    # last checkpoint (overwrite each run)\n    last_ckpt_path = os.path.join(results_dir, run_id, \"model_last.pth\")\n    torch.save({\n        \"model_state\": model.state_dict(),\n        \"cfg\": cfg,\n        \"epoch\": epochs,\n        \"val_acc\": epoch_metrics[-1][\"val_acc\"],\n    }, last_ckpt_path)\n\n    # save json   \n    save_json(os.path.join(results_dir, run_id, \"results.json\"), final_metrics)\n\n    # provide machine-readable output on stdout\n    print(json.dumps(final_metrics))\n\n\n# --------------------------------------------------------------------------- #\n\ndef parse_args():\n    p = argparse.ArgumentParser()\n    p.add_argument(\"--config\", required=True, help=\"Path to run-specific YAML config file\")\n    p.add_argument(\"--results-dir\", required=True, help=\"Root results directory for this whole experiment set\")\n    return p.parse_args()\n\n\nif __name__ == \"__main__\":\n    args = parse_args()\n    cfg = yaml.safe_load(open(args.config))\n    run(cfg, args.results_dir)\n"}


Compare the Base Code with the Derived Experiment Code for this specific experiment thoroughly. Ensure the derived code maintains all the quality, completeness, and functionality of the base code while properly replacing placeholders with specific implementations.
Output:
{
    "is_experiment_code_ready": true,
    "experiment_code_issue": ""
}
