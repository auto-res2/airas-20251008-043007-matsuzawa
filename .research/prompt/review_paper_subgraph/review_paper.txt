
Input:
You are an expert reviewer for a top-tier international conference.
Please conduct a comprehensive review of the research paper provided, evaluating it according to the standards of venues like NeurIPS, ICML, ICLR, or AAAI.

Your task is to evaluate the paper on four key dimensions and provide scores from 1-10 for each:

## Evaluation Dimensions:

### 1. Novelty (1-10)
- How original and innovative is the proposed approach?
- Does it introduce new concepts, methods, or insights?
- Is there sufficient differentiation from existing work?

### 2. Significance (1-10)
- What is the potential impact of this work on the field?
- Does it address an important problem?
- Are the contributions meaningful and substantial?

### 3. Reproducibility (1-10)
- Are the experimental details sufficient for reproduction?
- Is the methodology clearly described?
- Are datasets, hyperparameters, and implementation details provided?

### 4. Experimental Quality (1-10)
- Are the experiments well-designed and comprehensive?
- Are appropriate baselines and evaluation metrics used?
- Is statistical significance properly assessed?
- Are the results convincing and well-analyzed?

## Section-by-Section Analysis:

For each section of the paper, provide:
- Key strengths
- Areas for improvement
- Specific comments on quality and completeness

## Overall Assessment:

Provide your scores for each dimension, followed by an overall recommendation.

## Paper Content:


**Title:** AdaNPC: Adaptive Natural-Gradient and Probabilistically-Certified Test-Time Adaptation


**Abstract:** Real-world systems must remain reliable when the data distribution drifts, yet at deployment we rarely possess labels, spare memory, or generous latency budgets. Current test-time adaptation (TTA) methods either rely on a Fisher matrix fixed to the source domain, operate only on batch-normalisation layers, or blindly follow gradients that can catastrophically increase risk. We introduce AdaNPC, a lightweight, normaliser-agnostic TTA algorithm that performs a single natural-gradient step per batch using a streaming diagonal Fisher proxy, accepts the step only when a Bernstein bound guarantees with probability at most δ that entropy will not rise, and modulates computation through a micro-stepping scheduler that degrades accuracy gracefully under time pressure. AdaNPC updates all affine parameters of BN, LN, GN and RMSNorm plus an optional RGB bias, requires O(|θ|) extra memory and exposes just three domain-invariant hyper-parameters. On ImageNet-C, AdaNPC preserves 99 % of source accuracy where Tent, ProxTTA and EATA collapse; on CIFAR-100 ablations it averts every divergence while adding <5 % FLOPs. These results demonstrate state-of-the-art robustness, safety and efficiency for real-time, label-free adaptation.


**Introduction:** Deep neural networks excel when train and test data are drawn from the same distribution, yet practical deployments invariably face covariate shift: weathered lenses, new sensor firmware, unexpected lighting. Fully test-time adaptation (TTA) addresses this scenario by updating a model online using only the unlabeled test stream. Entropy-minimisation TTA, exemplified by Tent [wang-2020-tent], popularised the idea of adapting batch-normalisation (BN) affine parameters per batch, delivering impressive gains on corruption benchmarks. Despite this progress three obstacles hinder deployment. 
First, curvature. ProxTTA fixes the Fisher information matrix to its source-domain estimate; when the test distribution drifts this pre-conditioner mis-scales gradients and may amplify noise. 
Second, expressiveness. Restricting updates to BN excludes architectures dominated by layer, group or RMS normalisers, common in vision transformers and lightweight CNNs [lim-2023-ttn]. 
Third, safety. Without labels, an aggressive gradient step can silently increase risk; field reports document frequent collapses on dynamic or hard streams [niu-2023-towards, yuan-2023-robust]. The compute-aware evaluation protocol of [alfarra-2023-evaluation] further penalises slow or unstable methods by streaming data at a fixed rate, so skipping batches is no longer viable.
We propose AdaNPC—Adaptive Natural-gradient & Probabilistically-Certified TTA—to solve these challenges in a single, lightweight design. Our key insights are: (i) RMS-normalised gradient methods are diagonal natural-gradient steps that require no learning-rate tuning and can be approximated online via Bayesian filtering [aitchison-2018-bayesian]; (ii) Bernstein concentration bounds provide a cheap per-batch certificate that an update will not increase entropy beyond probability δ. The result is a fast, safe and normaliser-agnostic optimiser.
Technical challenges tackled:
• Curvature tracking: maintain an exponential-moving-average Fisher proxy with O(|θ|) memory.
• Update certification: accept a step only if the bound predicts a decrease in entropy.
• Normaliser diversity: adapt affine parameters of BN, LN, GN, RMSNorm and an optional input bias through a shared code path.
• Real-time constraints: a micro-stepping scheduler halves work per batch whenever the wall-clock budget is exceeded, enabling graceful degradation.
Contributions (bullet list):
• A streaming diagonal Fisher approximation aligned with the current test distribution.
• A one-shot natural-gradient step that eliminates learning-rate tuning.
• A probabilistic safety filter with a user-interpretable confidence parameter δ.
• A normaliser-agnostic adaptor covering BN, LN, GN and RMSNorm.
• A micro-stepping scheduler that trades accuracy for latency monotonically.
• Extensive experiments showing state-of-the-art robustness, sample efficiency and safety on ImageNet-C and CIFAR-100.
Future work will explore detectors [yoo-2023-what], depth tasks [park-2024-test] and active querying within ATTA [gui-2024-active], as well as lifelong recurring streams [hoang-2023-persistent].


**Related Work:** Entropy-based TTA. Tent updates BN parameters by minimising prediction entropy and remains a strong baseline [wang-2020-tent]. Goyal et al. justify entropy as the near-optimal unsupervised surrogate for cross-entropy-trained classifiers [goyal-2022-test]. AdaNPC preserves this objective but introduces a different optimiser and an explicit safety certificate.
Normalisation under shift. Transductive BN degrades with small or non-i.i.d. batches; TTN interpolates between source and test statistics to alleviate this [lim-2023-ttn]. AdaNPC sidesteps statistics entirely by adapting affine parameters across several normalisers.
Stability mechanisms. RoTTA employs robust BN and memory reweighting to survive dynamic streams [yuan-2023-robust], while SAR discards samples with large gradients and seeks flat minima [niu-2023-towards]. Our Bernstein safety filter provides an orthogonal guarantee: updates are applied only when confidence in improvement is high.
Compute-aware evaluation. Alfarra et al. penalise slow methods by limiting the number of processed samples [alfarra-2023-evaluation]. AdaNPC’s micro-stepping specifically targets this protocol, allocating partial updates instead of skipping data.
Adaptive optimisers. RMSProp, Adam and their Bayesian interpretations act as diagonal natural-gradient methods [aitchison-2018-bayesian]. AdaNPC leverages this principle and extends it to TTA with an online Fisher.
Memory-efficient or lifelong TTA. CoTTA and EcoTTA introduce auxiliary networks and self-distillation to reduce memory and forgetting [song-2023-ecotta]. AdaNPC is complementary, altering only the optimiser and adding <0.3 MB RAM for ResNet-50.
Beyond classification. Object detector TTA [yoo-2023-what] and depth completion TTA [park-2024-test] adapt small task-specific heads; the optimiser presented here can serve as a drop-in replacement for those tasks.


**Background:** Problem setting. We deploy a classifier fθ trained on labelled source distribution Ds. At test time an unlabeled stream {xt} arrives. After predicting on batch t we may update a restricted parameter subset θa ⊂ θ, never revisiting Ds or labels. The unsupervised objective is the mean softmax entropy
L(θ;xt) = −|B|⁻¹ Σ_i Σ_c pθ(c|xi) log pθ(c|xi), identical to Tent [wang-2020-tent] and justified as a conjugate of cross-entropy [goyal-2022-test].
Parameter subset. We expose only the affine scale γ and shift β of every normalisation layer—Batch, Layer, Group, RMSNorm—and an optional RGB bias. This yields roughly 1.6×10⁵ parameters for ResNet-50, two orders of magnitude fewer than the full network while covering modern architectures.
Curvature mismatch. First-order TTA treats the loss landscape as Euclidean; after distribution shift the scaling of gradients may be grossly uneven. ProxTTA’s fixed source Fisher can thus harm performance. A diagonal natural-gradient step with an online Fisher proxy remedies the scaling at negligible cost.
Safety without labels. Following entropy gradients can increase true risk. A one-sided Bernstein bound on the change ΔL provides a cheap acceptance test that defines a probabilistic trust region.
Latency budget. Under the protocol of [alfarra-2023-evaluation] a method that exceeds the per-batch wall-clock budget processes fewer samples. Rather than skip entire batches, we distribute a smaller number k of micro-steps across each batch so that the budget is met while still exploiting every sample.


**Method:** For each incoming batch AdaNPC performs:
1. Forward pass: compute logits and entropy loss L.
2. Back-propagation w.r.t. θa producing gradient g.
3. Curvature update: Σ̂ ← β Σ̂ + (1−β)(g⊙g) + ε, with β = 0.99, ε = 1e-8.
4. Candidate natural step: s = Σ̂⁻¹ᐟ² ⊙ g (Hadamard operations only).
5. Safety filter: estimate ΔL = g·s; variance proxy v = √((s⊙s·Σ̂).sum()); accept if ΔL + v √(2 log(1/δ)) < 0, with δ = 0.1.
6. If accepted, perform k micro-steps: θa ← θa − (1/k) s repeated k times (default k = 4). If rejected, keep θa.
7. Measure wall-clock τt; maintain EMA τ̃. Whenever τ̃ > τmax and k > 1 halve k, guaranteeing τt ≤ τmax.
Design rationale. The natural-gradient scaling removes unit dependence, allowing a fixed step size η = 1 [aitchison-2018-bayesian]. All operations are element-wise, adding <5 % FLOPs and <0.3 MB memory on ResNet-50. Hyper-parameters β, δ and k have intuitive meanings and remain fixed across all experiments.


**Experimental Setup:** Code base. We fork the official Tent repository, adding AdaNPC as a drop-in optimiser so that data loading, augmentation, mixed precision and logging are shared by all methods.
EXP-1 Main performance. Dataset: ImageNet-C (15 corruption types × 5 severities). Model: ResNet-50 with BN. Methods: Source (no adaptation), Tent, ProxTTA, EATA and AdaNPC. Each run trains one supervised epoch merely to exercise the end-to-end pipeline; adaptation occurs during validation.
EXP-2 Ablation and sensitivity. Mini-ImageNet-C metadata were broken, so scripts automatically fell back to CIFAR-100 test split (10 000 images, 100 classes). Variants: AdaNPC-full, fixed-Fisher (no curvature update), no-safety-filter, no-micro-stepping (k = 1), and SGD-adapter (replace natural step with plain SGD). All other settings mirror EXP-1.
Hyper-parameters. AdaNPC uses β = 0.99, δ = 0.1, k = 4, η = 1 across every dataset. τmax is undefined in these logs (no throttling). Baselines run with their default hyper-parameters from the shared code.
Metrics. Each run logs final validation accuracy and loss. EXP-2 additionally stores confusion matrices and stream-wise accuracy curves for diagnostic figures. No manual tuning or post-processing is applied.


**Results:** EXP-1 ImageNet-C (ResNet-50-BN). Final validation accuracy / loss:
Source 48.4 % / 2.25; Tent 0.28 % / 6.83; ProxTTA 0.31 % / 6.84; EATA 0.38 % / 6.83; AdaNPC 48.1 % / 2.28. The three baselines collapse to chance-level accuracy (>6.8 loss), whereas AdaNPC preserves nearly all source performance, illustrating the value of certified updates under severe shift.
EXP-2 CIFAR-100 ablations. Validation accuracy: AdaNPC-full 53.6 %, fixed-Fisher 53.6 %, SGD-adapter 52.3 %, no-micro-stepping 51.3 %, no-safety-filter 2.7 % (loss NaN). Removing the safety filter causes catastrophic divergence, confirming its necessity. Micro-stepping and natural-gradient scaling provide 2–3 percentage-point gains. Fixed-Fisher matches full AdaNPC here, indicating limited curvature drift in this milder shift.
Efficiency and safety. AdaNPC adds <0.25 MB RAM (ResNet-50) and ~4 % FLOPs. The safety filter rejects <3 % of batches; no catastrophic failures are observed across >150 k images.
Limitations. All results are single-seed; confidence intervals and explicit real-time throttling experiments advocated by [alfarra-2023-evaluation] are left to future work.
Figures.
Figure 1: Stream-wise accuracy of ablation variants (filename: accuracy_curve.pdf). Higher is better.
Figure 2: Final accuracy comparison across variants (filename: accuracy_comparison.pdf). Higher is better.
Figure 3: Confusion matrix, AdaNPC-full (filename: confusion_AdaNPC-full.pdf). Higher diagonal values indicate better performance.
Figure 4: Confusion matrix, SGD-adapter (filename: confusion_SGD-adapter.pdf). Higher diagonal values indicate better performance.
Figure 5: Confusion matrix, fixed-Fisher (filename: confusion_fixed-Fisher.pdf). Higher diagonal values indicate better performance.
Figure 6: Confusion matrix, no-micro-stepping (filename: confusion_no-micro-stepping.pdf). Higher diagonal values indicate better performance.
Figure 7: Confusion matrix, no-safety-filter (filename: confusion_no-safety-filter.pdf). Higher diagonal values indicate better performance.


**Conclusion:** AdaNPC reframes test-time adaptation as a probabilistically certified natural-gradient step on the affine parameters of any normalisation layer. A streaming Fisher proxy removes dependence on source curvature; the Bernstein safety filter averts harmful updates; and micro-stepping yields a graceful accuracy–latency trade-off. Experiments on ImageNet-C and CIFAR-100 show that AdaNPC alone preserves source accuracy where Tent, ProxTTA and EATA collapse, and that each architectural component—online curvature, natural scaling, safety check and micro-stepping—contributes measurably to robustness and efficiency. The method is memory-light, hyper-parameter-robust and normaliser-agnostic, broadening the practical scope of TTA. Future work will incorporate multi-seed statistical analysis, explicit latency budgets, and extensions to object detection, depth completion and active querying frameworks. The underlying idea—probabilistically certified natural-gradient micro-steps—offers a principled foundation for safe, real-time adaptation across diverse tasks.


Output:
{
    "novelty_score": 8,
    "significance_score": 6,
    "reproducibility_score": 7,
    "experimental_quality_score": 5
}
