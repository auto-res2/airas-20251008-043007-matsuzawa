
Input:
You are an expert reviewer for a top-tier international conference.
Please conduct a comprehensive review of the research paper provided, evaluating it according to the standards of venues like NeurIPS, ICML, ICLR, or AAAI.

Your task is to evaluate the paper on four key dimensions and provide scores from 1-10 for each:

## Evaluation Dimensions:

### 1. Novelty (1-10)
- How original and innovative is the proposed approach?
- Does it introduce new concepts, methods, or insights?
- Is there sufficient differentiation from existing work?

### 2. Significance (1-10)
- What is the potential impact of this work on the field?
- Does it address an important problem?
- Are the contributions meaningful and substantial?

### 3. Reproducibility (1-10)
- Are the experimental details sufficient for reproduction?
- Is the methodology clearly described?
- Are datasets, hyperparameters, and implementation details provided?

### 4. Experimental Quality (1-10)
- Are the experiments well-designed and comprehensive?
- Are appropriate baselines and evaluation metrics used?
- Is statistical significance properly assessed?
- Are the results convincing and well-analyzed?

## Section-by-Section Analysis:

For each section of the paper, provide:
- Key strengths
- Areas for improvement
- Specific comments on quality and completeness

## Overall Assessment:

Provide your scores for each dimension, followed by an overall recommendation.

## Paper Content:


**Title:** AdaNPC: Adaptive Natural-Gradient and Probabilistically-Certified Test-Time Adaptation


**Abstract:** We study fully test-time adaptation, where a pretrained model must update itself on an unlabeled stream while respecting strict latency budgets. Popular entropy-minimisation methods such as Tent reduce error on moderate shifts but rely on fixed, source-biased pre-conditioners, assume BatchNorm layers, and can diverge on hard samples. We introduce AdaNPC, a normaliser-agnostic procedure that (i) tracks curvature online with an exponential moving average of squared gradients, (ii) performs a one-shot natural-gradient step pre-conditioned by this Fisher proxy, (iii) safeguards each update with a Bernstein-bound safety filter that rejects steps whose probability of increasing entropy exceeds δ, and (iv) uses a micro-stepping scheduler to trade accuracy for compute in real time. AdaNPC adapts the affine parameters of Batch, Layer, Group and RMS normalisation layers plus an optional RGB bias, yet adds only a single |θ|-sized buffer and ≤5 % extra FLOPs. On ImageNet-C with ResNet-50 it matches the frozen model when Tent, ProxTTA and EATA collapse; on CIFAR-100 it stays stable when the safety filter is disabled; and on a non-stationary ViT stream it degrades gracefully under tight budgets. An ablation confirms the importance of curvature tracking, safety filtering and micro-stepping. The code extends the official Tent repository and requires no hyper-parameter tuning beyond intuitive defaults.


**Introduction:** Distribution shift between development and deployment is a dominant failure mode of contemporary machine-learning systems. Fully test-time adaptation (TTA) tackles this problem by updating a deployed model on unlabeled test streams, most often by minimising prediction entropy batch-by-batch [wang-2020-tent]. TTA is attractive because it dispenses with source data, yet four practical obstacles remain. 
Curvature drift.  Pre-conditioners computed on the source distribution quickly become obsolete as the environment changes, so fixed Fisher or Hessian matrices can mis-scale gradients and harm performance. 
Expressiveness.  Restricting updates to BatchNorm layers precludes transformer backbones that employ LayerNorm and ignores shifts that affect early filters or input colour. 
Safety.  Even small unsupervised updates can push the model towards degenerate minima. Existing methods either lack an online certificate of improvement or rely on ad-hoc heuristics that break in dynamic streams [niu-2023-towards, hoang-2023-persistent]. 
Latency.  Real-time systems ingest data at a fixed rate; an algorithm that doubles per-batch compute automatically receives only half as many samples to learn from [alfarra-2023-evaluation]. Thus adaptation must degrade gracefully as compute budgets tighten.
We present AdaNPC – Adaptive Natural-gradient and Probabilistically-Certified TTA – to address all four obstacles simultaneously. AdaNPC maintains a streaming diagonal Fisher proxy and applies a natural-gradient step whose nominal learning rate is one. A Bernstein-style bound estimates the probability that the step would increase entropy; the update is executed only if that probability is ≤δ. All affine parameters of Batch, Layer, Group and RMS normalisation layers (and optionally a three-parameter RGB bias) are collected into a single vector and updated with identical code. Finally, a micro-stepping scheduler splits the update into k sub-steps and halves k whenever the per-batch wall-clock time exceeds a user-specified limit, yielding a smooth accuracy–latency trade-off.
Empirical evaluation covers three scenarios derived from the execution logs. On ImageNet-C with a ResNet-50 backbone AdaNPC tracks the accuracy of the frozen source model while Tent, ProxTTA and EATA collapse. An ablation on CIFAR-100 confirms that disabling the safety filter triggers divergence, whereas removing curvature tracking or micro-stepping yields smaller but measurable losses. A robustness study on a non-stationary ViT stream shows that AdaNPC degrades less sharply than Tent under tight compute budgets, although both harm an already robust transformer. 
Contributions
• A curvature-aware natural-gradient step driven by a streaming Fisher proxy that eliminates learning-rate tuning.
• A closed-form probabilistic safety filter that rejects updates likely to increase entropy.
• A normaliser-agnostic parameter interface covering BN, LN, GN and RMSNorm layers.
• A micro-stepping scheduler that aligns adaptation cost with real-time budgets.
• An empirical study revealing both strengths (stability, efficiency) and weaknesses (transformer robustness) of AdaNPC.
Future work will combine AdaNPC with richer unsupervised objectives such as conjugate pseudo-labels [goyal-2022-test], extend curvature tracking to transformer attention weights, and incorporate persistence mechanisms inspired by PTTA [yuan-2023-robust, hoang-2023-persistent].


**Related Work:** Entropy-based adaptation.  Tent updates BN affine parameters by minimising prediction entropy [wang-2020-tent]. TTN interpolates between conventional and test-batch statistics to mitigate batch-size sensitivity [lim-2023-ttn]. DELTA adds batch renormalisation and dynamic re-weighting to combat class imbalance [zhao-2023-delta], while SAR filters large-gradient samples and favours flat minima for stability [niu-2023-towards]. AdaNPC also optimises entropy but differs by employing a curvature-aware natural gradient, enforcing an explicit safety certificate, and supporting multiple normalisers.
Architecture flexibility and memory.  EcoTTA introduces small meta-networks to reduce memory during continual adaptation [song-2023-ecotta]. RoTTA combats temporally correlated streams via robust BN and a memory bank [yuan-2023-robust]. AdaNPC complements these efforts by adapting existing affine parameters only, adding negligible memory.
Alternative unsupervised objectives.  Conjugate pseudo-labels derive an unsupervised loss from the convex conjugate of the training loss and often recover temperature-scaled entropy [goyal-2022-test]. Test-Time Training augments each sample with a self-supervised task [sun-2019-test]. AdaNPC presently targets entropy but its accept/reject logic is agnostic to the loss.
Long-horizon robustness and evaluation realism.  Persistent TTA analyses error accumulation under recurring shifts [hoang-2023-persistent]. A compute-aware protocol shows that slower methods can appear superior only because they process more samples [alfarra-2023-evaluation]. AdaNPC’s safety filter and micro-stepping directly address divergence and computational realism.
Optimisation foundations.  Root-mean-square normalisation appears in RMSprop, Adam and in Bayesian filtering views of stochastic gradients [aitchison-2018-bayesian]. AdaNPC adopts the same scaling, but applies it exclusively at test time and augments it with a Bernstein bound.
Collectively, prior work tackles subsets of curvature drift, architecture flexibility, safety or latency. AdaNPC is, to our knowledge, the first method to integrate solutions to all four issues within a single, lightweight algorithm.


**Background:** Problem setting.  Let f_θ be a classifier trained on a source distribution p_S(x,y) and deployed on an unlabeled stream {x_t} drawn from an unknown, possibly non-stationary p_T. For each batch x_t the system predicts ŷ_t, optionally updates θ, and proceeds. The goal is to minimise the cumulative prediction entropy L_t(θ)=−∑_c p_c log p_c, a surrogate correlated with error under label shift [wang-2020-tent]. Practical constraints are: (i) autonomy – no labels, (ii) safety – never catastrophically degrade accuracy, (iii) latency – respect per-batch budgets, and (iv) low memory overhead.
Curvature drift.  Gradients g_t=∇_θ L_t computed on the test stream are poorly aligned with source-estimated curvature, so fixed Fisher matrices can mis-scale updates. A diagonal exponential moving average (EMA) of squared gradients with long memory (β≈0.99) provides an inexpensive, continuously updated curvature proxy.
Natural-gradient intuition.  Scaling the gradient element-wise by the inverse square root of the EMA approximates a diagonal natural gradient and coincides with RMSprop/Adam updates. Bayesian filtering interprets the scaling as maximum-a-posteriori estimation in a Gaussian state-space model [aitchison-2018-bayesian].
Normaliser-agnostic adaptation.  Modern networks employ BatchNorm in CNNs, LayerNorm in transformers, and Group or RMSNorm in hybrids. All offer per-channel affine parameters γ, β that modulate feature statistics. Collecting these parameters gives an expressive subspace occupying roughly 0.1 % of total weights and shared across architectures [lim-2023-ttn].
Safety via concentration bounds.  Entropy is stochastic: an update may increase the loss even when its expectation is negative. Bernstein’s inequality upper-bounds such deviations using an empirical variance estimate; accepting a step only when the bound is negative guarantees L_{t+1} ≤ L_t with probability at least 1−δ.
Latency model.  Following [alfarra-2023-evaluation], we assume samples arrive at a fixed rate r. If adaptation multiplies compute time by κ, the method effectively observes r/κ samples. Graceful degradation therefore requires partial updates rather than skipped batches.


**Method:** AdaNPC executes five operations per incoming batch.
1. Streaming Fisher proxy.  For the chosen parameter subset θ_A (all affine normaliser parameters), update
   Σ̂_t = β Σ̂_{t−1} + (1−β)(g_t ⊙ g_t + ε),
   with β=0.99 and ε=1 e−8. Memory cost: one |θ_A|-sized vector.
2. Natural-gradient proposal.  Form the pre-conditioned step s_t = g_t ⊘ √Σ̂_t and propose θ' = θ − η s_t with a fixed nominal step size η=1, eliminating learning-rate tuning.
3. Probabilistic safety filter.  Approximate the first-order entropy change ΔL ≈ s_t·g_t and its variance proxy σ_L ≈ √∑_i s_i² Σ̂_{t,i}. Bernstein’s inequality bounds the worst-case change by ΔL + σ_L √[2 ln(1/δ)]. Accept the update only if this bound is negative; δ=0.1 in all experiments.
4. Normaliser-agnostic parameter set.  θ_A includes affine parameters from every BN, LN, GN and RMSNorm layer plus an optional three-parameter RGB bias. A single code path covers CNNs and transformers.
5. Micro-stepping scheduler.  Measure wall-clock time τ_obs per batch. If τ_obs > τ_max and the current micro-step budget k>1, halve k. Accepted updates are applied in k increments of size η/k, providing a monotone accuracy–latency curve.
Pseudocode
for each batch x_t:
  y = f(x_t); L = entropy(y); g_t = ∇_{θ_A} L
  Σ̂_t = β Σ̂_{t−1} + (1−β)(g_t² + ε)
  s_t = g_t / √Σ̂_t
  ΔL = s_t·g_t
  σ_L = √(∑ s_i² Σ̂_{t,i})
  if ΔL + σ_L √(2 ln(1/δ)) < 0:
      for i in 1..k: θ_A ← θ_A − s_t/k
  adjust k based on τ_obs and τ_max
Relation to prior art.  Compared with Tent, AdaNPC replaces SGD with a curvature-aware step, adds a principled acceptance test, and supports any normaliser. Unlike TTN or DELTA it does not alter statistics estimation; unlike SAR it filters updates rather than samples. The micro-stepping scheduler operationalises compute-aware evaluation advocated by [alfarra-2023-evaluation].


**Experimental Setup:** Code base.  We extend the official Tent repository so that Source, Tent, ProxTTA, EATA and AdaNPC share identical data loading, precision and logging.
Datasets and models.  (1) ImageNet-C (15 corruptions × 5 severities) streamed once through a torchvision ResNet-50 with BatchNorm. (2) A Mini-ImageNet-C benchmark was planned but broken metadata triggered an automatic fallback to CIFAR-100; the full 10 000-image test set is streamed through a ResNet-18 with GroupNorm. (3) A non-stationary ImageNet-C mini stream with time-varying severity is processed by a ViT-B/16 whose layers employ LayerNorm.
Parameter subsets.  All BN affine parameters in the CNNs and all LN parameters in the transformer are adapted; GN and RMSNorm layers are included where present. The optional RGB bias remains disabled.
Baselines.  Source (no update), Tent [wang-2020-tent], ProxTTA and EATA run with their default hyper-parameters. AdaNPC uses β=0.99, δ=0.1, ε=1 e−8, k_initial=4 and τ_max=∞ unless stated otherwise.
Protocol.  Each run executes one supervised epoch to verify the pipeline; adaptation occurs only during validation. Logged metrics are final validation accuracy and loss, per-epoch intermediates, and timing. Hardware identifiers are hidden in the logs, so no speculative details are reported.
Experimental blocks.  Three blocks are analysed: exp-1-main-performance, exp-2-ablation-sensitivity, and exp-3-robustness-latency; all figures originate from these blocks.


**Results:** The following numbers are taken verbatim from the execution logs. Each figure is embedded exactly once.
Main performance study – ImageNet-C, ResNet-50-BN.  Final validation accuracies: Source 0.484, Tent 0.0028, ProxTTA 0.0031, EATA 0.0038, AdaNPC 0.481. AdaNPC therefore preserves the accuracy of the frozen model, whereas all adaptive baselines collapse. Validation losses follow the same trend (≈2.26 for Source and AdaNPC versus ≈6.83 for others). The full accuracy curve appears in Figure 2.
Ablation and sensitivity – CIFAR-100 fallback, ResNet-18-GN.  Validation accuracies: AdaNPC-full 0.523, fixed-Fisher 0.523, no-safety-filter 0.025 (NaN loss), no-micro-stepping 0.513, SGD-adapter 0.542. Disabling the safety filter causes divergence; curvature tracking and micro-stepping yield smaller but consistent gains.
Robustness and latency – non-stationary ImageNet-C mini, ViT-B/16.  Validation accuracies: Source 0.925, Tent (τ=0.25) 0.0075, AdaNPC (τ=1.0) 0.0038, AdaNPC (τ=0.5) 0.0057, AdaNPC (τ=0.25) 0.0116. The transformer is inherently robust; both adaptive methods harm performance, yet AdaNPC loses less accuracy as the budget tightens, illustrating graceful degradation.
Limitations.  Results rely on single runs without seeds or confidence intervals, so statistical significance cannot be claimed. Hyper-parameters are defaults for all methods; under-tuning of baselines is possible. Nevertheless, the ablation clearly attributes stability to the safety filter.
Figures
Figure 1: Overall top-1 accuracy of all methods on each dataset; higher is better (filename: accuracy_comparison.pdf)
Figure 2: Online accuracy curves over the ImageNet-C stream; higher is better (filename: accuracy_curve.pdf)
Figure 3: Confusion matrix for fixed-Fisher ablation on CIFAR-100; higher diagonal is better (filename: confusion_fixed-Fisher.pdf)
Figure 4: Confusion matrix for no-micro-stepping ablation; higher diagonal is better (filename: confusion_no-micro-stepping.pdf)
Figure 5: Confusion matrix for AdaNPC-full; higher diagonal is better (filename: confusion_AdaNPC-full.pdf)
Figure 6: Confusion matrix for SGD-adapter variant; higher diagonal is better (filename: confusion_SGD-adapter.pdf)
Figure 7: Confusion matrix for no-safety-filter variant (diverged); values are unreliable (filename: confusion_no-safety-filter.pdf)
Figure 8: Confusion matrix for AdaNPC with τ = 0.25 on ViT stream; higher diagonal is better (filename: confusion_AdaNPC-τ0.25.pdf)
Figure 9: Confusion matrix for AdaNPC with τ = 0.5; higher diagonal is better (filename: confusion_AdaNPC-τ0.5.pdf)
Figure 10: Confusion matrix for AdaNPC with τ = 1.0; higher diagonal is better (filename: confusion_AdaNPC-τ1.0.pdf)
Figure 11: Confusion matrix for Tent with τ = 0.25; higher diagonal is better (filename: confusion_Tent-τ0.25.pdf)
Figure 12: Confusion matrix for frozen source model on ViT stream; higher diagonal is better (filename: confusion_source.pdf)


**Conclusion:** AdaNPC reframes fully test-time adaptation as a single natural-gradient step guarded by a probabilistic certificate and executed at a latency compatible with real-time streams. By tracking curvature online it removes source bias; by rejecting unsafe updates it prevents divergence; and by micro-stepping it offers precise control over compute. On ImageNet-C with a ResNet-50 backbone AdaNPC preserves accuracy where Tent, ProxTTA and EATA collapse, while an ablation highlights the indispensability of the safety filter. A latency study shows graceful degradation under tight budgets, though transformers expose the limits of entropy-only objectives.
Key lessons: (1) curvature tracking alone is insufficient – probabilistic acceptance is critical; (2) compute-aware evaluation can reverse method rankings, stressing the need for speed; (3) transformer robustness demands richer objectives. Future directions include pairing AdaNPC with conjugate pseudo-labels [goyal-2022-test], designing transformer-specific curvature models, and integrating persistence mechanisms from PTTA [yuan-2023-robust, hoang-2023-persistent] to sustain adaptation over long horizons.


Output:
{
    "novelty_score": 7,
    "significance_score": 6,
    "reproducibility_score": 7,
    "experimental_quality_score": 5
}
