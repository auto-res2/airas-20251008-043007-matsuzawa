
Input:
You are an expert in academic paper analysis. 
Your task is to extract reference paper titles from the full text of research papers.

Instructions:
- Analyze the provided full text of research papers
- Extract all reference paper titles mentioned in the text
- Focus on titles that appear in reference sections, citations, or are explicitly mentioned as related work
- Return only the exact titles as they appear in the text
- Exclude general topics or field names that are not specific paper titles
- If no clear reference titles are found, return an empty list

Full Text:
---------------------------------
arXiv:1908.04847v2  [math.ST]  5 Sep 2019 Convergence Rates of V ariational Inference in Sparse Deep Learning Convergence Rates of V ariational Inference in Sparse Deep Learning Badr-Eddine Chérief-Abdellatif badr.eddine.cherief.abdellatif@ensae.fr CREST, ENSAE, Institut Polytechnique de Paris Editor: Abstract V ariational inference is becoming more and more popular for approximating intractable pos- terior distributions in Bayesian statistics and machine le arning. Meanwhile, a few recent works have provided theoretical justiﬁcation and new insig hts on deep neural networks for estimating smooth functions in usual settings such as no nparametric regression. In this paper, we show that variational inference for sparse de ep learning retains the same generalization properties than exact Bayesian inference. In particular, we highlight the connection between estimation and approximation theories via the classical bias-variance trade-oﬀ and show that it leads to near-minimax rates of conv ergence for Hölder smooth functions. Additionally , we show that the model selection f ramework over the neural net- work architecture via ELBO maximization does not overﬁt and adaptively achieves the optimal rate of convergence. Keywords:V ariational Inference, Neural Networks, Deep Learning, Ge neralization 1. Introduction Deep learning (DL) is a ﬁeld of machine learning that aims to model data using complex ar- chitectures combining several nonlinear transformations with hundreds of parameters called Deep Neural Networks (DNN) ( LeCun et al. , 2015; Goodfellow et al. , 2016). Although gen- eralization theory that explains why DL generalizes so well is still an open problem, it is widely acknowledged that it mainly takes advantage of large datasets containing millions of samples and a huge computing power coming from clusters of gr aphics processing units. V ery popular architectures for deep neural networks such as the m ultilayer perceptron, the convo- lutional neural network ( Lecun et al. , 1998), the recurrent neural network ( Rumelhart et al. , 1986) or the generative adversarial network ( Goodfellow et al. , 2014) have shown impressive results and have enabled to perform better than humans in var ious important areas in ar- tiﬁcial intelligence such as image recognition, game playi ng, machine translation, computer vision or natural language processing, to name a few promine nt examples. An outstanding example is AlphaGo ( Silver et al. , 2017), an artiﬁcial intelligence developed by Google that learned to play the game of Go using deep learning techniques and even defeated the world champion in 2016. The Bayesian approach, leading to popular methods such as Hid den Markov Models (Baum and Petrie , 1966) and Particle Filtering ( Doucet and Johansen , 2009), provides a natural way to model uncertainty . Some prior distribution i s put over the space of parameters and represents the prior belief as to which parameters are li kely to have generated the data 1Chérief-Abdellatif before any datapoint is observed. Then this prior distribut ion is updated using the Bayes rule when new data arrive in order to capture the more likely param eters given the observations. Unfortunately , exact Bayesian inference is computationall y challenging for complex models as the normalizing constant of the posterior distribution i s often intractable. In such cases, approximate inference methods such as variational inferen ce (VI) ( Jordan et al. , 1999) and expectation propagation ( Minka, 2001) are popular to overcome intractability in Bayesian modeling. The idea of VI is to minimize the Kullback-Leibler (KL) divergence with respect to the posterior given a set of tractable distributions, whi ch is also equivalent to maximizing a numerical criterion called the Evidence Lower Bound (ELBO). Recent advances of VI have shown great performance in practice and have been appli ed to many machine learning problems ( Hoﬀman et al. , 2013; Kingma and W elling , 2013). The Bayesian approach to learning in neural networks has a lon g history . Bayesian Neural Networks (BNN) have been ﬁrst proposed in the 90s and wi dely studied since then (MacKay, 1992a; Neal, 1995). They oﬀer a probabilistic interpretation and a measure of uncertainty for DL models. They are more robust to overﬁttin g than classical neural net- works and still achieve great performance even on small data sets. A prior distribution is put on the parameters of the network, namely the weight matri ces and the bias vectors, for instance a Gaussian or a uniform distribution, and Bayesian i nference is done through the likelihood speciﬁcation. Nevertheless, state-of-the-ar t neural networks may contain millions of parameters and the form of a neural network is not adapted t o exact integration, which makes the posterior distribution be intractable in practic e. Modern approximate inference mainly relies on VI, with sometimes a ﬂavor of sampling techn iques. A lot of recent pa- pers have investigated variational inference for DNNs ( Hinton and van Camp , 1993; Graves, 2011; Blundell et al. , 2015) to ﬁt an approximate posterior that maximizes the evidence lower bound. F or instance, Blundell et al. (2015) introduced Bayes by Backprop, one of the most famous techniques of VI applied to neural networks, which de rives a fully factorized Gaussian approximation to the posterior: using the reparameterizat ion trick ( Opper and Archambeau , 2008), the gradients of ELBO towards parameters of the Gaussian ap proximation can be computed by backpropagation, and then be used for updates. A nother point of interest in DNNs is the choice of the prior. Blundell et al. (2015) introduced a mixture of Gaus- sians prior on the weights, with one mixture tightly concent rated around zero, imitating the sparsity-inducing spike-and-slab prior. This oﬀers a Ba yesian alternative to the dropout regularization procedure ( Srivastava et al. , 2014) which injects sparsity in the network by switching oﬀ randomly some of the weights of the network. Thi s idea goes back to David MacKay who discussed in his thesis the possibility of choosi ng a spike-and-slab prior over the weights of the neural network ( MacKay, 1992b). More recently , Rockova and Polson (2018) introduced Spike-and-Slab Deep Learning (SS-DL), a fully Bayesian alternative to dropout for improving generalizability of deep ReLU networ ks. 1.1 Related work Although deep learning is extremely popular, the study of ge neralization properties of DNNs is still an open problem. Some works have been conducted in or der to investigate the theoret- ical properties of neural networks from diﬀerent points of v iew. The literature developed in the past decades can be shared in three parts. First, the appr oximation theory wonders how 2Convergence Rates of V ariational Inference in Sparse Deep Learning well a function can be approximated by neural networks. The ﬁ rst studies were mostly con- ducted to obtain approximation guarantees for shallow neur al nets with a single hidden layer (Cybenko, 1989; Barron, 1993). Since then, modern research has focused on the expressive power of depth and extended the previous results to deep neur al networks with a larger num- ber of layers ( Bengio and Delalleau , 2011; Y arotsky, 2016; Petersen and V oigtländer , 2017; Grohs et al. , 2019). Indeed, even though the universal approximation theorem (Cybenko, 1989) states that a shallow neural network containing a ﬁnite num ber of neurons can approx- imate any continuous function on compact sets under mild ass umptions on the activation function, recent advances showed that a shallow network req uires exponentially many neu- rons in terms of the dimension to represent a monomial functi on, whereas linearly many neurons are suﬃcient for a deep network ( Rolnick and T egmark , 2018). Second, as the ob- jective function in deep learning is known to be nonconvex, t he optimization community has discussed the landscape of the objective as well as the dy namics of some learning algo- rithms such as Stochastic Gradient Descent (SGD) ( Baldi and Hornik , 1989; Stanford et al. , 2000; Soudry and Carmon , 2016; Kawaguchi, 2016; Kawaguchi et al. , 2019; Nguyen et al. , 2019; Allen-Zhu et al. , 2019; Du et al. , 2019). Finally , the statistical learning community has investigated generalization properties of DNNs, see Barron (1994); Zhang et al. (2017); Schmidt-Hieber (2017); Suzuki (2018); Imaizumi and F ukumizu (2019); Suzuki (2019). In particular, Schmidt-Hieber (2017) and Suzuki (2019) showed that estimators in nonparamet- ric regression based on sparsely connected DNNs with ReLU ac tivation function and wisely chosen architecture achieve the minimax estimation rates ( up to logarithmic factors) under classical smoothness assumptions on the regression functi on. In the same time, Bartlett et al. (2017) and Neyshabur et al. (2018) respectively used Rademacher complexity and covering number, and P AC-Bayes theory to get spectrally-normalized m argin bounds for deep ReLU networks. More recently , Imaizumi and F ukumizu (2019) and Hayakawa and Suzuki (2019) showed the superiority of DNNs over linear operators in some situations when DNNs achieve the minimax rate of convergence while alternative methods f ail. F rom a Bayesian point of view, Rockova and Polson (2018) and Suzuki (2018) studied the concentration of the pos- terior distribution while Vladimirova et al. (2019) investigated the regularization eﬀect of prior distributions at the level of the units. Such as for generalization properties of DNNs, only little a ttention has been put in the literature towards the theoretical properties of VI until r ecently . Alquier et al. (2016) stud- ied generalization properties of variational approximati ons of Gibbs distributions in machine learning for bounded loss functions. Alquier and Ridgway (2017); Zhang and Gao (2017); Sheth and Khardon (2017); Bhattacharya et al. (2018); Chérief-Abdellatif and Alquier (2018); Cherief-Abdellatif (2019); Jaiswal et al. (2019b) extended the previous guarantees to more general statistical models and studied the concentration o f variational approximations of the posterior distribution, while W ang and Blei (2018) provided Bernstein-von-Mises’ theorems for variational approximations in parametric models. Huggins et al. (2018); Campbell and Li (2019); Jaiswal et al. (2019a) discussed theoretical properties of variational inferen ce algo- rithms based on various divergences (respectively W assers tein and Hellinger distances, and Rényi divergence). More recently , Chérief-Abdellatif et al. (2019) presented generalization bounds for online variational inference. All these works sh ow that under mild conditions, the variational approximation is consistent and achieves t he same rate of convergence than the Bayesian posterior distribution it approximates. Note t hat Alquier and Ridgway (2017); 3Chérief-Abdellatif Bhattacharya et al. (2018); Chérief-Abdellatif and Alquier (2018); Cherief-Abdellatif (2019) restricted their studies to tempered versions of the poster ior distribution where the likelihood is raised to an α-power ( α < 1) as it is known to require less stringent assumptions to obta in consistency and to be robust to misspeciﬁcation, see respec tively Bhattacharya et al. (2016) and Grünwald and V an Ommen (2017). Nevertheless, some questions remain unanswered, as the theoretical study of generalization of variational i nference for deep neural networks. 1.2 Contributions This paper aims at ﬁlling the gap between theory and practice when using variational ap- proximations for tempered Bayesian Deep Neural Networks. T o the best of our knowledge, this is the ﬁrst paper to present theoretical generalizatio n error bounds of variational infer- ence for Bayesian deep learning. Inspired by the related lite rature, our work is motivated by the following questions: • Do consistency of Bayesian DNNs still hold when an approximat ion is used instead of the exact posterior distribution, and can we obtain the sa me rates of convergence than those obtained for the regular posterior distribution and frequentist estimators ? • Is it possible to obtain a nonasymptotic generalization err or bound that holds for (almost) any generating distribution function and that giv es a general formula ? • What about the consistency of numerical algorithms used to c ompute these variational approximations ? • Can we obtain new insights on the structure of the networks ? The main contribution of this paper, a nonasymptotic genera lization error bound for vari- ational inference in sparse DL in the nonparametric regress ion framework, answers the ﬁrst two questions. This generalization result is similar to the oretical inequalities in the seminal works of Suzuki (2018); Imaizumi and F ukumizu (2019); Rockova and Polson (2018) on gen- eralization properties of deep neural networks, and is insp ired by the general literature on the consistency of variational approximations ( Alquier and Ridgway , 2017; Bhattacharya et al. , 2018). In particular, it states that under the same conditions, s parse variational approxima- tions of posterior distributions of deep neural networks ar e consistent at the same rate of convergence than the exact posterior. It also raises the question of ﬁnding a relevant general deﬁn ition of consistency that can be used to provide theoretical properties for the exact Bayes ian DNNs distribution and their variational approximations. Indeed, a classical criterio n used to assess frequentist guarantees for Bayesian estimators is the concentration of the posterio r (to the true distribution) which is deﬁned as the asymptotic concentration of the Bayesian est imator to the true distribution (Ghosal et al. , 2000). Nevertheless, posterior concentration to the true distr ibution only applies when the model is well speciﬁed, or at least when the m odel contains distributions in the neighborhood of the true distribution, which is probl ematic for misspeciﬁed models e.g. when the neural network does not suﬃciently approximate the generating distribution. And although the posterior distribution may concentrate to the best approximation of the true distribution in KL divergence in such misspeciﬁed models, t here exists pathological cases 4Convergence Rates of V ariational Inference in Sparse Deep Learning where the regular Bayesian posterior is not consistent at all , see Grünwald and V an Ommen (2017). This is the reason why we focus here on tempered posteriors which are robust to such misspeciﬁcation. Therefore, we introduce in Section 2 a notion of consistency of a Bayesian estimator which is closely related to the notion of c oncentration - even stronger - and which enables a more robust formulation of generalizati on error bounds for variational approximations. See Appendix A for more details on the connection between the notions of consistency and concentration. Then we focus on optimization aspects. W e no longer assume an ideal optimization, as done for instance in Schmidt-Hieber (2017); Imaizumi and F ukumizu (2019). W e address in this paper the question of the consistency of numerical algo rithms used to compute our ideal approximations. W e consider an optimization error given by any algorithm and independent to the statistical error, and we show how it aﬀects our genera lization result. Our upper bound highlights the connection between the consistency of the va riational approximation and the convergence of the ELBO. W e also provide insights on the structure of the network whic h leads to optimal rates of convergence, i.e. its depth, its width and its sparsity . I ndeed, in our ﬁrst generalization error bound, the structure of the network is ideally tuned fo r some choice of the generating function, and we show how to choose such a structure. Neverth eless, the characteristics of the regression function may be unknown, e.g. we may know that the regression function is Hölder continuous but we ignore its level of smoothness. W e propose here an automated method for choosing the architecture of the network. W e introduce a cla ssical model selection framework based on the ELBO criterion ( Cherief-Abdellatif, 2019), and we show that the variational approximation associated with the selected structure does not overﬁt and adaptively achieves the optimal rate of convergence even without any oracle info rmation. The rest of this paper is organized as follows. Section 2 introduces the notations and the framework that will be considered in the paper, and prese nts sparse spike-and-slab variational inference for deep neural networks. Section 3 provides theoretical generalization error bounds for variational approximations of DNNs and sho ws the optimality of the method for estimating Hölder smooth functions. Finally , insights on the choice of the architecture of the network are given in Section 4 via the ELBO maximization framework. All the proofs are deferred to the appendix. 2. Sparse deep variational inference Let us introduce the notations and the statistical framework we adopt in this paper. F or any vector x = ( x1, ..., xd) ∈ [−1, 1]d and any real-valued function f deﬁned on [−1, 1]d, d > 0, we denote: ∥x∥∞ = max 1≤i≤d |xi| , ∥f∥2 = (∫ f2 ) 1/2 and ∥f∥∞ = sup y∈[−1,1]d |f(y)|. F or any k ∈ { 0, 1, 2, ...}d , we deﬁne |k| = ∑ d i=1 ki and the mixed partial derivatives when all partial derivatives up to order |k| exist: Dkf(x) = ∂|k|f ∂k1 x1...∂kd xd (x). 5Chérief-Abdellatif W e also introduce the notion of β-Hölder continuity for β > 0. W e denote ⌊β⌋ the largest integer strictly smaller than β. Then f is said to be β-Hölder continuous ( T sybakov, 2008) if all partial derivatives up to order ⌊β⌋ exist and are bounded, and if: ∥f∥Cβ := max |k|≤⌊β⌋ ∥Dkf∥∞ + max |k|=⌊β⌋ sup x,y∈[−1,1]d,x̸=y |Dkf(x) − Dkf(y)| ∥x − y∥β−⌊β⌋ ∞ < +∞. ∥f∥Cβ is the norm of the Hölder space Cβ = {f/∥f∥Cβ < +∞}. 2.1 Nonparametric regression W e consider the nonparametric regression framework. W e hav e a collection of random vari- ables (Xi, Yi) ∈ [−1, 1]d ×R for i = 1 , ..., n which are independent and identically distributed (i.i.d.) with the generating process: { Xi ∼ U ([−1, 1]d), Yi = f0(Xi) + ζi where U([−1, 1]d) is the uniform distribution on the interval [−1, 1]d, ζ1, ..., ζn are i.i.d. Gaus- sian random variables with mean 0 and known variance σ2, and f0 : [ −1, 1]d → R is the true unknown function. F or instance, the true regression functi on f0 may belong to the set Cβ of Hölder functions with level of smoothness β. 2.2 Deep neural networks W e call deep neural network any map fθ : Rd → R deﬁned recursively as follows:      x(0) := x, x(ℓ) := ρ(Aℓx(ℓ−1) + bℓ) for ℓ = 1 , ..., L − 1, fθ(x) := ALx(L−1) + bL where L ≥ 3. ρ is an activation function acting componentwise. F or instan ce, we can choose the ReLU activation function ρ(u) = max( u, 0). Each Aℓ ∈ RDℓ×Dℓ−1 is a weight matrix such that its (i, j) coeﬃcient, called edge weight, connects the j-th neuron of the (ℓ − 1)-th layer to the i-th neuron of the ℓ-th layer, and each bℓ ∈ RDℓ is a shift vector such that its i-th coeﬃcient, called node vector, represents the weight as sociated with the i-th node of layer ℓ. W e set D0 = d the number of units in the input layer, DL = 1 the number of units in the output layer and Dℓ = D the number of units in the hidden layers. The architecture of the network is characterized by its number o f edges S, i.e. the total number of nonzero entries in matrices Aℓ and vectors bℓ, its number of layers L ≥ 3 (excluding the input layer), and its width D ≥ 1. W e have S ≤ T where T = ∑ L ℓ=1 Dℓ(Dℓ−1 + 1) is the total number of coeﬃcients in a fully connected network. By no w, we consider that S, L and D are ﬁxed, and d = O(1) as n → +∞. In particular, we assume that d ≤ D, which implies that T ≤ LD(D + 1). W e also suppose that the absolute values of all coeﬃcients a re upper bounded by some positive constant B ≥ 2. This boundedness assumption will be relaxed in the appendix, see Appendix G. Then, the parameter of a DNN is θ = {(A1, b1), ..., (AL, bL)}, and we denote Θ S,L,D the set of all possible parameters. W e will also alternative ly consider the stacked coeﬃcients parameter θ = ( θ1, ..., θT ). 6Convergence Rates of V ariational Inference in Sparse Deep Learning 2.3 Bayesian modeling W e adopt a Bayesian approach, and we place a spike-and-slab pr ior π (Castillo et al. , 2015) over the parameter space Θ S,L,D (equipped with some suited sigma-algebra) that is deﬁned hierarchically . The spike-and-slab prior is known to be a re levant alternative to dropout for Bayesian deep learning, see Rockova and Polson (2018). First, we sample a vector of binary indicators γ = ( γ1, ..., γT ) ∈ { 0, 1}T uniformly among the set SS T of T -dimensional binary vectors with exactly S nonzero entries, and then given γt for each t = 1 , ..., T , we put a spike-and-slab prior on θt that returns 0 if γt = 0 and a random sample from a uniform distribution on [−B, B ] otherwise: { γ ∼ U (SS T ), θt|γt ∼ γt U([−B, B ]) + (1 − γt)δ{0}, t = 1 , ..., T where δ{0} is a point mass at 0 and U([−B, B ]) is a uniform distribution on [−B, B ]. W e recall that the sparsity level S is ﬁxed here and that this assumption will be relaxed in Section 4. Remark 2.1. We consider uniform distributions for simplicity as in simi lar works ( Rockova and Polson , 2018; Suzuki, 2018), but Gaussian distributions can be used as well when workin g on an un- bounded parameter set Θ S,L,D, see Theorem 7 in Appendix G. Then we deﬁne the tempered posterior distribution πn,α on parameter θ ∈ Θ S,L,D using prior π for any α ∈ (0, 1): πn,α(dθ) ∝ exp ( − α 2σ2 n∑ i=1 (Yi − fθ(Xi))2 ) π(dθ), which is a slight variant of the deﬁnition of the regular Bayes ian posterior (for which α = 1 ). This distribution is known to be easier to sample from, to req uire less stringent assumptions to obtain concentration, and to be robust to misspeciﬁcatio n, see respectively Behrens et al. (2012), Bhattacharya et al. (2016) and Grünwald and V an Ommen (2017). 2.4 Sparse variational inference The variational Bayes approximation ˜πn,α of the tempered posterior is deﬁned as the projec- tion (with respect to the Kullback-Leibler divergence) of t he tempered posterior onto some set FS,L,D: ˜πn,α = arg min q∈FS,L,D KL(q∥πn,α). which is equivalent to: ˜πn,α = arg min q∈FS,L,D { α 2σ2 n∑ i=1 ∫ (Yi − fθ(Xi))2q(dθ) + KL(q∥π) } (1) where the function inside the argmin operator in ( 1) is the opposite of the evidence lower bound Ln(q). 7Chérief-Abdellatif W e choose a sparse spike-and-slab variational set FS,L,D - see for instance T onolini et al. (2019) - which can be seen as an extension of the popular mean-ﬁeld v ariational set with a dependence assumption specifying the number of active neu rons. The mean-ﬁeld ap- proximation is based on a decomposition of the space of param eters Θ S,L,D as a prod- uct θ = ( θ1, ..., θT ) and consists in compatible product distributions on each pa rameter θt, t = 1 , ..., T . Here, we ﬁt a distribution in the family that matches the pri or: we ﬁrst choose a distribution πγ on the set SS T that selects a T -dimensional binary vector γ with S nonzero entries, and then we place a spike-and-slab variational app roximation on each θt given γt: { γ ∼ πγ , θt|γt ∼ γt U([lt, ut]) + (1 − γt)δ{0} for each t = 1 , ..., T where −1 ≤ lt ≤ ut ≤ 1, with the distribution πγ and the intervals [lt, ut], t = 1 , ..., T as the hyperparameters of the variational set FS,L,D. In particular, if we choose a deterministic πγ = δ{γ′} with γ′ ∈ S S T , then we will obtain a parametric mean-ﬁeld approximation. See Section 6.6 of the PhD thesis of Gal (2016) for a more detailed discussion on the connection between Gaussian mean-ﬁeld and sparse spike-and-slab post erior approximations. The generalization error of the tempered posterior πn,α and of its variational approxima- tion ˜πn,α is the expected average of the squared L2-distance to the true generating function over the Bayesian estimator: E [ ∫ ∥fθ − f0∥2 2πn,α(dθ) ] and E [ ∫ ∥fθ − f0∥2 2˜πn,α(dθ) ] . W e say that a Bayesian estimator is consistent at rate rn → 0 if its generalization error is up- per bounded by rn. Notice that consistency of the Bayesian estimator implies c oncentration to f0. Again, see Appendix A for the connection between these two notions. 3. Generalization of variational inference for neural networks The ﬁrst result of this section is an extension of the result o f Rockova and Polson (2018) on the Bayesian distribution for Hölder regression functions. Indeed, we provide a concentration result on the posterior distribution for the expected L2-distance instead of the empirical L2- distance, which enables generalization instead of reconst ruction on the training datapoints. This result is then extended again to the variational approx imation for our deﬁnition of consistency: we show that we can still achieve near-optimal ity using an approximation of the posterior without any additional assumption. Finally , we explain how we can incorporate optimization error in our generalization results. 3.1 Concentration of the posterior Rockova and Polson (2018) gives the ﬁrst posterior concentration result for deep ReL U net- works when estimating Hölder smooth functions in nonparame tric regression with empirical L2-distance. The authors highlight the ﬂexibility of DNNs ove r other methods for estimating β-Hölder smooth functions as there is a large range of values o f the level of smoothness β for which one can obtain concentration, e.g. 0 < β < d for a DNN against 0 < β < 1 for a Bayesian tree. 8Convergence Rates of V ariational Inference in Sparse Deep Learning The following theorem provides the concentration of the tem pered posterior distribution πn,α for deep ReLU neural networks when using the expected L2-distance for some suitable architecture of the network: Theorem 1. Let us assume that α ∈ (0, 1), that f0 is β-Hölder smooth with 0 < β < d and that the activation function is ReLU. We consider the archit ecture of Rockova and Polson (2018) for some positive constant CD independent of n: L = 8 + ( ⌊log2 n⌋ + 5)(1 + ⌈log2 d⌉), D = CD⌊n d 2β+d / log n⌋, S ≤ 94d2(β + 1)2dD(L + ⌈log2 d⌉). Then the tempered posterior distribution πn,α concentrates at the minimax rate rn = n −2β 2β+d up to a (squared) logarithmic factor for the expected L2-distance in the sense that: πn,α ( θ ∈ Θ S,L,D / ∥fθ − f0∥2 2> M n · n −2β 2β+d · log2 n ) − − − − − → n→+∞ 0 in probability as n → +∞ for any Mn → +∞. In order to prove Theorem 1, we actually have to check that the so-called prior mass condition is satisﬁed: π ( θ ∈ Θ S,L,D / ∥fθ − f0∥2 2≤ rn ) ≥ e−nrn . (2) This assumption, introduced in Ghosal et al. (2000) in order to obtain the concentration of the regular posterior distribution states that the prior must give enough mass to some neighborhood of the true parameter. As shown in Bhattacharya et al. (2016), this condition is even suﬃcient for tempered posteriors. Actually , this in equality was ﬁrst stated using the KL divergence instead of the expected L2-distance (see Condition 2.4 in Theorem 2.1 in Ghosal et al. (2000)), but the KL metric is equivalent to the squared L2-metric in regression problems with Gaussian noise. This prior mass condition giv es us the rate of convergence of the tempered posterior rn = n −2β 2β+d (up to a squared logarithmic factor) which is known to be optimal when estimating β-Hölder smooth functions ( T sybakov, 2008). Note that the log2 n term is common in the theoretical deep learning literature ( Imaizumi and F ukumizu , 2019; Suzuki, 2019; Schmidt-Hieber, 2017). Remark 3.1. The number of parameters of order n 2d 2β+d / log n ∈ [n2/3/ log(n), n2/ log(n)] is high compared to standard machine learning methods, whic h may lead to overﬁtting and hence prevent the procedure from achieving the minimax rate of convergence. The sparsity parameter S which gives a network with a small number of nonzero paramete rs along with the spike-and-slab prior help us tackle this issue and obtai n optimal rates of convergence (up to logarithmic factors). 9Chérief-Abdellatif 3.2 A generalization error bound The result we state in this subsection applies to a wide range of activation functions, includ- ing the popular ReLU activation and the identity map: Assumption 3.1. In the following, we assume that the activation function ρ is 1-Lispchitz continuous (with respect to the aboluste value) and is such t hat for any x ∈ R, |ρ(x)| ≤ | x|. W e do not assume any longer that the regression function is β-Hölder and we consider any structure (S, L, D). The following theorem gives a generalization error bound w hen using variational approximations instead of exact tempere d posteriors for DNNs. The proof is given in Appendix B and is based on P AC-Bayes theory ( Catoni, 2007; Guedj, 2019): Theorem 2. For any α ∈ (0, 1), E [ ∫ ∥fθ − f0∥2 2˜πn,α(dθ) ] ≤ 2 1 − α inf θ∗∈Θ S,L,D ∥fθ∗ − f0∥2 2+ 2 1 − α ( 1 + σ2 α ) rS,L,D n , (3) with rS,L,D n = LS n log(BD) + 2S n log(BLD) + S n log ( 7dL max (n S , 1 ) ) . The oracle inequality ( 3) ensures consistency of variational Bayes for estimating ne ural networks and provides the associated rate of convergence gi ven the structure (S, L, D). In- deed, if f0 is a neural network with structure (S, L, D), then the inﬁmum term on the right hand side of the inequality vanishes and we obtain a rate of co nvergence of order rS,L,D n ∼ max (S log(nL/S) n , LS log D n ) , which underlines a linear dependence on the number of layers and the sparsity . In fact, this rate of convergence is determined by the extended prior mass condition (Alquier and Ridgway , 2017; Chérief-Abdellatif and Alquier , 2018; Cherief-Abdellatif, 2019), which requires that in addition to the previous prior mass condition of Ghosal et al. (2000) and Bhattacharya et al. (2016), the variational set FS,L,D must contain probability distributions q that are concen- trated enough around the true generating function f0. One of the main ﬁndings of Theorem 2 is that our choice of the sparse spike-and-slab variational set FS,L,D is rich enough and that both conditions are actually similar and lead to the sam e rate of convergence. Hence, the rate of convergence is the one that satisﬁes the prior mas s condition ( 2). In particular, as the prior distribution is uniform over the parameter spac e, the negative logarithm of the prior mass of the neighborhood of the true regression functi on in Equation ( 2) is a local covering entropy , that is the logarithm of the number of rS,L,D n -balls needed to cover a neigh- borhood of the true regression function. Especially , it has been shown in previous studies that this local covering entropy fully characterizes the ra te of convergence of the empirical risk minimizer for DNNs ( Schmidt-Hieber, 2017; Suzuki, 2019). The rate rS,L,D n we obtain in this work is exactly of the same order than the upper bound on t he covering entropy number given in Lemma 5 in Schmidt-Hieber (2017) and in Lemma 3 in Suzuki (2019) which derive rates of convergence for the empirical risk minimizer using diﬀerent proof techniques. Note 10Convergence Rates of V ariational Inference in Sparse Deep Learning that replacing a uniform by a Gaussian in the prior and variat ional distributions leads to the same rate of convergence, see Appendix G. Nevertheless, deep neural networks are mainly used for thei r computational eﬃciency and their ability to approach complex functions, which makes th e task of estimating a neural network not so popular in machine learning. As said earlier, Imaizumi and F ukumizu (2019) used neural networks for estimating non-smooth functions. In such a context where the neural network model is misspeciﬁed, our generalization er ror bound is robust and still holds, and satisﬁes the best possible balance between bias a nd variance. Indeed, the upper bound on the generalization error on the ri ght-hand-side of ( 3) is mainly divided in two parts: the approximation error of f0 by a DNN fθ∗ in Θ S,L,D (i.e. the bias) and the estimation error rS,L,D n of a neural network fθ∗ in Θ S,L,D (i.e. the variance). F or instance, even if the generalization power is decreasing li nearly with respect to the number of layers compared to the logarithmic dependence on the width d ue to the variance term, this eﬀect is compensated by the beneﬁts of depth in the approxima tion theory of deep learning. Then, as there exists relationships between the bias/the va riance and the architecture of a neural network (respectively due to the approximation theo ry/the form of rS,L,D n ), Theorem 2 gives both a general formula for deriving rates of convergen ce for variational approximations and insight on the way to choose the architecture. W e choose t he architecture that minimizes the right-hand-side of ( 3), which can lead to minimax estimators for smooth functions . It also connects the approximation and estimation theories fo llowing previous studies. This was done for instance by Schmidt-Hieber (2017); Suzuki (2019); Imaizumi and F ukumizu (2019) who exploited the eﬀectiveness of ReLU activation function in terms of approximation ability (Y arotsky, 2016; Petersen and V oigtländer , 2017) for Hölder/Besov smooth and piecewise smooth generating functions. Now we illustrate Theorem 2 on Hölder smooth functions. The following result shows that the variational approximation achieves the same rate o f convergence than the posterior distribution it approximates, and even the minimax rate of c onvergence if the architecture is well chosen. W e present both consistency and concentrati on results. Corollary 3. Let us ﬁx α ∈ (0, 1). We consider the ReLU activation function. Assume that f0 is β-Hölder smooth with 0 < β < d . Then with L, D and S deﬁned as in Theorem 1, the variational approximation of the tempered posterior distr ibution ˜πn,α is consistent and hence concentrates at the minimax rate rn = n −2β 2β+d (up to a squared logarithmic factor): ˜πn,α ( θ ∈ Θ S,L,D / ∥fθ − f0∥2 2> M n · n −2β 2β+d · log2 n ) − − − − − → n→+∞ 0 in probability as n → +∞ for any Mn → +∞. 3.3 Optimization error In this subsection, we discuss the eﬀect of an optimization error that is independent on the previous statistical error. Indeed, in the variational Bayes community , people use ap- proximate algorithms in practice to solve the optimization problem ( 1) when the model is non-conjugate, i.e. the VB solution is not available in clos ed-form. This is the case here when considering a sparse spike-and-slab variational appr oximation in FS,L,D for DNNs with 11Chérief-Abdellatif hyperparameters φ = ( πγ, (φt)1≤t≤T ) and an algorithm that gives a sequence of hyperparam- eters (φk)k≥1 and associated variational approximations (˜πk n,α)k≥1. The following theorem gives a statistical guarantee for any approximation ˜πk n,α, k ≥ 1: Theorem 4. For any α ∈ (0, 1), E [ ∫ ∥fθ − f0∥2 2˜πk n,α(dθ) ] ≤ 2 1 − α inf θ∗ ∥fθ∗ − f0∥2 2+ 2 1 − α ( 1 + σ2 α ) rS,L,D n + 2σ2 α(1 − α) · E[L∗ n− Lk n] n , where L∗ n is the maximum of the evidence lower bound i.e. the ELBO evalu ated at ˜πn,α, while Lk nis the ELBO evaluated at ˜πk n,α. W e establish a clear connection between the convergence (in mean) of the ELBO Lk nto L∗ nand the consistency of our algorithm ˜πk n,α. Indeed, as soon as the ELBO Lk nconverges at rate ck,n, then our variational approximation ˜πk n,α is consistent at rate: max (ck,n n , S log(nL/S) n , SL log D n ) . In particular, as soon as k is such that ck,n ≤ max(S log n, S log D), then we obtain consis- tency of ˜πk n,α at rate rS,L,D n , i.e. ˜πk n,α and ˜πn,α have the same rate of convergence. However, deriving the convergence of the ELBO is a hard task. F or instance, when consid- ering a simple Gaussian mean-ﬁeld approximation without sp arsity , the variational objective Ln can be maximized using either stochastic ( Graves, 2011; Blundell et al. , 2015) or natu- ral gradient methods ( Khan et al. , 2018) on the parameters of the Gaussian approximation. The convergence of the ELBO is often met in practice ( Buchholz et al. , 2018; Mishkin et al. , 2018) and the recent work of Osawa et al. (2019) even showed that Bayesian deep learning enables practical deep learning and matches the performanc e of standard methods while pre- serving beneﬁts of Bayesian principles. Nevertheless, the o bjective is nonconvex and hence it is diﬃcult to prove the convergence to a global maximum in t heory . Some recent papers studied global convergence properties of gradient descent algorithms for frequentist classi- ﬁcation and regression losses ( Du et al. , 2019; Allen-Zhu et al. , 2019) that we may extend to gradient descent algorithms for the ELBO objective such as V ariational Online Gauss Newton or V adam ( Khan et al. , 2018; Osawa et al. , 2019). Another point is to develop and study more complex algorithm s than simple gradient descent that deal with spike-and-slab sparsity-inducing v ariational inference, as for instance Titsias and Lázaro-Gredilla (2011) did for multi-task and multiple kernel learning. Also, Louizos et al. (2018) connected sparse spike-and-slab variational inference w ith L0-norm regularization for neural networks and proposed a solution to the intractability of the L0- penalty term through the use of non-negative stochastic gat es, while Bellec et al. (2018) proposed an algorithm preserving sparsity during training . Nevertheless, these optimization concerns fall beyond the scope of this paper and are left for f urther research. 12Convergence Rates of V ariational Inference in Sparse Deep Learning 4. Architecture design via ELBO maximization W e saw in Section 3 that the choice of the architecture of the neural network is c rucial and can lead to faster convergence and better approximation. In this section, we formulate the architecture design of DNNs as a model selection problem and we investigate the ELBO maximization strategy which is very popular in the variatio nal Bayes community . This approach is diﬀerent from Rockova and Polson (2018) which is fully Bayesian and treats the parameters of the network architecture, namely the depth, t he width and the sparsity , as random variables. W e show that the ELBO criterion does not ove rﬁt and is adaptive: it provides a variational approximation with the optimal rate of convergence, and it does not require the knowledge of the unknown aspects of the regressi on function f0 (e.g. the level of smoothness for smooth functions) to select the optimal va riational approximation. W e denote MS,L,D the statistical model associated with the parameter set Θ S,L,D. W e consider a countable number of models, and we introduce prio r beliefs πS,L,D over the sparsity , the depth and the width of the network, that can be d eﬁned hierarchically and that are known beforehand. F or instance, the prior beliefs can be chosen such that πL = 2 −L, πD|L follows a uniform distribution over {d, ..., max(eL, d)} given L, and πS|L,D a uniform distribution over {1, ..., T } given L and D (we recall that T is the number of coeﬃcients in a fully connected network). This particular choice is sen sible as it allows to consider any number of hidden layers and (at most) an exponentially la rge width with respect to the depth of the network. W e still consider spike-and-slab p riors on θS,L,D ∈ Θ S,L,D given model MS,L,D. Each tempered posterior associated with model MS,L,D is denoted πS,L,D n,α . W e recall that the variational approximation ˜πS,L,D n,α associated with model MS,L,D is deﬁned as the distribution into the variational set FS,L,D that maximizes the Evidence Lower Bound: ˜πS,L,D n,α = arg max qS,L,D∈FS,L,D Ln(qS,L,D). W e will simply denote in the following L∗ n(S, L, D) the closest approximation to the log- evidence i.e., the value of the ELBO evaluated at its maximum: L∗ n(S, L, D) = Ln(˜πS,L,D n,α ). The model selection criterion we use here to select the archi tecture of the network is a slight penalized variant of the classical ELBO criterion ( Blei et al. , 2017) with strong theoretical guarantees ( Cherief-Abdellatif, 2019) : ( ˆS, ˆL, ˆD) = arg max S,L,D { L∗ n(S, L, D) − log ( 1 πS,L,D )} . F or any choice of the prior beliefs πS,L,D, compute the ELBO for each model MS,L,D using an algorithm that will converge to L∗ n(S, L, D) and choose the architecture that maximizes the penalized ELBO criterion. It is possible to restrict to a ﬁ nite number of layers in practice (for instance, a factor of n or log n). The following theorem shows that this ELBO criterion leads to a variational approxima- tion with the optimal rate of convergence: 13Chérief-Abdellatif Theorem 5. For any α ∈ (0, 1), E [ ∫ ∥fθ − f0∥2 2˜π ˆS, ˆL, ˆD n,α (dθ) ] ≤ inf S,L,D { 2 1 − α inf θ∗∈Θ S,L,D ∥fθ∗ − f0∥2 2+ 2 1 − α ( 1 + σ2 α ) rS,L,D n + 2σ2 α(1 − α) log( 1 πS,L,D ) n } . This inequality shows that as soon as the complexity term log(1/πS,L,D)/n that reﬂects the prior beliefs is lower than the eﬀective rate of converge nce that balances the accuracy and the estimation error rS,L,D n , the selected variational approximation adaptively achie ves the best possible rate. F or instance, it leads to (near-)min imax rates for Hölder smooth functions and selects the optimal architecture even withou t the knowledge of β, which was required in the previous section. Note that for the previous choice of prior beliefs πL = 2 −L, πD|L = 1 /(max(eL, d) − d + 1), πS|L,D = 1 /T , we get: log( 1 πS,L,D ) n ≤ 2 log(D + 1) + log L + max(L, log d) + L log 2 n that is lower than rS,L,D n (up to a factor) and hence the ELBO criterion does not overﬁt. 5. Discussion In this paper, we provided theoretical justiﬁcations for neural networks from a Bayesian point of view using sparse variational inference. W e derive d new generalization error bounds and we showed that sparse variational approximations of DNN s achieve (near-)minimax optimality when the regression function is Hölder smooth. A ll our results directly imply concentration of the approximation of the posterior distri bution. W e also proposed an automated method for selecting an architecture of the netwo rk with optimal consistency guarantees via the ELBO maximization framework. W e think that one of the main challenges here is the design of n ew computational algo- rithms for spike-and-slab deep learning in the wake of the wo rk of Titsias and Lázaro-Gredilla (2011) for multi-task and multiple kernel learning, or those of Louizos et al. (2018) and Bellec et al. (2018). In the latter paper, the authors designed an algorithm for training deep networks while simultaneously learning their sparse c onnectivity allowing for fast and computationally eﬃcient learning, whereas most approache s have focused on compressing already trained neural networks. In the same time, a future point of interest is the study of the global convergence of these approximate algorithms in nonconvex settings i.e. st udy of the theoretical conver- gence of the ELBO. This work was conducted for frequentist gra dient descent algorithms (Allen-Zhu et al. , 2019; Du et al. , 2019). Such studies should be investigated for Bayesian gradient descents, as well as for algorithms that preserve t he sparsity of the network during training. Acknowledgments W e would like to warmly thank Pierre Alquier for his helpful s uggestions on early versions of this work. 14Convergence Rates of V ariational Inference in Sparse Deep Learning References Zeyuan Allen-Zhu, Y uanzhi Li, and Zhao Song. A convergence theory for deep learning via over-parameterization. In Kamalika Chaudhuri and Ruslan S alakhutdinov, editors, Pro- ceedings of the 36th International Conference on Machine Le arning, volume 97 of Proceed- ings of Machine Learning Research , pages 242–252, Long Beach, California, USA, 09–15 Jun 2019. PMLR. URL http://proceedings.mlr.press/v97/allen-zhu19a.html. P . Alquier and J. Ridgway . Concentration of tempered poster iors and of their variational approximations. arXiv preprint arXiv:1706.09293 , 2017. P . Alquier, J. Ridgway , and N. Chopin. On the properties of va riational approximations of Gibbs posteriors. JMLR, 17(239):1–41, 2016. Pierre Baldi and Kurt Hornik. Neural networks and principal c omponent analysis: Learning from examples without local minima”, ne. Neural Networks , 2:53–58, 12 1989. doi: 10. 1016/0893-6080(89)90014-2. Andrew Barron. Barron, a.e.: Universal approximation bounds for superpositions of a sigmoidal function. ieee trans. on information theory 39, 9 30-945. Information Theory, IEEE Transactions on , 39:930 – 945, 06 1993. doi: 10.1109/18.256500. Andrew R Barron. Approximation and estimation bounds for art iﬁcial neural networks. Machine Learning , 14(1):115–133, 1994. Peter L Bartlett, Dylan J F oster, and Matus J T elgarsky . Spect rally-normalized margin bounds for neural networks. In I. Guyon, U. V. Luxburg, S. Beng io, H. W allach, R. F ergus, S. Vishwanathan, and R. Garnett, editors, Advances in Neural Infor- mation Processing Systems 30 , pages 6240–6249. Curran Associates, Inc., 2017. URL http://papers.nips.cc/paper/7204-spectrally-normalized-margin-bounds-for-neural-networks.pdf . Leonard E. Baum and T ed Petrie. Statistical inference for pro babilistic functions of ﬁnite state markov chains. Ann. Math. Statist. , 37(6):1554–1563, 12 1966. doi: 10.1214/aoms/ 1177699147. URL https://doi.org/10.1214/aoms/1177699147. G. Behrens, N. F riel, and M. Hurn. T uning tempered transition s. Statistics and computing , 22(1):65–78, 2012. Guillaume Bellec, David Kappel, W olfgang Maass, and Robert L egenstein. Deep rewiring: T raining very sparse deep networks. In International Conference on Learning Represen- tations, 2018. URL https://openreview.net/forum?id=BJ_wN01C-. Y oshua Bengio and Olivier Delalleau. On the expressive power of deep architectures. In Pro- ceedings of the 22Nd International Conference on Algorithm ic Learning Theory , AL T’11, pages 18–36, Berlin, Heidelberg, 2011. Springer-V erlag. IS BN 978-3-642-24411-7. URL http://dl.acm.org/citation.cfm?id=2050345.2050349. A. Bhattacharya, D. Pati, and Y. Y ang. Bayesian fractional pos teriors. arXiv preprint arXiv:1611.01125, to appear in the Annals of Statistics , 2016. 15Chérief-Abdellatif A. Bhattacharya, D. Pati, and Y. Y ang. On statistical optimal ity of variational Bayes. Proceedings of Machine Learning Research , 84 - AIST A T, 2018. David M Blei, Alp Kucukelbir, and Jon D McAuliﬀe. V ariational inference: A review for statisticians. Journal of the American Statistical Association , 112(518):859–877, 2017. Charles Blundell, Julien Cornebise, Koray Kavukcuoglu, and Daan Wierstra. W eight un- certainty in neural networks. In Proceedings of the 32Nd International Conference on International Conference on Machine Learning - Volume 37 , ICML’15, pages 1613–1622. JMLR.org, 2015. URL http://dl.acm.org/citation.cfm?id=3045118.3045290. Stéphane Boucheron, Gábor Lugosi, and Pascal Massart. Conce ntration inequalities us- ing the entropy method. Ann. Probab. , 31(3):1583–1614, 07 2003. doi: 10.1214/aop/ 1055425791. URL https://doi.org/10.1214/aop/1055425791. Alexander Buchholz, Florian W enzel, and Stephan Mandt. Quas i-Monte Carlo variational inference. In Jennifer Dy and Andreas Krause, editors, Proceedings of the 35th Interna- tional Conference on Machine Learning , volume 80 of Proceedings of Machine Learning Research, pages 668–677, Stockholmsmässan, Stockholm Sweden, 10–1 5 Jul 2018. PMLR. URL http://proceedings.mlr.press/v80/buchholz18a.html. T revor Campbell and Xinglong Li. Universal boosting variat ional inference. volume arXiv:1903.05220, 2019. Ismaël Castillo, Johannes Schmidt-Hieber, and Aad van der V aart. Bayesian linear regression with sparse priors. Ann. Statist. , 43(5):1986–2018, 10 2015. doi: 10.1214/15-AOS1334. URL https://doi.org/10.1214/15-AOS1334. O. Catoni. PAC-Bayesian supervised classiﬁcation: the thermodynami cs of statistical learn- ing. Institute of Mathematical Statistics Lecture Notes—Mono graph Series, 56. Institute of Mathematical Statistics, Beachwood, OH, 2007. B. Chérief-Abdellatif and P . Alquier. Consistency of variat ional bayes inference for estima- tion and model selection in mixtures. Electronic Journal of Statistics , 12(2):2995–3035, 2018. ISSN 1935-7524. doi: 10.1214/18-EJS1475. B.-E. Chérief-Abdellatif, P . Alquier, and M.E. Khan. A gener alization bound for online variational inference. Preprint arXiv:1904.03920v1, 201 9. Badr-Eddine Cherief-Abdellatif. Consistency of elbo maxim ization for model selection. In F rancisco Ruiz, Cheng Zhang, Dawen Liang, and Thang Bui, ed itors, Proceedings of The 1st Symposium on Advances in Approximate Bayesian Inf erence, volume 96 of Proceedings of Machine Learning Research , pages 11–31. PMLR, 02 Dec 2019. URL http://proceedings.mlr.press/v96/cherief-abdellatif19a.html. G. Cybenko. Approximation by superpositions of a sigmoidal function. Mathematics of Control, Signals, and Systems (MCSS) , 2(4):303–314, December 1989. ISSN 0932-4194. doi: 10.1007/BF02551274. URL http://dx.doi.org/10.1007/BF02551274. 16Convergence Rates of V ariational Inference in Sparse Deep Learning A. Doucet and A. Johansen. A tutorial on particle ﬁltering an d smoothing: Fifteen years later. Handbook of Nonlinear Filtering , 12, 01 2009. Simon Du, Jason Lee, Haochuan Li, Liwei W ang, and Xiyu Zhai. G radient descent ﬁnds global minima of deep neural networks. In Kamalika Chaudhur i and Ruslan Salakhutdinov, editors, Proceedings of the 36th International Conference on Machin e Learning , volume 97 of Proceedings of Machine Learning Research , pages 1675–1685, Long Beach, California, USA, 09–15 Jun 2019. PMLR. URL http://proceedings.mlr.press/v97/du19c.html. Y arin Gal. Uncertainty in Deep Learning . PhD thesis, University of Cambridge, 2016. Subhashis Ghosal, Jayanta K. Ghosh, and Aad W. van der V aart. Convergence rates of pos- terior distributions. Ann. Statist. , 28(2):500–531, 04 2000. doi: 10.1214/aos/1016218228. URL https://doi.org/10.1214/aos/1016218228. Ian Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing Xu, Da vid W arde-F arley , Sherjil Ozair, Aaron Courville, and Y oshua Bengio. Generative adver sarial nets. In Z. Ghahra- mani, M. W elling, C. Cortes, N. D. Lawrence, and K. Q. W einber ger, editors, Advances in Neural Information Processing Systems 27 , pages 2672–2680. Curran Associates, Inc., 2014. URL http://papers.nips.cc/paper/5423-generative-adversarial-nets.pdf. Ian Goodfellow, Y oshua Bengio, and Aaron Courville. Deep Learning . MIT Press, 2016. http://www.deeplearningbook.org. Alex Graves. Practical variational inference for neural ne tworks. In J. Shawe-T aylor, R. S. Zemel, P . L. Bartlett, F. Pereira, and K. Q. W einberger, edito rs, Advances in Neural Information Processing Systems 24 , pages 2348–2356. Curran Associates, Inc., 2011. URL http://papers.nips.cc/paper/4329-practical-variational-inference-for-neural-networks.pdf . Philipp Grohs, Dmytro Perekrestenko, Dennis Elbrächter, a nd Helmut Bölcskei. Deep neural network approximation theory , 01 2019. P . D. Grünwald and T. V an Ommen. Inconsistency of Bayesian inf erence for misspeciﬁed linear models, and a proposal for repairing it. Bayesian Analysis , 12(4):1069–1103, 2017. B. Guedj. A primer on pac-bayesian learning. arXiv preprint arXiv:1901.05353 , 2019. Satoshi Hayakawa and T aiji Suzuki. On the minimax optimalit y and superiority of deep neural network learning over sparse parameter spaces. arXiv preprint arXiv:1905.09195 , 2019. Geoﬀrey E. Hinton and Drew van Camp. Keeping the neural netwo rks simple by min- imizing the description length of the weights. In Proceedings of the Sixth Annual Conference on Computational Learning Theory , COL T ’93, pages 5–13, New Y ork, NY, USA, 1993. ACM. ISBN 0-89791-611-5. doi: 10.1145/168304 .168306. URL http://doi.acm.org/10.1145/168304.168306. M. D. Hoﬀman, D. M. Blei, C. W ang, and J. Paisley . Stochastic va riational inference. The Journal of Machine Learning Research , 14(1):1303–1347, 2013. 17Chérief-Abdellatif Jonathan H. Huggins, T revor Campbell, Mikolaj Kasprzak, an d T amara Broderick. Practical bounds on the error of bayesian posterior approximations: A nonasymptotic approach. ArXiv, abs/1809.09505, 2018. Masaaki Imaizumi and Kenji F ukumizu. Deep neural networks l earn non-smooth functions ef- fectively . In Kamalika Chaudhuri and Masashi Sugiyama, edi tors, Proceedings of Machine Learning Research, volume 89 of Proceedings of Machine Learning Research , pages 869–878. PMLR, 16–18 Apr 2019. URL http://proceedings.mlr.press/v89/imaizumi19a.html. P . Jaiswal, V. A. Rao, and H. Honnappa. Asymptotic consisten cy of α-rényi-approximate posteriors. Preprint arXiv:1902.01902, 2019a. Prateek Jaiswal, Harsha Honnappa, and Vinayak A. Rao. Risk- sensitive variational bayes: F ormulations and bounds. volume arXiv:1906.01235, 2019b. M. I. Jordan, Z. Ghahramani, T. S. Jaakkola, and L. K. Saul. An introduction to variational methods for graphical models. Machine Learning , 37:183–233, 1999. Kenji Kawaguchi. Deep learning without poor local minima. I n D. D. Lee, M. Sugiyama, U. V. Luxburg, I. Guyon, and R. Garnett, editors, Advances in Neural Informa- tion Processing Systems 29 , pages 586–594. Curran Associates, Inc., 2016. URL http://papers.nips.cc/paper/6112-deep-learning-with out-poor-local-minima.pdf. Kenji Kawaguchi, Jiaoyang Huang, and Leslie Pack Kaelbling . Eﬀect of depth and width on local minima in deep learning. Neural Computation , 31(6):1462–1498, 2019. Mohammad Khan, Didrik Nielsen, V oot T angkaratt, W u Lin, Y ar in Gal, and Akash Sri- vastava. F ast and scalable Bayesian deep learning by weight- perturbation in Adam. In Jennifer Dy and Andreas Krause, editors, Proceedings of the 35th International Con- ference on Machine Learning , volume 80 of Proceedings of Machine Learning Research , pages 2611–2620, Stockholmsmässan, Stockholm Sweden, 10– 15 Jul 2018. PMLR. URL http://proceedings.mlr.press/v80/khan18a.html. Diederik P Kingma and Max W elling. Auto-encoding variation al bayes. arXiv preprint arXiv:1312.6114, 2013. Y ann Lecun, Léon Bottou, Y oshua Bengio, and Patrick Haﬀner. Gr adient-based learning applied to document recognition. In Proceedings of the IEEE , pages 2278–2324, 1998. Y ann LeCun, Y oshua Bengio, and Geoﬀrey Hinton. Deep learning . Nature, 521(7553): 436–444, 5 2015. ISSN 0028-0836. doi: 10.1038/nature14539 . Christos Louizos, Max W elling, and Diederik P . Kingma. Lear ning sparse neural networks through l0-regularization. In International Conference on Learning Representations , 2018. URL https://openreview.net/forum?id=H1Y8hhg0b. David J. C. MacKay . A practical bayesian framework for backp ropagation networks. Neural Computation , 4(3):448–472, 1992a. doi: 10.1162/neco.1992.4.3.448. U RL https://doi.org/10.1162/neco.1992.4.3.448. 18Convergence Rates of V ariational Inference in Sparse Deep Learning David J. C. MacKay . Bayesian methods for adaptive models . PhD thesis, California Institute of T echnology , 1992b. T. P . Minka. Expectation propagation for approximate bayes ian inference. In Proceedings of the 17th Conference in Uncertainty in Artiﬁcial Intellig ence, UAI ’01, pages 362–369, San F rancisco, CA, USA, 2001. Morgan Kaufmann Publishers In c. ISBN 1-55860-800-1. URL http://dl.acm.org/citation.cfm?id=647235.720257. Aaron Mishkin, F rederik Kunstner, Didrik Nielsen, Mark Sch midt, and Mohammad Emtiyaz Khan. Slang: F ast structured covariance approximations fo r bayesian deep learning with natural gradient. In S. Bengio, H. W allach, H. Larochelle, K. Grauman, N. Cesa-Bianchi, and R. Garnett, editors, Advances in Neural Information Processing Systems 31 , pages 6245–6255. Curran Associates, Inc., 2018. Radford. M. Neal. Bayesian learning for neural networks . PhD thesis, University of T oronto, 1995. Behnam Neyshabur, Srinadh Bhojanapalli, and Nathan Srebro. A P AC-bayesian approach to spectrally-normalized margin bounds for neural networks. In International Conference on Learning Representations, 2018. URL https://openreview.net/forum?id=Skz_WfbCZ. Quynh Nguyen, Mahesh Chandra Mukkamala, and Matthias Hein. On the loss landscape of a class of deep neural networks with no bad local valleys. In International Conference on Learning Representations, 2019. URL https://openreview.net/forum?id=HJgXsjA5tQ. Manfred Opper and Cedric Archambeau. The variational gauss ian approximation revisited. Neural computation , 21:786–92, 10 2008. doi: 10.1162/neco.2008.08-07-592. Kazuki Osawa, Siddharth Swaroop, Anirudh Jain, Runa Eschen hagen, Richard E. T urner, Rio Y okota, and Mohammad Emtiyaz Khan. Practical deep learn ing with bayesian prin- ciples, 2019. URL http://arxiv.org/abs/1906.02506. cite arxiv:1906.02506Comment: Under review. Philipp Petersen and F elix V oigtländer. Optimal approxima tion of piecewise smooth func- tions using deep relu neural networks. Neural Networks , 09 2017. doi: 10.1016/j.neunet. 2018.08.019. V eronika Rockova and nicholas Polson. Posterior concentra tion for sparse deep learning. In S. Bengio, H. W allach, H. Larochelle, K. Grauman, N. Cesa-Bian chi, and R. Garnett, editors, Advances in Neural Information Processing Systems 31 , pages 930–941. Curran Associates, Inc., 2018. David Rolnick and Max T egmark. The power of deeper networks f or expressing natural functions. In 6th International Conference on Learning Representations , ICLR 2018, Van- couver, BC, Canada, April 30 - May 3, 2018, Conference Track P roceedings, 2018. URL https://openreview.net/forum?id=SyProzZAW. David E. Rumelhart, Geoﬀrey E. Hinton, and Ronald J. William s. Learning Representations by Back-propagating Errors. Nature, 323(6088):533–536, 1986. doi: 10.1038/323533a0. URL http://www.nature.com/articles/323533a0. 19Chérief-Abdellatif Johannes Schmidt-Hieber. Nonparametric regression using deep neural networks with relu activation function. ArXiv, arxiv:1708.06633, 2017. Rishit Sheth and Roni Khardon. Excess risk bounds for the bay es risk using variational inference in latent gaussian models. In I. Guyon, U. V. Luxbu rg, S. Bengio, H. W allach, R. F ergus, S. Vishwanathan, and R. Garnett, editors, Advances in Neural Information Processing Systems 30 , pages 5151–5161. Curran Associates, Inc., 2017. David Silver, Julian Schrittwieser, Karen Simonyan, Ioann is Antonoglou, Aja Huang, Arthur Guez, Thomas Hubert, Lucas Baker, Matthew Lai, Adrian Bolton, Y utian Chen, Timothy Lillicrap, F an Hui, Laurent Sifre, George van den Driessche , Thore Graepel, and Demis Hassabis. Mastering the game of go without human knowledge. Nature, 550:354–, October 2017. URL http://dx.doi.org/10.1038/nature24270. Daniel Soudry and Y air Carmon. No bad local minima: Data inde pendent training error guarantees for multilayer neural networks. 05 2016. Nitish Srivastava, Geoﬀrey Hinton, Alex Krizhevsky , Ilya S utskever, and Ruslan Salakhutdinov. Dropout: A simple way to prevent neural netw orks from over- ﬁtting. Journal of Machine Learning Research , 15:1929–1958, 2014. URL http://jmlr.org/papers/v15/srivastava14a.html. J.A. Stanford, K Giardina, G.A. Gerhardt, Kenji F ukumizu, a nd Shun-ichi Amari. Local minima and plateaus in hierarchical structures of multilay er perceptrons. Neural Networks , 13, 05 2000. doi: 10.1016/S0893-6080(00)00009-5. T aiji Suzuki. F ast generalization error bound of deep learn ing from a kernel perspective. In Amos Storkey and F ernando Perez-Cruz, editors, Proceedings of the Twenty-First Inter- national Conference on Artiﬁcial Intelligence and Statist ics, volume 84 of Proceedings of Machine Learning Research , pages 1397–1406, Playa Blanca, Lanzarote, Canary Islands, 09–11 Apr 2018. PMLR. URL http://proceedings.mlr.press/v84/suzuki18a.html. T aiji Suzuki. Adaptivity of deep reLU network for learning i n besov and mixed smooth besov spaces: optimal rate and curse of dimensionality . In International Conference on Learning Representations, 2019. URL https://openreview.net/forum?id=H1ebTsActm. Michalis K. Titsias and Miguel Lázaro-Gredilla. Spike and s lab variational inference for multi-task and multiple kernel learning. In J. Shawe-T aylo r, R. S. Zemel, P . L. Bartlett, F. Pereira, and K. Q. W einberger, editors, Advances in Neural Information Processing Systems 24 , pages 2339–2347. Curran Associates, Inc., 2011. F rancesco T onolini, Bjorn Sand Jensen, and Roderick Murray- Smith. V ariational sparse coding, 2019. URL https://openreview.net/forum?id=SkeJ6iR9Km. Alexandre B. T sybakov. Introduction to Nonparametric Estimation . Springer Publishing Company , Incorporated, 1st edition, 2008. ISBN 0387790519, 9780387790510. 20Convergence Rates of V ariational Inference in Sparse Deep Learning Mariia Vladimirova, Jakob V erbeek, Pablo Mesejo, and Julya n Arbel. Understand- ing priors in Bayesian neural networks at the unit level. In Ka malika Chaud- huri and Ruslan Salakhutdinov, editors, Proceedings of the 36th International Con- ference on Machine Learning , volume 97 of Proceedings of Machine Learning Re- search, pages 6458–6467, Long Beach, California, USA, 09–15 Jun 201 9. PMLR. URL http://proceedings.mlr.press/v97/vladimirova19a.html. Y. W ang and D. M. Blei. F requentist consistency of variationa l Bayes. Journal of the American Statistical Association (to appear), 2018. Dmitry Y arotsky . Error bounds for approximations with deep relu networks. Neural Net- works, 94, 10 2016. doi: 10.1016/j.neunet.2017.07.002. Chiyuan Zhang, Samy Bengio, Moritz Hardt, Benjamin Recht, and Oriol Vinyals. Understanding deep learning requires rethinking generali zation. 2017. URL https://arxiv.org/abs/1611.03530. F. Zhang and C. Gao. Convergence rates of variational poster ior distributions. arXiv preprint arXiv:1712.02519v1, 2017. 21Chérief-Abdellatif Appendix A. Connection between concentration and consistency In this appendix, we show the connection between the notions of consistency and concen- tration. The Bayesian estimator ρ (e.g. the tempered posterior πn,α or its variational approxima- tion ˜πn,α) is said to be consistent if its generalization error goes to zero as n → +∞: E [ ∫ ∥fθ − f0∥2 2ρ(dθ) ] − − − − − → n→+∞ 0. W e say that the Bayesian estimator ρ concentrates at rate rn ( Ghosal et al. , 2000) if in probability (with respect to the random variables distribu ted according to the generating process), the estimator concentrates asymptotically arou nd the true distribution as n → +∞, i.e.: ρ ( θ ∈ Θ S,L,D / ∥fθ − f0∥2 2> M nrn ) − − − − − → n→+∞ 0. in probability as n → +∞ for any Mn → +∞. The consistency of the Bayesian distribution ρ at rate rn implies its concentration at rate rn. Indeed, if we we assume that ρ is consistent at rate rn, i.e.: E [ ∫ ∥fθ − f0∥2 2ρ(dθ) ] ≤ rn, then, using Markov’s inequality for any Mn → +∞ as n → +∞: E [ ρ ( θ ∈ Θ S,L,D / ∥fθ − f0∥2 2> M nrn )] ≤ E [ ∫ ∥fθ − f0∥2 2ρ(dθ) ] Mnrn ≤ rn Mnrn = 1 Mn → 0. Hence, we have the convergence in mean of ρ ( θ ∈ Θ S,L,D / ∥fθ − f0∥2 2> M nrn ) to 0, and then the convergence in probability of ρ ( θ ∈ Θ S,L,D / ∥fθ − f0∥2 2 > M nrn ) to 0, i.e. the concentration of ρ to f0 at rate rn. Appendix B. Proof of Theorem 2 The structure of the proof of Theorem 2 is composed of three main steps. The ﬁrst one consists in obtaining the general shape of the inequality us ing P AC-Bayes inequalities, and the two others in ﬁnding a rate that satisﬁes the extended pri or mass condition. First step : we obtain the general inequality W e start from inequality 2.6 in Alquier and Ridgway (2017) that provides an upper bound on the generalization error but in α-Rényi divergence. W e denote P 0 the generating distribution of any (Xi, Yi) and Pθ the distribution characterizing the model. Then, for any α ∈ (0, 1): E [ ∫ Dα(Pθ, P 0)˜πn,α(dθ) ] ≤ inf q∈FS,L,D { α 1 − α ∫ KL(P 0, Pθ)q(dθ) + KL(q∥π) n(1 − α) } . 22Convergence Rates of V ariational Inference in Sparse Deep Learning Moreover, the α-Rényi divergence is equal to Dα(Pθ, P 0) = α 2σ2 ∥fθ − f0∥2 2 and the KL divergence is KL (P 0∥Pθ) = 1 2σ2 ∥fθ − f0∥2 2, and for any θ∗, ∥fθ − f0∥2 2 ≤ 2∥fθ − fθ∗ ∥2 2+ 2∥fθ∗ − f0∥2 2. Hence, for any θ∗ ∈ Θ S,L,D: E [ ∫ α 2σ2 ∥fθ − f0∥2 2˜πn,α(dθ) ] ≤ α 1 − α 2 2σ2 ∥fθ∗ − f0∥2 2+ inf q∈FS,L,D { α 1 − α ∫ 2 2σ2 ∥fθ − fθ∗ ∥2 2q(dθ) + KL(q∥π) n(1 − α) } , i.e. for any θ∗ ∈ Θ S,L,D, E [ ∫ ∥fθ − f0∥2 2˜πn,α(dθ) ] ≤ 2 1 − α∥fθ∗ − f0∥2 2+ inf q∈FS,L,D { 2 1 − α ∫ ∥fθ − fθ∗ ∥2 2q(dθ) + 2σ2 α KL(q∥π) n(1 − α) } . F rom now on, the rest of the proof consists in ﬁnding a distrib ution q∗ n ∈ F S,L,D that satisﬁes for θ∗ = arg min θ∈Θ S,L,D ∥fθ − f0∥2 the extended prior mass condition, i.e. that satisﬁes both: ∫ ∥fθ − fθ∗ ∥2 2q∗ n(dθ) ≤ rn (4) and KL(q∗ n∥π) ≤ nrn (5) with rn = SL n log(BD) + S n log(BL(D + 1)2) + S 2n log ( 4n S { 3 + ( d + 2)2L2 }) that is smaller than rS,L,D n as 3 + ( x + 2)2L2 ≤ 10x2L2 for x ≥ 1 and L ≥ 3. This will lead to: E [ ∫ ∥fθ − f0∥2 2˜πn,α(dθ) ] ≤ 2 1 − α inf θ∗∈Θ S,L,D ∥fθ∗ − f0∥2 2+ 2 1 − α ( 1 + σ2 α ) rS,L,D n . Second step : we prove Inequality (4) T o begin with, we deﬁne the loss of the ℓth layer of the neural network fθ: rℓ(θ) = sup x∈[−1,1]d sup 1≤i≤D |fℓ θ (x)i − fℓ θ∗ (x)i| where fℓ θ s are deﬁned as the partial networks: { f0 θ (x) := x, fℓ θ (x) := ρ(Aℓfℓ−1 θ (x) + bℓ) for ℓ = 1 , ..., L. W e also deﬁne the loss of the output layer: rℓ(θ) = sup x∈[−1,1]d |fL θ (x) − fL θ∗ (x)| = sup x∈[−1,1]d |fθ(x) − fθ∗ (x)|. 23Chérief-Abdellatif W e will prove by induction that for any ℓ = 1 , ..., L: rℓ(θ) ≤ (BD)ℓ ( d + 1 + 1 BD − 1 ) ℓ∑ u=1 ˜Au + ℓ∑ u=1 (BD)ℓ−u˜bu where ˜Au = sup i,j |Au,i,j − A∗ u,i,j| and ˜bu = sup j |bu,j − b∗ u,j|. T o do so, we will also prove by induction that: cℓ ≤ BℓDℓ−1 ( d + 1 + 1 BD − 1 ) where { cℓ = sup x∈[−1,1]d sup1≤i≤D |fℓ θ∗ (x)i| for ℓ = 1 , ..., L, cL = sup x∈[−1,1]d |fθ∗ (x)|, using the formula: xn ≤ unxn−1 + vn =⇒ xn ≤ n∑ i=2 ( n∏ j=i+1 uj ) vi + ( n∏ j=2 uj ) x1 (6) for any n ≥ 2 with the convention ∏ n j=n+1 uj = 1 . Indeed, we have according to Assumption 3.1: • Initialization: c1 = sup x∈[−1,1]d sup 1≤i≤D |f1 θ∗ (x)i| ≤ sup x∈[−1,1]d sup 1≤i≤D ⏐ ⏐ ⏐ ⏐ d∑ j=1 A∗ 1ij xj + b∗ 1i ⏐ ⏐ ⏐ ⏐ ≤ sup x∈[−1,1]d sup 1≤i≤D { d∑ j=1 |A∗ 1ij | · | xj | + |b∗ 1i| } ≤ d · B · 1 + B = ( d + 1)B. • F or any layer ℓ: cℓ ≤ sup x∈[−1,1]d sup 1≤i≤D ⏐ ⏐ ⏐ ⏐ D∑ j=1 A∗ ℓijfℓ−1 θ∗ (x)j + b∗ ℓi ⏐ ⏐ ⏐ ⏐ ≤ sup x∈[−1,1]d sup 1≤i≤D { D∑ j=1 |A∗ ℓij| · | fℓ−1 θ∗ (x)j | + |b∗ ℓi| } ≤ D · B · cℓ−1 + B. 24Convergence Rates of V ariational Inference in Sparse Deep Learning • Hence, using F ormula ( 6), we get: cℓ ≤ ℓ∑ u=2 ( ℓ∏ v=u+1 DB ) B + ( ℓ∏ v=2 BD ) c1 ≤ B ℓ∑ u=2 (DB)ℓ−u + (BD)ℓ−1(d + 1)B = B ℓ−2∑ u=0 (DB)u + (d + 1)Dℓ−1Bℓ = B (BD)ℓ−1 − 1 BD − 1 + (d + 1)Dℓ−1Bℓ ≤ BℓDℓ−1 ( d + 1 + 1 BD − 1 ) . Let us now come back to ﬁnding an upper bound on losses of the pa rtial networks fℓ θ s. As previously , we have: • Initialization: r1(θ) = sup x∈[−1,1]d sup 1≤i≤D |f1 θ∗ (x)i − f1 θ (x)i| ≤ sup x∈[−1,1]d sup 1≤i≤D { d∑ j=1 |A1ij − A∗ 1ij | · | xj| + |b1i − b∗ 1i| } ≤ d · ˜A1 + ˜b1. • F or any layer ℓ: rℓ(θ) ≤ sup x∈[−1,1]d sup 1≤i≤D { D∑ j=1 |Aℓij fℓ−1 θ (x)j − A∗ ℓijfℓ−1 θ∗ (x)j | + |bℓi − b∗ ℓi| } ≤ sup x∈[−1,1]d sup 1≤i≤D { D∑ j=1 [ |Aℓij − A∗ ℓij| · | fℓ−1 θ∗ (x)j | + |Aℓij | · | fℓ−1 θ∗ (x)j − fℓ−1 θ (x)j | ] + |bℓi − b∗ ℓi| } ≤ Dcℓ−1 ˜Aℓ + BDrℓ−1(θ) + ˜bℓ ≤ BDrℓ−1(θ) + ˜AℓBℓ−1Dℓ−1 ( d + 1 + 1 BD − 1 ) + ˜bℓ. 25Chérief-Abdellatif • Finally , using F ormula ( 6): rℓ(θ) ≤ ℓ∑ u=2 ( ℓ∏ v=u+1 BD )( ˜Au(BD)u−1 { d + 1 + 1 BD − 1 } + ˜bu ) + ( ℓ∏ v=2 BD ) r1(θ) = ℓ∑ u=2 (BD)ℓ−u ˜Au(BD)u−1 ( d + 1 + 1 BD − 1 ) + ℓ∑ u=2 (BD)ℓ−u˜bu + (BD)ℓ−1r1(θ) ≤ ( d + 1 + 1 BD − 1 ) ℓ∑ u=2 (BD)ℓ−1 ˜Au + ℓ∑ u=2 (BD)ℓ−u˜bu + (BD)ℓ−1d ˜A1 + (BD)ℓ−1˜b1 ≤ (BD)ℓ−1 ( d + 1 + 1 BD − 1 ) ℓ∑ u=1 ˜Au + ℓ∑ u=1 (BD)ℓ−u˜bu. Then, for any distribution q: ∫ ∥fθ − fθ∗ ∥2 2q(dθ) ≤ ∫ ∥fθ − fθ∗ ∥2 ∞q(dθ) = ∫ rL(θ)2q(dθ) ≤ ∫ 2(BD)2L−2 ( d + 1 + 1 BD − 1 ) 2( L∑ ℓ=1 ˜Aℓ ) 2 q(dθ) + ∫ 2 ( L∑ ℓ=1 (BD)L−ℓ˜bu ) 2 q(dθ) = 2( BD)2L−2 ( d + 1 + 1 BD − 1 ) 2(∫ L∑ ℓ=1 ˜A2 ℓq(dθ) + 2 ∫ L∑ ℓ=1 ℓ−1∑ k=1 ˜Aℓ ˜Akq(dθ) ) + 2 (∫ L∑ ℓ=1 (BD)2(L−ℓ)˜b2 lq(dθ) + 2 ∫ L∑ ℓ=1 ℓ−1∑ k=1 (BD)L−ℓ(BD)L−k˜bℓ˜bkq(dθ) ) . Here, we deﬁne q∗ n(θ) as follows: { γ∗ t = I(θ∗ t ̸= 0) , θt ∼ γ∗ t U([θ∗ t − sn, θ∗ t + sn]) + (1 − γ∗ t )δ{0} for each t = 1 , ..., T. with s2 n= S 4n (BD)−2L {( d + 1 + 1 BD−1 ) 2 L2 (BD)2 + 1 (BD)2−1 + 2 (BD−1)2 }−1 . Hence: ∫ ˜A2 ℓq∗ n(dθ) = ∫ sup i,j (Aℓ,i,j − A∗ ℓ,i,j)2q∗ n(dAℓ,i,j ) ≤ s2 n, and ∫ ˜Aℓ ˜Akq∗ n(dθ) = (∫ sup i,j |Aℓ,i,j − A∗ ℓ,i,j|q∗ n(dθ) )( ∫ sup i,j |Ak,i,j − A∗ k,i,j|q∗ n(dθ) ) ≤ | sn| · | sn| = s2 n, and similarly , ∫˜b2 ℓq∗ n(dθ) ≤ s2 nand ∫˜bℓ˜bkq∗ n(dθ) ≤ s2 n. 26Convergence Rates of V ariational Inference in Sparse Deep Learning Then ∫ ∥fθ − fθ∗ ∥2 2q∗ n(dθ) ≤ 2(BD)2L−2 ( d + 1 + 1 BD − 1 ) 2(∫ L∑ ℓ=1 ˜A2 ℓq(dθ) + 2 ∫ L∑ ℓ=1 ℓ−1∑ k=1 ˜Aℓ ˜Akq(dθ) ) + 2 (∫ L∑ ℓ=1 (BD)2(L−ℓ)˜b2 lq(dθ) + 2 ∫ L∑ ℓ=1 ℓ−1∑ k=1 (BD)L−ℓ(BD)L−k˜bℓ˜bkq(dθ) ) ≤ 2(BD)2L−2 ( d + 1 + 1 BD − 1 ) 2 s2 n ( L + 2 L−1∑ ℓ=0 ℓ ) + 2s2 n L−1∑ ℓ=0 (BD)2ℓ + 4s2 n L∑ ℓ=1 L−1∑ k=L−ℓ+1 (BD)L−ℓ(BD)k = 2( BD)2L−2 ( d + 1 + 1 BD − 1 ) 2 s2 nL2 + 2s2 n (BD)2L − 1 (BD)2 − 1 + 4s2 n L∑ ℓ=1 ℓ−2∑ k=0 (BD)L−ℓ(BD)k(BD)L−ℓ+1 = 2 s2 n(BD)2L−2 ( d + 1 + 1 BD − 1 ) 2 L2 + 2s2 n (BD)2L − 1 (BD)2 − 1 + 4s2 n L∑ ℓ=1 (BD)L−ℓ (BD)ℓ−1 − 1 BD − 1 (BD)L−ℓ+1 ≤ 2s2 n(BD)2L−2 ( d + 1 + 1 BD − 1 ) 2 L2 + 2s2 n (BD)2L − 1 (BD)2 − 1 + 4s2 n 1 BD − 1 L∑ ℓ=1 (BD)2L−ℓ = 2 s2 n(BD)2L−2 ( d + 1 + 1 BD − 1 ) 2 L2 + 2s2 n (BD)2L − 1 (BD)2 − 1 + 4s2 n 1 BD − 1(BD)L (BD)L − 1 BD − 1 ≤ 2s2 n(BD)2L−2 ( d + 1 + 1 BD − 1 ) 2 L2 + 2s2 n (BD)2L − 1 (BD)2 − 1 + 4s2 n 1 (BD − 1)2 (BD)2L ≤ 2s2 n(BD)2L {( d + 1 + 1 BD − 1 ) 2 L2 (BD)2 + 1 (BD)2 − 1 + 2 (BD − 1)2 } = S 2n ≤ rn which proves Equation ( 4). 27Chérief-Abdellatif Third step : we prove Inequality (5) W e will use the fact that for any K, any p, p0 ∈ [0, 1]K such that ∑ K k=1 pk = ∑ K k=1 p0 k= 1 and any distributions Qk, Q0 kfor k = 1 , ..., K , we have: K (K∑ k=1 p0 kQ0 k     K∑ k=1 pkQk ) ≤ K (p0∥p) + K∑ k=1 p0 kK(Q0 k∥Qk). (7) Please refer to Lemma 6.1 in Chérief-Abdellatif and Alquier (2018) for a proof. Then we write q∗ n and π as mixtures of independent products of mixtures of two compo nents: q∗ n = ∑ γ∈SS T I(γ = γ∗) T⨂ t=1 { γt U([lt, ut]) + (1 − γt)δ{0} } and π = ∑ γ∈SS T (T S∗ ) −1 T⨂ t=1 { γt U([−B, B ]) + (1 − γt)δ{0} } Hence, using Inequality 7 twice and the additivity of KL for independent distribution s: KL(q∗ n∥π) ≤ KL ( {I(γ = γ∗)}γ∈SS T     {(T ∗ S∗ ) −1} γ∈SS T ) + ∑ γ∈SS T I(γ = γ∗) KL ( T⨂ t=1 { γt U([lt, ut]) + (1 − γt)δ{0} }    T⨂ t=1 { γt U([−B, B ]) + (1 − γt)δ{0} }) = log (T S ) + T∑ t=1 KL ( γ∗ t U([lt, ut]) + (1 − γ∗ t )δ{0}    γ∗ t U([−B, B ]) + (1 − γ∗ t )δ{0} ) ≤ log (T S ) + T∑ t=1 γ∗ t KL ( U([lt, ut])    U([−B, B ]) ) + T∑ t=1 (1 − γ∗ t )KL ( δ{0}∥δ{0} ) ≤ S log(T ) + T∑ t=1 γ∗ t log ( 2B ut − lt ) = S log(T ) + T∑ t=1 γ∗ t log (2B 2sn ) = S log(T ) + S log(B) + S 2 log (1 s2 n ) = S log(T ) + S log(B) + S 2 log (4n S (BD)2L {( d + 1 + 1 BD − 1 ) 2 L2 + 1 (BD)2 − 1 + 2 (BD − 1)2 }) , 28Convergence Rates of V ariational Inference in Sparse Deep Learning and hence, KL(q∗ n∥π) ≤ S log(T ) + S log(B) + S 2 log (4n S (BD)2L {( d + 1 + 1 BD − 1 ) 2 L2 + 1 (BD)2 − 1 + 2 (BD − 1)2 }) ≤ S log(L(D + 1)2) + S log(B) + LS log(BD) + S 2 log (4n S {( d + 1 + 1 BD − 1 ) 2 L2 + 1 (BD)2 − 1 + 2 (BD − 1)2 }) ≤ nrn, which ends the proof. Appendix C. Proof of Corollary 3 Corollary 3 is a direct consequence of Theorem 2, and we just need to ﬁnd an upper bound on infθ∗∈Θ S,L,D ∥fθ∗ − f0∥2 ∞and rS,L,D n . Indeed, according to Theorem 2: E [ ∫ ∥fθ − f0∥2 2˜πn,α(dθ) ] ≤ 2 1 − α inf θ∗∈Θ S,L,D ∥fθ∗ − f0∥2 ∞+ 2 1 − α ( 1 + σ2 α ) rn. (8) W e directly use the rate rn in the proof of Theorem 2 rather than rS,L,D n . Let us assume that f0 is β-Hölder smooth with 0 < β < d . Then according to Lemma 5.1 in Rockova and Polson (2018), we have for some positive constant CD independent of n (see Theorem 6.1 in Rockova and Polson (2018)) a neural network with architecture : L = 8 + ( ⌊log2 n⌋ + 5)(1 + ⌈log2 d⌉), D = CD⌊n d 2β+d / log n⌋, S ≤ 94d2(β + 1)2dD(L + ⌈log2 d⌉), with an error ∥f −f0∥∞ that is at most a constant multiple of D n +D−β/d ≤ CDn −2β 2β+d / log n+ C−β/d D n −β 2β+d logβ/d n ≤ (CD/ log n + C−β/d D log n)n −β 2β+d , which gives an upper bound on the ﬁrst term of the right-hand-side of Inequality 8 of order n −2β 2β+d log2 n. In the same time, we have for some constants C, C ′ that do not depend on n: rn ≤ SL n log(BD) + S n log(2BL(D + 1)2) + S 2n log (4n S { 3 + ( d + 2)2L2 }) ≤ C (DL2 n log D + DL n log(LD) + DL n log n ) ≤ C′ n d 2β+d n log2 n = C′n −2β 2β+d log2 n. 29Chérief-Abdellatif Then the tempered posterior distribution πn,α concentrates at the minimax rate rn = n −2β 2β+d up to a (squared) logarithmic factor for the expected L2-distance in the sense that: πn,α ( θ ∈ Θ S,L,D / ∥fθ − f0∥2 2> M nn −2β 2β+d log2 n ) − − − − − → n→+∞ 0. in probability as n → +∞ for any Mn → +∞. Appendix D. Proof of Theorem 1 W e could prove Theorem 1 using the prior mass condition ( 2) but we will use instead the same proof than for Theorem 2. Indeed, we can easily show that for any θ∗ ∈ Θ S,L,D, E [ ∫ ∥fθ−f0∥2 2πn,α(dθ) ] ≤ 2 1 − α∥fθ∗ −f0∥2 2+inf q { 2 1 − α ∫ ∥fθ−fθ∗ ∥2 2q(dθ)+2σ2 α KL(q∥π) n(1 − α) } where the inﬁmum is taken over all the probability distribut ions on Θ S,L,D. W e have: inf q { 2 1 − α ∫ ∥fθ − fθ∗ ∥2 2q(dθ) + 2σ2 α KL(q∥π) n(1 − α) } ≤ inf q∈FS,L,D { 2 1 − α ∫ ∥fθ − fθ∗ ∥2 2q(dθ) + 2σ2 α KL(q∥π) n(1 − α) } ≤ 2 1 − α ( 1 + σ2 α ) rS,L,D n , which implies E [ ∫ ∥fθ − f0∥2 2˜πn,α(dθ) ] ≤ 2 1 − α inf θ∗∈Θ S,L,D ∥fθ∗ − f0∥2 2+ 2 1 − α ( 1 + σ2 α ) rS,L,D n ≤ 2 1 − α inf θ∗∈Θ S,L,D ∥fθ∗ − f0∥2 ∞+ 2 1 − α ( 1 + σ2 α ) rS,L,D n . The rest of the proof follows the same lines than the one of Cor ollary 3. Appendix E. Proof of Theorem 4 First, we need Donsker and V aradhan’s variational formula. Refer to Lemma 1.1.3. in Catoni (2007) for a proof. Theorem 6. For any probability λ on some measurable space (E, E) and any measurable function h : E → R such that ∫ ehdλ < ∞, log ∫ ehdλ = sup q { ∫ hdq − KL(q, λ) } , where the supremum is taken over all probability distributi ons over E and with the convention ∞ − ∞ = −∞. Moreover, if h is upper-bounded on the support of λ, then the supremum is reached by the distribution of the form: λh(dβ) = eh(β) ∫ ehdλλ(dβ). 30Convergence Rates of V ariational Inference in Sparse Deep Learning Let us come back to the proof of Theorem 4. Here, we can not directly use Theorem 2.6 in Alquier and Ridgway (2017). Thus we begin from scratch. F or any α ∈ (0, 1) and θ ∈ Θ S,L,D, using the deﬁnition of Rényi divergence and Dα(P ⊗n, R⊗n) = nDα(P, R) as data are i.i.d. E [ exp ( − αrn(Pθ, P 0) + (1 − α)nDα(Pθ, P 0) )] = 1 where rn(Pθ, P 0) = 1 2σ2 ∑ n i=1{(Yi − fθ(Xi))2 − (Yi − f0(Xi))2} is the negative log-likelihood ratio. Then we integrate and use F ubini’s theorem, E [ ∫ exp ( − αrn(Pθ, P 0) + (1 − α)nDα(Pθ, P 0) ) π(dθ) ] = 1 . According to Theorem 6, E [ exp ( sup q { ∫ ( − αrn(Pθ, P 0) + (1 − α)nDα(Pθ, P 0) ) q(dθ) − KL(q||π) })] = 1 where the supremum is taken over all probability distributi ons over Θ S,L,D. Then, using Jensen’s inequality , E [ sup q { ∫ ( − αrn(Pθ, P 0) + (1 − α)nDα(Pθ, P 0) ) q(dθ) − KL(q||π) }] ≤ 0, and then, E [ ∫ ( − αrn(Pθ, P 0) + (1 − α)nDα(Pθ, P 0) ) ˜πk n,α(dθ) − KL(˜πk n,α||π) ] ≤ 0. W e rearrange terms: E [ ∫ Dα(Pθ, P 0)˜πk n,α(dθ) ] ≤ E [ α 1 − α ∫ rn(Pθ, P 0) n ˜πk n,α(dθ) + KL(˜πk n,α||π) n(1 − α) ] , that we can write: E [ ∫ Dα(Pθ, P 0)˜πk n,α(dθ) ] ≤ E [ α 1 − α ∫ rn(Pθ, P 0) n ˜πn,α(dθ) + KL(˜πn,α||π) n(1 − α) ] + E [ α 1 − α ∫ rn(Pθ, P 0) n ˜πk n,α(dθ) + KL(˜πk n,α||π) n(1 − α) ] − E [ α 1 − α ∫ rn(Pθ, P 0) n ˜πn,α(dθ) + KL(˜πn,α||π) n(1 − α) ] . Let us precise that E [ rn(Pθ,P 0) n ] = KL(P 0||Pθ) = ∥f0−fθ∥2 2 2σ2 , and: Ln(q) = − α 2σ2 n∑ i=1 ∫ (Yi − fθ(Xi))2q(dθ) − KL(q∥π) up to a constant. 31Chérief-Abdellatif Then: E [ ∫ Dα(Pθ, P 0)˜πk n,α(dθ) ] ≤ E [ α 1 − α ∫ rn(Pθ, P 0) n ˜πn,α(dθ) + KL(˜πn,α||π) n(1 − α) ] + E[L∗ n− Lk n] n(1 − α) . W e conclude by interverting the inﬁmum and the expectation a nd the same inequalities than in Theorem 2: E [ α 1 − α ∫ rn(Pθ, P 0) n ˜πn,α(dθ) + KL(˜πn,α∥π) n(1 − α) ] = E [ inf q∈FS,L,D { α 1 − α ∫ rn(Pθ, P 0) n q(dθ) + KL(q∥π) n(1 − α) }] ≤ inf q∈FS,L,D { E [ α 1 − α ∫ rn(Pθ, P 0) n q(dθ) + KL(q∥π) n(1 − α) ]} ≤ α 1 − α 2 2σ2 inf θ∗∈Θ S,L,D ∥fθ∗ − f0∥2 2+ α 2σ2 2 1 − α ( 1 + σ2 α ) rS,L,D n . Appendix F. Proof of Theorem 5 W e start from the last inequality obtained in the proof of The orem 3 in Cherief-Abdellatif (2019) that provides an upper bound in α-Rényi divergence for the ELBO model selection framework. W e still denote P 0 the generating distribution and Pθ the distribution charac- terizing the model. Then, for any α ∈ (0, 1): E [ ∫ Dα(Pθ, P 0)˜π ˆS, ˆL, ˆD n,α (dθ) ] ≤ inf S,L,D { inf q∈FS,L,D { α 1 − α ∫ KL(P 0, PθS,L,D )q(dθS,L,D) + KL(q, Π S,L,D) n(1 − α) } + log( 1 πS,L,D ) n(1 − α) } where Π S,L,D denotes the prior over the parameter set Θ S,L,D and πS,L,D the prior belief over model (S, L, D). As for the proof of Theorem 2, for any S, L, D and any θ∗ ∈ Θ S,L,D: E [ ∫ α 2σ2 ∥fθ − f0∥2 2˜π ˆS, ˆL, ˆD n,α (dθ) ] ≤ α 1 − α 2 2σ2 ∥fθ∗ − f0∥2 2+ inf q∈FS,L,D { α 1 − α ∫ 2 2σ2 ∥fθ − fθ∗ ∥2 2q(dθ) + KL(q, Π S,L,D) n(1 − α) } + log( 1 πS,L,D ) n(1 − α) , and then for any S, L, D and any θ∗ ∈ Θ S,L,D, E [ ∫ ∥fθ − f0∥2 2˜π ˆS, ˆL, ˆD n,α (dθ) ] ≤ 2 1 − α∥fθ∗ − f0∥2 2+ 2 1 − α ( 1+σ2 α ) rS,L,D n + 2σ2 α(1 − α) log( 1 πS,L,D ) n , which ﬁnally leads to Theorem 5. 32Convergence Rates of V ariational Inference in Sparse Deep Learning Appendix G. Result for sparse Gaussian approximations In this appendix, we consider non-bounded parameter sets Θ S,L,D and Gaussians instead of uniform distributions in spike-and-slab priors on θ ∈ Θ S,L,D: { γ ∼ U (SS T ), θt|γt ∼ γt N (0, 1) + (1 − γt)δ{0}, t = 1 , ..., T and Gaussian-based sparse spike-and-slab approximations : { γ ∼ πγ , θt|γt ∼ γt N (mt, s2 n) + (1 − γt)δ{0} for each t = 1 , ..., T. The following theorem states that using Gaussians instead o f uniform distributions still leads to consistency with the same rate of convergence. Note that t he inﬁmum in the RHS of the inequality is taken over a bounded neural network model. Theorem 7. Let us introduce the sets Θ B S,L,Dthat contain the neural network parameters upper bounded by B (in L∞-norm). Then for any α ∈ (0, 1), for any B ≥ 2, E [ ∫ ∥fθ − f0∥2 2˜πn,α(dθ) ] ≤ 2 1 − α inf θ∗∈Θ B S,L,D ∥fθ∗ − f0∥2 2+ 2 1 − α ( 1 + σ2 α ) rS,L,D n with rS,L,D n = SL n log(2BD) + S 4n ( 12 log(LD) + B2 ) + S n log ( 11d max( n S , 1) ) . Proof. The proof follows the same structure than for Theorem 2. W e ﬁx B ≥ 2. First step : we obtain the general inequality W e can directly write for any θ∗ ∈ Θ S,L,D, E [ ∫ ∥fθ − f0∥2 2˜πn,α(dθ) ] ≤ 2 1 − α∥fθ∗ − f0∥2 2+ inf q∈FS,L,D { 2 1 − α ∫ ∥fθ − fθ∗ ∥2 2q(dθ) + 2σ2 α KL(q∥π) n(1 − α) } . W e deﬁne θ∗ = arg min θ∈Θ B S,L,D ∥fθ − f0∥2. Again, the rest of the proof consists in ﬁnding a distribution q∗ n ∈ F S,L,D that satisﬁes the extended prior mass condition: ∫ ∥fθ − fθ∗ ∥2 2q∗ n(dθ) ≤ rn (9) and KL(q∗ n∥π) ≤ nrn (10) with rn = SL n log(2BD)+ S n log(L(D+1)2)+ S log log(3D) n + SB2 4n + S 2n log ( 16n S { 3+(d+2)2 }) ≤ rS,L,D n as 3 + ( x + 2)2 ≤ 7x2 for x ≥ 1. 33Chérief-Abdellatif Second step : we prove Inequality (9) All coeﬃcients of parameter θ∗ are upper bounded by B. Hence, we still have: cℓ ≤ BℓDℓ−1 ( d + 1 + 1 BD − 1 ) . However, the upper bound on rℓ(θ) is not the same, as |Aℓ,i,j | can not be upper bounded by B directly and must be upper bounded by |A∗ ℓ,i,j| + ˜Aℓ ≤ B + ˜Aℓ: rℓ(θ) ≤ sup x∈[−1,1]d sup 1≤i≤D { D∑ j=1 [ |Aℓij − A∗ ℓij| · | fℓ−1 θ∗ (x)j | + |Aℓij | · | fℓ−1 θ∗ (x)j − fℓ−1 θ (x)j | ] + |bℓi − b∗ ℓi| } ≤ sup x∈[−1,1]d sup 1≤i≤D { D∑ j=1 [ |Aℓij − A∗ ℓij| · | fℓ−1 θ∗ (x)j | + (B + ˜Aℓ) · |fℓ−1 θ∗ (x)j − fℓ−1 θ (x)j | ] + |bℓi − b∗ ℓi| } ≤ Dcℓ−1 ˜Aℓ + (B + ˜Aℓ)Drℓ−1(θ) + ˜bℓ ≤ (B + ˜Aℓ)Drℓ−1(θ) + ˜AℓBℓ−1Dℓ−1 ( d + 1 + 1 BD − 1 ) + ˜bℓ. Then, using F ormula 6: rℓ(θ) ≤ ℓ∑ u=2 ( ℓ∏ v=u+1 (B + ˜Av)D )( ˜Au(BD)u−1 { d + 1 + 1 BD − 1 } + ˜bu ) + ( ℓ∏ v=2 (B + ˜Av)D ) r1(θ) ≤ ℓ∑ u=2 Dℓ−u ℓ∏ v=u+1 (B + ˜Av) ˜Au(BD)u−1 ( d + 1 + 1 BD − 1 ) + ℓ∑ u=2 Dℓ−u ℓ∏ v=u+1 (B + ˜Av)˜bu + Dℓ−1 ℓ∏ v=2 (B + ˜Av)r1(θ), and using inequality r1(θ) ≤ d · ˜A1 + ˜b1: rℓ(θ) ≤ Dℓ−1 ( d + 1 + 1 BD − 1 ) ℓ∑ u=2 Bu−1 ℓ∏ v=u+1 (B + ˜Av) ˜Au + ℓ∑ u=2 Dℓ−u ℓ∏ v=u+1 (B + ˜Av)˜bu + dDℓ−1 ℓ∏ v=2 (B + ˜Av) ˜A1 + Dℓ−1 ℓ∏ v=2 (B + ˜Av)˜b1 ≤ Dℓ−1 ( d + 1 + 1 BD − 1 ) ℓ∑ u=1 Bu−1 ℓ∏ v=u+1 (B + ˜Av) ˜Au + ℓ∑ u=1 Dℓ−u ℓ∏ v=u+1 (B + ˜Av)˜bu. 34Convergence Rates of V ariational Inference in Sparse Deep Learning Then we have for any distribution q(θ) = q1(θ1) × ... × qT (θT ): ∫ ∥fθ − fθ∗ ∥2 2q(dθ) ≤ ∫ ∥fθ − fθ∗ ∥2 ∞q(dθ) = ∫ rL(θ)2q(dθ) ≤ ∫ 2D2L−2 ( d + 1 + 1 BD − 1 ) 2( L∑ ℓ=1 Bℓ−1 L∏ v=ℓ+1 (B + ˜Av) ˜Aℓ ) 2 q(dθ) + ∫ 2 ( L∑ ℓ=1 DL−ℓ L∏ v=ℓ+1 (B + ˜Av)˜bℓ ) 2 q(dθ) = 2 D2L−2 ( d + 1 + 1 BD − 1 ) 2(∫ L∑ ℓ=1 B2ℓ−2 L∏ v=ℓ+1 (B + ˜Av)2 ˜A2 ℓq(dθ) + 2 ∫ L∑ ℓ=1 ℓ−1∑ k=1 Bℓ−1Bk−1 L∏ v=ℓ+1 (B + ˜Av) ˜Aℓ L∏ v=k+1 (B + ˜Av) ˜Akq(dθ) ) + 2 (∫ L∑ ℓ=1 D2(L−ℓ) L∏ v=ℓ+1 (B + ˜Av)2˜b2 ℓq(dθ) + 2 ∫ L∑ ℓ=1 ℓ−1∑ k=1 DL−ℓDL−k L∏ v=ℓ+1 (B + ˜Av)˜bℓ L∏ v=k+1 (B + ˜Av)˜bkq(dθ) ) = 2 D2L−2 ( d + 1 + 1 BD − 1 ) 2( L∑ ℓ=1 B2ℓ−2 L∏ v=ℓ+1 ∫ (B + ˜Av)2q(dθ) ∫ ˜A2 ℓqℓ(dθℓ) + 2 L∑ ℓ=1 ℓ−1∑ k=1 Bℓ−1Bk−1 L∏ v=ℓ+1 ∫ (B + ˜Av)2q(dθ) ∫ ˜Aℓqℓ(dθℓ) ℓ∏ v=k+1 ∫ (B + ˜Av)q(dθ) ∫ ˜Akq(dθ) ) + 2 ( L∑ ℓ=1 D2(L−ℓ) L∏ v=ℓ+1 ∫ (B + ˜Av)2q(dθ) ∫ ˜b2 ℓq(dθ) + 2 L∑ ℓ=1 ℓ−1∑ k=1 DL−ℓDL−k L∏ v=ℓ+1 ∫ (B + ˜Av)2q(dθ) ∫ ˜bℓq(dθ) ℓ∏ v=k+1 ∫ (B + ˜Av)q(dθ) ∫ ˜bkq(dθ) ) . Here, we deﬁne q∗ n(θ) as follows: { γ∗ t = I(θ∗ t ̸= 0) , θt ∼ γ∗ t N (θ∗ t , s2 n) + (1 − γ∗ t )δ{0} for each t = 1 , ..., T. with s2 n= S 16n log(3D)−1(2BD)−2L {( d + 1 + 1 BD−1 ) 2 + 1 (2BD)2−1 + 2 (2BD−1)2 }−1 . W e upper bound the expectation of the supremum of absolute va lues of Gaussian vari- ables: ∫ ˜Aℓq∗ n(dθ) = ∫ sup i,j |Aℓ,i,j − A∗ ℓ,i,j|q∗ n(dθ) ≤ √ 2s2 nlog(2D2) = √ 4s2 nlog(3D), 35Chérief-Abdellatif and use Example 2.7 in Boucheron et al. (2003): ∫ ˜A2 ℓq∗ n(dθ) = ∫ sup i,j (Aℓ,i,j − A∗ ℓ,i,j)2q∗ n(dθ) ≤ s2 n(1 + 2 √ log(D2) + log( D2)) = 4 s2 nlog(3D), which also give: ∫ (B + ˜Aℓ)q∗ n(dθ) = B + ∫ ˜Aℓq∗ n(dθ) ≤ B + √ 4s2 nlog(3D) ≤ 2B, and ∫ (B + ˜Aℓ)2q∗ n(dθ) = B2 + 2B ∫ ˜Aℓq∗ n(dθ) + ∫ ˜A2 ℓq∗ n(dθ) ≤ B2 + 2B √ 4s2 nlog(3D) + 4 s2 nlog(3D) ≤ 4B2 as √ 4s2 nlog(3D) ≤ B (s2 n≤ LD(D+1) 16n (2BD)−2L ≤ 2LD2 16n 4−2LD−2L ≤ 1). Similarly , ∫ ˜bℓq∗ n(dθ) ≤ √ 4s2 nlog(3D) and ∫ ˜b2 ℓq∗ n(dθ) ≤ 4s2 nlog(3D). Then ∫ ∥fθ − fθ∗ ∥2 2q∗ n(dθ) ≤ 2D2L−2 ( d + 1 + 1 BD − 1 ) 2( L∑ ℓ=1 B2ℓ−2(4B2)L−ℓ4s2 nlog(3D) + 2 L∑ ℓ=1 ℓ−1∑ k=1 Bℓ−1Bk−1(4B2)L−ℓ√ 4s2 nlog(3D)(2B)ℓ−k√ 4s2 nlog(3D) ) + 2 ( L∑ ℓ=1 D2(L−ℓ)(4B2)L−ℓ4s2 nlog(3D) + 2 L∑ ℓ=1 ℓ−1∑ k=1 DL−ℓDL−k(4B2)L−ℓ√ 4s2 nlog(3D)(2B)ℓ−k√ 4s2 nlog(3D) ) , 36Convergence Rates of V ariational Inference in Sparse Deep Learning i.e. ∫ ∥fθ − fθ∗ ∥2 2q∗ n(dθ) ≤ 2D2L−2 ( d + 1 + 1 BD − 1 ) 2( B2L−24s2 nlog(3D) L−1∑ ℓ=0 4ℓ + 2B2L−24s2 nlog(3D) L∑ ℓ=1 ℓ−1∑ k=1 2L−ℓ2L−k ) + 2 ( 4s2 nlog(3D) L∑ ℓ=1 (2BD)2L−2ℓ + 8s2 nlog(3D) L∑ ℓ=1 ℓ−1∑ k=1 (2BD)L−ℓ(2BD)L−k ) ≤ 2D2L−2 ( d + 1 + 1 BD − 1 ) 2( B2L−24s2 nlog(3D)4L − 1 4 − 1 + 2B2L−24s2 nlog(3D) L∑ ℓ=1 2L−ℓ2L−ℓ+1 ℓ−2∑ k=0 2k ) + 2 ( 4s2 nlog(3D) L−1∑ ℓ=0 (2BD)2ℓ + 8s2 nlog(3D) L∑ ℓ=1 (2BD)L−ℓ(2BD)L−ℓ+1 ℓ−2∑ k=0 (2BD)k ) ≤ 2D2L−2 ( d + 1 + 1 BD − 1 ) 2( B2L−24s2 nlog(3D)4L 3 + 2B2L−24s2 nlog(3D) L∑ ℓ=1 2L−ℓ2L−ℓ+12ℓ−1 ) + 2 ( 4s2 nlog(3D) (2BD)2L (2BD)2 − 1 + 8s2 nlog(3D) L∑ ℓ=1 (2BD)L−ℓ(2BD)L−ℓ+1 (2BD)ℓ−1 2BD − 1 ) ≤ 2D2L−2 ( d + 1 + 1 BD − 1 ) 2( B2L−24s2 nlog(3D)4L 3 + 2B2L−24s2 nlog(3D)2L L−1∑ ℓ=0 2ℓ ) + 2 ( 4s2 nlog(3D) (2BD)2L (2BD)2 − 1 + 8s2 nlog(3D) L−1∑ ℓ=0 (2BD)ℓ (2BD)L 2BD − 1 ) ≤ 2D2L−2 ( d + 1 + 1 BD − 1 ) 2( B2L−24s2 nlog(3D)4L 3 + 2B2L−24s2 nlog(3D)22L ) + 2 ( 4s2 nlog(3D) (2BD)2L (2BD)2 − 1 + 8s2 nlog(3D) (2BD)2L (2BD − 1)2 ) = 2 D2L−2 ( d + 1 + 1 BD − 1 ) 2 4s2 nlog(3D) ( B2L−2 4L 3 + 2B2L−222L ) + 2 ( (2BD)2L (2BD)2 − 1 + 2 (2BD)2L (2BD − 1)2 ) 4s2 nlog(3D), 37Chérief-Abdellatif and consequently , as BD ≥ 2, ∫ ∥fθ − fθ∗ ∥2 2q∗ n(dθ) ≤ 8s2 nlog(3D) { D2L−2 ( d + 1 + 1 BD − 1 ) 2 7 3B2L−222L + (2BD)2L ( 1 (2BD)2 − 1 + 2 (2BD − 1)2 )} = 8 s2 nlog(3D) { (2BD)2L 1 (BD)2 ( d + 1 + 1 BD − 1 ) 2 7 3 + (2BD)2L ( 1 (2BD)2 − 1 + 2 (2BD − 1)2 )} ≤ 8s2 nlog(3D)(2BD)2L {( d + 1 + 1 BD − 1 ) 2 + 1 (2BD)2 − 1 + 2 (2BD − 1)2 } = S 2n ≤ rn. which ends Step 2. Third step : we prove Inequality (10) W e end the proof: KL(q∗ n∥π) ≤ log (T S ) + T∑ t=1 γ∗ t KL ( N (θ∗ t , s2 n)    N (0, 1) ) ≤ S log(T ) + T∑ t=1 γ∗ t {1 2 log (1 s2 n ) + s2 n+ θ∗2 t 2 − 1 2 } ≤ S log(T ) + T∑ t=1 γ∗ t {1 2 log (1 s2 n ) + s2 n+ B2 2 − 1 2 } = S log(T ) + S 2 s2 n+ S 2 B2 − 1 2 + S 2 log (1 s2 n ) ≤ S log(T ) + S 2 + S 2 B2 − 1 2 + S 2 log (16n S log(3D)(2BD)2L {( d + 1 + 1 BD − 1 ) 2 + 1 (2BD)2 − 1 + 2 (2BD − 1)2 }) ≤ S log(L(D + 1)2) + B2S 4 + LS log(2BD) + S 2 log log(3D) + S 2 log (16n S {( d + 1 + 1 BD − 1 ) 2 + 1 (BD)2 − 1 + 2 (BD − 1)2 }) ≤ nrn. 38
---------------------------------

Please extract all reference paper titles and return them as a list of strings.
Output:
{
    "reference_titles": [
        "A convergence theory for deep learning via over-parameterization",
        "Concentration of tempered posteriors and of their variational approximations",
        "On the properties of variational approximations of Gibbs posteriors",
        "Neural networks and principal component analysis: Learning from examples without local minima",
        "Universal approximation bounds for superpositions of a sigmoidal function",
        "Approximation and estimation bounds for artificial neural networks",
        "Spectrally-normalized margin bounds for neural networks",
        "Statistical inference for probabilistic functions of finite state markov chains",
        "Tuning tempered transitions",
        "Deep rewiring: Training very sparse deep networks",
        "On the expressive power of deep architectures",
        "Bayesian fractional posteriors",
        "On statistical optimality of variational Bayes",
        "Variational inference: A review for statisticians",
        "Weight uncertainty in neural networks",
        "Concentration inequalities using the entropy method",
        "Quasi-Monte Carlo variational inference",
        "Universal boosting variational inference",
        "Approximation by superpositions of a sigmoidal function",
        "Bayesian linear regression with sparse priors",
        "PAC-Bayesian supervised classification: the thermodynamics of statistical learning",
        "Consistency of variational bayes inference for estimation and model selection in mixtures",
        "A generalization bound for online variational inference",
        "Consistency of elbo maximization for model selection",
        "Generative adversarial nets",
        "Deep Learning",
        "Practical variational inference for neural networks",
        "Deep neural network approximation theory",
        "Inconsistency of Bayesian inference for misspeciﬁed linear models, and a proposal for repairing it",
        "A primer on pac-bayesian learning",
        "On the minimax optimality and superiority of deep neural network learning over sparse parameter spaces",
        "Deep neural networks learn non-smooth functions effectively",
        "Fast generalization error bound of deep learning from a kernel perspective",
        "Adaptivity of deep relu network for learning in besov and mixed smooth besov spaces: optimal rate and curse of dimensionality",
        "Local minima and plateaus in hierarchical structures of multilayer perceptrons",
        "Expectation propagation for approximate bayesian inference",
        "Mastering the game of go without human knowledge",
        "No bad local minima: Data independent training error guarantees for multilayer neural networks",
        "Dropout: A simple way to prevent neural networks from over-ﬁtting",
        "A convergence theory for deep learning via over-parameterization",
        "Gradient descent ﬁnds global minima of deep neural networks",
        "Uncertainty in Deep Learning",
        "Bayesian learning for neural networks",
        "PAC-Bayesian approach to spectrally-normalized margin bounds for neural networks",
        "On the loss landscape of a class of deep neural networks with no bad local valleys",
        "Spike and slab variational inference for multi-task and multiple kernel learning",
        "Variational sparse coding",
        "Introduction to Nonparametric Estimation",
        "Understanding priors in Bayesian neural networks at the unit level",
        "Frequentist consistency of variational Bayes",
        "Error bounds for approximations with deep relu networks",
        "Understanding deep learning requires rethinking generalization",
        "Convergence rates of variational posterior distributions",
        "Connection between concentration and consistency"
    ]
}
