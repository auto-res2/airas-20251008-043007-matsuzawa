
Input:
You are an expert in academic paper analysis. 
Your task is to extract reference paper titles from the full text of research papers.

Instructions:
- Analyze the provided full text of research papers
- Extract all reference paper titles mentioned in the text
- Focus on titles that appear in reference sections, citations, or are explicitly mentioned as related work
- Return only the exact titles as they appear in the text
- Exclude general topics or field names that are not specific paper titles
- If no clear reference titles are found, return an empty list

Full Text:
---------------------------------
EcoTTA: Memory-Efficient Continual Test-time Adaptation via Self-distilled Regularization Junha Song1,2* , Jungsoo Lee 1, In So Kweon 2, Sungha Choi 1‚Ä† 1Qualcomm AI Research‚Ä°, 2KAIST Abstract This paper presents a simple yet effective approach that improves continual test-time adaptation (TTA) in a memory- efficient manner. TTA may primarily be conducted on edge devices with limited memory, so reducing memory is cru- cial but has been overlooked in previous TTA studies. In addition, long-term adaptation often leads to catastrophic forgetting and error accumulation, which hinders apply- ing TTA in real-world deployments. Our approach con- sists of two components to address these issues. First, we present lightweight meta networks that can adapt the frozen original networks to the target domain. This novel archi- tecture minimizes memory consumption by decreasing the size of intermediate activations required for backpropaga- tion. Second, our novel self-distilled regularization controls the output of the meta networks not to deviate significantly from the output of the frozen original networks, thereby preserving well-trained knowledge from the source domain. Without additional memory, this regularization prevents er- ror accumulation and catastrophic forgetting, resulting in stable performance even in long-term test-time adaptation. We demonstrate that our simple yet effective strategy out- performs other state-of-the-art methods on various bench- marks for image classification and semantic segmentation tasks. Notably, our proposed method with ResNet-50 and WideResNet-40 takes 86% and 80% less memory than the recent state-of-the-art method, CoTTA. 1. Introduction Despite recent advances in deep learning [15, 24, 23, 22], deep neural networks often suffer from performance degra- dation when the source and target domains differ signifi- cantly [8, 43, 38]. Among several tasks addressing such domain shifts, test-time adaptation (TTA) has recently re- ceived a significant amount of attention due to its practi- cality and wide applicability especially in on-device set- *Work done during an internship at Qualcomm AI Research. ‚Ä†Corresponding author. ‚Ä° Qualcomm AI Research is an initiative of Qualcomm Technologies, Inc. Memory (MB) CIFAR100-C  Error (%) CIFAR10-C  Error (%) ResNet-50 WideResNet-40 CoTTA SWR&NSP TTT++ Con6nual TENT Single domain TENT EATA CoTTA SWR&NSP TTT++ EATA         NOTE Memory (MB)   0 450 900 1350 1800 Param Ac6va6on 86% 72% 0 100 200 300 400 Param Ac6va6on TENT/EATA  CoTTA  Ours 80% 59% (a) (b) Con6nual TENT Single domain TENT             ResNet-50 WideResNet-40 Ours (K=4) Ours (K=5) ÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩ ÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩ Figure 1. (a) Memory cost comparison between TTA methods. The size of activations, not the parameters, is the primary mem- ory bottleneck during training. (b) CIFAR-C adaptation perfor- mance. We perform the continual online adaptation on CIFAR-C dataset. The x- and y-axis are the average error of all corruptions and the total memory consumption including the parameters and activations, respectively. Our approach, EcoTTA, achieves the best results while consuming the least amount of memory, where K is the model partition factor used in our method. tings [65, 42, 32, 16]. This task focuses on adapting the model to unlabeled online data from the target domain with- out access to the source data. While existing TTA methods show improved TTA per- formances, minimizing the sizes of memory resources have been relatively under-explored, which is crucial considering the applicability of TTA in on-device settings. For example, several studies [66, 42, 9] update entire model parameters 1 arXiv:2303.01904v4  [cs.CV]  23 May 2023TENTùê∑! EATAùê∑! weight regularization: freeze: update: meta networks (Ours) randomrestoration CoTTA Ours (EcoTTA)ùê∑!ùê∑!transformmovingaverage bnbnbnconv blockmain networksmain networkssource modelmain networksconv blockconv block teacher modelsource modelùê∑!:unlabeledonlinetestdata ‚Ä¶bn ‚Ä¶bn Figure 2. Architecture for test-time adaptation. We illustrate TTA methods: TENT [65], EATA [50], CoTTA [66], and Ours (EcoTTA). TENT and EATA update multiple batch norm layers, in which large activations have to be stored for gradient calculation. In CoTTA, an entire network is trained with additional strategies for continual adaptation that requires a significant amount of both memory and time. In contrast, our approach requires a minimum size of activations by updating onlya few layers. Also, stable long-term adaptation is performed by our proposed regularization, named self-distilled regularization. to achieve large performance improvements, which may be impractical when the available memory sizes are limited. Meanwhile, several TTA approaches update only the batch normalization (BN) parameters [65, 50, 17] to make the optimization efficient and stable However, even updating only BN parameters is not memory efficient enough since the amount of memory required for training models signifi- cantly depends on the size of intermediate activations rather than the learnable parameters [4, 14, 69]. Throughout the paper, activations refer to the intermediate features stored during the forward propagation, which are used for gradi- ent calculations during backpropagation. Fig. 1 (a) demon- strates such an issue. Moreover, a non-trivial number of TTA studies assume a stationary target domain [65, 42, 9, 57], but the target do- main may continuously change in the real world (e.g., con- tinuous changes in weather conditions, illuminations, and location [8] in autonomous driving). Therefore, it is nec- essary to consider long-term TTA in an environment where the target domain constantly varies. However, there exist two challenging issues: 1) catastrophic forgetting [66, 50] and 2) error accumulation. Catastrophic forgetting refers to degraded performance on the source domain due to long- term adaptation to target domains [66, 50]. Such an issue is important since the test samples in the real world may come from diverse domains, including the source and tar- get domains [50]. Also, since target labels are unavailable, TTA relies on noisy unsupervised losses, such as entropy minimization [19], so long-term continual TTA may lead to error accumulation [75, 2]. To address these challenges, we propose memory- Efficient continual Test-Time Adaptation (EcoTTA), a sim- ple yet effective approach for 1) enhancing memory effi- ciency and 2) preventing catastrophic forgetting and error accumulation. First, we present a memory-efficient archi- tecture consisting of frozen original networks and our pro- posed meta networks attached to the original ones. During the test time, we freeze the original networks to discard the intermediate activations that occupy a significant amount of memory. Instead, we only adapt lightweight meta networks to the target domain, composed of only one batch normal- ization and one convolution block. Surprisingly, updating only the meta networks, not the original ones, can result in significant performance improvement as well as consider- able memory savings. Moreover, we propose a self-distilled regularization method to prevent catastrophic forgetting and error accumulation. Our regularization leverages the pre- served source knowledge distilled from the frozen original networks to regularize the meta networks. Specifically, we control the output of the meta networks not to deviate from the one extracted by the original networks significantly. No- tably, our regularization leads to negligible overhead be- cause it requires no extra memory and is performed in par- allel with adaptation loss, such as entropy minimization. Recent TTA studies require access to the source databe- fore model deployments[42, 9, 34, 1, 40, 50]. Similarly, our method uses the source data to warm up the newly attached meta networks for a small number of epochs before model deployment. If the source dataset is publicly available or the owner of the pre-trained model tries to adapt the model to a target domain, access to the source data is feasible [9]. Here, we emphasize that pre-trained original networks are frozen throughout our process, and our method is applicable to any pre-trained model because it is agnostic to the archi- tecture and pre-training method of the original networks. Our paper presents the following contributions: ‚Ä¢ We present novel meta networks that help the frozen original networks adapt to the target domain. This architecture significantly minimize memory consump- tion up to 86% by reducing the activation sizes of the original networks. ‚Ä¢ We propose a self-distilled regularization that controls the output of meta networks by leveraging the output of frozen original networks to preserve the source knowl- edge and prevent error accumulation. ‚Ä¢ We improve both memory efficiency and TTA perfor- mance compared to existing state-of-the-art methods on 1) image classification task ( e.g., CIFAR10/100-C and ImageNet-C) and 2) semantic segmentation task (e.g., Cityscapes with weather corruption) 22. Related Work Mitigating domain shift. One of the fundamental issues of DNNs is the performance degradation due to the domain shift between the train (i.e. source) and test (i.e. target) dis- tributions. Several research fields attempt to address this problem, such as unsupervised domain adaptation [64, 6, 53, 56, 46, 58] and domain generalization [76, 8]. In par- ticular, domain generalization aims to learn invariant rep- resentation so as to cover the possible shifts of test data. They simulate the possible shifts using a single or multiple source dataset [76, 74, 39] or force to minimize the depen- dence on style information [52, 8]. However, it is challeng- ing to handle all potential test shifts using the given source datasets [20]. Thus, instead of enhancing generalization ability during the training time, TTA [65] overcomes the domain shift by directly adapting to the test data. Test-time adaptation. Test-time adaptation allows the model to adapt to the test data ( i.e., target domain) in a source-free and online manner [33, 62, 65]. Existing works improve TTA performance with sophisticated designs of un- supervised loss [48, 72, 42, 9, 45, 57, 5, 16, 1, 3, 12, 59] or enhance the usability of small batch sizes [36, 70, 31, 51, 40] considering streaming test data. They focus on improv- ing the adaptation performance with a stationary target do- main (i.e., single domain TTA setup). In such a setting, the model that finished adaptation to a given target domain is reset to the original model pre-trained with the source do- main in order to adapt to the next target domain. Recently, CoTTA [66] has proposed continual TTA setup to address TTA under a continuously changing target do- main which also involves a long-term adaptation. This setup frequently suffers from error accumulation [75, 2, 63] and catastrophic forgetting [66, 35, 50]. Specifically, perform- ing a long-term adaptation exposes the model to unsuper- vised loss from unlabeled test data for a long time, so er- rors are accumulated significantly. Also, the model focuses on learning new knowledge and forgets about the source knowledge, which becomes problematic when the model needs to correctly classify the test sample as similar to the source distribution. To address such issues, CoTTA [66] randomly restores the updated parameters to the source one, while EATA [50] proposed a weight regularization loss. Efficient on-device learning. Since the edge device is likely to be memory constrained ( e.g., a Raspberry Pi with 512MB and iPhone 13 with 4GB), it is necessary to take account of the memory usage when deploying the models on the device [41]. TinyTL [4], a seminal work in on- device learning, shows that the activation size, not learn- able parameters, bottlenecks the training memory. Follow- ing this, recent on-device learning studies [4, 68, 69] target- ing fine-tuning task attempt to decrease the size of interme- diate activations. In contrast, previous TTA studies [65, 50] have overlooked these facts and instead focused on reduc- ing learnable parameters. This paper, therefore, proposes a method that not only reduces the high activation sizes re- quired for TTA, but also improves adaptation performance. 3. Approach Fig. 3 illustrates our simple yet effective approach which only updates the newly added meta networks on the tar- get domain while regularizing them with the knowledge distilled from the frozen original network. This section describes how such a design promotes memory efficiency and prevents error accumulation and catastrophic forgetting which are frequently observed in long-term adaptation. 3.1. Memory-efficient Architecture Prerequisite. We first formulate the forward and the back- ward propagation. Assume that the ith linear layer in the model consists of weight W and bias b, and the input and output features of this layer are fi and fi+1, respectively. Given that the forward propagation of fi+1 = fiW + b, the backward propagation from the i+1th layer to the ith layer, and the weight gradient are respectively formulated as: ‚àÇL ‚àÇfi = ‚àÇL ‚àÇfi+1 WT , ‚àÇL ‚àÇW = fT i ‚àÇL ‚àÇfi+1 . (1) Eq. (1) means that the learnable layers whose weight W need to be updated must store intermediate activations fi to compute the weight gradient. In contrast, the backward propagation in frozen layers can be accomplished without saving the activations, only requiring its weightW. Further descriptions are provided in Appendix A. TinyTL [4] shows that activations occupy the majority of the memory required for training the model rather than learnable parameters. Due to this fact, updating the entire model (e.g., CoTTA [66]) requires a substantial amount of memory. Also, updating only parameters in batch normal- ization (BN) layers (e.g., TENT [65] and EATA [50]) is not an effective approach enough since they still save the large intermediate activations for multiple BN layers. While pre- vious studies fail to reduce memory by utilizing large ac- tivations, this work proposes a simple yet effective way to reduce a significant amount of memory by discarding them. Before deployment. As illustrated in Fig. 3 (a, b), we first take a pre-trained model using any pre-training method. We divide the encoder of the pre-trained model into K number of parts and attach lightweight meta networks to each part of the original network. The details of how to divide the model into K number of parts are explained in the next sec- tion. One group of meta network composes of one batch normalization layer and one convolution block ( i.e., Conv- BN-Relu). Before the deployment, we pre-train the meta networks on the source dataset Ds for a small number of 3: backpropagation: freeze: update: meta networks (Ours)ùê∑!:labeled source dataùê∑":unlabeled online test data Our proposed method (EcoTTA)Anypre-training method(a) partition (K=3)(b) attach and warm up meta networks Any pre-trained model (c) test-time adaptation Deploy=‚Äñùë•%#-ùë•#‚Äñ$ cross entropy ùê∑! K=3 entropy min. input convencoderclassifier ùë•%#%$ ùë•# convblockbn ùë•%# ùë•%#%$ ùë•#bn ùë•%# ùê∑" self-distilled reg. convblock Figure 3. Overview of our approach. (a) The encoder of the pre-trained model is divided into K parts (i.e., model partition factor K). (b) Before deployment, the meta networks are attached to each part of the original networks and pre-trained with source dataset Ds. (c) After the model is deployed, only the meta networks are updated with unsupervised loss (i.e., entropy minimization) on target data Dt, while the original networks are frozen. To avoid error accumulation and catastrophic forgetting by the long-term adaptation, we regularize the output Àúxk of each group of the meta networks leveraging the output xk of the frozen original network, which preserves the source knowledge. epochs (e.g., 10 epochs for CIFAR dataset) while freezing the original networks. Such a warm-up process is com- pleted before the model deployment, similarly done in sev- eral TTA works [9, 34, 40, 50]. Note that we do not require source dataset Ds during test time. Pre-trained model partition. Previous TTA studies ad- dressing domain shifts [9, 48] indicate that updating shal- low layers is more crucial for improving the adaptation per- formance than updating the deep layers. Inspired by such a finding, given that the encoder of the pre-trained model is split into model partition factor K ( e.g., 4 or 5), we par- tition the shallow parts of the encoder more ( i.e., densely) compared to the deep parts of it. Table 4c shows how per- formance changes as we vary the model partition factor K. After deployment. During the test-time adaptation, we only adapt meta networks to target domains while freezing the original networks. Following EATA [50], we use the entropy minimization H(ÀÜy) = ‚àíP c p(ÀÜy) logp(ÀÜy) to the samples achieving entropy less than the pre-defined entropy threshold H0, where ÀÜy is the prediction output of a test im- age from test dataset Dt and p(¬∑) is the softmax function. Thus, the main task loss for adaptation is defined as Lent = I{H(ÀÜy)<H0} ¬∑ H(ÀÜy), (2) where I{¬∑} is an indicator function. In addition, in order to prevent catastrophic forgetting and error accumulation, we apply our proposed regularization loss Rk, which is de- scribed next in detail. Consequently, the overall loss of our method is formulated as, Ltotal Œ∏ = Lent Œ∏ + Œª KX k Rk Œ∏k , (3) where Œ∏ and Œ∏k denotes parameters of all meta networks and those of k-th group of meta networks, respectively, and Œª is used to balance the scale of the two loss functions. Note that our architecture requires less memory than pre- vious works [66, 65] since we use frozen original networks and discard its intermediate activations. To be more spe- cific, our architecture uses 82% and 60% less memory on average than CoTTA and TENT/EATA. 3.2. Self-distilled Regularization The unsupervised loss from unlabeled test data Dt is likely to provide a false signal ( i.e., noise) to the model (ÀÜy Ã∏= yt where yt is the ground truth test label). Previ- ous works have verified that long-term adaptation with un- supervised loss causes overfitting due to error accumula- tion [75, 2] and catastrophic forgetting [66, 35]. To prevent the critical issues, we propose a self-distilled regularization utilizing our architecture. As shown in Fig. 3, we regularize the output Àúxk of each k-th group of the meta networks not to deviate from the outputxk of the k-th part of frozen orig- inal networks. Our regularization loss which computes the mean absolute error (i.e., L1 loss) is formulated as follows: Rk Œ∏k = ‚à•Àúxk ‚àí xk‚à•1 . (4) Since the original networks are not updated, the output xk,k‚àºK extracted from them can be considered as contain- ing the knowledge learned from the source domain. Taking advantage of this fact, we let the output of meta networks Àúxk be regularized with knowledge distilled from the origi- nal networks. By preventing the adapted model to not sig- nificantly deviate from the original model, we can prevent 1) catastrophic forgetting by maintaining the source domain knowledge and 2) error accumulation by utilizing the class discriminability of the original model. Remarkably, unlike previous works [66, 50], our regularization does not require saving additional original networks, which accompanies ex- tra memory usage. Moreover, it only needs a negligible 4WideResNet-40 (AugMix) WideResNet-28 ResNet-50Method Avg. err‚Üì Mem. (MB) Avg. err‚Üì Mem. (MB) Avg. err‚Üì Mem. (MB) Source 36.7 11 43.5 58 48.8 91BN Stats Adapt [49] 15.4 11 20.9 58 16.6 91Single do. TENT [65] 12.7 188 19.2 646 15.0 925Continual TENT 13.3 188 20.0 646 15.2 925TTT++ [42] 14.6 391 20.3 1405 16.1 1877SWR&NSP [9] 12.1400 17.2 1551 15.4 1971NOTE [17] 13.4 188 20.2 646 - -EATA [50] 13.0 188 18.6 646 14.2 925CoTTA [66] 14.0 409 17.0 1697 14.4 2066Ours (K=4)12.2 80(80, 58%‚Üì) 16.9404(76, 38%‚Üì) 14.4296(86, 68%‚Üì) Ours (K=5)12.1 92(77, 51%‚Üì) 16.8471(72, 27%‚Üì) 14.1498(76, 46%‚Üì) (a) CIFAR10-C with severity level 5 WideResNet-40 (AugMix) ResNet-50Method Avg. err‚Üì Mem. (MB) Avg. err‚Üì Mem. (MB) Source 69.7 11 73.8 91BN Stats Adapt [49] 41.1 11 44.5 91Single do. TENT [65] 36.7 188 40.1 926Continual TENT 38.3 188 45.9 926TTT++ [42] 41.0 391 44.2 1876SWR&NSP [9] 36.6 400 44.1 1970NOTE [17] 42.8 188 - -EATA [50] 37.1 188 39.9 926CoTTA [66] 38.1 409 40.2 2064Ours (K=4)36.4 80(80, 58%‚Üì) 39.5296(86, 68%‚Üì) Ours (K=5)36.3 92(77, 51%‚Üì) 39.3498(76, 46%‚Üì) (b) CIFAR100-C with severity level 5 Table 1. Comparison of error rate ( %) on CIFAR-C. We report an average error of 15 corruptions on continual TTA and a memory requirement including model parameters and activation sizes. The lowest error is in bold, and the second lowest error is underlined. The memory reduction rates compared to CoTTA and TENT are presented sequentially. WideResNet-40 was pre-trained with AugMix [26] that is a data processing to increase the robustness of the model. Source denotes the pre-trained model without adaptation. Single domain (in short, single do.) TENT resets the model when adapting to a new target domain, so the domain labeles are required. ResNet-50 (AugMix) ResNet-50(MB)Total Mem.‚ÜìMethod Avg. err‚Üì Avg. err‚Üì Source 74.36 82.35 91BN Stats Adapt [49] 57.87 72.18 91Continual TENT [65] 56.1 (0.6) 66.2 (1.1) 1486EATA [50] 54.9 (2.3) 63.8 (2.7) 1486CoTTA [66] 54.6(3.9) 62.6(3.1) 3132Ours (K=4) 55.2 (3.0) 64.6 (3.2)438(86, 72%‚Üì) Ours (K=5) 54.4(2.7) 63.4(3.0) 747(75, 51%‚Üì) Table 2. Comparison of error rate ( %) on ImageNet-C with severity level 5. Standard deviation for ten diverse corruption se- quences is denoted by the parentheses values. The total memory refers to the sum of model parameters and activations. Avg. err (%) CIFAR10-C CIFAR100-CMethod Mem. (MB)single do. continual single do. continual BN Stats Adapt [49] 91 16.6 16.6 44.5 44.5TinyTL‚Ä†[4] 379 15.8 21.9 40.5 77.4RepNet‚Ä†[69] 508 15.2 20.9 41.5 52.1AuxAdapt‚Ä†[73] 207 16.0 16.7 44.0 45.8Ours (K=4) 296 14.4 14.4 39.5 39.2 Table 3. Comparison with methods for on-device learning. The backbone is ResNet-50. ‚Ä† denotes our own re-implemented mod- els. single do. indicates the singe domain TTA setup. amount of computational overhead because it is performed in parallel with the entropy minimization loss Lent. 4. Classification Experiments We evaluate our approach to image classification tasks based on the continual test-time adaptation setup with three datasets: CIFAR10-C, CIFAR100-C, and ImageNet-C. Experimental setup. Following CoTTA [66], we conduct most experiments on the continual TTA task, where we continually adapt the deployed model to each corruption type sequentially without resetting the model. This task is more challenging but more realistic than single domain TTA task [65] in which the adapted model is periodically reset to the original pre-trained model after finishing adaptation to each target, so they require additional domain information. Moreover, we evaluate our approach on the long-term TTA setup, which is detailed in Section 4.2. Following the previous TTA studies [65, 66], we eval- uate models with {CIFAR10, CIFAR10-C}, {CIFAR100, CIFAR100-C}, and {ImageNet, ImageNet-C } where the first and the second dataset in each bracket refers to the source and the target domain, respectively. The target do- mains include 15 types of corruptions ( e.g. noise, blur, weather, and digital) with 5 levels of severity, which are widely used in conventional benchmarks [25]. Implementation Details. We evaluate our approach within the frameworks officially provided by previous state-of- the-art methods [66, 50]. For fair comparisons, we use the same pre-trained model, which are WideResNet-28 and WideResNet-40 [71] models from the RobustBench [11], and ResNet-50 [24] model from TTT++ [42, 9]. Before the deployment, we pre-train the meta networks on the source dataset using a cross-entropy loss with SGD optimizer with the learning rate of 5e-2. Since the meta networks contain only a few layers, we pre-train them with a small number of epochs: 10 and 3 epochs for CIFAR and ImageNet, re- spectively. After deployment, similar to EATA [50], we use the same SGD optimizer with the learning rate of 5e-3. In Eq. (2), the entropy threshold H0 is set to 0.4 √ó ln C where C denotes the number of task classes. The batch size is 64 and 32 for CIFAR and ImageNet, respectively. We set the importance of the regularization Œª in Eq. (3) to 0.5 to balance it with the entropy minimization loss. Additional implementation details can be found in Appendix C. Evaluation Metric. For all the experiments, we report error rates calculated during testing and the memory consump- tion, including the model parameter and the activation stor- 5Ours(i) (ii) (iii)(iv)(v)(vi) =‚Äñ			-			‚Äñ! convbn convbn bnconv CBAMSE : update conv (a) Visualization of networks variants Avr. errArch CIFAR10-CWRN-28CIFAR10-CWRN-40CIFAR100-CWRN-40(i) 18.1 12.637.2(ii) Ours w\o BN 18.7 13.7 38.2(iii) Ours w\o Conv 20.7 14.9 40.1(iv) Conv 60.6 73.3 77.2(v) CBAM [67] 21.4 15.1 40.9(vi) SE [30] 22.3 16.2 40.5Ours 16.812.136.3 (b) Meta network design (K=5) Model #Block Avg. err WRN-28 (12)CIFAR10-C 3,3,3,3 17.34,4,2,2 17.92,2,4,416.9 WRN-40 (18)CIFAR10-C 4,4,5,5 12.86,6,3,3 13.73,3,6,612.2 WRN-40 (18)CIFAR100-C 4,4,5,5 36.96,6,3,3 38.53,3,6,636.4 (c) # of blocks of each partition (K=4) Table 4. Architecture ablation experiments. (a,b) We compare continual TTA performance on several memory-efficient designs. WRN refers to WideResNet [71] backbone. (c) We report the performance based on different designs of partitioning the model. The value next to the backbone‚Äôs name denotes the total number of residual blocks of a model. age. We demonstrate the memory efficiency of our work by using the official code provided by TinyTL [4]. 4.1. Comparisons Comparisons with TTA methods. We compare our ap- proach to competing TTA methods on extensive bench- marks and various pre-trained models. The results of CIFAR10/100-C are detailed in Table 1. The model par- tition factor K are set to 4 and 5. Our approach outperforms existing TTA methods with the lowest memory usage in all pre-trained models. Specifically, in WideResNet-40, our method achieves superior performance while requiring 80% and 58% less memory than CoTTA [66] and EATA [50], re- spectively, which are also designed for continual TTA. Ap- proaches targeting single domain TTA [65, 42, 9] show poor performance due to error accumulation and catastrophic for- getting, as observed in CoTTA. The error rates for each cor- ruption type are provided in Appendix F. Table 2 shows the experiment for ImageNet-C. Two ResNet-50 backbones from RobustBench [11] are lever- aged. Following CoTTA, evaluations are conducted on ten diverse corruption-type sequences. We achieve comparable performance to CoTTA while utilizing 86% and 75% less memory with K=4 and 5, respectively. In addition, we ob- serve that our approach shows superior performance when adopting the model pre-trained with strong data augmenta- tion methods (e.g., Augmix [26]). Comparisons with on-device learning methods.We com- pare our approach with methods for memory-efficient on- device learning. TinyTL [4] and RepNet [69] focus on su- pervised on-device learning ( i.e., requiring labeled target data). However, since TTA assumes that we do not have access to the target labels, utilizing such methods to TTA di- rectly is infeasible. Therefore, we experimented by replac- ing supervised loss ( i.e., cross-entropy) with unsupervised loss (i.e., entropy minimization) in TinyTL and RepNet. As shown in Table 3, they suffer from performance degradation in continual TTA, showing inferior performance compared to our proposed approach even in the single domain TTA. Memory (MB) Avg. error (%)  (0.8%)              CIFAR100-C                      WideResNet-40 K=1 K=2 (3.7%) K=3 (4.3%) K=4 (10.8%) K=5 (11.3%) K=6 (12.8%) K=7 (13.3%) ÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩ Figure 4. Ablation study of K. We uniformly divide the encoder of the pre-trained model into the model partition factor K. The x- axis indicates the memory size including both model parameter size and activation size while the y-axis indicates the average error rate. The values in parentheses show the rate of increase for the model parameters compared to the original model. Similar to ours, AuxAdapt [73] adds and updates a small network ( i.e., ResNet-18) while freezing the pre-trained model. Unlike our approach, they only modify a prediction output, not intermediate features. While AuxAdapt requires the least memory usage, it fails to improve TTA performan- ce in single domain TTA. Nevertheless, since the original model is frozen, it suffers less from catastrophic forgetting and error accumulation than TinyTL [4] and RepNet [69] in the continual TTA. Through the results, we confirm that our proposed method brings both memory efficiency and a significant performance improvement in both TTA setups. 4.2. Empirical Study Architecture design. An important design of our meta net- works is injecting a single BN layer before the original net- works and utilizing a residual connection with one conv block. Table 4b studies the effectiveness of the proposed design by comparing it with six different variants. From the results, we observe that using only either conv block (ii) or BN (iii) aggravates the performance: error rate increases by 1.4% and 3.8% on CIFAR-100-C with WideResNet-40. In design (i), we enforce both BN parameters and Conv layers in the meta networks to take the output of the origi- nal networks as inputs. Such a design brings performance drop. We speculate that it is because the original network, 6Gaus.ShotImpu.Defo.Glas.Moti.ZoomSnowFros.Fog Brig.Cont.Elas.Pixe.Jpeg 27.5 30.0 32.5 35.0 37.5 40.0 42.5 45.0 25 26 27 28 29 30 31                                Corruption type in 1 round                                Ours (Clean) TENT (Clean) Ours (Corrupt) ÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩ (a) Catastrophic forgetting effect Corrup&on Error (%) Round (b) Error accumulation effect Figure 5. Regularization ablation experiments. We conduct experiments with WideResNet-40 on CIFAR100-C. (a) We utilize a test set of the CIFAR-100 dataset to measure clean error after adapting to each corruption. Maintaining clean errors at a stable level indicates that our approach helps the model robust to catastrophic forgetting. (b) We simulate a long-term adaptation scenario by repeating 100 rounds of 15 corruption sequences. In the absence of regularization, error accumulation can lead to overfitting (i.e., the case of the error increases exponentially). However, our approach does not suffer from such an error accumulation. We set K to 5 in the above experiments. Batch size 16 8 4 2 1 Non training Source 69.7 69.7 69.7 69.7 69.7 BN Stats Adapt [49] 41.1 50.2 59.9 81.0 99.1 AdaptBN [55] 39.1 41.2 45.2 49.0 54.0 Training Con. TENT [65] 40.9 47.8 58.6 82.2 99.0 Con. TENT+AdaptBN 38.2 40.2 43.2 47.7 52.2 Ours (K=5) 40.0 45.8 63.4 80.8 99.0 Ours (K=5)+AdaptBN36.9 39.3 42.2 46.5 51.8 Table 5. Experiments with small batch sizes. We evaluate all baselines with WideResNet-40 on CIFAR100-C. Con. TENT is the abbreviation for continual TENT. which is not adapted to the target domain, lacks the ability to extract sufficiently meaningful features from the target image. Also, we observed a significant performance degra- dation after removing the residual connection in design (iv). In addition, since attention mechanisms [67, 30] generally have improved classification accuracy, we study how atten- tion mechanisms can further boost TTA performance of our approach in design (v, vi). The results show that it is diffi- cult for the attention module to train ideally in TTA setup using unsupervised learning, unlike when applying it to su- pervised learning. An ablation study on each element of meta networks can be found in Appendix D. Number of blocks in each partition. ResNet [24] consists of multiple residual blocks (e.g., BasicBlock and Bottleneck in Pytorch [54]). For instance, WideResNet-28 has 12 resid- ual blocks. By varying the number of blocks for each part of the original networks, we analyze TTA performance in Ta- ble 4c. We observe that splitting the shallow parts of the en- coder densely (e.g., 2,2,4,4 blocks, from the shallow to the deep parts sequentially) brings more performance gain than splitting the deep layers densely ( e.g., 4,4,2,2 blocks). We suggest that it is because we modify the lower-level feature more as we split shallow layers densely. Our observation is aligned with the finding of previous TTA works [9, 48], which show that updating the shallow layers more than the deep layers improves TTA performance. Number of model partition K. Fig. 4 shows both memory requirement and adaptation performance according to the model partition factor K. With a small K ( e.g., 1 or 2), the intermediate outputs are barely modified, making it difficult to achieve a reasonable level of performance. We achieve the best TTA performance with K of 4 or 5 as adjusting a greater numver of intermediate features. In the meanwhile, we observe that the average error rate is saturated and re- mains consistent when K is set to large values (e.g. 6,7 or 8) even with the increased amount of activations and learnable parameters. Therefore, we set K to 4 and 5. Catastrophic forgetting. We conduct experiments to con- firm the catastrophic forgetting effect (Fig. 5a). Once fin- ishing adaptation to each corruption, we evaluate the model on clean target data ( i.e., test-set of CIFAR dataset) with- out updating the model. For TENT with no regulariza- tion, the error rates for the clean target data ( i.e., clean er- ror (%)) increase gradually, which can be seen as the phe- nomenon of catastrophic forgetting. In contrast, our ap- proach consistently maintains the error rates for the clean target data, proving that our regularization loss effectively prevents catastrophic forgetting. These results indicate that our method can be reliably utilized in various domains, in- cluding the source and target domains. Error accumulation in long-term adaptation. To evalu- ate the error accumulation effect, we repeat all the corrup- tion sequences for 100 rounds. The results are described in Fig. 5b. For TENT, a gradual increase in error rates is ob- served in later rounds, even with small learning rates. For example, TENT [65] with the learning rate of 1e-5 achieves the error rate of 39.7%, and reached its lowest error rate of 36.5% after 8 rounds. However, it shows increased error rate of 38.6% after 100 rounds due to overfitting. It suggests 7Time t‚àí ‚àí ‚àí ‚àí ‚àí ‚àí ‚àí ‚àí ‚àí ‚àí ‚àí ‚àí ‚àí ‚àí ‚àí ‚àí ‚àí ‚àí ‚àí ‚àí ‚àí ‚àí ‚àí ‚àí ‚àí ‚àí ‚àí ‚àí ‚àí ‚àí ‚àí ‚àí ‚àí ‚àí ‚àí ‚àí ‚àí ‚àí ‚àí ‚àí ‚àí ‚àí ‚àí ‚àí ‚àí ‚àí ‚àí ‚àí ‚àí ‚àí ‚àí ‚àí ‚àí ‚àí ‚àí ‚àí ‚àí ‚àí ‚àí ‚àí ‚àí ‚àí ‚àí ‚àí ‚àí ‚àí ‚àí ‚àí ‚àí ‚àí ‚àí ‚àí ‚àí ‚àí ‚àí ‚àí ‚àí ‚àí ‚àí ‚àí ‚àí ‚ÜíRound 1 4 7 10 AllMethod Mem. (MB)Brig. Fog Fros. SnowBrig. Fog Fros. SnowBrig. Fog Fros. SnowBrig. Fog Fros. SnowMean Source 280 60.4 54.3 30.0 4.160.4 54.3 30.0 4.160.4 54.3 30.0 4.160.4 54.3 30.0 4.137.2BN Stats Adapt [49]280 69.1 61.0 44.8 39.169.1 61.0 44.8 39.169.1 61.0 44.8 39.169.1 61.0 44.8 39.153.6Continual TENT [65]2721 70.1 62.1 46.1 40.262.2 53.7 44.4 37.950.0 41.5 31.6 26.639.2 32.6 25.3 22.442.9Ours (K=4) 918(66%‚Üì) 70.262.446.341.970.062.846.542.270.062.846.542.170.162.846.642.255.3 Table 6. Semantic segmentation results in continual test-time adaptation tasks. We conduct experiments on Cityscapes [10] with four weather corruptions [25] applied. The four conditions are repeated ten times to simulate continual domain shifts. All results are evaluated based on DeepLabV3Plus-ResNet-50. that without regularization, TTA methods eventually face overfitting in long-term adaptation [75, 2, 35]. Our method in the absence of regularization (Œª = 0) also causes overfit- ting. On the other hand, when self-distilled regularization is involved (Œª >0), the performance remains consistent even in the long-term adaptation. Small batch size. We examine the scalability of our ap- proach with a TTA method designed for small batches size, named adapting BN statistics (i.e., AdaptBN [55, 72]). When the number of batches is too small, the estimated statistics can be unreliable [55]. Thus, they calibrate the source and target statistics for the normalization of BN lay- ers so as to alleviate the domain shift and preserve the dis- criminative structures. As shown in Table 5, training mod- els with small batch sizes (e.g., 2 or 1) generally increase the error rates. However, such an issue can be addressed by appying AdaptBN to our method. To be more sepcific, we achieve an absolute improvement of 17.9% and 2.2% from Source and AdaptBN, respectively, in the batch size of 1. Number of the source samples for meta networks. Like previous TTA works [9, 42, 34, 40] including EATA [50], our approach requires access to the source data for pre- training our proposed meta networks before model deploy- ment. In order to cope with the situation where we can only make use of a subset of the source dataset, we study the TTA performance of our method according to the number of ac- cessible source samples. The results are specified in Table 7 where we use WideResNet-40. We observe that our method outperforms the baseline model even with small number of training samples ( e.g., 10% or 20%) while showing com- parable performance with excessively small numbers ( e.g. 5%). Note that we still reduce the memory usage of about 51% compared to EATA. 5. Segmentation Experiments We investigate our approach in semantic segmentation. First, we create Cityscapes-C by applying the weather cor- ruptions (brightness, fog, frost, and snow [25]) to the vali- dation set of Cityscapes [10]. Then, to simulate continual distribution shifts, we repeat the four types of Cityscapes-C ten times. In this scenario, we conduct continual TTA using the publicly-available ResNet-50-based DeepLabV3 + [7], which is pre-trained on Cityscapes for domain generaliza- EATA [50] (188MB) # of source samples Target domain Ours(K=5) (92MB) 10k (20%) 5k (10%) 2.5k (5%) CIFAR10-C 13.0 12.1 12.4 12.9 13.1 CIFAR100-C 37.1 36.3 36.4 36.6 37.2 Table 7. Ablation of # of source samples to warm up the meta networks. Before deployment, we pre-trained the meta networks using only a subset of the source dataset (e.g., 20%, 10%, and 5%). The memory usage (MB) of each method is also presented. tion task [8] in semantic segmentation. For TTA, we use the batch size of 2. More details are specified in Appendix C. Results. We report the results based on mean intersection over union (mIoU) in Table 6. It demonstrates that our ap- proach helps to both minimize memory consumption and performs long-term adaptation stably for semantic segmen- tation. Unlike continual TENT, our method avoids catas- trophic forgetting and error accumulation, allowing us to achieve the highest mIoU score while using 66% less mem- ory usage in a continual TTA setup. Additional experiment results can be found in Appendix B. 6. Conclusion This paper proposed a simple yet effective approach that improves continual TTA performance and saves a signifi- cant amount of memory, which can be applied to edge de- vices with limited memory. First, we presented a memory- efficient architecture that consists of original networks and meta networks. This architecture requires much less mem- ory size than the previous TTA methods by decreasing the intermediate activations used for gradient calculations. Sec- ond, in order to preserve the source knowledge and prevent error accumulation during long-term adaptation with noisy unsupervised loss, we proposed self-distilled regularization that controls the output of meta networks not to deviate sig- nificantly from the output of the original networks. With ex- tensive experiments on diverse datasets and backbone net- works, we verified the memory efficiency and TTA perfor- mance of our approach. In this regard, we hope that our efforts will facilitate a variety of studies that make test-time adaptation for edge devices feasible in practice. Acknowledgments. We would like to thank Kyuwoong Hwang, Simyung Chang, and Byeonggeun Kim for their valuable feedback. We are also grateful for the helpful dis- cussions from Qualcomm AI Research teams. 8References [1] Kazuki Adachi, Shin‚Äôya Yamaguchi, and Atsutoshi Kuma- gai. Covariance-aware feature alignment with pre-computed source statistics for test-time adaptation. arXiv preprint arXiv:2204.13263, 2022. 2, 3 [2] Eric Arazo, Diego Ortego, Paul Albert, Noel E O‚ÄôConnor, and Kevin McGuinness. Pseudo-labeling and confirmation bias in deep semi-supervised learning. In IJCNN, 2020. 2, 3, 4, 8 [3] Kambiz Azarian, Debasmit Das, Hyojin Park, and Fatih Porikli. Test-time adaptation vs. training-time generaliza- tion: A case study in human instance segmentation using keypoints estimation. In WACV Workshops, 2023. 3 [4] Han Cai, Chuang Gan, Ligeng Zhu, and Song Han. Tinytl: Reduce memory, not parameters for efficient on-device learning. In NeurIPS, 2020. 2, 3, 5, 6, 12, 16 [5] Dian Chen, Dequan Wang, Trevor Darrell, and Sayna Ebrahimi. Contrastive test-time adaptation. In CVPR, 2022. 3, 13 [6] Lin Chen, Huaian Chen, Zhixiang Wei, Xin Jin, Xiao Tan, Yi Jin, and Enhong Chen. Reusing the task-specific classifier as a discriminator: Discriminator-free adversarial domain adap- tation. In CVPR, 2022. 3 [7] Liang-Chieh Chen, Yukun Zhu, George Papandreou, Florian Schroff, and Hartwig Adam. Encoder-decoder with atrous separable convolution for semantic image segmentation. In ECCV, 2018. 8, 14 [8] Sungha Choi, Sanghun Jung, Huiwon Yun, Joanne T Kim, Seungryong Kim, and Jaegul Choo. Robustnet: Improving domain generalization in urban-scene segmentation via in- stance selective whitening. In CVPR, 2021. 1, 2, 3, 8, 14, 15 [9] Sungha Choi, Seunghan Yang, Seokeon Choi, and Sun- grack Yun. Improving test-time adaptation via shift-agnostic weight regularization and nearest source prototypes. In ECCV, 2022. 1, 2, 3, 4, 5, 6, 7, 8, 13, 15, 16, 17 [10] Marius Cordts, Mohamed Omran, Sebastian Ramos, Timo Rehfeld, Markus Enzweiler, Rodrigo Benenson, Uwe Franke, Stefan Roth, and Bernt Schiele. The cityscapes dataset for semantic urban scene understanding. In CVPR, 2016. 8, 14 [11] Francesco Croce, Maksym Andriushchenko, Vikash Se- hwag, Edoardo Debenedetti, Nicolas Flammarion, Mung Chiang, Prateek Mittal, and Matthias Hein. Robustbench: a standardized adversarial robustness benchmark. In NeurIPS Datasets and Benchmarks Track, 2021. 5, 6 [12] Debasmit Das, Shubhankar Borse, Hyojin Park, Kambiz Azarian, Hong Cai, Risheek Garrepalli, and Fatih Porikli. Transadapt: A transformative framework for online test time adaptive semantic segmentation. In ICASSP, 2023. 3 [13] Zhiwei Deng and Olga Russakovsky. Remember the past: Distilling datasets into addressable memories for neural net- works. arXiv preprint arXiv:2206.02916, 2022. 13 [14] Sauptik Dhar, Junyao Guo, Jiayi Liu, Samarth Tripathi, Un- mesh Kurup, and Mohak Shah. A survey of on-device ma- chine learning: An algorithms and learning theory perspec- tive. ACM Transactions on Internet of Things, 2021. 2 [15] Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner, Mostafa Dehghani, Matthias Minderer, Georg Heigold, Syl- vain Gelly, et al. An image is worth 16x16 words: Trans- formers for image recognition at scale. In ICLR, 2021. 1 [16] Yossi Gandelsman, Yu Sun, Xinlei Chen, and Alexei A Efros. Test-time training with masked autoencoders. In NeurIPS, 2022. 1, 3 [17] Taesik Gong, Jongheon Jeong, Taewon Kim, Yewon Kim, Jinwoo Shin, and Sung-Ju Lee. Robust continual test- time adaptation: Instance-aware bn and prediction-balanced memory. In NeurIPS, 2022. 2, 5, 16, 17 [18] Priya Goyal, Piotr Doll ¬¥ar, Ross Girshick, Pieter Noord- huis, Lukasz Wesolowski, Aapo Kyrola, Andrew Tulloch, Yangqing Jia, and Kaiming He. Accurate, large mini- batch sgd: Training imagenet in 1 hour. arXiv preprint arXiv:1706.02677, 2017. 16 [19] Yves Grandvalet and Yoshua Bengio. Semi-supervised learning by entropy minimization. In NeurIPS, 2004. 2, 15 [20] Ishaan Gulrajani and David Lopez-Paz. In search of lost do- main generalization. In ICLR, 2021. 3 [21] Chuan Guo, Geoff Pleiss, Yu Sun, and Kilian Q Weinberger. On calibration of modern neural networks. In ICML, 2017. 13 [22] Kaiming He, Xinlei Chen, Saining Xie, Yanghao Li, Piotr Doll¬¥ar, and Ross Girshick. Masked autoencoders are scalable vision learners. In CVPR, 2022. 1 [23] Kaiming He, Haoqi Fan, Yuxin Wu, Saining Xie, and Ross Girshick. Momentum contrast for unsupervised visual rep- resentation learning. In CVPR, 2020. 1 [24] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image recognition. In CVPR, 2016. 1, 5, 7, 12, 16 [25] Dan Hendrycks and Thomas Dietterich. Benchmarking neu- ral network robustness to common corruptions and perturba- tions. In ICLR, 2019. 5, 8 [26] Dan Hendrycks, Norman Mu, Ekin D. Cubuk, Barret Zoph, Justin Gilmer, and Balaji Lakshminarayanan. AugMix: A simple data processing method to improve robustness and uncertainty. In ICLR, 2020. 5, 6, 14 [27] Geoffrey Hinton, Oriol Vinyals, and Jeff Dean. Distilling the knowledge in a neural network. In NeurIPS, 2014. 13 [28] Neil Houlsby, Andrei Giurgiu, Stanislaw Jastrzebski, Bruna Morrone, Quentin De Laroussilhe, Andrea Gesmundo, Mona Attariyan, and Sylvain Gelly. Parameter-efficient transfer learning for nlp. In ICML, 2019. 13 [29] Edward J Hu, yelong shen, Phillip Wallis, Zeyuan Allen- Zhu, Yuanzhi Li, Shean Wang, Lu Wang, and Weizhu Chen. Lora: Low-rank adaptation of large language models. In ICLR, 2022. 13 [30] Jie Hu, Li Shen, and Gang Sun. Squeeze-and-excitation net- works. In CVPR, 2018. 6, 7 [31] Xuefeng Hu, Gokhan Uzunbas, Sirius Chen, Rui Wang, Ashish Shah, Ram Nevatia, and Ser-Nam Lim. Mixnorm: Test-time adaptation through online normalization estima- tion. arXiv preprint arXiv:2110.11478, 2021. 3 9[32] Yusuke Iwasawa and Yutaka Matsuo. Test-time classifier ad- justment module for model-agnostic domain generalization. In NeurIPS, 2021. 1 [33] Vidit Jain and Erik Learned-Miller. Online domain adapta- tion of a pre-trained cascade of classifiers. In CVPR, 2011. 3 [34] Sanghun Jung, Jungsoo Lee, Nanhee Kim, and Jaegul Choo. Cafa: Class-aware feature alignment for test-time adaptation. arXiv preprint arXiv:2206.00205, 2022. 2, 4, 8 [35] Tommie Kerssies, Joaquin Vanschoren, and Mert Kƒ±lƒ±c ¬∏kaya. Evaluating continual test-time adaptation for contextual and semantic domain shifts. arXiv preprint arXiv:2208.08767 , 2022. 3, 4, 8 [36] Ansh Khurana, Sujoy Paul, Piyush Rai, Soma Biswas, and Gaurav Aggarwal. Sita: Single image test-time adaptation. arXiv preprint arXiv:2112.02355, 2021. 3 [37] Andreas Krause, Pietro Perona, and Ryan Gomes. Discrim- inative clustering by regularized information maximization. In NeurIPS, 2010. 15 [38] Daiqing Li, Junlin Yang, Karsten Kreis, Antonio Torralba, and Sanja Fidler. Semantic segmentation with generative models: Semi-supervised learning and strong out-of-domain generalization. In CVPR, 2021. 1 [39] Xiaotong Li, Yongxing Dai, Yixiao Ge, Jun Liu, Ying Shan, and Ling-Yu Duan. Uncertainty modeling for out-of- distribution generalization. In ICLR, 2022. 3 [40] Hyesu Lim, Byeonggeun Kim, Jaegul Choo, and Sungha Choi. TTN: A domain-shift aware batch normalization in test-time adaptation. In ICLR, 2023. 2, 3, 4, 8, 13, 16 [41] Ji Lin, Wei-Ming Chen, Yujun Lin, Chuang Gan, Song Han, et al. Mcunet: Tiny deep learning on iot devices. InNeurIPS, 2020. 3 [42] Yuejiang Liu, Parth Kothari, Bastien van Delft, Baptiste Bellot-Gurlet, Taylor Mordan, and Alexandre Alahi. Ttt++: When does self-supervised test-time training fail or thrive? In NeurIPS, 2021. 1, 2, 3, 5, 6, 8, 13, 16, 17 [43] Mingsheng Long, Han Zhu, Jianmin Wang, and Michael I Jordan. Unsupervised domain adaptation with residual trans- fer networks. In NeurIPS, 2016. 1 [44] David Lopez-Paz and Marc‚ÄôAurelio Ranzato. Gradient episodic memory for continual learning. In NeurIPS, 2017. 13 [45] Robert A Marsden, Mario D ¬®obler, and Bin Yang. Gradual test-time adaptation by self-training and style transfer. arXiv preprint arXiv:2208.07736, 2022. 3 [46] Ke Mei, Chuang Zhu, Jiaqi Zou, and Shanghang Zhang. In- stance adaptive self-training for unsupervised domain adap- tation. In ECCV, 2020. 3 [47] Rafael M ¬®uller, Simon Kornblith, and Geoffrey E Hinton. When does label smoothing help? In NeurIPS, 2019. 13 [48] Chaithanya Kumar Mummadi, Robin Hutmacher, Kilian Rambach, Evgeny Levinkov, Thomas Brox, and Jan Hendrik Metzen. Test-time adaptation to distribution shift by confi- dence maximization and input transformation.arXiv preprint arXiv:2106.14999, 2021. 3, 4, 7 [49] Zachary Nado, Shreyas Padhy, D Sculley, Alexander D‚ÄôAmour, Balaji Lakshminarayanan, and Jasper Snoek. Evaluating prediction-time batch normalization for robust- ness under covariate shift. arXiv preprint arXiv:2006.10963, 2020. 5, 7, 8, 16, 17 [50] Shuaicheng Niu, Jiaxiang Wu, Yifan Zhang, Yaofo Chen, Shijian Zheng, Peilin Zhao, and Mingkui Tan. Efficient test- time model adaptation without forgetting. In ICML, 2022. 2, 3, 4, 5, 6, 8, 12, 13, 14, 15, 16, 17 [51] Shuaicheng Niu, Jiaxiang Wu, Yifan Zhang, Zhiquan Wen, Yaofo Chen, Peilin Zhao, and Mingkui Tan. Towards stable test-time adaptation in dynamic wild world. In ICLR, 2023. 3 [52] Xingang Pan, Ping Luo, Jianping Shi, and Xiaoou Tang. Two at once: Enhancing learning and generalization capacities via ibn-net. In ECCV, 2018. 3 [53] Kwanyong Park, Sanghyun Woo, Inkyu Shin, and In So Kweon. Discover, hallucinate, and adapt: Open compound domain adaptation for semantic segmentation. In NeurIPS, 2020. 3 [54] Adam Paszke, Sam Gross, Francisco Massa, Adam Lerer, James Bradbury, Gregory Chanan, Trevor Killeen, and et al. Lin. Pytorch: An imperative style, high-performance deep learning library. In NeurIPS, 2019. 7, 12, 16 [55] Steffen Schneider, Evgenia Rusak, Luisa Eck, Oliver Bring- mann, Wieland Brendel, and Matthias Bethge. Improving robustness against common corruptions by covariate shift adaptation. In NeurIPS, 2020. 7, 8, 16 [56] Inkyu Shin, Dong-Jin Kim, Jae Won Cho, Sanghyun Woo, Kwanyong Park, and In So Kweon. Labor: Labeling only if required for domain adaptive semantic segmentation. In ICCV, 2021. 3 [57] Inkyu Shin, Yi-Hsuan Tsai, Bingbing Zhuang, Samuel Schulter, Buyu Liu, Sparsh Garg, In So Kweon, and Kuk- Jin Yoon. Mm-tta: Multi-modal test-time adaptation for 3d semantic segmentation. In CVPR, 2022. 2, 3 [58] Inkyu Shin, Sanghyun Woo, Fei Pan, and InSo Kweon. Two- phase pseudo label densification for self-training based do- main adaptation. In ECCV, 2020. 3 [59] Junha Song, Kwanyong Park, Inkyu Shin, Sanghyun Woo, Chaoning Zhang, and In So Kweon. Test-time adaptation in the dynamic world with compound domain knowledge man- agement. arXiv preprint arXiv:2212.08356, 2023. 3 [60] Nitish Srivastava, Geoffrey Hinton, Alex Krizhevsky, Ilya Sutskever, and Ruslan Salakhutdinov. Dropout: a simple way to prevent neural networks from overfitting. The journal of machine learning research, 2014. 13 [61] Qianru Sun, Yaoyao Liu, Tat-Seng Chua, and Bernt Schiele. Meta-transfer learning for few-shot learning. InCVPR, 2019. 13 [62] Yu Sun, Xiaolong Wang, Zhuang Liu, John Miller, Alexei A. Efros, and Moritz Hardt. Test-time training with self- supervision for generalization under distribution shifts. In ICML, 2020. 3 [63] Antti Tarvainen and Harri Valpola. Mean teachers are better role models: Weight-averaged consistency targets improve semi-supervised deep learning results. In NeurIPS, 2017. 3 [64] Tuan-Hung Vu, Himalaya Jain, Maxime Bucher, Mathieu Cord, and Patrick P¬¥erez. Advent: Adversarial entropy mini- 10mization for domain adaptation in semantic segmentation. In CVPR, 2019. 3 [65] Dequan Wang, Evan Shelhamer, Shaoteng Liu, Bruno Ol- shausen, and Trevor Darrell. Tent: Fully test-time adaptation by entropy minimization. In ICLR, 2021. 1, 2, 3, 4, 5, 6, 7, 8, 12, 13, 14, 16, 17 [66] Qin Wang, Olga Fink, Luc Van Gool, and Dengxin Dai. Con- tinual test-time domain adaptation. In CVPR, 2022. 1, 2, 3, 4, 5, 6, 12, 13, 14, 16, 17 [67] Sanghyun Woo, Jongchan Park, Joon-Young Lee, and In So Kweon. Cbam: Convolutional block attention module. In ECCV, 2018. 6, 7 [68] Li Yang, Adnan Siraj Rakin, and Deliang Fan. Da3: Dy- namic additive attention adaption for memory-efficient on- device multi-domain learning. In CVPR Workshops, 2022. 3 [69] Li Yang, Adnan Siraj Rakin, and Deliang Fan. Rep-net: Efficient on-device learning via feature reprogramming. In CVPR, 2022. 2, 3, 5, 6, 16 [70] Fuming You, Jingjing Li, and Zhou Zhao. Test-time batch statistics calibration for covariate shift. arXiv preprint arXiv:2110.04065, 2021. 3 [71] Sergey Zagoruyko and Nikos Komodakis. Wide residual net- works. In BMVC, 2016. 5, 6, 12 [72] Marvin Zhang, Sergey Levine, and Chelsea Finn. Memo: Test time robustness via adaptation and augmentation. In NeurIPS, 2021. 3, 8 [73] Yizhe Zhang, Shubhankar Borse, Hong Cai, and Fatih Porikli. Auxadapt: Stable and efficient test-time adaptation for temporally consistent video semantic segmentation. In WACV, 2022. 5, 6, 16 [74] Yabin Zhang, Minghan Li, Ruihuang Li, Kui Jia, and Lei Zhang. Exact feature distribution matching for arbitrary style transfer and domain generalization. In CVPR, 2022. 3 [75] Zixing Zhang, Fabien Ringeval, Bin Dong, Eduardo Coutinho, Erik Marchi, and Bj¬®orn Sch¬®uller. Enhanced semi- supervised learning for multimodal emotion recognition. In ICASSP, 2016. 2, 3, 4, 8 [76] Kaiyang Zhou, Yongxin Yang, Yu Qiao, and Tao Xiang. Do- main generalization with mixstyle. In ICLR, 2021. 3 11Appendix In this supplementary material, we provide, A. Efficiency for TTA methods B. Discussion and further experiments C. Further implementation details D. Additional ablations E. Baseline details F. Results of all corruptions A. Efficiency for TTA methods Memory efficiency. Existing TTA works [65, 50, 66] up- date model parameters to adapt to the target domain. This process inevitably requires additional memory to store the activations. Fig. 6 describes Eq. (1) of the main paper in more detail. For instance, 1) the backward propagation from the layer (c) to the layer ( b) can be accomplished without saving intermediate activations fi and fi+1, since it only re- quires ‚àÇL ‚àÇfi+1 =‚àÇL ‚àÇLWT i+1 and ‚àÇL ‚àÇfi = ‚àÇL ‚àÇfi+1 WT i =‚àÇL ‚àÇLWT i+1WT i op- erations. 2) During the forward propagation, the learnable layer (a) has to store the intermediate activation fi‚àí1 to cal- culate the weight gradient ‚àÇL ‚àÇWi‚àí1 =fT i‚àí1 ‚àÇL ‚àÇfi . : freeze: update ‚Ñíùúï‚Ñíùúï‚Ñíùúï‚Ñíùúïùëì!"#ùúï‚Ñíùúïùëì!ùúï‚Ñíùúïùëì!$# ùíáùíä$ùüè ùëì! ùëì!"#ùëä!$#,ùëè!$# ùëä!,ùëè! ùëä!"#,ùëè!"#(ùëé) (ùëè) (ùëê)ùúï‚Ñíùúïùëì!=ùúï‚Ñíùúïùëì!"#ùëä!' ùúï‚Ñíùúïùëì!"#=ùúï‚Ñíùúï‚Ñíùëä!"#'ùúï‚Ñíùúïùëì!$#=ùúï‚Ñíùúïùëì!ùëä!$#'ùúï‚Ñíùúïùëä!$#=ùíáùíä$ùüèùëªùúï‚Ñíùúïùëì! ùëì!"#=ùëì!ùëä!+ùëè!ùúï‚Ñíùúïùëè!$#=ùúï‚Ñíùúïùëì! Figure 6. Forward and backward propagation. The black and red lines refer to forward and backward propagation, respectively. f and (a, b, c) are the activations and the linear layers, respectively. Computational efficiency. Wall-clock time and floating point operations (FLOPs) are standard measures of com- putational cost. We utilize wall-clock time to compare the computational cost of TTA methods since most libraries computing FLOPs only support inference, not training. Unfortunately, wall-clock time of EATA [50] and our ap- proach can not truly represent its computational efficiency since the current Pytorch version [54] does not support fine-grained implementation [4]. For example, EATA fil- ters samples to improve its computational efficiency. How- ever, its gradient computation is performed on the full mini- batch, so the wall-clock time for backpropagation in EATA is almost the same as that of TENT [65]. In our approach, our implementation follows Algorithm 1 to make each reg- ularization loss Rk Œ∏k applied to parameters of k-th group of meta networks Œ∏k in Eq. (3). In order to circumvent such an issue, the authors of EATA report the theoretical time, which assumes that PyTorch handles gradient back- propagation at an instance level. Similar to EATA, we also report both theoretical time and wall-clock time in Ta- ble 8. To compute the theoretical time of our approach, we simply subtract the time for re-forward (in Algorithm 1) from wall-clock time. We emphasize that this is mainly an engineering-based issue, and the optimized implementation can further improve computational efficiency. [50]. Using a single NVIDIA 2080Ti GPU, we measure the total time required to adapt to all 15 corruptions, includ- ing the time to load test data and perform TTA. The results in Table 8 show that our proposed method requires neg- ligible overhead compared to CoTTA [66]. For example, CoTTA needs approximately 10 times more training time than Continual TENT [65] with WideResNet-40. Note that meta networks enable our approach to use 80% and 58% less memory than CoTTA and EATA, even with such minor extra operations. B. Discussion and further experiments Comparison on gradually changing setup. In Table 1 and Table 2, we evaluate all methods on the continual TTA task, proposed in CoTTA [66] and EATA [50], where we continually adapt the deployed model to each cor- ruption type sequentially. Additionally, we conduct ex- periments on the gradually changing setup. This grad- WideResNet-40 [71] Avg. err Mem. (MB) Theo. time Wall time Source 69.7 11 - 40s Con. TENT [65] 38.3 188 - 2m 18s CoTTA [66] 38.1 409 - 22m 52s EATA [50] 37.1 188 2m 8s 2m 22s Ours (K=4) 36.4 80(80, 58%‚Üì) 2m 27s 2m 49s Ours (K=5) 36.3 92(77, 51%‚Üì) 2m 31s 2m 52s ResNet-50 [24] Avg. err Mem. (MB) Theo. time Wall time Source 73.8 91 - 1m 8s Con. TENT [65] 45.9 926 - 4m 2s CoTTA [66] 40.2 2064 - 38m 24s EATA [50] 39.9 926 3m 45s 4m 15s Ours (K=4) 39.5 296(86, 68%‚Üì) 4m 16s 4m 41s Ours (K=5) 39.3 498(76, 46%‚Üì) 4m 26s 5m 14s Table 8. Comparison of training time on CIFAR100-C. We re- port both theoretical time (in short, theo. time) and wall-clock time, taking to adapt to all 15 corruption types. Theoretical time is calculated by assuming that the ML frameworks ( e.g., Py- torch [54]) provide fine-grained implementations [4]. Con. TENT refers to continual TENT. 12Algorithm 1: PyTorch-style pseudocode for EcoTTA. # img_t: test image # model: original and meta networks # # ent_min(): Entropy minimization loss # Detach_parts(): Detach the graph connection # between each partition of networks # Attach_parts(): Attach the graph connection # between each partition of networks for img_t in test_loader: # 1. Forward output = model(img_t) # 2. Compute entropy loss loss_ent = ent_min(output) loss_ent.backward() # 3. Re-forward # (This process is not required # in fine-grained ML frameworks.) Detach_parts(model) _ = model(img_t) # 4. Compute regularization loss reg_loss = 0 for k_th_meta in meta_networks: reg_loss += k_th_meta.get_l1_loss() reg_loss.backward() # 5. Update params of meta networks optim.step() optim.zero_grad() Attach_parts(model) ual setup, proposed in CoTTA, represents the sequence by gradually changing severity for the 15 corruption types: . . .2‚àí ‚Üí1| {z } t-1 and before change ‚àí ‚àí ‚àí ‚àí ‚àí ‚Üí type 1‚àí ‚Üí2‚àí ‚Üí3‚àí ‚Üí4‚àí ‚Üí5‚àí ‚Üí4‚àí ‚Üí3‚àí ‚Üí2‚àí ‚Üí1| {z } corruption type t, gradually changing severity change ‚àí ‚àí ‚àí ‚àí ‚àí ‚Üí type 1‚àí ‚Üí2. . .| {z } t+1 and on , The results in Table 9 indicate that our approach outper- forms previous TTA methods [65, 50, 66] even with the gradually changing setup. Comparisons with methods for parameter efficient transfer learning. While our framework may be similar to parameter-efficient transfer learning (PETL) [29, 28, 61] in that only partial parameters are updated during training time for PETL or test time for TTA, we utilized meta net- works to minimize intermediate activations, which is crucial for memory-constrained edge devices. We conduct experi- ments by applying a PETL method [28] to the TTA setup. The adapter module is constructed by using 3x3 Conv and ReLU layers as the projection layer and the nonlinearity, re- spectively, and these modules are attached after each resid- ual block of the backbone networks. The Table 10 shows that PETA+SDR needs a 177% increase in memory usage with a 6.1% drop in performance, compared to our method. Comparisons with methods for continual learning. Typ- ical continual learning (CL) and continual TTA assume su- pervised and unsupervised learning, respectively. However, since both are focused on alleviating catastrophic forget- ting, we believe that CL methods can also be applied in continual TTA settings. The methods for addressing catas- trophic forgetting can be divided into regularization- and Method Con. TENT [65] EATA [50] CoTTA [66]Ours (K=4) Avg. err (%) 38.5 31.8 32.5 31.4 Mem. (MB) 188 188 409 80(58, 80%‚Üì) Table 9. Comparision on gradually changing setup. To con- duct experiments, we use WRN-40 backbone on CIFAR100-C. The values in parentheses refer to memory reduction rates com- pared to TENT/EATA and CoTTA, sequentially. Method Con. TENT [65]PETL [28] PETL+SDROurs (K=4) Avg. err (%)38.3 73.3 42.5 36.4Mem. (MB) 188 141 141 80 Table 10. Comparisons with methods for PETL. We com- pare our method with methods [28] for parameter-efficient trans- fer learning (PETL) with WRN-40 on CIFAR100-C. PETL+SDR refers to PETL with our proposed self-distilled regularization. RoundCon. TENTTS DO LS KD Ours (K=4) 1 38.3 37.4 41.0 38.4 39.8 36.410 99.0 96.1 96.3 41.1 40.4 36.3 Table 11. Comparisons with methods for continual learning. We report an average error rate (%) of 15 corruptions using WRN- 40 on CIFAR100-C. In the table, TS: Entropy minimization with temperature scaling [21], DO: Dropout [60], LS: Label smoothing with the pseudo label [47], and KD: Knowledge distillation [27]. replay-based methods. The former can be subdivided into weight regularization ( e.g., CoTTA [66] and EATA [50]) and knowledge distillation [27], while the latter includes GEM [44] and dataset distillation [13]. Suppose dataset dis- tillation is applied to the continual TTA setup; for example, we can periodically replay synthetic samples distilled from the source dataset to prevent the model from forgetting the source knowledge during TTA. Notably, our self-distilled regularization (SDR) is superior to conventional CL meth- ods in terms of the efficiency of TTA in on-device settings. Specifically, unlike previous regularization- or replay-based methods, we do not require storing a copy of the original model or a replay-and-train process. To further compare our SDR with existing regulariza- tion methods, we conduct experiments while keeping our architecture and adaptation loss but replacing SDR with other regularizations, as shown in Table 11. The results demonstrate that our SDR achieves superior performance compared to other regularizations. In addition, Knowledge distillation [27] alleviates the error accumulation effect in long-term adaptation ( e.g., round 10), while showing lim- ited performance for adapting to the target domain. Superiority of our approach compared to existing TTA methods. Our work focuses on proposing an efficient ar- chitecture for continual TTA, which has been overlooked in previous TTA studies [65, 66, 5, 42, 9, 40] by introduc- ing meta networks and self-distilled regularization, rather than adaptation loss such as entropy minimization proposed 13Method Mem. (MB) Round 1 Round 4 Round 7 Round 10 Source 280 37.2 37.2 37.2 37.2Con. TENT 2721 54.6 49.6 37.4 29.9Con. TENT* 2721 56.5 52.7 42.7 36.5CoTTA* 6418 56.7 56.7 56.7 56.7Ours 918(66, 85%‚Üì) 55.2 55.4 55.4 55.4Ours* 918(66, 85%‚Üì) 56.7 56.8 56.9 56.9 Table 12. Further experiments in semantic segmentation. We represent the results based on mean intersection over union (mIoU). * means that the method utilizes the same cross-entropy consistency loss. The values in parentheses refer to memory re- duction rates compared to TENT/EATA and CoTTA, sequentially. #Partitions WRN-28 (12) WRN-40 (18) ResNet-50 (16) K=4 2,2,4,4 3,3,6,6 3,3,5,5 K=5 2,2,2,2,4 3,3,3,3,6 2,2,4,4,4 Table 13. Details of # of blocks of each partition. The list of numbers denotes the number of residual blocks for each part of the original networks, from the shallow to the deep parts sequentially. The values in parentheses are the total number of residual blocks. in TENT [65] and EATA [50]. Thus, our method can be used with various adaptation losses. Moreover, even though our self-distilled regularization can be regarded as a teacher-student distillation from original networks to meta networks, it does not require a large activation size or the storage of an extra source model, unlike CoTTA [66]. In addition to the results in Table 6, we improve the segmentation experiments by comparing our approach with CoTTA [66]. As we aforementioned, our approach has scalability with diverse adaptation loss. Thus, as shown in Table 12, we additionally apply cross-entropy consis- tency loss* with multi-scaling input as proposed in CoTTA, where we use the multi-scale factors of [0.5, 1.0, 1.5, 2.0] and flip. Our method not only achieves comparable per- formance with 85% less memory than CoTTA, but shows consistent performance even for multiple rounds while con- tinual TENT [65] suffers from the error accumulation effect. C. Further implementation details Partition of a pre-trained model. As illustrated in Fig. 3, the given pre-trained model consists of three parts: clas- sifier, encoder, and input conv, where the encoder denotes layer1 to 4 in the case of ResNet. Our method is applied to the encoder and we divide it into K parts. Table 13 describes the details of the number of residual blocks for each part of the encoder. Our method is designed to divide the shallow layers more (i.e., densely) than the deep layers, improving the TTA performance as shown in Table 4c. Convolution layer in meta networks. As the hyperparam- eters of the convolution layer1, we set the bias to false and 1https://pytorch.org/docs/stable/generated/torch.nn.Conv2d.html (K=4) Kernel size= 1, padding=0Kernel size=3, padding=1 Arch Avg. err params‚Üë Mem. Avg. err params‚Üë Mem. WRN-28 17.2 0.8% 396 16.9 9.5% 404 WRN-40 12.4 0.6% 80 12.2 6.4% 80 ResNet-50 14.4 11.8% 296 14.2 142.2% 394 Table 14. Kernel size in the conv layer.We report the average er- ror rate (%), the increase rate of the model parameters compared to the original model (%), and the total memory consumption (MB) including the model and activation sizes, based on the kernel size of the conv layer in meta networks. (K=4) Transformations Dataset Arch EATA [50]None +Color +Blur +Gray CIFAR10-C WRN-40 13.0 12.5 12.3 12.3 12.2 CIFAR10-C WRN-28 18.6 17.8 17.4 17.2 16.9 CIFAR100-C WRN-4037.1 36.9 36.7 36.6 36.4 Table 15. Ablation of the combination of transformations. To warm up the meta networks, we use the following transformations in Pytorch: ColorJitter (Color), GaussianBlur (Blur), and Ran- domGrayscale (Gray). We report the average error rate (%). the stride to two if the corresponding part of the encoder in- cludes the stride of two; otherwise, one. As shown in the gray area in Table 14, we conduct experiments by modify- ing the kernel size and padding for each architecture. To be more specific, we obtain better performances by setting the kernel size to three with WideResNet (with 10% additional number of model parameters). On the other hand, utilizing the kernel size of three with ResNet leads to significant in- creases in parameters and memory sizes. Thus, we use one and three as the kernel size with ResNet and WideResNet, respectively. Warming up meta networks. Before the model deploy- ment, we warm up meta networks with the source data by applying the following transformations, which prevent the meta networks from being overfitted to the source domain. Regardless of the pre-trained model‚Äôs architecture and pre-training method, we use the same transformations to warm up meta networks. Even for WideResNet-40 pre- trained with AugMix [26], a strong data augmentation tech- nique, the following simple transformations are enough to warm up the meta networks. In addition, we provide the ablation of the combination of transformations in Table 15. from t o r c h v i s i o nimport t r a n s f o r m s a s T TRANSFORMS = t o r c h . nn . S e q u e n t i a l ( RandomApply ( T . C o l o r J i t t e r ( 0 . 4 , 0 . 4 , 0 . 4 , 0 . 1 ) , p = 0 . 4 ) RandomApply ( T . G a u s s i a n B l u r ( ( 3 , 3 ) , p = 0 . 2 ) T . RandomGrayscale ( P = 0 . 1 ) ) Semantic segmentation. For semantic segmentation exper- iments, we utilize ResNet-50-based DeepLabV3+ [7] from RobustNet repository2 [8]. We warm up the meta networks on the train set of Cityscapes [10] with SGD optimizer with the learning rate of 5e-2 and the epoch of 5. Image trans- 2https://github.com/shachoi/RobustNet 14: freeze : update ‚Ñí ùúï‚Ñí ùúï‚Ñí ùúï‚Ñí ùúïùëìùëñ+1 ùúï‚Ñí ùúïùëìùëñ ùúï‚Ñí ùúïùëìùëñ‚àí1 ùëìùëñ‚àí1 ùëìùëñ ùëìùëñ+1 ùëä1,ùëè1 ùëä2,ùëè2 ùëä3,ùëè3 ùêø1 ùêø2 ùêø3 Appendix entropy min. ùê∑ùë° ‡∑§ùë•ùëò‚àí1 ùë•ùëò bn ‡∑§ùë•ùëò relu bn conv Affine tra. Standard. ‚ë† ‚ë° ‚ë¢ ‚ë£ ‚ë§ ‚ë• ÔøΩ (a) Visualization of meta networks (K=5) CIFAR10-CWRN-28CIFAR10-CWRN-40CIFAR100-CWRN-40Variants1 2 3 4 5 6 7I ‚úì ‚úì ‚úì ‚úì ‚úì19.9 15.4 39.2II ‚úì ‚úì ‚úì ‚úì ‚úì18.6 13.4 38.0III ‚úì ‚úì ‚úì ‚úì18.7 13.7 38.2IV ‚úì ‚úì ‚úì ‚úì ‚úì18.6 12.4 36.7V ‚úì ‚úì ‚úì ‚úì ‚úì19.8 12.9 37.2VI ‚úì ‚úì ‚úì ‚úì 32.3 14.5 51.8VII‚úì ‚úì ‚úì 20.7 14.9 40.1XIII‚úì ‚úì ‚úì ‚úì ‚úì ‚úì18.1 12.6 37.2IX ‚úì ‚úì ‚úì ‚úì60.6 73.3 77.2Ours‚úì‚úì‚úì‚úì‚úì ‚úì 16.8 12.1 36.3 (b) Comparison of average error rate (%) on continual TTA setup (K=5) Table 16. Components of meta networks. We conduct an ablation study on components of meta networks ( i.e., 1‚Éù ‚àº7‚Éù). Here, 1‚Éù and 2‚Éù refer to affine transformation and standardization in a BN layer after the original networks. 3‚Éù‚àº 5‚Éù and 6‚Éù‚àº 7‚Éù, respectively, indicate modules in a convolution block and two kinds of inputs of it. In table (b), ‚úì means applying the component to meta networks. formations follow the implementation details of [8]. After model deployment, we perform TTA using SGD optimizer with the learning rate of 1e-5, the image size of 1600√ó800, the batch size of 2, and the importance of regularizationŒª of 2. The main loss for adaptation is same as Lent in Eq. (2). D. Additional ablations Main task loss for adaptation. To adapt to the target do- main effectively, selecting the main task loss for adaptation is a non-trivial problem. So, we conduct a comparative experiment on three types of adaptation loss: L1) entropy minimization [19], L2) entropy minimization with mean en- tropy maximization [37], and L3) filtering samples using entropy minimization [50]. With a mini-batch of N test im- ages, the three adaptation losses are formulated as follows: L1 = 1 N NX i=1 H(ÀÜyi), (5) L2 = Œªm1 1 N NX i=1 H(ÀÜyi) ‚àí Œªm2 H(y), (6) L3 = 1 N NX i=1 I{H(ÀÜyi)<H0} ¬∑ H(ÀÜyi), (7) where ÀÜyi is the logits output of i-th test data, y = 1 N PN i=1 p(ÀÜyi), H(y) = ‚àíP C p(y) logp(y), p( ¬∑) is the softmax function, C is the number of classes, and I{¬∑} is an indicator function. Œªm1 and Œªm2 indicate the importance of each term in Eq. (6) which are set to 0.2 and 0.25, respec- tively, following SWR&NSP [9]. The entropy thresholdH0 is set to 0.4 √ó ln C following EATA [50]. The results are described in Table 17. Particularly, apply- ing any of the three losses, our method achieves comparable performance to EATA. Among them, using L3 of Eq. (7) achieves the lowest error rate in most cases. Therefore, we apply L3 to our approach as mentioned in Section 3.1. Components of meta networks. As shown in Table 16, we (K=5) Ours Dataset Arch EATA[50] L1 L2 L3 CIFAR10-C WRN-28 18.6 17.3 16.9 16.9 WRN-40 13.0 12.2 12.3 12.1 Resnet-50 14.2 15.0 14.3 14.1 CIFAR100-C WRN-40 37.1 36.5 36.4 36.3 Resnet-50 39.9 40.7 38.8 39.4 Table 17. Ablation study of main task loss. We compare the average error rate (%) of three types of adaptation losses. (K=5) Ours Dataset Arch MSE loss (Eq. (8))L1 loss (Eq. (4)) CIFAR10-C WRN-28 16.9 16.9 WRN-40 12.3 12.1 Resnet-50 14.1 14.1 CIFAR100-CWRN-40 36.6 36.3 Resnet-50 39.5 39.4 Table 18. Ablation study of loss function of our regularization. We present the average error (%) according to two types of loss functions for self-distilled regularization. conduct an ablation study on each element of our proposed meta networks. We observe that the affine transformation is more critical than standardization in a BN layer after the original networks. Specifically, removing the standardiza- tion (variant II) causes less performance drop than remov- ing the affine transformation (variant I). In addition, using only a conv layer in conv block (variant VI) also cause per- formance degradation, so it is crucial to use the ReLU and BN layers together in the conv block. Loss function choice of our regularization.As mentioned in Section 3.2, self-distilled regularization loss computes the mean absolute error ( i.e., L1 loss) of Eq. (4). This loss regularizes the output Àúxk of each k-th group of the meta networks not to deviate from the outputxk of each k-th part of frozen original networks. The mean squared error ( i.e., MSE loss) also can be used to get a similar effect which is defined as: MSE = (Àúxk ‚àí xk)2. (8) 15We compare two kinds of loss functions for our regular- ization in Table 18. By observing a marginal performance difference, our method is robust to the loss function choice. Robustness to the importance of regularization Œª. We show that our method is robust to the regularization term Œª. We conduct experiments using a wide range of Œª as shown in Fig. 5 and the following table. Round\Œª 0 0.1 0.5 1 2 5 10 1 36.31 36.30 36.29 36.56 37.20 38.41 39.58 10 55.47 43.83 36.42 36.14 36.48 37.47 38.95 The experiments are performed with WideResNet-40 on CIFAR100-C. When Œª is changed from 0.5 to 1, the per- formance difference was only 0.27% in the first round. We also test Œª to be extremely large ( e.g., 5, 8, and 10). Since setting Œª to 10 may mean that we hardly adapt the meta net- works to the target domain, the error rate (39.58%) with Œª of 10 was close to the one (41.1%) of BN Stats Adapt [49]. E. Baseline details E.1. TTA works We refer to the baselines for which the code was of- ficially released: TENT 3, TTT++4, CoTTA5, EATA6, and NOTE7. We did experiments on their code by adding the needed data loader or pre-trained model loader. In this sec- tion, implementation details of the baselines are provided. BN Stats Adapt [49] is one of the non-training TTA ap- proaches. It can be implemented by setting the model to the train mode8 of Pytorch [54] during TTA. TTT+++ [42] was originally implemented as the offline adaptation, i.e., multi-epoch training. So, we modified their setup to continual TTA. We further tuned the learn- ing rate as 0.005 and 0.00025 for adapting to CIFAR10-C and CIFAR100-C, respectively. NOTE [17] proposed the methods named IABN and PBRS with taking account of temporally correlated target data. However, our experiments were conducted with target data that was independent and identically distributed (i.i.d.). Hence, we adapted NOTE-i.i.d ( i.e., NOTE* in their git repository), which is a combination of TENT [65] and IABN without using PBRS. We fine-tuned the Œ± of their main paper ( i.e., self.k in the code 9) to 8 and the learning rate to 1e-5. Others (e.g., TENT [65], SWR&NSP [9], CoTTA [66], and EATA [50]). We utilized the best hyperparameters specified in their paper and code. In the case where the batch size of 3https://github.com/DequanWang/tent 4https://github.com/vita-epfl/ttt-plus-plus 5https://github.com/qinenergy/cotta 6https://github.com/mr-eggplant/EATA 7https://github.com/TaesikGong/NOTE 8pytorch.org/docs/stable/generated/torch.nn.Module.html#torch.nn.Module.train 9https://github.com/TaesikGong/NOTE/blob/main/utils/iabn.py their works (e.g., 200 and 256) differs from one for our ex- periments (e.g., 64), we decreased the learning rate linearly based on the batch size [18]. AdaptBN [55]. We set the hyperparameter N of their main paper to 8. When AdaptBN is employed alongside TENT or our approach, we set the learning rate to 1e-5 or 5e-6 [40]. E.2. On-device learning works To unify the backbone network as ResNet-50 [24], we reproduced the following works by referencing their paper and published code: TinyTL 10, Rep-Net11, and AuxAdapt. This section presents additional implementation details for reproducing the above three works. TinyTL [4]. We attach the LiteResidualModules 12 to layer1 to 4 in the case of ResNet-50 13. As the hyperpa- rameters of the LiteResidualModules, the hyperparameter expand is set to 4 while the other hyperparameters follow the default values. Rep-Net [69]. We divide the encoder of ResNet-50 into six parts, as each part of the encoder has 2,2,3,3,3,3 resid- ual blocks (e.g., BasicBlock or Bottleneck in Pytorch) from the shallow to the deep parts sequentially. Then, we connect the ProgramModules14 to each corresponding part of the en- coder. For the ProgramModule, we set the hyperparameter expand to 4 while the rest hyperparameters are used as their default values. We copy the input conv of ResNet-50 and make use of it as the input conv of Rep-Net. AuxAdapt [73]. We use ResNet-18 as the AuxNet. We create pseudo labels by fusing the logits output of ResNet- 50 and ResNet-18, and optimize all parameters of ResNet- 18 using the pseudo labels with cross-entropy loss. Warming up the additional modules. Before model de- ployment, we pre-train the additional modules ( i.e., the LiteResidualModule of TinyTL [4], the ProgramModule of Rep-Net [69], and the AuxNet of AuxAdapt [73]) on the source data using the same strategy warming up the meta networks as mentioned in Section C. F. Results of all corruptions We report the error rates (%) of all corruptions on con- tinual TTA and memory consumption (MB) including the model parameters and activations in Table 19 and Table 20. These tables contain additional details to Table 1. 10https://github.com/mit-han-lab/tinyml/tree/master/tinytl 11https://github.com/ASU-ESIC-FAN-Lab/RepNet 12https://github.com/mit-han-lab/tinyml/blob/master/tinytl/tinytl/model/modules.py 13https://github.com/pytorch/vision/blob/main/torchvision/models/resnet.py 14github.com/ASU-ESIC-FAN-Lab/RepNet/blob/master/repnet/model/reprogram.py 16Time t‚àí ‚àí ‚àí ‚àí ‚àí ‚àí ‚àí ‚àí ‚àí ‚àí ‚àí ‚àí ‚àí ‚àí ‚àí ‚àí ‚àí ‚àí ‚àí ‚àí ‚àí ‚àí ‚àí ‚àí ‚àí ‚àí ‚àí ‚àí ‚àí ‚àí ‚àí ‚àí ‚àí ‚àí ‚àí ‚àí ‚àí ‚àí ‚àí ‚àí ‚àí ‚àí ‚àí ‚àí ‚àí ‚àí ‚àí ‚àí ‚àí ‚àí ‚àí ‚àí ‚àí ‚àí ‚àí ‚àí ‚àí ‚àí ‚àí ‚àí ‚àí ‚àí ‚àí ‚àí ‚àí ‚àí ‚àí ‚àí ‚àí ‚àí ‚àí ‚àí ‚àí ‚àí ‚àí ‚àí ‚àí ‚àí ‚àí ‚àí ‚àí ‚àí ‚àí ‚àí ‚àí ‚àí ‚àí ‚àí ‚ÜíArch Method Gaus. Shot Impu. Defo. Glas. Moti. Zoom Snow Fros. Fog Brig. Cont. Elas. Pixe. JpegAvg. err Mem. WRN-40(AugMix) Source 44.3 37.0 44.8 30.6 43.9 32.6 29.4 23.9 30.1 39.7 12.9 66.4 32.7 58.4 23.5 36.7 11tBN [49] 19.5 17.6 23.8 9.6 23.1 11.1 10.3 13.4 14.2 15.0 8.0 13.9 17.3 16.0 18.8 15.4 11Single do. TENT [65]16.4 13.9 19.1 8.3 19.1 9.3 8.6 10.9 11.3 12.0 6.9 11.6 14.6 12.2 15.6 12.7 188TENT continual [65]16.4 12.2 17.1 9.1 18.7 11.4 10.4 12.7 12.4 14.8 10.1 13.0 17.0 13.3 19.0 13.3 188TTT++ [42] 19.1 16.9 22.2 9.3 21.6 10.8 9.8 12.7 13.1 14.3 7.8 13.9 15.9 14.2 17.2 14.6 391SWRNSP [9] 15.9 13.3 18.2 8.4 18.5 9.5 8.6 11.0 10.2 11.7 7.0 8.1 14.6 11.3 15.1 12.1 400NOTE [17] 19.6 16.4 19.9 9.4 20.3 10.3 10.1 11.6 10.6 13.3 7.9 7.7 15.4 12.0 17.3 13.4 188EATA [50] 15.2 13.1 17.5 9.5 19.9 11.6 9.3 11.4 11.5 12.4 7.8 11.1 16.1 12.2 16.1 13.0 188CoTTA [66] 15.6 13.6 17.3 9.8 19.0 11.0 10.2 13.5 12.6 17.4 7.8 17.3 16.2 12.9 16.0 14.0 409Ours (K=4) 16.1 13.2 18.3 8.0 18.3 9.3 8.6 10.5 10.1 12.2 6.8 11.3 14.5 11.0 14.8 12.2 80Ours (K=5) 15.9 12.6 17.2 8.2 18.4 9.3 8.6 10.6 10.4 12.4 6.7 11.7 14.3 11.3 14.9 12.1 92 WRN-28 Source 72.3 65.7 72.9 46.9 54.3 34.8 42.0 25.1 41.3 26.0 9.3 46.7 26.6 58.5 30.3 43.5 58tBN [49] 28.6 26.8 37.0 13.2 35.4 14.4 12.6 18.0 18.2 16.0 8.6 13.3 24.0 20.3 27.8 20.9 58Single do. TENT [65]25.2 23.8 33.5 12.8 32.3 14.1 11.7 16.4 17.0 14.4 8.4 12.2 22.8 18.0 24.8 19.2 646Continual TENT [65]25.2 20.8 29.8 14.4 31.5 15.4 14.2 18.8 17.5 17.3 10.9 14.9 23.6 20.2 25.6 20.0 646TTT++ [42] 27.9 25.8 35.8 13.0 34.3 14.2 12.2 17.4 17.6 15.5 8.6 13.1 23.1 19.6 26.6 20.3 1405SWRNSP [9] 24.6 20.5 29.3 12.4 31.1 13.0 11.3 15.3 14.7 11.7 7.8 9.3 21.5 15.6 20.3 17.2 1551NOTE [17] 30.4 26.7 34.6 13.6 36.3 13.7 13.9 17.2 15.8 15.2 9.1 7.5 24.1 18.4 25.9 20.2 646EATA [50] 23.8 18.8 27.3 13.9 29.7 16.0 13.3 18.0 16.9 15.7 10.5 12.2 22.9 17.1 23.0 18.6 646CoTTA [66] 24.6 21.6 26.5 12.1 28.0 13.0 10.9 15.3 14.6 13.6 8.1 12.2 20.0 14.9 19.5 17.0 1697Ours (K=4) 23.5 19.0 26.6 11.5 30.6 13.1 10.9 15.2 14.5 13.1 7.8 11.4 20.9 15.4 20.8 16.9 404Ours (K=5) 23.8 18.7 25.7 11.5 29.8 13.3 11.3 15.3 15.0 13.0 7.9 11.3 20.2 15.1 20.5 16.8 471 Resnet-50 Source 65.6 60.7 74.4 28.9 79.9 46.0 25.7 35.0 49.4 54.7 13.0 83.2 41.2 46.7 27.7 48.8 91tBN [49] 18.0 17.2 29.3 10.7 27.2 15.5 8.9 16.7 14.6 21.0 9.3 12.7 20.9 12.4 14.8 16.6 91Single do. TENT [65]16.6 15.7 25.7 10.0 24.8 13.8 8.3 14.9 13.8 17.6 8.7 10.0 19.1 11.5 13.8 15.0 925TENT continual [65]16.6 14.4 22.9 10.4 22.6 13.4 10.3 15.8 14.6 18.0 10.5 11.7 18.4 13.1 15.3 15.2 925TTT++ [42] 18.2 16.9 28.7 10.5 26.5 14.5 8.9 16.5 14.5 20.9 9.0 9.0 20.4 12.3 14.7 16.1 1877SWRNSP [9] 17.3 16.1 26.1 10.6 25.6 14.1 8.7 15.6 13.6 18.6 8.8 10.0 19.3 12.0 14.2 15.4 1971EATA [50] 17.2 14.9 23.6 10.2 23.3 13.2 8.5 14.0 12.5 16.6 8.6 9.4 17.2 11.0 12.7 14.2 925CoTTA [66] 16.2 15.0 21.2 10.4 22.8 13.9 8.4 15.1 12.9 19.8 8.6 11.3 17.5 10.5 12.2 14.4 2066Ours (K=4) 16.5 14.5 24.3 9.7 23.7 13.3 8.8 14.7 12.9 17.0 9.1 9.4 17.6 11.4 13.1 14.4 296Ours (K=5) 16.6 14.4 23.6 9.8 23.4 12.7 8.6 14.5 12.6 16.6 8.7 9.0 17.0 11.3 12.6 14.1 498 Table 19. Comparison of error rate (%) on CIFARC10-C with severity level 5. We conduct experiments on continual TTA setup. Avg. err means the average error rate (%) of all 15 corruptions, and Mem. denotes total memory consumption, including model parameter sizes and activations. WRN refers to WideResNet. The implementation details of the baselines are described in Section E.1. Time t‚àí ‚àí ‚àí ‚àí ‚àí ‚àí ‚àí ‚àí ‚àí ‚àí ‚àí ‚àí ‚àí ‚àí ‚àí ‚àí ‚àí ‚àí ‚àí ‚àí ‚àí ‚àí ‚àí ‚àí ‚àí ‚àí ‚àí ‚àí ‚àí ‚àí ‚àí ‚àí ‚àí ‚àí ‚àí ‚àí ‚àí ‚àí ‚àí ‚àí ‚àí ‚àí ‚àí ‚àí ‚àí ‚àí ‚àí ‚àí ‚àí ‚àí ‚àí ‚àí ‚àí ‚àí ‚àí ‚àí ‚àí ‚àí ‚àí ‚àí ‚àí ‚àí ‚àí ‚àí ‚àí ‚àí ‚àí ‚àí ‚àí ‚àí ‚àí ‚àí ‚àí ‚àí ‚àí ‚àí ‚àí ‚àí ‚àí ‚àí ‚àí ‚àí ‚àí ‚àí ‚àí ‚àí ‚àí ‚àí ‚ÜíArch Method Gaus. Shot Impu. Defo. Glas. Moti. Zoom Snow Fros. Fog Brig. Cont. Elas. Pixe. JpegAvg. err Mem. WRN-40(AugMix) Source 80.1 77.0 76.4 59.9 77.6 64.2 59.3 64.8 71.3 78.3 48.1 83.4 65.8 80.4 59.2 69.7 11tBN [49] 45.9 45.6 48.2 33.6 47.9 34.5 34.1 40.3 40.4 47.1 31.7 39.7 42.7 39.2 45.6 41.1 11Single do. TENT [65]41.2 40.6 42.2 30.9 43.4 31.8 30.6 35.3 36.2 40.1 28.5 35.5 39.1 33.9 41.7 36.7 188continual TENT [65]41.2 38.2 41.0 32.9 43.9 34.9 33.2 37.7 37.2 41.5 33.2 37.2 41.1 35.9 45.1 38.3 188TTT++ [42] 46.0 45.4 48.2 33.5 47.7 34.4 33.8 39.9 40.2 47.1 31.8 39.7 42.5 38.9 45.5 41.0 391SWRNSP [9] 42.4 40.9 42.7 30.6 43.9 31.7 31.3 36.1 36.2 41.5 28.7 34.1 39.2 33.6 41.3 36.6 400NOTE [17] 50.9 47.4 49.0 37.3 49.6 37.3 37.0 41.3 39.9 47.0 35.2 34.7 45.2 40.9 49.9 42.8 188EATA [50] 41.6 39.9 41.2 31.7 44.0 32.4 31.9 36.2 36.8 39.7 29.1 34.4 39.9 34.2 42.2 37.1 188CoTTA [66] 43.5 41.7 43.7 32.2 43.7 32.8 32.2 38.5 37.6 45.9 29.0 38.1 39.2 33.8 39.4 38.1 409Ours (K=4) 42.7 39.6 42.4 31.4 42.9 31.9 30.8 35.1 34.8 40.7 28.1 35.0 37.5 32.1 40.5 36.4 80Ours (K=5) 41.8 39.0 41.9 31.2 42.7 32.5 31.0 35.0 35.0 39.9 28.8 34.5 37.5 32.8 40.5 36.3 92 Resnet-50 Source 84.7 83.5 93.3 59.6 92.5 71.9 54.8 66.6 77.6 81.8 44.3 91.2 72.2 76.6 56.5 73.8 91tBN [49] 48.1 46.7 60.6 35.1 58.0 41.8 33.2 47.3 43.5 54.9 33.5 35.3 49.8 38.4 40.8 44.5 91Single do. TENT [65]44.1 42.7 53.9 32.6 52.0 37.5 30.5 43.4 40.2 45.7 30.4 31.4 45.1 35.0 37.6 40.1 926continual TENT [65]44.0 40.1 49.9 34.7 50.6 40.0 33.6 47.0 45.7 53.4 42.5 46.2 56.1 51.2 53.3 45.9 926TTT++ [42] 48.1 46.5 60.8 35.1 57.8 41.6 32.9 46.8 43.3 55.0 33.3 34.0 50.0 38.1 40.6 44.2 1876SWRNSP [9] 48.3 46.5 60.5 35.1 57.9 41.7 32.9 47.1 43.5 54.7 33.5 35.1 49.9 38.3 40.7 44.1 1970EATA [50] 44.8 41.9 52.6 33.0 51.1 37.8 30.3 43.0 40.1 45.1 30.1 31.8 45.2 35.2 37.4 39.9 926CoTTA [66] 43.6 42.8 50.4 34.2 51.6 39.2 31.4 43.4 39.6 47.4 31.3 32.2 43.4 35.8 36.7 40.2 2064Ours (K=4) 44.8 40.3 49.2 32.3 50.1 36.3 29.5 41.0 39.9 44.6 31.5 33.7 45.3 36.3 37.7 39.5 296Ours (K=5) 44.9 40.4 48.9 32.7 49.7 36.9 29.3 40.8 39.0 44.4 31.1 33.6 44.0 35.7 37.8 39.3 498 Table 20. Comparison of error rate (%) on CIFARC100-C with severity level 5.We conduct experiments on continual TTA setup. Avg. err means the average error rate (%) of all 15 corruptions, and Mem. denotes total memory consumption, including model parameter sizes and activations. WRN refers to WideResNet. The implementation details of the baselines are described in Section E.1. 17
---------------------------------

Please extract all reference paper titles and return them as a list of strings.
Output:
{
    "reference_titles": [
        "Covariance-aware feature alignment with pre-computed source statistics for test-time adaptation.",
        "Pseudo-labeling and confirmation bias in deep semi-supervised learning.",
        "Test-time adaptation vs. training-time generalization: A case study in human instance segmentation using keypoints estimation.",
        "TinyTL: Reduce memory, not parameters for efficient on-device learning.",
        "Contrastive test-time adaptation.",
        "Reusing the task-specific classifier as a discriminator: Discriminator-free adversarial domain adaptation.",
        "Encoder-decoder with atrous separable convolution for semantic image segmentation.",
        "RobustNet: Improving domain generalization in urban-scene segmentation via instance selective whitening.",
        "Improving test-time adaptation via shift-agnostic weight regularization and nearest source prototypes.",
        "The Cityscapes dataset for semantic urban scene understanding.",
        "RobustBench: a standardized adversarial robustness benchmark.",
        "TransAdapt: A transformative framework for online test time adaptive semantic segmentation.",
        "Remember the past: Distilling datasets into addressable memories for neural networks.",
        "A survey of on-device machine learning: An algorithms and learning theory perspective.",
        "An image is worth 16x16 words: Transformers for image recognition at scale.",
        "Test-time training with masked autoencoders.",
        "Momentum contrast for unsupervised visual representation learning.",
        "Deep residual learning for image recognition.",
        "Benchmarking neural network robustness to common corruptions and perturbations.",
        "AugMix: A simple data processing method to improve robustness and uncertainty.",
        "Distilling the knowledge in a neural network.",
        "Parameter-efficient transfer learning for NLP.",
        "LoRA: Low-rank adaptation of large language models.",
        "Squeeze-and-excitation networks.",
        "MixNorm: Test-time adaptation through online normalization estimation.",
        "Test-time classifier adjustment module for model-agnostic domain generalization.",
        "Online domain adaptation of a pre-trained cascade of classifiers.",
        "Cafa: Class-aware feature alignment for test-time adaptation.",
        "Evaluating continual test-time adaptation for contextual and semantic domain shifts.",
        "SITA: Single image test-time adaptation.",
        "Robust continual test-time adaptation: Instance-aware BN and prediction-balanced memory.",
        "MCUNet: Tiny deep learning on IoT devices.",
        "TTT++: When does self-supervised test-time training fail or thrive?",
        "Unsupervised domain adaptation with residual transfer networks.",
        "Gradual test-time adaptation by self-training and style transfer.",
        "Instance adaptive self-training for unsupervised domain adaptation.",
        "When does label smoothing help?",
        "Two at once: Enhancing learning and generalization capacities via IBN-Net.",
        "Test-time adaptation to distribution shift by confidence maximization and input transformation.",
        "Evaluating prediction-time batch normalization for robustness under covariate shift.",
        "Efficient test-time model adaptation without forgetting.",
        "Towards stable test-time adaptation in dynamic wild world.",
        "Mm-tta: Multi-modal test-time adaptation for 3d semantic segmentation.",
        "Two-phase pseudo label densification for self-training based domain adaptation.",
        "Test-time adaptation in the dynamic world with compound domain knowledge management.",
        "Mean teachers are better role models: Weight-averaged consistency targets improve semi-supervised deep learning results.",
        "Advent: Adversarial entropy minimization for domain adaptation in semantic segmentation.",
        "Tent: Fully test-time adaptation by entropy minimization.",
        "Continual test-time domain adaptation.",
        "CBAM: Convolutional block attention module.",
        "DA3: Dynamic additive attention adaption for memory-efficient on-device multi-domain learning.",
        "Rep-Net: Efficient on-device learning via feature reprogramming.",
        "Test-time batch statistics calibration for covariate shift.",
        "Wide residual networks.",
        "Memo: Test time robustness via adaptation and augmentation.",
        "AuxAdapt: Stable and efficient test-time adaptation for temporally consistent video semantic segmentation.",
        "Domain generalization with mixstyle.",
        "ImageNet in 1 hour."
    ]
}
