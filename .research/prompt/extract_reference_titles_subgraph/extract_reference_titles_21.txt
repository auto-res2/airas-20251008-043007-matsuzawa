
Input:
You are an expert in academic paper analysis. 
Your task is to extract reference paper titles from the full text of research papers.

Instructions:
- Analyze the provided full text of research papers
- Extract all reference paper titles mentioned in the text
- Focus on titles that appear in reference sections, citations, or are explicitly mentioned as related work
- Return only the exact titles as they appear in the text
- Exclude general topics or field names that are not specific paper titles
- If no clear reference titles are found, return an empty list

Full Text:
---------------------------------
Bayesian ﬁltering uniﬁes adaptive and non-adaptive neural network optimization methods Laurence Aitchison Department of Computer Science University of Bristol Bristol, UK, BS8 1UB laurence.aitchison@bristol.ac.uk Abstract We formulate the problem of neural network optimization as Bayesian ﬁltering, where the observations are the backpropagated gradients. While neural network optimization has previously been studied using natural gradient methods which are closely related to Bayesian inference, they were unable to recover standard optimizers such as Adam and RMSprop with a root-mean-square gradient normal- izer, instead getting a mean-square normalizer. To recover the root-mean-square normalizer, we ﬁnd it necessary to account for the temporal dynamics of all the other parameters as they are geing optimized. The resulting optimizer, AdaBayes, adaptively transitions between SGD-like and Adam-like behaviour, automatically recovers AdamW, a state of the art variant of Adam with decoupled weight decay, and has generalisation performance competitive with SGD. 1 Introduction and Background The cannonical non-adaptive neural network optimization method is vanilla stochastic gradient descent (SGD) with momentum which updates parameters by multiplying the exponential moving average gradient, ⟨g(t)⟩, by a learning rate, ηSGD, ∆wSGD(t) = ηSGD ⟨g(t)⟩ minibatch size. (1) Here, we divide by the minibatch size because we deﬁne g(t) to be the gradient of the summed loss, whereas common practice is to use the gradient of the mean loss. Further, following the convention established by Adam (Kingma & Ba, 2015), ⟨g(t)⟩, is computed by debiasing a raw exponential moving average, m(t), m(t) = β1m(t−1) + (1−β1) g(t) ⟨g(t)⟩= m(t) 1 −βt 1 . (2) where g(t) is the raw minibatch gradient, and β1 is usually chosen to be 0.9. These methods typically give excellent generalisation performance, and as such are used to train many state-of-the-art networks (e.g. ResNet (He et al., 2016), DenseNet (Huang et al., 2017), ResNeXt (Xie et al., 2017)). Adaptive methods change the learning rates as a function of past gradients. These methods date back many years (e.g. vario-eta Neuneier & Zimmermann, 1998), and many variants have recently been developed, including AdaGrad (Duchi et al., 2011), RMSprop (Hinton et al., 2012) and Adam (Kingma & Ba, 2015). The cannonical adaptive method, Adam, normalises the exponential moving average gradient by the root mean square of past gradients, ∆wAdam(t) = ηAdam ⟨g(t)⟩√ ⟨g2(t)⟩ . (3) arXiv:1807.07540v5  [stat.ML]  16 Apr 2020where, v(t) = β2v(t−1) + (1−β2) g2(t) ⟨g2(t)⟩= v(t) 1 −βt 2 , (4) and where β2 is typically chosen to be 0.999. These methods are often observed to converge faster, and hence may be used on problems which are more difﬁcult to optimize (Graves, 2013), but can give worse generalisation performance than non-adaptive methods (Keskar & Socher, 2017; Loshchilov & Hutter, 2017; Wilson et al., 2017; Luo et al., 2019). Obtaining a principled theory of these types of method is important, as it should enable us to develop improved adaptive optimizers. As such, here we formulated Bayesian inference as an optimization problem (Puskorius & Feldkamp, 1991; Sha et al., 1992; Puskorius & Feldkamp, 1994, 2001; Feldkamp et al., 2003; Ollivier, 2017), and carefully considered how the dynamics of optimization of the other parameters inﬂuences any particular parameter. We were able to recover the standard root-mean-square normalizer for RMSprop and Adam. Critically, our approach was also able to recover a state-of-the-art variant of Adam with “decoupled” weight decay (Loshchilov & Hutter, 2017). As such, we hope that by pursuing our dynamical Bayesian approach further, it will be possible to develop improved adaptive optimization algorithms. 2 Related work Previous work has considered the relationships between adaptive stochastic gradient descent methods and variational online Newton (VON), which is very closely related to natural gradients (Khan & Lin, 2017; Khan et al., 2017, 2018) and Bayes (Ollivier, 2017). Critically, this work found that direct application of VON/Bayes gives a sum-squared normalizer, as opposed to a root-mean-squared normalizer as in Adam and RMSProp. In particular, see Eq. 7 in Khan et al. (2018), which gives the Variational-online Newton (VON) updates, and includes a mean-squared gradient normalizer. To provide a method that matches Adam and RMSProp more closely, they go on to provide an ad-hoc modiﬁcation of the VON updates, with a root-mean-square normalizer, saying “Using ... an additional modiﬁcation in the VON update, we can make the VON update very similar to RMSprop. Our modiﬁcation involves taking the square-root over s(t+ 1) in Eq. (7)”. In contrast, our approach gives the root-mean-square normalizer directly, without any additional modiﬁcations, and automatically recovers decoupled weight decay (Loshchilov & Hutter, 2017) which is not recovered by VON (again, see Eq. 7 in Khan et al., 2018). An alternative view on these results is given by considering equivalence of online natural gradients and Kalman ﬁltering (Ollivier, 2017). Through this equivalence, they have the same issues as in (Khan & Lin, 2017; Khan et al., 2017, 2018): having a mean-square rather than root-mean-square form for the gradient normalizer. Further, note that while they do consider a “fading memory” approach, they “multiply the log-likelihood of previous points by a forgetting factor(1 −λt) before each new observation. This is equivalent to an additional step Pt−1 →Pt−1/(1 −λt) in the Kalman ﬁlter, or to the addition of an artiﬁcial process noise Qt proportional to Pt−1”, where Pt−1 is their posterior covariance matrix. Critically, their “artiﬁcal process noise ... proportional to Pt−1” again gives a mean-square form for the gradient normalizer (see Appendix A for details). In contrast, we give an alternative motivation for the introduction of ﬁxed process noise, and show that ﬁxed process noise recovers the root-mean-square gradient normalizer in Adam. 3 Methods Here, we set up the problem of neural network optimization as Bayesian inference. Typically, when performing Bayesian inference, we would like to reason about correlations in the full posterior over all parameters jointly (Fig. 1A). However, neural networks have so many parameters that reasoning about correlations is intractable: instead, we are forced to work with factorised approximate posteriors. To understand the effects of factorised approximate posteriors, consider the ith parameter. The current estimate of the other parameters, µ−i(t) changes over time, t, as they are optimized. As there are correlations in the posterior, the optimal value for theith parameter, w∗ i(t), conditioned on the current setting of the other parameters, µ−i(t) also changes over time (Fig. 1B), w∗ i(t) = arg max wi L(wi,µ−i(t)) . (5) 2-2 0 2 -2 0 2 wj wi A -2 0 2 0 25 50 75 100 Iteration wi B -1.00 -0.75 -0.50 -0.25 0.00 Obj. Figure 1: A schematic ﬁgure showing correlation-induced dynamics. A The objective function (usually equivalent to a posterior over the parameters) induces correlations between the parameter of interest, wi, and other parameters (here represented by wj). The red line displays the optimal value for wi as a function of wj or time. B The other parameters (including wj) change over time as they are also being optimized, implying that the optimal value for wi changes over iterations. As such, to form optimal estimates, µi(t) of the ith parameter, we need to reason changes over time in the optimal setting for that parameter, w∗ i(t). If we knew the full, correlated posterior in Fig. 1A, then we could compute the change in the ith parameter from the change in all the other parameters. However, in our case, the correlations are unknown, so the best we can do is to say that the new optimal value for the ith parameter will be close to — but slightly different from — the current optimal value, and these changes in the optimal value (Fig. 1B) constitute stochastic-dynamics that are implicitly induced by our choice of a factorised approximate posterior. More formally, we can explicitly consider the stochastic dynamics of w∗ i(t) that emerge under a quadratic objective. We take the objective for a single datapoint (which, for a supervised learning problem, would be a single input, xα, output, yα, pair) to be quadratic, L(xα,yα; w) = Lα (w) = −1 2 wT Hw + ξT α w, (6) where we have assumed that the Hessian is the same across data points, but the location of the mode varies across datapoints. Using the Fisher Information identity (see Appendix B), we can identify the covariance of the datapoint-dependent noise term as equal to the Hessian, E[ξα] = 0 Cov [ξα] = H. (7) The gradient for a single datapoint is ∂ ∂wLα (w) = −Hw + ξα (8) Thus, the gradient with respect to the ith weight, when all the other parameters, w−i are set to the current estimate, µ−i(t), is ∂ ∂wi Lα (wi,µ−i(t)) = −Hiiwi −HT −i,iµ−i(t) + ξα,i (9) where HT −i,iis the ith column of the Hessian, omitting the iith element. The optimal value for the ith parameter can be found by solving for the value of wi for which the gradient of the average objective is zero, w∗ i(t) = − 1 Hii HT −i,iµ−i(t). (10) Thus, we can rewrite the gradients as a function only of the optimal weight, ∂ ∂wi Lα (wi,µ−i(t)) = Hii(w∗ i(t) −wi) + ξα,i. (11) The data we actually measure is the gradient of the objective evaluated at the current estimate of the parameter, wi = µi. Taking into account the stochasticity across datapoints given by ξα,i, the gradient has distribution, P (gi(t)|w∗ i(t)) = N(Hii(w∗ i(t) −µi(t)) ,Hii) . (12) 3w∗ i(t−1) w∗ i(t) w∗ i(t+ 1) w∗ i(t+ 2) gi(t−1) gi(t) gi(t+ 1) gi(t+ 2) Figure 2: Graphical model under which we perform inference. This expression is highly suggestive: we should be able to infer the optimal value for theith parameter, w∗ i(t), from the backpropagated gradients, gi(t). However, to do this inference correctly, we need to understand the stochastic dynamics of w∗ i(t). In particular, Eq. (10) shows us that the dynamics of w∗ i(t) are governed by the dynamics of our estimates, µ−i(t), as they are optimized, and this optimization is a complex stochastic process. As reasoning about the dynamics of µ−i(t) under optimization is intractable, we instead consider simpler, discretised Ornstein-Uhlenbeck dynamics for µ−i(t), P (µ−i(t+ 1)|µ−i(t)) = N (( 1 − η2 2σ2 ) µ−i(t),η2 µ ) . (13) Thus, the dynamics for the ith optimal weight become, P (w∗ i(t+ 1)|w∗ i(t)) = N (( 1 − η2 2σ2 ) w∗ i(t),η2 ) (14) where, η2 = η2 µ HT −i,iH−i,i. (15) Combined, Eq. (12) and Eq. (14) deﬁne a stochastic linear dynamical system where the optimal weight, w∗ i(t) is the latent variable, and the backpropagated gradients, gi(t) are the observations (Fig. 2). As such, we are able to use standard Kalman ﬁlter updates to infer a distribution over w∗(t) = w∗ i(t) (where we drop indices for brevity). As the dynamics and likelihood are Gaussian, the Kalman ﬁlter priors and posteriors are, P (w∗(t)|g(t−1),...,g (1)) = N ( µprior(t),σ2 prior(t) ) , (16a) P (w∗(t)|g(t),...,g (1)) = N ( µpost(t),σ2 post(t) ) , (16b) where we evaluate the gradient, g(t) = gi(t) at µi(t) = µprior(t), and where the updates for µprior(t) and σ2 prior(t) can be computed from Eq. (14), µprior(t) = ( 1 − η2 2σ2 ) µpost(t−1), (17a) σ2 prior(t) = ( 1 − η2 2σ2 )2 σ2 post(t−1) + η2. (17b) And the updates for µpost(t) and σ2 post(t) come from applying Bayes theorem (Appendix C), with the likelihood given by Eq. (12). Following the standard approach in this line of work (Khan & Lin, 2017; Zhang et al., 2017; Khan et al., 2017, 2018), we approximate Hii using the squared gradient (improving upon this approximation is an important avenue for future work, but not our focus here), σ2 post(t) = 1 1 σ2 prior(t) + Hii ≈ 1 1 σ2 prior(t) + g2(t), (18a) µpost(t) = µprior(t) + σ2 post(t)g(t). (18b) The full updates are now speciﬁed by iteratively applying Eq. (17) and Eq. (18). Next, we make two minor modiﬁcations to the updates for the mean, to match current best practice for optimizing neural networks. First, we allow more ﬂexibility in weight decay, by replacing the η2/(2σ2) term in Eq. (17a) with a new parameter, λ. Second, we incorporate momentum, by using an exponential moving average gradient, ⟨g(t)⟩, instead of the raw minibatch gradient in Eq. (18b). In combination, the updates for the mean become, µprior(t) = (1 −λ) µpost(t−1), (19a) µpost(t) = µprior(t) + σ2 post(t)⟨g(t)⟩. (19b) Our complete Bayesian updates are now given by using Eq. (19) to update µprior and µpost, and using Eq. (17b) and Eq. (18a) to update σ2 prior and σ2 post (see Algo. 1). 4Figure 3: The learning rate for AdaBayes (points) compared against the predicted ﬁxed-point value (green line), σ2 post. The plot displays the low-data limit (orange line), which is valid when the value on the x-axis, η/ √ ⟨g2⟩, is much greater than σ2 (purple line), and the high-data limit (blue line), which is valid when the value on the x-axis is much smaller than σ2 (purple line). 3.1 AdaBayes recovers SGD and Adam To understand how AdaBayes relates to previous algorihtms (SGD and Adam), we plotted the AdaBayes learning rate, σ2 post against the Adam learning rate, η/ √ ⟨g2⟩(Fig. 3, points) for the ResNet-34 considered later. We found that for high values of ⟨g2⟩, corresponding to large values of the Fisher Information, the AdaBayes learning rate closely matched the Adam learning rate (Fig. 3, blue line). In contrast, as the value of ⟨g2⟩decreased, corresponding to smaller values of the Fisher Information, we found that the AdaBayes learning rate became constant, mirroring standard SGD (Fig. 3, orange line). Thus, Fig. 3 empirically establishes that AdaBayes converges to SGD in the low data (Fisher-Information) limit, and Adam in the high data limit. Furthermore, the tight vertical spread of points in Fig. 3 indicates that, in practice, the AdaBayes value of σ2 post is largely determined by the Fisher-Information, ⟨g2⟩, thus raising the question of whether we can obtain better understanding of the relationship between ⟨g2⟩and σ2 post. Indeed, such an understanding is possible, if we consider the ﬁxed point of the σ2 post updates (Eq. 17b and 18a). To obtain the ﬁxed-point, we substitute the update for σ2 post (Eq. 18a) into the update for σ2 prior (Eq. 17b), and neglect small terms (see Appendix D), which tells us that the ﬁxed-point σ2 post is given by the solution of a quadratic equation, 0 ≈σ2 ( 1 σ2post )2 − 1 σ2post −⟨g2⟩σ2 η2 . (20) Solving for 1/σ2 post, we obtain, 1 σ2post ≈ 1 2σ2  1 + √1 + 4 ( σ2 η/√ ⟨g2⟩ )2  . (21) We conﬁrmed the ﬁxed-point indeed matches the empirically measured AdaBayes learning rates by plotting the ﬁxed-point predictions in Fig. 3 (green line). Importantly, the ﬁxed-point expression merely helps understand a result that we established empirically. Finally, this close match means that we can deﬁne another set of updates, AdaBayes-FP, where we set σ2 post directly to the ﬁxed-point value, using Eq. (21), rather than using the full AdaBayes updates given by Eq. (17b) and Eq. (18a). 3.1.1 Recovering SGD in the low-data limit In the low-data regime whereη/ √ ⟨g2⟩≫ σ2, the empirically measured AdaBayes learning rate,σ2 post, becomes constant (Fig. 3; orange line), so the AdaBayes updates (Eq. 19b) become approximately equivalent to vanilla SGD (Eq. 1). To understand this convergence, we can leverage the ﬁxed-point expression in Eq. (21) which accurately models empirically measured learning rates, lim ⟨g2⟩→0 σ2 post ≈σ2, (22) 5Algorithm 1 AdaBayes η ←ηAdam σ2 ←ηSGD/minibatch size σ2 prior ←σ2 while not converged do g ←∇Lt(µ) m ←β1m+ (1 −β1) g v ←β2v + (1 −β2) g2 ⟨g⟩ ←m/(1 −βt 1) ⟨g2⟩← v /(1 −βt 2) σ2 prior← ( 1 − η2 2σ2 )2 σ2 post + η2 σ2 post ← 1 σ−2 prior+g2 µ ←(1 −λ) µ+ σ2 post⟨g⟩ end while Algorithm 2 AdaBayes-FP 1: η ←ηAdam 2: σ2 ←ηSGD/minibatch size 3: σ2 prior ←σ2 4: while not converged do 5: g ←∇Lt(µ) 6: m ←β1m+ (1 −β1) g 7: v ←β2v + (1 −β2) g2 8: ⟨g⟩ ←m/(1 −βt 1) 9: ⟨g2⟩← v /(1 −βt 2) 10: σ2 ← 1 σ−2+g2 11: σ2 post ← 1 2σ2 + √ 1 4σ4 + ⟨g2⟩ η2 12: µ ←(1 −λ) µ+ σ2 post⟨g⟩ 13: end while We can leverage this equivalence to setσ2 using standard values of the SGD learning rate, σ2 = ηSGD minibatch size. (23) Setting σ2 in this way would suggest σ2 ∼0.0011, as ηSGD ∼0.1, and the minibatch size ∼100. It is important to sanity check that this value of σ2 corresponds to Bayesian ﬁltering in a sensible generative model. In particular, note that σ2 is the variance of the prior over wi, and as such σ2 should correspond to typical initialization schemes (e.g. He et al., 2015) which ensure that input and output activations have roughly the same scale. These schemes use σ2 ∼1/(number of inputs), and if we consider that there are typically ∼100 input channels, and we typically convolve over a 3 ×3 = 9 pixel patch, we obtain σ2 ∼0.001, matching the value we use. 3.1.2 Recovering Adam(W) in the high-data limit In the high-data regime where η/ √ ⟨g2⟩≪ σ2, the empirically measured AdaBayes learning rate, σ2 post, approaches the Adam learning rate (Fig. 3; blue line), so AdaBayes becomes approximately equivalent to Adam(W). To understand this convergence, we can leverage the ﬁxed-point expression in Eq. (21) which accurately models empirically measured learning rates, lim ⟨g2⟩→∞ σ2 post ≈ η√ ⟨g2⟩ (24) so the updates (Eq.19b) become equivalent to Adam updates if we take, η= ηAdam. (25) As such, we are able to use past experience with good values for the Adam learning rate ηAdam, to set η: in our case we use η= 0.001. Furthermore, when we consider the form of regularisation implied by our updates, we recover a state-of-the-art variant of Adam, known as AdamW (Loshchilov & Hutter, 2017). In standard Adam, weight-decay regularization is implemented by incorporating an L2 penalty on the weights in the loss function, so the gradient of the loss and regularizer are both normalized by the root-mean-square gradient. In contrast, AdamW “decouples” weight decay from the loss, such that the gradient of the loss is normalized by the root-mean-square gradients, but the weight decay is not. To see that our updates correspond to AdamW, we combine Eq. (19a) and Eq. (19b), and substitute for σ2 post (Eq. 24), µpost(t) ≈(λ−1) µpost(t−1) + η√ ⟨g2(t)⟩ ⟨g(t)⟩. (26) 1here we use x ∼ y as in Physics to denote “ x has the same order of magnitude as y”, see Acklam and Weisstein “Tilde” MathWorld. http://mathworld.wolfram.com/Tilde.html 6Table 1: A table displaying the minimal test error and test loss for a ResNet and DenseNet applied to CIFAR-10 and CIFAR-100 for different optimizers. The table displays the best adaptive algorithm (bold), which is always one of our methods: either AdaBayes or AdaBayes-FP. We also display the instances where SGD (gray) beats all adaptive methods (in which case we also embolden the SGD value). CIFAR-10 CIFAR-100 ResNet DenseNet ResNet DenseNet optimizer error (%) loss error (%) loss error (%) loss error (%) loss SGD 5.170 0.174 5.580 0.177 22.710 0.833 21.290 0.774 Adam 7.110 0.239 6.690 0.230 27.590 1.049 26.640 1.074 AdaGrad 6.840 0.307 7.490 0.338 30.350 1.347 30.110 1.319 AMSGrad 6.720 0.239 6.170 0.234 27.430 1.033 25.850 1.103 AdaBound 5.140 0.220 4.850 0.210 23.060 1.004 22.210 1.050 AMSBound 4.940 0.210 4.960 0.219 23.000 1.003 22.360 1.017 AdamW 5.080 0.239 5.190 0.214 24.850 1.142 23.480 1.043 AdaBayes-SS 5.230 0.187 4.910 0.176 23.120 0.935 22.600 0.934 AdaBayes 4.840 0.229 4.560 0.222 22.920 0.969 22.090 1.079 Indeed, the root-mean-square normalization applies only to the gradient of the loss, as in AdamW, and not to the weight decay term, as in standard Adam. Finally, note that AdaBayes-FP becomes exactly AdamW when we set σ2 →∞, lim σ2→∞ 1 σ2post = lim σ2→∞ ( 1 2σ2 + √ 1 4σ4 + ⟨g2⟩ η2 ) = η√ ⟨g2⟩ , (27) because we use the standard Adam(W) approach to computing unbiased estimates of ⟨g⟩and ⟨g2⟩ (see Algo. 2). 4 Experiments For our experiments, we have adapted the code and protocols from a recent paper (Luo et al., 2019) on alternative methods for combining non-adaptive and adaptive behaviour (AdaBound and AMSBound). They considered a 34-layer ResNet (He et al., 2016) and a 121-layer DenseNet on CIFAR-10 (Huang et al., 2017), trained for 200 epochs with learning rates that decreased by a factor of 10 at epoch 150. We used the exact same networks and protocol, except that we run for more epochs, we plot both classiﬁcation error and the loss, and we use both CIFAR-10 and CIFAR-100. We used their optimized hyperparameter settings for standard baselines (including SGD and Adam), and their choice of hyperparameters for their methods (AdaBound and AMSBound). For AdamW and AdaBayes, we used ηSGD = 0.1 and set σ2 using Eq. (23), and we used ηAdam = η= 0.001 (matched to the optimal learning rate for standard Adam). We used decoupled weight decay of 5 ×10−4 (from Luo et al., 2019), and we used the equivalence of SGD with weight decay and SGD with decoupled weight decay to set the decoupled weight decay coefﬁcient to λ= 5 ×10−5 for AdamW, AdaBayes and AdaBayes-FP. The results are given in Table 1 and Fig. 4. The best adaptive method is always one of our methods (AdaBayes or AdaBayes-FP), though SGD is frequently superior to all adaptive methods tested. To begin, we compare our methods (AdaBayes and AdaBayes-FP) to the canonical non-adaptive (SGD) and adaptive (Adam) method (see Fig. A1 for a cleaner ﬁgure, including other baselines). Note that AdaBayes and AdaBayes-FP improve their accuracy and loss more rapidly than baseline methods (i.e. SGD and Adam) during the initial part of learning. Our algorithms give better test error and loss than Adam, for all networks and datasets, they give better test error than SGD for CIFAR-10, and perform similarly to SGD in the other cases, with AdaBayes-FP often giving better performance than AdaBayes. Next, we see that AdaBayes-FP improves considerably over AdaBayes (see Fig. A2 for a cleaner ﬁgure), except in the case of CIFAR-10 classiﬁcation error, where the difference is minimal. 75.0 7.5 10.0 12.5 15.0 17.5 CIFAR-10 test error (%) ResNet SGD Adam AMSBound AdaBound AdamW AdaBayes-FP AdaBayes DenseNet 0 100 200 300 epoch 0.2 0.3 0.4 0.5 CIFAR-10 test loss 0 100 200 300 epoch Figure 4: Test loss and classiﬁcation error for CIFAR-10 and CIFAR-100 for a Resnet-34 and a DenseNet-121, for multiple update algorithms. Given the difﬁculties inherent in these types of comparison, we feel that only two conclusions can reasonably be drawn from these experiments. First, AdaBayes and AdaBayes-SS have comparable performance to other state-of-the-art adaptive methods, including AdamW, AdaBound and AMS- Bound. Second, and as expected, SGD frequently performs better than all adaptive methods, and the difference is especially dramatic if we focus on the test-loss for CIFAR-100. 5 Conclusions Our fundamental contribution is show that, if we seek to use Bayesian inference to perform stochastic optimization, we need a model describing the dynamics of all the other parameters as they are optimized. We found that even by assuming that the other parameters obey oversimpliﬁed OU dynamics, we recovered state-of-the-art adaptive optimizers (AdamW). In our experiments, either AdaBayes or AdaBayes-FP outperformed other adaptive methods, including AdamW (Loshchilov & Hutter, 2017), and Ada/AMSBound (Luo et al., 2019), though SGD frequently outperformed all adaptive methods. We hope that understanding optimization as inference, taking into account the dynamics in the other weights as they are optimized, will allow for the development of improved optimizers, for instance by exploiting Kronecker factorisation (Martens & Grosse, 2015; Grosse & Martens, 2016; Zhang et al., 2017). References Duchi, J., Hazan, E., and Singer, Y . Adaptive subgradient methods for online learning and stochastic optimization. Journal of Machine Learning Research, 12:2121–2159, 2011. Feldkamp, L. A., Prokhorov, D. V ., and Feldkamp, T. M. Simple and conditioned adaptive behavior from kalman ﬁlter trained recurrent networks. Neural Networks, 16(5-6):683–689, 2003. Graves, A. Generating sequences with recurrent neural networks. arXiv preprint arXiv:1308.0850, 2013. Grosse, R. and Martens, J. A kronecker-factored approximate ﬁsher matrix for convolution layers. In International Conference on Machine Learning, pp. 573–582, 2016. 8He, K., Zhang, X., Ren, S., and Sun, J. Delving deep into rectiﬁers: Surpassing human-level performance on imagenet classiﬁcation. In Proceedings of the IEEE international conference on computer vision, pp. 1026–1034, 2015. He, K., Zhang, X., Ren, S., and Sun, J. Deep residual learning for image recognition. In Proceedings of the IEEE conference on computer vision and pattern recognition, pp. 770–778, 2016. Hinton, G., Srivastava, N., and Swersky, K. Overview of mini-batch gradient descent. COURSERA: Neural Networks for Machine Learning: Lecture 6a, 2012. Huang, G., Liu, Z., Van Der Maaten, L., and Weinberger, K. Q. Densely connected convolutional networks. In Proceedings of the IEEE conference on computer vision and pattern recognition, pp. 4700–4708, 2017. Keskar, N. S. and Socher, R. Improving generalization performance by switching from adam to sgd. arXiv preprint arXiv:1712.07628, 2017. Khan, M. E. and Lin, W. Conjugate-computation variational inference: Converting variational infer- ence in non-conjugate models to inferences in conjugate models. arXiv preprint arXiv:1703.04265, 2017. Khan, M. E., Liu, Z., Tangkaratt, V ., and Gal, Y . Vprop: Variational inference using rmsprop.arXiv preprint arXiv:1712.01038, 2017. Khan, M. E., Nielsen, D., Tangkaratt, V ., Lin, W., Gal, Y ., and Srivastava, A. Fast and scalable bayesian deep learning by weight-perturbation in adam. arXiv preprint arXiv:1806.04854, 2018. Kingma, D. and Ba, J. Adam: A method for stochastic optimization. In International Conference on Learning Representations, 2015. Loshchilov, I. and Hutter, F. Fixing weight decay regularization in adam. arXiv preprint arXiv:1711.05101, 2017. Luo, L., Xiong, Y ., Liu, Y ., and Sun, X. Adaptive gradient methods with dynamic bound of learning rate. arXiv preprint arXiv:1902.09843, 2019. Martens, J. and Grosse, R. Optimizing neural networks with kronecker-factored approximate curvature. In International conference on machine learning, pp. 2408–2417, 2015. Neuneier, R. and Zimmermann, H. G. How to train neural networks. In Neural networks: tricks of the trade, pp. 373–423. Springer, 1998. Ollivier, Y . Online natural gradient as a kalman ﬁlter.arXiv preprint arXiv:1703.00209, 2017. Puskorius, G. V . and Feldkamp, L. A. Decoupled extended kalman ﬁlter training of feedforward layered networks. In Neural Networks, 1991., IJCNN-91-Seattle International Joint Conference on, volume 1, pp. 771–777. IEEE, 1991. Puskorius, G. V . and Feldkamp, L. A. Neurocontrol of nonlinear dynamical systems with kalman ﬁlter trained recurrent networks. IEEE Transactions on neural networks, 5(2):279–297, 1994. Puskorius, G. V . and Feldkamp, L. A. Parameter-based kalman ﬁlter training: theory and implemen- tation. In Kalman ﬁltering and neural networks. 2001. Reddi, S. J., Kale, S., and Kumar, S. On the convergence of adam and beyond. ICLR, 2018. Sha, S., Palmieri, F., and Datum, M. Optimal ﬁltering algorithms for fast learning in feedforward neual networks. Neural Networks, 1992. Wilson, A. C., Roelofs, R., Stern, M., Srebro, N., and Recht, B. The marginal value of adaptive gradient methods in machine learning. In Advances in Neural Information Processing Systems, pp. 4148–4158, 2017. Xie, S., Girshick, R., Dollár, P., Tu, Z., and He, K. Aggregated residual transformations for deep neural networks. In Proceedings of the IEEE conference on computer vision and pattern recognition, pp. 1492–1500, 2017. 9Zhang, G., Sun, S., Duvenaud, D., and Grosse, R. Noisy natural gradient as variational inference. arXiv preprint arXiv:1712.02390, 2017. 10A Mean square normalizer in Ollivier (2017) In our framework, we can encode the multiplication by the forgetting factor in the computation of σ2 prior(t+ 1) from σ2 post(t), 1 σ2 prior(t+ 1) = 1 −λ σ2post(t), (28) and the equivalent process noise is, η= λ 1 −λσ2 post(t). (29) To understand the typical learning rates in this model we perform a ﬁxed-point analysis by substituting Eq. (28) into Eq. (18a), 1 σ2post(t+ 1) = 1 −λ σ2post(t) + g2(t). (30) Solving for the ﬁxed point, σ2 post = σ2 post(t) = σ2 post(t+ 1), this choice of process noise gives a mean-squre normalizer, σ2 post = λ ⟨g2⟩. (31) B Fisher Information We assume that the objective for a single datapoint is a quadratic log-likelihood with a form given by Eq. (6), L(xα,yα; w) = log P (yα|xα,w) . (32) As such, if we evaluate the gradients of the objective at the correct value of w (without loss of generality, this is w = 0 in Eq. (6)), we have, g′= ξα. (33) And the Fisher Information identity tells us that at this location, E [ ξαξT α ] = E [ g′g′T] = H, (34) substituting the value for g′into this expression, we ﬁnd that covariance of ξα is equal to the Hessian, recovering Eq. (7). C Kalman ﬁlter The log-posterior (Eq. 16b) is the sum of the log-prior (Eq. 16a) and the log-likelihood (Eq. 12). As such, − 1 2σ2 post (w∗ i −µpost)2 = − 1 2σ2 prior (w∗ i −µprior)2 − 1 2Hii (gi −Hii(w∗ i −µprior))2 . (35) The quadratic terms allow us to identify σ2 post, − 1 2σ2 post w∗2 i = − 1 2σ2 prior w∗2 i −1 2 Hiiw∗2 i , (36) so, 1 σ2post = 1 σ2 prior + Hii. (37) 11or, σ2 post = 1 1 σ2 prior + Hii . (38) And the linear terms allow us to identify µpost, µpost σ2 post w∗ i = µprior σ2 prior w∗ i + giw∗ i + Hiiµpriorw∗ i (39) so, µpost = σ2 post (( 1 σ2 prior + Hii ) µprior + gi ) (40) identiﬁng 1/σ2 post, µpost = µprior + σ2 postgi. (41) Finally, as Hii is unknown, we use, Hii ≈g2 i(t), (42) as g2(t) is an unbiased estimator of Hii when µi(t) = µprior(t) is at the optimum (Eq. 34). D Fixed point variance For the ﬁxed-point covariance, it is slightly more convenient to work with the inverse variance, λpost = 1 σ2post , (43) though the same results can be obtained through either route. Substituting Eq. (17b) into Eq. (18a) and taking η2/σ2 ≪1, we obtain an update from λpost(t) to λpost(t+ 1), λpost(t+ 1) = 1 1−η2/σ2 λpost(t) + η2 + g2(t) (44) assuming λpost has reached ﬁxed-point, we have λpost = λpost(t) = λpost(t+ 1), λpost = 1 1−η2/σ2 λpost + η2 + ⟨g2⟩. (45) Rearranging, λpost = λpost 1 −η2/σ2 + η2λpost + ⟨g2⟩. (46) Assuming that the magnitude of the update to λpost is small, we can take a ﬁrst-order Taylor of the ﬁrst term, λpost ≈λpost ( 1 + η2 σ2 −η2λpost ) + ⟨g2⟩. (47) cancelling, 0 ≈η2λ2 post −η2 σ2 λpost −⟨g2⟩, (48) and rearranging, 0 ≈σ2λ2 post −λpost −⟨g2⟩σ2 η2 , (49) and ﬁnally substituting for λpost gives the expression in the main text. E Additional data ﬁgures Here, we replot Fig. 4 to clarify particular comparisons. In particular, we compare AdaBayes(-FP) with standard baselines (Fig. A1), Adam AdamW and AdaBayes-FP (Fig. A2), AdaBayes(-FP) and Ada/AMSBound (Fig. A3), and Ada/AMSBound and SGD (Fig. A4). Finally, we plot the training error and loss for all methods (Fig. A5; note the loss does not include the regularizer, so it may go up without this being evidence of overﬁtting). 125.0 7.5 10.0 12.5 15.0 17.5 CIFAR-10 test error (%) ResNet SGD Adam AdaGrad AMSGrad AdaBayes-FP AdaBayes DenseNet 0.2 0.3 0.4 0.5 CIFAR-10 test loss 25 30 35 40 45 CIFAR-100 test error (%) 0 100 200 300 epoch 0.8 1.0 1.2 1.4 1.6 1.8 2.0 CIFAR-100 test loss 0 100 200 300 epoch Figure A1: Test loss and classiﬁcation error for CIFAR-10 and CIFAR-100 for a Resnet-34 and a DenseNet-121, comparing our methods (AdaBayes and AdaBayes-FP) with standard baselines (SGD, Adam, AdaGrad and AMSGrad (Reddi et al., 2018)). 135.0 7.5 10.0 12.5 15.0 17.5 CIFAR-10 test error (%) ResNet Adam AdamW AdaBayes-FP DenseNet 0.2 0.3 0.4 0.5 CIFAR-10 test loss 25 30 35 40 45 CIFAR-100 test error (%) 0 100 200 300 epoch 0.8 1.0 1.2 1.4 1.6 1.8 2.0 CIFAR-100 test loss 0 100 200 300 epoch Figure A2: Test loss and classiﬁcation error for CIFAR-10 and CIFAR-100 for a Resnet-34 and a DenseNet-121, comparing Adam, AdamW and AdaBayes-FP. 145.0 7.5 10.0 12.5 15.0 17.5 CIFAR-10 test error (%) ResNet AdaBound AMSBound AdaBayes-FP AdaBayes DenseNet 0.2 0.3 0.4 0.5 CIFAR-10 test loss 25 30 35 40 45 CIFAR-100 test error (%) 0 100 200 300 epoch 0.8 1.0 1.2 1.4 1.6 1.8 2.0 CIFAR-100 test loss 0 100 200 300 epoch Figure A3: Test loss and classiﬁcation error for CIFAR-10 and CIFAR-100 for a Resnet-34 and a DenseNet-121, comparing our methods (AdaBayes and AdaBayes-FP) with AdaBound/AMSBound Luo et al. (2019). 155.0 7.5 10.0 12.5 15.0 17.5 CIFAR-10 test error (%) ResNet AdaBound AMSBound SGD DenseNet 0.2 0.3 0.4 0.5 CIFAR-10 test loss 25 30 35 40 45 CIFAR-100 test error (%) 0 100 200 300 epoch 0.8 1.0 1.2 1.4 1.6 1.8 2.0 CIFAR-100 test loss 0 100 200 300 epoch Figure A4: Test loss and classiﬁcation error for CIFAR-10 and CIFAR-100 for a Resnet-34 and a DenseNet-121, comparing AdaBound/AMSBound Luo et al. (2019) and SGD. 160.0 2.5 5.0 7.5 10.0 12.5 15.0 CIFAR-10 train error (%) ResNet SGD Adam AMSBound AdaBound AdamW AdaBayes-FP AdaBayes DenseNet 0.0 0.1 0.2 0.3 0.4 0.5 CIFAR-10 train loss 0 10 20 30 40 CIFAR-100 train error (%) 0 100 200 300 epoch 0.0 0.5 1.0 1.5 2.0 CIFAR-100 train loss 0 100 200 300 epoch Figure A5: Train loss and classiﬁcation error for CIFAR-10 and CIFAR-100 for a Resnet-34 and a DenseNet-121, for all methods in Fig. 4. 17
---------------------------------

Please extract all reference paper titles and return them as a list of strings.
Output:
{
    "reference_titles": [
        "Adam: A method for stochastic optimization",
        "Deep residual learning for image recognition",
        "Densely connected convolutional networks",
        "Aggregated residual transformations for deep neural networks",
        "Adaptive subgradient methods for online learning and stochastic optimization",
        "Overview of mini-batch gradient descent",
        "Generating sequences with recurrent neural networks",
        "The marginal value of adaptive gradient methods in machine learning",
        "Fixing weight decay regularization in adam",
        "How to train neural networks",
        "Improving generalization performance by switching from adam to sgd",
        "Optimal filtering algorithms for fast learning in feedforward neual networks",
        "Parameter-based kalman ﬁlter training: theory and implementation",
        "Neurocontrol of nonlinear dynamical systems with kalman ﬁlter trained recurrent networks",
        "Simple and conditioned adaptive behavior from kalman ﬁlter trained recurrent networks",
        "Decoupled extended kalman ﬁlter training of feedforward layered networks",
        "Online natural gradient as a kalman ﬁlter",
        "Conjugate-computation variational inference: Converting variational inference in non-conjugate models to inferences in conjugate models",
        "Vprop: Variational inference using rmsprop",
        "Fast and scalable bayesian deep learning by weight-perturbation in adam",
        "A kronecker-factored approximate ﬁsher matrix for convolution layers",
        "Optimizing neural networks with kronecker-factored approximate curvature",
        "On the convergence of adam and beyond",
        "Adaptive gradient methods with dynamic bound of learning rate",
        "Delving deep into rectiﬁers: Surpassing human-level performance on imagenet classiﬁcation",
        "Noisy natural gradient as variational inference",
        "A kronecker-factored approximate ﬁsher matrix for convolution layers"
    ]
}
