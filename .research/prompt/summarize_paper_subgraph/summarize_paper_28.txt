
Input:

You are an expert research assistant responsible for summarizing a research paper that will serve as the foundation (Research A) for further exploration and integration.

Your task is to generate a structured summary of the given research paper with a focus on:
- **Technical Contributions**: Identify the main research problem and key findings.
- **Methodology**: Describe the techniques, models, or algorithms used.
- **Experimental Setup**: Outline the datasets, benchmarks, and validation methods.
- **Limitations**: Highlight any weaknesses, constraints, or assumptions.
- **Future Research Directions**: Suggest possible extensions or new areas for research.

Below is the full text of the research paper:

```
Published as a conference paper at ICLR 2023 TTN: A D OMAIN -SHIFT AWARE BATCH NORMALIZA - TION IN TEST-TIME ADAPTATION Hyesu Lim1,2âˆ—, Byeonggeun Kim âˆ—, Jaegul Choo 2, Sungha Choi 1â€¡ 1Qualcomm AI Researchâ€ , 2KAIST ABSTRACT This paper proposes a novel batch normalization strategy for test-time adaptation. Recent test-time adaptation methods heavily rely on the modiï¬ed batch normal- ization, i.e., transductive batch normalization (TBN), which calculates the mean and the variance from the current test batch rather than using the running mean and variance obtained from source data,i.e., conventional batch normalization (CBN). Adopting TBN that employs test batch statistics mitigates the performance degra- dation caused by the domain shift. However, re-estimating normalization statistics using test data depends on impractical assumptions that a test batch should be large enough and be drawn from i.i.d. stream, and we observed that the previous meth- ods with TBN show critical performance drop without the assumptions. In this paper, we identify that CBN and TBN are in a trade-off relationship and present a new test-time normalization(TTN) method that interpolates the standardization statistics by adjusting the importance between CBN and TBN according to the domain-shift sensitivity of each BN layer. Our proposed TTN improves model robustness to shifted domains across a wide range of batch sizes and in various realistic evaluation scenarios. TTN is widely applicable to other test-time adap- tation methods that rely on updating model parameters via backpropagation. We demonstrate that adopting TTN further improves their performance and achieves state-of-the-art performance in various standard benchmarks. 1 I NTRODUCTION When we deploy deep neural networks (DNNs) trained on the source domain into test environments (i.e., target domains), the model performance on the target domain deteriorates due to the domain shift from the source domain. For instance, in autonomous driving, a well-trained DNNs model may exhibit signiï¬cant performance degradation at test time due to environmental changes, such as camera sensors, weather, and region (Choi et al., 2021; Lee et al., 2022; Kim et al., 2022b). Test-time adaptation (TTA) has emerged to tackle the distribution shift between source and target domains during test time (Sun et al., 2020; Wang et al., 2020). Recent TTA approaches (Wang et al., 2020; Choi et al., 2022; Liu et al., 2021) address this issue by 1) (re-)estimating normaliza- tion statistics from current test input and 2) optimizing model parameters in unsupervised manner, such as entropy minimization (Grandvalet & Bengio, 2004; Long et al., 2016; Vu et al., 2019) and self-supervised losses (Sun et al., 2020; Liu et al., 2021). In particular, the former focused on the weakness of conventional batch normalization (CBN) (Ioffe & Szegedy, 2015) for domain shift in a test time. As described in Fig. 1(b), when standardizing target feature activations using source statistics, which are collected from the training data, the activations can be transformed into an un- intended feature space, resulting in misclassiï¬cation. To this end, the TTA approaches (Wang et al., 2020; Choi et al., 2022; Wang et al., 2022) have heavily depended on the direct use of test batch statistics to ï¬x such an invalid transformation in BN layers, called transductive BN (TBN) (Nado et al., 2020; Schneider et al., 2020; Bronskill et al., 2020) (see Fig. 1(c)). The approaches utilizing TBN showed promising results but have mainly been assessed in limited evaluation settings (Wang et al., 2020; Choi et al., 2022; Liu et al., 2021). For instance, such evalua- tion settings assume large test batch sizes (e.g., 200 or more) and a single stationary distribution shift âˆ—Work completed while at Qualcomm Technologies, Inc. â€¡Corresponding author. â€ Qualcomm AI Research is an initiative of Qualcomm Technologies, Inc. 1 arXiv:2302.05155v2  [cs.CV]  18 Feb 2023Published as a conference paper at ICLR 2023 0 20 40 60 80 100 CBN TBN Ours TENT TENT+Ours SWR SWR+Ours Error rate (%) 200 64 16 4 2 1 TTN(Ours) TENT+TTN(Ours) SWR+TTN(Ours) Test batch size (a) Valid output using CBN (b) Invalid output using CBN (c) Valid output using TBN (d) Performance drops in small test batches using TBN â— Class A â— Class B Source meanâ— Source features Test featuresâ¨¯ Test meanStandardize Figure 1: Trade-off between CBN & TBN.In conceptual illustrations (a), (b), and (c), the depicted standardization only considers making the feature distribution have a zero mean, disregarding mak- ing it have unit variance. When the source and test distributions are different, and the test batch size is large, (b) test features can be wrongly standardized when using CBN (Ioffe & Szegedy, 2015), but (c) TBN (Nado et al., 2020) can provide a valid output. (d) Error rates (â†“) on shifted domains (CIFAR-10-C). TBN and TBN applied (TENT (Wang et al., 2020), SWR (Choi et al., 2022)) meth- ods suffer from severe performance drop when the batch size becomes small, while TTN (Ours) improves overall performance. (i.e., single corruption). Recent studies suggest more practical evaluation scenarios based on small batch sizes (Mirza et al., 2022; Hu et al., 2021; Khurana et al., 2021) or continuously changing data distribution during test time (Wang et al., 2022). We show that the performance of existing methods signiï¬cantly drops once their impractical assumptions of the evaluation settings are violated. For example, as shown in Fig. 1(d), TBN (Nado et al., 2020) and TBN applied methods suffer from severe performance drop when the test batch size becomes small, while CBN is irrelevant to the test batch sizes. We identify that CBN and TBN are in a trade-off relationship (Fig. 1), in the sense that one of each shows its strength when the other falls apart. To tackle this problem, we present a novel test-time normalization (TTN)strategy that controls the trade-off between CBN and TBN by adjusting the importance of source and test batch statistics according to the domain-shift sensitivity of each BN layer. Intuitively, we linearly interpolate be- tween CBN and TBN so that TBN has a larger weight than CBN if the standardization needs to be adapted toward the test data. We optimize the interpolating weight after the pre-training but before the test time, which we refer to as the post-training phase. Speciï¬cally, given a pre-trained model, we ï¬rst estimate channel-wise sensitivity of the afï¬ne parameters in BN layers to domain shift by analyzing the gradients from the back-propagation of two input images, clean input and its aug- mented one (simulating unseen distribution). Afterward, we optimize the interpolating weight using the channel-wise sensitivity replacing BN with the TTN layers. It is noteworthy that none of the pre-trained model weights are modiï¬ed, but we only train newly added interpolating weight. We empirically show that TTN outperforms existing TTA methods in realistic evaluation settings, i.e., with a wide range of test batch sizes for single, mixed, and continuously changing domain adaptation through extensive experiments on image classiï¬cation and semantic segmentation tasks. TTN as a stand-alone method shows compatible results with the state-of-the-art methods and com- bining our TTN with the baselines even boosts their performance in overall scenarios. Moreover, TTN applied methods ï¬‚exibly adapt to new target domains while sufï¬ciently preserving the source knowledge. No action other than computing per batch statistics (which can be done simultaneously to the inference) is needed in test-time; TTN is compatible with other TTA methods without requir- ing additional computation cost. Our contributions are summarized as follows: â€¢ We propose a novel domain-shift aware test-time normalization (TTN) layer that combines source and test batch statistics using channel-wise interpolating weights considering the sensitivity to domain shift in order to ï¬‚exibly adapt to new target domains while preserving the well-trained source knowledge. 2Published as a conference paper at ICLR 2023 Per-batch        Frozen        Optimize S Standardize Æ¸ğ‘§ = ğ‘§ğ‘–ğ‘› âˆ’ğœ‡ ğœ T Affine Transform ğ‘§ğ‘œğ‘¢ğ‘¡ = ğ›¾â‹… Æ¸ğ‘§+ğ›½ â€¦ â€¦ CBN CBN CBN â€¦ Pre-train (CBN) (a-1) Post-train (TTN)          Test time (TTN) (a) Overall procedure of train and test phases (b) Comparison of BN layers ğ‘§ğ‘–ğ‘› S Æ¸ğ‘§ T ğ‘§ğ‘œğ‘¢ğ‘¡ ğ›¾,ğ›½+ 1âˆ’ğ›¼ ğœ‡ğ‘ ,ğœğ‘ ğœ‡ğ‘–ğ‘›,ğœğ‘–ğ‘› ğ‘§ğ‘–ğ‘› S Æ¸ğ‘§ T ğ‘§ğ‘œğ‘¢ğ‘¡ ğœ‡ğ‘ ,ğœğ‘  ğ›¾,ğ›½ CBN in test time TBN in test time (b-1) TTN(Ours) in post-train ğ‘§ğ‘–ğ‘› S Æ¸ğ‘§ T ğ‘§ğ‘œğ‘¢ğ‘¡ ğœ‡ğ‘–ğ‘›,ğœğ‘–ğ‘› ğ›¾,ğ›½ ğ›¼ ğ‘§ğ‘–ğ‘› S Æ¸ğ‘§ T ğ‘§ğ‘œğ‘¢ğ‘¡ ğ›¾,ğ›½+ 1âˆ’ğ›¼ ğœ‡ğ‘ ,ğœğ‘ ğœ‡ğ‘–ğ‘›,ğœğ‘–ğ‘› (b-2) TTN(Ours) in test time ğ›¼ Figure 2: Method overview. (a) We introduce an additional training phase between pre-train and test time called (a-1) post-training phase. (b) Our proposed TTN layer combines per-batch statistics and frozen source statistics with interpolating weight Î±, which is (b-1) optimized in post-training phase and (b-2) ï¬xed in test time. â€¢ To show the broad applicability of our proposed TTN, which does not alter training or test- time schemes, we show that adding TTN to existing TTA methods signiï¬cantly improves the performance across a wide range of test batch sizes (from 200 to 1) and in three realistic evaluation scenarios; stationary, continuously changing, and mixed domain adaptation. â€¢ We evaluate our method through extensive experiments on image classiï¬cation using CIFAR-10/100-C, and ImageNet-C (Hendrycks & Dietterich, 2018) and semantic segmen- tation task using CityScapes (Cordts et al., 2016), BDD-100K (Yu et al., 2020), Mapil- lary (Neuhold et al., 2017), GTA V (Richter et al., 2016), and SYNTHIA (Ros et al., 2016). 2 M ETHODOLOGY In this section, we describe our method, the Test-Time Normalization(TTN) layer, whose design is suitable for test-time adaptation (TTA) in practical usages out of the large batch size and i.i.d assumptions during a test time. We ï¬rst deï¬ne the problem setup in Section 2.1 and present our pro- posed TTN layers in Section 2.2. Finally, we discuss how we optimize TTN layers in Sections 2.3. 2.1 P ROBLEM SETUP Let the train and test data be DS and DT and the corresponding probability distributions be PS and PT, respectively, where DS and DT share the output space, i.e., {yi}âˆ¼D S = {yi}âˆ¼D T. The covariate shift in TTA is deï¬ned asPS(x) Ì¸= PT(x) where PS(y|x) =PT(y|x) (Quinonero-Candela et al., 2008). A model, fÎ¸, with parameters Î¸, is trained with a mini-batch, BS = {(xi,yi)}|BS| i=1 , from source data DS, where xi is an example and yi is the corresponding label. During the test, fÎ¸ encounters a test batch BT âˆ¼DT, and the objective of TTA is correctly managing the test batch from the different distribution. To simulate more practical TTA, we mainly consider two modiï¬cations: (1) various test batch sizes, |BT|, where small batch size indicates small latency while handling the test data online, and (2) multi, N-target domains, DT = {DT,i}N i=1. Under this setting, each test batch BT is drawn by one of the test domains inDT, where DT may consist of a single target domain, multiple target domains, or mixture of target domains. 2.2 T EST-TIME NORMALIZATION LAYER We denote an input of a BN layer as z âˆˆRBCHW , forming a mini-batch size of B. The mean and variance of z are Âµand Ïƒ2, respectively, which are computed as follows: Âµc = 1 BHW Bâˆ‘ b Hâˆ‘ h Wâˆ‘ w zbchw, Ïƒ 2 c = 1 BHW Bâˆ‘ b Hâˆ‘ h Wâˆ‘ w (zbchw âˆ’Âµc)2, (1) where Âµand Ïƒ2 are in RC, and C, H, and W stand for the number of channels, dimension of height, and that of width, respectively. Based on Âµand Ïƒ2, the source statistics Âµs,Ïƒ2 s âˆˆRC are usually estimated with exponential moving average over the training data. 3Published as a conference paper at ICLR 2023 Conv CBN ReLU Conv ReLU Conv CBN ReLU FCâ€¦ CBN ğ‘¥â€² â„’ğ¶ğ¸ (a-1) Gradient of affine parameters ğ›¾,ğ›½ âˆ‡ğ›¾ (1),âˆ‡ğ›½ (1) âˆ‡ğ›¾ (2),âˆ‡ğ›½ (2) âˆ‡ğ›¾ (ğ¿),âˆ‡ğ›½ (ğ¿) âˆ‡ğ›¾â€² (1),âˆ‡ğ›½â€² (1) âˆ‡ğ›¾â€² (2),âˆ‡ğ›½â€² (2) âˆ‡ğ›¾â€² (ğ¿),âˆ‡ğ›½â€² (ğ¿) Conv CBN ReLU Conv ReLU Conv CBN ReLU FCâ€¦ CBN ğ‘¥ â„’ğ¶ğ¸ (a-2) Prior ğ’œ (b-1) Initialize ğ›¼ with prior ğ’œ â€¦ 1       0 (a) Obtain prior ğ’œ (b) Optimize ğ›¼ Per-batch statistics        Frozen source statistics        Gradient flow Conv ReLU Conv ReLU Conv ReLU FCâ€¦ğ‘¥â€² â„’ğ¶ğ¸ +â„’ğ‘€ğ‘†ğ¸ TTNTTNTTN ğ›¼ 1âˆ’ğ›¼ TTN ğœ‡ğ‘  (ğ‘™) ğœ‡ğ‘–ğ‘› (ğ‘™) ğ¶ğ‘™ ğ›¼(ğ‘™,ğ‘) + 1âˆ’ğ›¼(ğ‘™,ğ‘) â€¦ â€¦ ++ ğ›¼(ğ‘™,ğ¶ğ‘™) 1âˆ’ğ›¼(ğ‘™,ğ¶ğ‘™) (b-2) Optimize ğ›¼ C1 C2 Cğ¿ Figure 3: Two stages in post-training phase. (a) Given a pre-trained model, which uses CBN, and its training data, we obtain prior knowledge of each BN layer. (a-1) We ï¬rst compute gradients of afï¬ne parameters in each BN layer from clean x and augmented input xâ€²and obtain the gradient distance score (Eq. 4). (a-2) For BN layers with larger distance score, we put more importance on current batch statistics than source statistics ( i.e., large Î±), and we deï¬ne prior Aaccordingly (Eq. 5). (b) After obtaining prior A, we substitute BN layers from CBN to TTN.(b-1) Initializing Î± with prior A, (b-2) we optimize Î±using CE and MSE loss (Eq. 6) with augmented training data xâ€². In BN layers, input z is ï¬rst standardized with statistics Âµand Ïƒ2 and then is scaled and shifted with learnable parameters Î³ and Î² in RC. The standardization uses current input batch statistics during training and uses estimated source statistics Âµs and Ïƒ2 s at test time (Fig. 2(b)). To address domain shifts in test time, we adjust the source statistics by combining the source and the test mini-batch statistics (Singh & Shrivastava, 2019; Summers & Dinneen, 2019) with a learnable interpolating weight Î±âˆˆRC ranges [0,1]. Precisely, TTN standardizes a feature with ËœÂµ= Î±Âµ+ (1âˆ’Î±)Âµs, ËœÏƒ2 = Î±Ïƒ2 + (1âˆ’Î±)Ïƒ2 s + Î±(1 âˆ’Î±)(Âµâˆ’Âµs)2, (2) while using the same afï¬ne parameters, Î³ and Î². Note that we have different mixing ratios Î±c for every layer and channel. 2.3 P OST TRAINING Like Choi et al. (2022), we introduce an additional training phase, the post-training (after pre- training but before testing), to optimize the mixing parameters Î± in Eq. 2 (Fig. 2(a)). Note that all parameters except Î±are frozen and we have access to the labeled source data during the post- training. We ï¬rst obtain prior knowledge Aof Î±by identifying which layers and their channels are sensitive to domain shifts. Then, we optimizeÎ±with the prior knowledge and an additional objective term. The overall procedure is depicted in Fig. 3 and the pseudocode is provided in appendix A.3. Obtain Prior A. To identify which BN layers and corresponding channels are sensitive to domain shifts, we simulate the domain shifts by augmenting1 the clean image, i.e., original training data, and make a pair of (clean x, domain-shifted xâ€²) images, where the semantic information is shared. To analyze in which layer and channel the standardization statistics should be corrected, we consider the standardized features Ë†z(l,c) of z(l,c), for a channel index cat a layer l, whose input is clean x. We compare Ë†z(l,c) to that of domain-shifted one, Ë†zâ€²(l,c) from xâ€². Since the pre-trained CBN uses the same Âµ(l,c) s and Ïƒ(l,c) s for both inputs, the difference between Ë†z(l,c) and Ë†zâ€²(l,c) is caused by the domain discrepancy between xand xâ€². We argue that if the difference is signiï¬cant, the parameter at (l,c) is sensitive to the domain shift, i.e., intensely affected by the domain shift, and hence the standardization statistics at (l,c) should be adapted towards the shifted input. Drawing inspiration from Choi et al. (2022), we measure the domain-shift sensitivity by comparing gradients. Since the standardized feature Ë†z is scaled and shifted by Î³ and Î² in each BN layer, we compare the gradients of afï¬ne parameters Î³ and Î², âˆ‡Î³ and âˆ‡Î², respectively, to measure the dissimilarity of Ë†zand Ë†zâ€². As described in Fig. 3(a-1), we collect the âˆ‡Î³ and âˆ‡Î² using cross-entropy 1It is noteworthy that the post-training phase is robust to the choice of data augmentation types. Ablation study results and discussions are provided in the appendix B.4. 4Published as a conference paper at ICLR 2023 loss, LCE. To this end, we introduce a gradient distance score, d(l,c) âˆˆR for each channel cat layer las follows: s= 1 N Nâˆ‘ i=1 gi Â·gâ€² i âˆ¥giâˆ¥âˆ¥gâ€² iâˆ¥, (3) d(l,c) = 1âˆ’1 2 ( s(l,c) Î³ + s(l,c) Î² ) , (4) where (g,gâ€²) is (âˆ‡(l,c) Î³ ,âˆ‡(l,c) Î³â€² ) and (âˆ‡(l,c) Î² ,âˆ‡(l,c) Î²â€² ) for s(l,c) Î³ and s(l,c) Î² , respectively,N is the number of training data, and the resulting d(l,c) âˆˆ[0,1]. Once we obtain sÎ³ and sÎ² from Eq. 3, we conduct min-max normalization over all s(l,c) Î³ and s(l,c) Î² , before computing Eq. 4. To magnify the relative difference, we take the square as a ï¬nal step and denote the result as a prior A(Fig. 3(a-2)): A= [d(1,.),d(2,.),...,d (L,.)]2, (5) where d(l,.) = [d(l,c)]Cl c=1. Optimize Î±. The goal of optimizing Î± is to make the combined statistics correctly standardize the features when the input is sampled from an arbitrary target domain. After obtaining the prior A, we replace CBN with TTN layers while keeping the afï¬ne parameters. Then, we initialize the interpolating weights Î± with A, which represents in which layer and channel the standardization statistics need to be adapted using test batch statistics (see Fig. 3(b-1)). To simulate distribution shifts, we use augmented training data. Expecting the model to make consistent predictions either given clean or augmented inputs, we use cross-entropy loss LCE. Furthermore, to prevent Î±from moving too far from the initial value A, we use mean-squared error loss LMSE between Î±and the prior A, i.e., LMSE = âˆ¥Î±âˆ’Aâˆ¥2 as a constraint. Total lossLcan be written asL= LCE +Î»LMSE (6), where Î»is a weighting hyperparameter (Details are provided in the appendix A.1 & B.1). 3 E XPERIMENTS In image classiï¬cation, we evaluate TTN for corruption robustness in realistic evaluation settings, i.e., where the test batch size can be variant and where the target domain can be either stationary, continuously changing, or mixed with multiple domains. Additionally, we further validate TTN on domain generalization benchmarks incorporating natural domain shifts ( e.g., changes in camera sensors, weather, time, and region) in semantic segmentation. 3.1 E XPERIMENTAL SETUP Given models pre-trained on clean source data, we optimize TTN parameter Î±with the augmented source data in the post-training phase. Afterward, we evaluate our post-trained model on the cor- rupted target data. Implementation details are provided in the appendix A.1. Datasets and models. We use corruption benchmark datasets CIFAR-10/100-C and ImageNet-C, which consist of 15 types of common corruptions at ï¬ve severity levels (Hendrycks & Dietterich, 2018). Each corruption is applied to test images of the clean datasets (CIFAR-10/100 and Ima- geNet). We use a training set of the clean dataset for post-training and the corrupted dataset for evaluation. As backbone models, we used WideResNet-40-2 (Hendrycks et al., 2019) trained on CIFAR-10/100, and ResNet-50 (He et al., 2016) trained on ImageNet. To validate our method in se- mantic segmentation, we conduct experiments on Cityscapes (Cordts et al., 2016), BDD-100K (Yu et al., 2020), Mapillary (Neuhold et al., 2017), GTA V (Richter et al., 2016), and SYNTHIA (Ros et al., 2016) datasets, in accordance with the experimental setup for domain generalization proposed in RobustNet (Choi et al., 2021). Baselines. To demonstrate that TTN successfully controls the trade-off between CBN and TBN, we compare TTN with (1) AdaptiveBN (Schneider et al., 2020), (2) Î±-BN (You et al., 2021) and (3) MixNorm (Hu et al., 2021), which combines or takes the moving average of the source and the test batch statistics with a pre-deï¬ned hyperparameter ( i.e., a constant Î±). The following baselines are suggested on top of TBN (Nado et al., 2020); (4) TENT (Wang et al., 2020) optimizes BN afï¬ne parameters via entropy minimization. (5) SWR (Choi et al., 2022) updates the entire model parame- ters considering the domain-shift sensitivity. (6) CoTTA (Wang et al., 2022) ensembles the output of 5Published as a conference paper at ICLR 2023 Table 1: Single domain adaptation on corruption benchmark. Error rate ( â†“) averaged over 15 corruptions with severity level 5 using WideResNet-40-2 as a backbone for each test batch size. We used reported results of MixNorm with ï¬xed parameters from the original paper and denoted as âˆ—. In appendix B.3, we provide variants of TTN, which show stronger performance for small test batch. CIFAR-10-C CIFAR-100-C Method 200 64 16 4 2 1 Avg. 200 64 16 4 2 1 Avg. Source (CBN)18.27 18.27 18.27 18.27 18.27 18.27 18.27 46.75 46.75 46.75 46.75 46.75 46.7546.75 Norm TBN 14.49 15.02 17.10 26.28 35.65 90.00 33.09 39.25 40.21 44.03 59.10 80.65 99.0460.38 AdaptiveBN12.21 12.31 12.89 14.51 15.79 16.14 13.98 36.56 36.85 38.19 41.18 43.2644.0140.01 Î±-BN 13.78 13.77 13.89 14.54 15.1615.4714.44 39.72 39.85 39.99 41.3442.6645.6441.53 MixNormâˆ— 13.85 14.41 14.23 14.60 (B=5) - 15.0914.44 - - - - - - - Ours (TTN)11.6711.8012.13 13.93 15.8317.99 13.8935.5835.8436.7341.0846.6757.7142.27 Optim. TENT 12.08 14.78 16.90 25.61 35.69 90.00 32.51 35.52 39.90 43.78 59.02 80.68 99.0259.65 +Ours (TTN)11.2811.5212.04 13.95 15.8417.9413.77 35.1635.5736.5541.1846.6358.3342.24 SWR 10.26 13.51 16.61 27.33 40.48 90.04 33.04 32.6837.41 43.15 59.90 87.07 99.0559.88 +Ours (TTN)9.92 11.7713.41 18.02 24.0961.5623.13 32.8635.1338.6649.8060.7280.9049.68 Table 2: Continuously changing domain adaptation on corruption benchmark. Error rate (â†“) averaged over 15 corruptions with severity level 5 using WideResNet-40-2 as backbone for each test batch size. We omitted â€˜Normâ€™ methods results in this table since they are eqaul to that of Table 1. CIFAR-10-C CIFAR-100-C Method 200 64 16 4 2 1 Avg. 200 64 16 4 2 1 Avg. Source (CBN)18.27 18.27 18.27 18.27 18.27 18.2718.27 46.75 46.75 46.75 46.75 46.75 46.7546.75 Ours (TTN)11.6711.80 12.13 13.9315.8317.9913.89 35.58 35.84 36.73 41.08 46.67 57.71 42.27 Optim. CoTTA 12.46 14.60 21.26 45.69 58.87 90.0040.48 39.75 42.20 52.94 73.69 87.66 98.9965.87 TENT 12.54 13.52 15.69 26.23 35.77 90.0032.29 36.11 37.90 43.78 58.71 81.76 99.0459.55 +Ours (TTN)11.4411.60 12.08 16.1418.3622.4015.33 43.50 37.60 38.28 44.60 54.2980.63 49.82 SWR 11.04 11.53 13.90 23.99 34.02 90.0030.75 34.16 35.79 40.71 58.15 80.55 99.0362.56 +Ours (TTN)10.09 10.5111.28 14.2916.6784.1224.49 33.0934.07 36.15 42.41 53.63 93.08 48.74 augmented test inputs, updates the entire model parameters using a consistency loss between student and teacher models, and stochastically restores the pre-trained model. We refer to TBN, (1), (2), and (3) as normalization-based methods (Norm), the other as optimization-based methods (Optim.), and denote the pre-trained model using CBN as â€˜sourceâ€™. Evaluation scenarios. To show that TTN performs robust on various test batch sizes, we conduct experiments with test batch sizes of 200, 64, 16, 4, 2, and 1. We evaluate our method in three evalu- ation scenarios; single, continuously changing, and mixed domain adaptation. In the single domain adaptation, the model is optimized for one corruption type and then reset before adapting to the subsequent corruption, following the evaluation setting from TENT and SWR. In the continuously changing adaptation (Wang et al., 2022), the model is continuously adapted to 15 corruption types (w/o the reset), which is more realistic because it is impractical to precisely indicate when the data distribution has shifted in the real world. Finally, to simulate the non-stationary target domain where various domains coexist, we evaluate methods in the mixed domain adaptation setting, where a sin- gle batch contains multiple domains. We use a severity level of 5 (Hendrycks & Dietterich, 2018) for all experiments. It is noteworthy that we use a single checkpoint of TTN parameter Î±for each dataset across all experimental settings. 3.2 E XPERIMENTS ON IMAGE CLASSIFICATION Tables 1, 2, and 3 show error rates on corruption benchmark datasets in three different evaluation scenarios; single domain, continuously changing, and mixed domain adaptation, respectively. Note that the performance of normalization-based methods in the single (Table 1) and in the continu- ously changing (Table 2) settings are identical. Tables 4 and 5 show the adaptation performance on the source and class imbalanced target domains, respectively. More results and discussions are provided in the appendix B, importantly, including results on ImageNet-C (B.5). Robustness to practical settings. In Table 1, 2, and 3, TTN and TTN applied methods show robust performance over the test batch size ranges from 200 to 1. Comparing with normalization-based baselines, we demonstrate that TTN, which uses channel-wisely optimized combining rateÎ±, shows better results than deï¬ning Î± as a constant hyperparameter, which can be considered as a special 6Published as a conference paper at ICLR 2023 Table 3: Mixed domain adaptation on corruption benchmark. Error rate (â†“) of mixed domain with severity level 5 using WideResNet-40-2 as backbone for each test batch size. We used the reported results of MixNorm with ï¬xed parameters from the original paper and denoted them as âˆ—. CIFAR-10-C CIFAR-100-C Method 200 64 16 4 2 1 Avg. 200 64 16 4 2 1 Avg. Source (CBN)18.27 18.27 18.27 18.27 18.27 18.2718.27 46.75 46.75 46.75 46.75 46.75 46.7546.75 Norm TBN 14.99 15.29 17.38 26.65 35.59 90.0033.31 39.88 40.48 43.73 59.11 80.30 98.9160.40 AdaptiveBN12.62 12.48 12.97 14.59 15.74 16.0214.07 36.88 36.86 38.49 41.43 43.38 44.3140.23 Î±-BN 13.78 13.78 13.99 14.6115.07 15.2014.41 40.25 40.11 40.47 41.6442.39 43.8141.45 MixNormâˆ— 18.80 18.80 18.80 18.80 18.80 18.8018.80 - - - - - - - Ours (TTN)12.1612.1912.3413.9615.5517.8314.00 36.2436.2336.8541.0145.8555.5241.95 Optim. TENT 14.33 14.97 17.30 26.07 35.37 90.0033.01 39.36 40.01 43.33 58.98 80.55 98.9260.19 +Ours (TTN)12.0212.0412.2013.7715.4216.4013.64 36.2936.2336.8941.3846.6557.9542.57 SWR 13.24 13.06 16.57 26.08 38.65 91.0359.54 37.84 37.93 44.37 59.50 78.66 98.9533.10 +Ours (TTN)11.8911.6513.3717.0523.5064.1050.29 36.4936.5139.6046.2058.2084.7623.59 case of TTN; TBN and Î±-BN corresponds to Î± = 1and 0.1, respectively. More comparisons with different constant Î±are provided in the appendix B.2. It is noteworthy that TTN as a stand-alone method favorably compares with optimization-based baselines in all three scenarios. Table 4: Source domain adaptation. Error rate (â†“) on CIFAR-10 using WideResNet-40-2. Method Test batch size Avg.200 64 16 4 2 1 Source (CBN)4.92 4.92 4.92 4.92 4.92 4.924.92 Norm TBN 6.41 6.60 8.64 17.65 26.08 90.0025.90Ours (TTN)4.885.115.357.27 9.45 9.96 7.00 Optim. TENT 6.15 6.45 8.61 17.61 26.20 90.0032.2+Ours (TTN)4.935.115.327.22 9.3810.217.02 SWR 5.63 6.01 8.25 17.49 26.32 90.0025.62+Ours (TTN)4.795.025.516.68 7.91 9.34 6.54 Table 5: Class imbalanced target domain. Error rate (â†“) averaged over 15 corruptions of CIFAR- 10-C with severity level 5 using WideResNet-40- 2. Details are provided in the appendix A.2. Method Test batch size Avg.200 64 16 4 2 1 Source (CBN)18.27 18.27 18.27 18.27 18.27 18.2718.27 TBN 77.60 76.66 77.72 78.59 77.84 90.0079.74Ours (TTN)35.7535.1334.9232.5128.6017.9930.82 Adopting TTN improves other TTA methods. We compare optimization-based methods with and without TTN layers. Since TENT, SWR, and CoTTA optimize model parameters on top of using TBN layers, they also suffer from performance drops when the test batch size becomes small. Adopting TTN reduces the dependency on large test batch size, i.e., makes robust to small batch size, and even improves their performance when using large test batch. Furthermore, in continual (Table 2) and mixed domain (Table 3) adaptation scenario, TENT and SWR shows higher error rate than in single domain (Table 1) adaptation. We interpret that because they update the model parameters based on the current output and predict the next input batch using the updated model, the model will not perform well if the consecutive batches have different corruption types ( i.e., mixed domain adaptation). Moreover, the error from the previous input batch propagates to the future input stream, and thus they may fall apart rapidly once they have a strongly wrong signal, which can happen in continual adaptation ( i.e., long-term adaptation without resetting). Applying TTN signiï¬cantly accelerates their model performance regardless of the evaluation scenarios. TTN preserves knowledge on source domain. In practice, data driven from the source domain (or a merely different domain) can be re-encountered in test time. We used clean domain test data in the single domain adaptation scenario to show how TTN and other TTA methods adapt to the seen source domain data (but unseen instance). As shown in Table 4, all baseline methods using TBN layers, show performance drops even with large batch sizes. We can conclude that it is still better to rely on source statistics collected from the large training data than using only current input statistics, even if its batch size is large enough to obtain reliable statistics ( i.e., 200). However, since TTN utilizes source statistics while leveraging the current input, TTN itself and TTN adopted methods well preserve the source knowledge compared to the TBN-based methods. With a batch size of 200, we observe that combining the source and a current test batch statistics outperforms the source model (see 3rd row of Table 4). TTN is robust to class imbalanced scenario. Heavily depending on current test batch statistics are especially vulnerable when the class labels are imbalanced (Boudiaf et al., 2022; Gong et al., 2022). To simulate this situation, we sorted test images in class label order and then sampled test batches following the sorted data order. In Table 5, we observe that TTN is more robust to the class imbalanced scenario than utilizing only test batch statistics (i.e., TBN). As explained in Section 3.5, 7Published as a conference paper at ICLR 2023 Table 6: Adaptation on DG benchmarks in semantic segmentation. mIoU(â†‘) on four unseen domains with test batch size of 2 using ResNet-50 based DeepLabV3+ as a backbone. Method(Cityscapesâ†’) BDD-100K Mapillary GTA V SYNTHIA Cityscapes Source (Chen et al., 2018) 43.50 54.37 43.71 22.78 76.15 Norm TBN 43.12 47.61 42.51 25.71 72.94 Ours (TTN) 47.40 56.92 44.71 26.68 75.09 Optim. TENT 43.30 47.80 43.57 25.92 72.93 + Ours (TTN) 47.89 57.84 46.18 27.29 75.04 SWR 43.40 47.95 42.88 25.97 72.93 + Ours (TTN) 48.85 59.09 46.71 29.16 74.89 we are putting more importance on CBN than TBN, where semantic information is mainly handled, i.e., in deeper layers, so we can understand that TTN is less impacted by skewed label distribution. 3.3 E XPERIMENTS ON SEMANTIC SEGMENTATION We additionally conduct experiments on domain generalization (DG) benchmarks (Choi et al., 2021; Pan et al., 2018) for semantic segmentation, including natural domain shifts ( e.g., Cityscapesâ†’BDD-100K), to demonstrate the broad applicability of TTN. Table 6 shows the results of evaluating the ResNet-50-based DeepLabV3+ (Chen et al., 2018) model trained on the Cityscapes training set using the validation set of real-world datasets such as Cityscapes, BDD-100K, and Map- illary, and synthetic datasets including GTA V and SYNTHIA. We employ a test batch size of 2 for test-time adaptation in semantic segmentation. We observe that even when exploiting test batch statistics for standardization in BN layers (TBN) or updating the model parameters on top of TBN (TENT, SWR) does not improve the model performance (i.e., perform worse than the source model), adopting TTN helps the model make good use of the strength of the test batch statistics. Implemen- tation details and additional results are provided in the appendix A.1 and B.7, respectively. 3.4 A BLATION STUDIES Prior Aregularizes Î±to be robust to overall test batch sizes. We conduct an ablation study on the importance of each proposed component,i.e., initializing Î±with prior A, optimizing Î±using CE and MSE losses, and the results are shown in Table 7. Using Afor initialization and MSE loss aims to optimize Î±following our intuition that we discussed in Section 2.3. Optimizing Î±using CE loss improves the overall performance, but without regularizing with MSE loss, Î±may overï¬t to large batch size (rows 2 & 3). Initialization with Aor not does not show a signiï¬cant difference, but A provides a better starting point than random initialization when comparing the left and right of the 2nd row. We observe that when using MSE loss, regardless of initialization using A, the optimized Î±sufï¬ciently reï¬‚ects our intuition resulting in a low error rate to overall batch sizes (row 3). Table 7: Ablation study on importance of each component Method Test batch size Avg. Method Test batch size Avg.Init. CE MSE200 64 16 4 2 1 Init. CE MSE200 64 16 4 2 1 - -  13.36 13.43 13.85 15.50 17.43 20.0715.61  - - 13.37 13.43 13.85 15.50 17.44 20.0715.61 -  - 11.64 11.7312.26 14.46 16.94 19.8814.49   - 11.73 11.82 12.23 14.18 16.41 19.2714.27 -   11.6411.7812.21 13.97 15.86 18.0013.91    11.67 11.80 12.13 13.93 15.83 17.9913.89 3.5 V ISUALIZATION OF Î± Fig. 4 shows the visualization of optimized Î±for CIFAR-10 using WideResNet-40-2. We observe that Î± decreases from shallow to deep layers (left to right), which means CBN is more active in deeper layers, and TBN is vice versa. As shown in Table 4 and 6, CBN employing source statistics is superior to TBN when the distribution shift between source and target domains is small. Assum- ing that the Î±we obtained is optimal, we can conjecture that CBN is more active ( i.e., Î±closer to 0) in deeper layers because domain information causing the distribution shift has been diminished. In contrast, TBN has a larger weight ( i.e., Î± closer to 1) in shallower layers since the domain in- formation induces a large distribution shift. This interpretation is consistent with the observations of previous studies (Pan et al., 2018; Wang et al., 2021; Kim et al., 2022a) that style information mainly exists in shallower layers, whereas only content information remains in deeper layers. 8Published as a conference paper at ICLR 2023 ğ¶ğ‘™ 0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1 Channel-wise ğ›¼ Channel mean ğ›¼ per layer Channels in all layers1 2661 Figure 4: Optimized Î±. x- and y-axes indicate all channels in order from shallow to deep layers and the interpolating weight Î±, respectively. Cl denotes the channel size of layer l. 4 R ELATED WORK Test-time adaptation/training (TTA) aims to adapt models towards test data to overcome the per- formance degradation caused by distribution shifts (Sun et al., 2020; Wang et al., 2020). There are other related problems, unsupervised domain adaptation (UDA) (Sun & Saenko, 2016; Ganin et al., 2016) and source-free domain adaptation (SFDA) (Liang et al., 2020; Huang et al., 2021; Liu et al., 2021). Both UDA and SFDA have access to sufï¬ciently large enough unlabeled target datasets, and their objective is to achieve high performance on that particular target domain. Unlike UDA and SFDA, TTA utilizes test data in an online manner. There are two key factors of recent approaches: adapting standardization statistics in normalization layers and adapting model parameters. Normalization in Test Time. Nado et al. (2020) suggested prediction-time BN, which uses test batch statistics for standardization and Schneider et al. (2020) introduced to adapt BN statistics by combining source and test batch statistics considering the the test batch size to mitigate the inter- mediate covariate shift. In this paper, we refer to the former method as TBN. Similarly, You et al. (2021) and Khurana et al. (2021) mixed the statistics using predeï¬ned hyperparameter. Also, Mirza et al. (2022) and Hu et al. (2021) adapted the statistics using moving average while augmenting the input to form a pseudo test batch when only a single instance is given. The primary difference with the existing approaches is that we consider the channel-wise domain-shift sensitivity of BN layers to optimize the interpolating weights between CBN and TBN. Concurrently, Zou et al. (2022) pro- posed to adjust the standardization statistics using a learnable calibration strength and showed its effectiveness focusing on the semantic segmentation task. Optimization in Test Time.TENT (Wang et al., 2020), SWR (Choi et al., 2022), and CoTTA (Wang et al., 2022) updated model parameters while using TBN. TENT optimized afï¬ne parameters in BN layers using entropy minimization while freezing the others. To maximize the adaptability, SWR updated the entire model parameters minimizing the entropy loss based on the domain-shift sensitivity. To stabilize the adaptation in continuously changing domains, CoTTA used consistency loss between student and teacher models and stochastically restored random parts of the pre-trained model. Liu et al. (2021) and Chen et al. (2022) suggested to update the model through contrastive learning. We focus on correcting the standardization statistics using domain-shift aware interpolating weight Î±. Similar to Choi et al. (2022), we measure the domain-shift sensitivity by comparing gradients. The principle difference is that we use channel-wise sensitivity when optimizing Î±in post-training, while SWR used layer-wise sensitivity regularizing the entire model update in test time. 5 C ONCLUSIONS This paper proposes TTN, a novel domain-shift aware batch normalization layer, which combines the beneï¬ts of CBN and TBN that have a trade-off relationship. We present a strategy for mixing CBN and TBN based on the interpolating weight derived from the optimization procedure utilizing the sensitivity to domain shift and show that our method signiï¬cantly outperforms other normaliza- tion techniques in various realistic evaluation settings. Additionally, our method is highly practical because it can complement other optimization-based TTA methods. The oracle mixing ratio between CBN and TBN can vary depending on the domain gap difference. However, our proposed method employs a ï¬xed mixing ratio during test time, where the mixing ratio is optimized before model deployment. If we could ï¬nd the optimal mixing ratio according to the distribution shift during test time, we can expect further performance improvement. We consider it as future work. In this regard, our efforts encourage this ï¬eld to become more practical and inspire new lines of research. 9Published as a conference paper at ICLR 2023 REPRODUCIBILITY STATEMENT To ensure the reproducibility of our method, we provide the experimental setup in Section 3.1. Moreover, the details on implementation and evaluation settings can be found in the appendix A.1 and A.2, respectively. The pseudocode for overall training and testing scheme is provided in the appendix A.3. Together with related references and publicly available codes, we believe our paper contains sufï¬cient information for reimplementation. ACKNOWLEDGEMENT We would like to thank Kyuwoong Hwang, Simyung Chang, and Seunghan Yang of the Qualcomm AI Research team for their valuable discussions. REFERENCES Malik Boudiaf, Romain Mueller, Ismail Ben Ayed, and Luca Bertinetto. Parameter-free online test-time adaptation. In IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2022. John Bronskill, Jonathan Gordon, James Requeima, Sebastian Nowozin, and Richard Turner. Tas- knorm: Rethinking batch normalization for meta-learning. In International Conference on Ma- chine Learning (ICML). PMLR, 2020. Dian Chen, Dequan Wang, Trevor Darrell, and Sayna Ebrahimi. Contrastive test-time adaptation. In IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2022. Liang-Chieh Chen, Yukun Zhu, George Papandreou, Florian Schroff, and Hartwig Adam. Encoder- decoder with atrous separable convolution for semantic image segmentation. In Proceedings of the European conference on computer vision (ECCV), 2018. Sungha Choi, Sanghun Jung, Huiwon Yun, Joanne T Kim, Seungryong Kim, and Jaegul Choo. Robustnet: Improving domain generalization in urban-scene segmentation via instance selective whitening. In IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2021. Sungha Choi, Seunghan Yang, Seokeon Choi, and Sungrack Yun. Improving test-time adaptation via shift-agnostic weight regularization and nearest source prototypes. In European Conference on Computer Vision (ECCV), 2022. Marius Cordts, Mohamed Omran, Sebastian Ramos, Timo Rehfeld, Markus Enzweiler, Rodrigo Benenson, Uwe Franke, Stefan Roth, and Bernt Schiele. The cityscapes dataset for semantic urban scene understanding. In IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2016. Yaroslav Ganin, Evgeniya Ustinova, Hana Ajakan, Pascal Germain, Hugo Larochelle, Franc Â¸ois Laviolette, Mario Marchand, and Victor Lempitsky. Domain-adversarial training of neural net- works. The journal of machine learning research, 2016. Taesik Gong, Jongheon Jeong, Taewon Kim, Yewon Kim, Jinwoo Shin, and Sung-Ju Lee. Note:robust continual test-time adaptation against temporal correlation. Advances in Neural In- formation Processing Systems (NeurIPS), 2022. Priya Goyal, Piotr DollÂ´ar, Ross Girshick, Pieter Noordhuis, Lukasz Wesolowski, Aapo Kyrola, An- drew Tulloch, Yangqing Jia, and Kaiming He. Accurate, large minibatch sgd: Training imagenet in 1 hour. arXiv preprint arXiv:1706.02677, 2017. Yves Grandvalet and Yoshua Bengio. Semi-supervised learning by entropy minimization. In Ad- vances in Neural Information Processing Systems (NeurIPS), 2004. Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image recog- nition. In IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2016. 10Published as a conference paper at ICLR 2023 Dan Hendrycks and Thomas Dietterich. Benchmarking neural network robustness to common cor- ruptions and perturbations. In International Conference on Learning Representations (ICLR), 2018. Dan Hendrycks, Norman Mu, Ekin Dogus Cubuk, Barret Zoph, Justin Gilmer, and Balaji Lakshmi- narayanan. Augmix: A simple data processing method to improve robustness and uncertainty. In International Conference on Learning Representations (ICLR), 2019. Xuefeng Hu, Gokhan Uzunbas, Sirius Chen, Rui Wang, Ashish Shah, Ram Nevatia, and Ser-Nam Lim. Mixnorm: Test-time adaptation through online normalization estimation. arXiv preprint arXiv:2110.11478, 2021. Jiaxing Huang, Dayan Guan, Aoran Xiao, and Shijian Lu. Model adaptation: Historical contrastive learning for unsupervised domain adaptation without source data. 2021. Sergey Ioffe and Christian Szegedy. Batch normalization: Accelerating deep network training by reducing internal covariate shift. InInternational Conference on Machine Learning (ICML), 2015. Ansh Khurana, Sujoy Paul, Piyush Rai, Soma Biswas, and Gaurav Aggarwal. Sita: Single image test-time adaptation. arXiv preprint arXiv:2112.02355, 2021. Byeonggeun Kim, Seunghan Yang, Jangho Kim, Hyunsin Park, Juntae Lee, and Simyung Chang. Domain Generalization with Relaxed Instance Frequency-wise Normalization for Multi-device Acoustic Scene Classiï¬cation. In Conference of the International Speech Communication Asso- ciation (INTERSPEECH), 2022a. Jin Kim, Jiyoung Lee, Jungin Park, Dongbo Min, and Kwanghoon Sohn. Pin the memory: Learn- ing to generalize semantic segmentation. In IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2022b. Diederik P Kingma and Jimmy Ba. Adam: A method for stochastic optimization. 2015. Suhyeon Lee, Hongje Seong, Seongwon Lee, and Euntai Kim. Wildnet: Learning domain general- ized semantic segmentation from the wild. In IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2022. Jian Liang, Dapeng Hu, and Jiashi Feng. Do we really need to access the source data? source hypothesis transfer for unsupervised domain adaptation. In International Conference on Machine Learning (ICML). PMLR, 2020. Yuejiang Liu, Parth Kothari, Bastien van Delft, Baptiste Bellot-Gurlet, Taylor Mordan, and Alexan- dre Alahi. Ttt++: When does self-supervised test-time training fail or thrive? Advances in Neural Information Processing Systems (NeurIPS), 2021. Mingsheng Long, Han Zhu, Jianmin Wang, and Michael I Jordan. Unsupervised domain adap- tation with residual transfer networks. In Advances in Neural Information Processing Systems (NeurIPS), 2016. Ilya Loshchilov and Frank Hutter. Sgdr: Stochastic gradient descent with warm restarts. 2017. M Jehanzeb Mirza, Jakub Micorek, Horst Possegger, and Horst Bischof. The norm must go on: Dynamic unsupervised domain adaptation by normalization. In IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2022. Zachary Nado, Shreyas Padhy, D Sculley, Alexander Dâ€™Amour, Balaji Lakshminarayanan, and Jasper Snoek. Evaluating prediction-time batch normalization for robustness under covariate shift. arXiv preprint arXiv:2006.10963, 2020. Gerhard Neuhold, Tobias Ollmann, Samuel Rota Bulo, and Peter Kontschieder. The mapillary vistas dataset for semantic understanding of street scenes. In International Conference on Computer Vision (ICCV), 2017. Xingang Pan, Ping Luo, Jianping Shi, and Xiaoou Tang. Two at once: Enhancing learning and generalization capacities via ibn-net. InEuropean Conference on Computer Vision (ECCV), 2018. 11Published as a conference paper at ICLR 2023 Joaquin Quinonero-Candela, Masashi Sugiyama, Anton Schwaighofer, and Neil D Lawrence. Dataset shift in machine learning. Mit Press, 2008. Stephan R. Richter, Vibhav Vineet, Stefan Roth, and Vladlen Koltun. Playing for data: Ground truth from computer games. In European Conference on Computer Vision (ECCV), 2016. German Ros, Laura Sellart, Joanna Materzynska, David Vazquez, and Antonio M Lopez. The synthia dataset: A large collection of synthetic images for semantic segmentation of urban scenes. In IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2016. Steffen Schneider, Evgenia Rusak, Luisa Eck, Oliver Bringmann, Wieland Brendel, and Matthias Bethge. Improving robustness against common corruptions by covariate shift adaptation. Ad- vances in Neural Information Processing Systems (NeurIPS), 2020. Saurabh Singh and Abhinav Shrivastava. Evalnorm: Estimating batch normalization statistics for evaluation. In International Conference on Computer Vision (ICCV), 2019. Cecilia Summers and Michael J Dinneen. Four things everyone should know to improve batch normalization. In International Conference on Learning Representations (ICLR), 2019. Baochen Sun and Kate Saenko. Deep coral: Correlation alignment for deep domain adaptation. In European Conference on Computer Vision (ECCV), 2016. Yu Sun, Xiaolong Wang, Zhuang Liu, John Miller, Alexei Efros, and Moritz Hardt. Test-time train- ing with self-supervision for generalization under distribution shifts. In International Conference on Machine Learning (ICML), 2020. Tuan-Hung Vu, Himalaya Jain, Maxime Bucher, Matthieu Cord, and Patrick PÂ´erez. Advent: Adver- sarial entropy minimization for domain adaptation in semantic segmentation. InIEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2019. Dequan Wang, Evan Shelhamer, Shaoteng Liu, Bruno Olshausen, and Trevor Darrell. Tent: Fully test-time adaptation by entropy minimization. In International Conference on Learning Repre- sentations (ICLR), 2020. Qin Wang, Olga Fink, Luc Van Gool, and Dengxin Dai. Continual test-time domain adaptation. In IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2022. Yulin Wang, Zanlin Ni, Shiji Song, Le Yang, and Gao Huang. Revisiting locally supervised learning: an alternative to end-to-end training. In International Conference on Learning Representations (ICLR), 2021. Fuming You, Jingjing Li, and Zhou Zhao. Test-time batch statistics calibration for covariate shift. arXiv preprint arXiv:2110.04065, 2021. Fisher Yu, Haofeng Chen, Xin Wang, Wenqi Xian, Yingying Chen, Fangchen Liu, Vashisht Madha- van, and Trevor Darrell. Bdd100k: A diverse driving dataset for heterogeneous multitask learning. In IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2020. Yuliang Zou, Zizhao Zhang, Chun-Liang Li, Han Zhang, Tomas Pï¬ster, and Jia-Bin Huang. Learn- ing instance-speciï¬c adaptation for cross-domain segmentation. European Conference on Com- puter Vision (ECCV), 2022. 12Published as a conference paper at ICLR 2023 A A PPENDIX : F OR REPRODUCIBILITY This section provides supplemental material for Section 2 and 3.1. A.1 I MPLEMENTATION DETAILS Datasets and Models. For CIFAR-10/100-C, we optimized Î± using augmented CIFAR-10/100 training set on the pre-trained WideResNet-40-2 (WRN-40) (Hendrycks et al., 2019). For ImageNet- C, we used augmented ImageNet training set (randomly sampled 64000 instances per epoch) on the pre-trained ResNet-50. Augmentation. Following the setting of SWR (Choi et al., 2022), we used color jittering, random invert and random grayscale when obtaining the prior A. When optimizing Î±, we followed the augmentation choice of CoTTA (Wang et al., 2022), which are color jittering, padding, random afï¬ne, gaussian blur, center crop and random horizontal ï¬‚ip. We excluded for gaussian noise to avoid any overlap with corruption type of common corruptions (Hendrycks & Dietterich, 2018). For ImageNet, we only used the same augmentation both for obtaining Aand optimizing Î±following the SWR augmentation choices. Post-training. When obtaining prior, we used randomly selected 1024 samples from the training set, following the setting of SWR. We used Adam (Kingma & Ba, 2015) optimizer using a learning rate (LR) of 1e-3, which is decayed with cosine schedule (Loshchilov & Hutter, 2017) for 30 epochs and used 200 training batch for CIFAR-10/100. For ImageNet, we lowered LR to 2.5e-4 and used 64 batch size. For semantic segmentation task, we trained TTN using Cityscapes training set, and training batch size of 2. We resized the image height to 800 while preserving the original aspect ratio Cordts et al. (2016). We can terminate the training when MSE loss saturates (We observed that Î±does not show signiï¬cant difference after the MSE loss is saturated). We used the weighting hyperparmeter to MSE loss Î»as 1. Test time. For AdaptiveBN, which adjusts the interpolating weight using two factors: hyperpa- rameter N and test batch size n, we followed the suggested N, which is empirically obtained best hyperparameter from the original paper, for each n(Figure 11, ResNet architecture from Schneider et al. (2020)). In detail, we set N as 256, 128, 64, 32, and 16 for test batch size 200, 64, 16, 4, 2, and 1, which yields Î±as 0.44, 0.33, 0.2, 0.11, 0.06, and 0.06, respectively. For optimization-based TTA methods, we followed default setting in TENT, SWR, and CoTTA for test-time adaptation. We used LR of 1e-3 to test batch size of 200 for CIFAR-10/100-C in single domain (TENT, SWR) and continuously changing domain (CoTTA) scenarios. To avoid rapid error accumulation, we lowered LR to 1e-4 for TENT and SWR in continual and mixed do- main scenarios. Moreover, we updated model parameters after accumulating the gradients for 200 samples for CIFAR-10/100-C. In other words, we compute gradients per batch, but update, i.e., optimizer.step(), after seeing 200 data samples. Exceptionally, we used LR of 5e-5 for SWR and SWR+TTN in mixed domain setting. Additionally, in continuously changing and mixed domain scenarios, we used the stable version of SWR, which updates the model parameter based on the frozen source parameters instead of previously updated parameters (original SWR). For semantic segmentation, we set the test batch size as 2 and learning rate for optimization-based methods as 1e-6 for all datasets. For SWR, we set the importance of the regularization term Î»r as 500. The other hyperparameters are kept the same as Choi et al. (2022) choices. For all test-time adaptation, we used constant learning rate without scheduling. A.2 E VALUATION SCENARIO DETAILS Class imbalanced setting. In the main paper Table 5, we show the results under class imbalanced settings. In the setting, we sorted the test dataset of each corruption in the order of class labels, i.e., from class 0 to 9 for CIFAR-10-C. Then, we comprised test batches following the sorted order. Therefore, most batches consist of single class input data, which leads to biased test batch statistics. For larger batch size, the statistics are more likely to be extremely skewed, and that explains why error rates are higher with larger batch sizes than with the small ones. 13Published as a conference paper at ICLR 2023 A.3 P SEUDOCODE Pseudocode for post-training, i.e., obtaining Aand optimizing Î±, is provided in Algorithms 1 and 2, respectively, and that for test time is in Algorithm 3. Moreover, we provide PyTorch-friendly pseudocode for obtaining Ain Listing 1. Please see Section 2 for equations and terminologies used in the algorithms. Algorithm 1 Obtain prior A 1: Require: Pre-trained model fÎ¸; source training data DS = (X,Y ) 2: Output: Prior A 3: for all (x,y) in DS do 4: Augment x: xâ€² 5: Collect gradients (âˆ‡Î³,âˆ‡Î³â€² ) and (âˆ‡Î²,âˆ‡Î²â€² ) from fÎ¸ using clean xand augmented xâ€² 6: end for 7: Compute gradient distance score dusing Eq. 4 8: Deï¬ne prior Ausing Eq. 5 Algorithm 2 Post-train 1: Require: Pre-trained model fÎ¸; source training data DS = (X,Y ); step size hyperparameter Î·; regularization weight hyperparameter Î» 2: Output: Optimized interpolating weight Î± 3: Obtain prior Ausing Algorithm 1 4: Initialize Î±with prior A 5: Replace all BN layers of fÎ¸ to TTN layers using Î±and Eq. 2 6: while not done do 7: Sample minibatches BS from DS 8: for all minibatches do 9: Augment all xin BS: xâ€² 10: Evaluate âˆ‡Î±Lusing fÎ¸ given {(xâ€² i,yi)}|BS| i=1 using Eq. 6 while adapting standardization statistics using Eq. 2 11: Update Î±â†Î±âˆ’Î·âˆ‡Î±L 12: end for 13: end while Algorithm 3 Inference (Test time) 1: Require: Pre-trained model fÎ¸; optimized Î±, target test data DT = (X); 2: Replace all BN layers of fÎ¸ to TTN layers using Î±and Eq. 2 3: Sample minibatches BT from DT 4: for all minibatches do 5: Make prediction using fÎ¸ given BT while adapting standardization statistics using Eq. 2 6: end for 1 def obtain_prior(model, train_data): 2 # make weight and bias of BN layers requires_grad=True, otherwise False 3 params = {n: p for n, p in model.named_parameters() if p.requires_grad} 4 5 grad_sim = {} 6 for x, y in train_data: 7 # collect gradients for clean and augmented input 8 grad_org = collect_grad(model, x, y, params) 9 grad_aug = collect_grad(model, augment(x), y, params) 10 11 # compute grad similarity 12 for n, p in params.items(): 13 grad_sim[n].data = cosine_sim(grad_org, grad_aug) 14 14Published as a conference paper at ICLR 2023 15 # average over data samples 16 for n, p in params.items(): 17 grad_sim[n].data /= len(train_data) 18 19 # min max normalization 20 max_grad = get_max_value(grad_sim) # scalar 21 min_grad = get_min_value(grad_sim) # scalar 22 grad_dist = {} 23 for n, p in grad_sim.items(): 24 grad_dist[n] = (p - min_grad) / (max_grad - min_grad) 25 26 prior = [] 27 j = 0 28 # integrate gradients of weight(gamma) and bias(beta) for each BN layer 29 for n, p in grad_dist.items(): 30 if "weight" in n: 31 prior.append(p) 32 elif "bias" in n: 33 prior[j] += p 34 prior[j] /= 2 35 prior[j] = (1-prior[j])**2 36 j += 1 37 38 return prior 39 40 def collect_grad(model, x, y, params): 41 model.zero_grad() 42 out = model(x) 43 loss = ce_loss(out, y) 44 loss.backward() 45 46 grad = {} 47 for n, p in params.items(): 48 grad[n].data = p.data 49 50 return grad Listing 1: PyTorch-friendly pseudo code for obtaining prior B A PPENDIX : E XPERIMENTAL RESULTS B.1 A BLATION STUDIES Channel-wise Î±. In Table 8, we compared different granularity levels of interpolating weightÎ±, i.e., channel-wise, layer-wise, and a constant value with CIFAR-10-C and backbone WRN-40. We ob- served that channel-wise optimized Î±shows the best performance. We take average of the channel- wisely optimized Î± (the 1st row) over channels to make a layer-wise Î± (the 2nd row), and take average over all channels and layers to make a constant value (the 3rd row). Î±of the 1st and 2nd rows are visualized in the main paper Figure 4 colored in blue and red, respectively. The constant value Î±(the 3rd row) is 0.3988. We also optimized Î±layer-wisely (the 4th row). Table 8: Ablation study on granularity level of Î± # Method Test batch size Avg. 200 64 16 4 2 1 1 Channel-wise (Optimized) 11.67 11.80 12.13 13.93 15.83 17.99 13.89 2 Layer-wise (Channel mean) 12.75 12.84 13.16 14.66 16.40 18.82 14.77 3 Constant (Mean) 12.07 12.21 13.05 16.72 20.04 21.26 15.89 4 Layer-wise (Optimized) 13.11 13.21 13.51 14.84 16.46 18.62 14.96 15Published as a conference paper at ICLR 2023 MSE loss strength Î». We empirically set the MSE loss strengthÎ»in Eq. 6 as 1 through the ablation study using CIFAR-10-C and WRN-40 (Table 9). Using the MSE regularizer prevents the learnable Î±from moving too far from the prior, thus letting Î±follow our intuition, i.e., putting smaller im- portance on the test batch statistics if the layer or channel is invariant to domain shifts. However, with too strong regularization (Î»=10.0), the overall error rates are high, which means the Î±needs to be sufï¬ciently optimized. On the other hand, with too small regularization, the Î±may overï¬t to the training batch size (B=200) and lose the generalizability to the smaller batch size. Table 9: MSE loss strength Î» Î» Test batch size Avg. 200 64 16 4 2 1 0.0 11.73 11.82 12.23 14.18 16.41 19.27 14.27 0.1 11.65 11.91 12.09 13.84 15.71 18.24 13.91 1.0 11.67 11.80 12.13 13.93 15.83 17.99 13.89 10.0 12.45 12.63 12.82 14.55 16.32 18.56 14.56 B.2 M ORE COMPARISONS ON CIFAR10-C Constant Î±. Table 10 shows results of simple baseline for normalization-based methods where the Î±is a constant value ranging from 0 to 1. Î±= 0 is identical to CBN (Ioffe & Szegedy, 2015), and Î± = 1 is identical to TBN (Nado et al., 2020). We observe that the lower Î±, i.e., using less test batch statistics, shows better performance for small test batch sizes. This observation is analogous to the ï¬nding of the previous work (Schneider et al., 2020). Table 10: Constant Î±. Error rate (â†“) averaged over 15 corruptions of CIFAR-10-C (WRN-40). Î± Test batch size Avg. 200 64 16 4 2 1 0 18.26 18.39 18.26 18.26 18.25 18.25 18.28 0.1 13.95 14.1 14.05 14.65 15.14 15.45 14.56 0.2 12.46 12.66 12.89 14.30 15.53 15.64 13.91 0.3 12.05 12.29 12.72 15.18 17.35 17.42 14.50 0.4 12.13 12.41 13.12 16.69 19.81 20.51 15.78 0.5 12.42 12.78 13.73 18.32 22.52 24.88 17.44 0.6 12.88 13.32 14.48 20.02 25.17 31.97 19.64 0.7 13.37 13.9 15.23 21.75 27.91 46.65 23.14 0.8 13.82 14.37 15.94 23.44 30.59 77.15 29.22 0.9 14.18 14.8 16.58 24.94 33.12 89.81 32.24 1 14.50 15.15 17.10 26.29 35.67 90.00 33.12 B.3 V ARIANTS OF TTN FOR SMALL TEST BATCH SIZE Online TTN. TTN interpolating weight Î±can also be adapted during test time. Table 11 shows the results of the TTN online version, which further optimizes the post-trained alpha using the entropy minimization and mean-squared error (MSE) loss between the updated alpha and the initial post- trained alpha. We followed entropy minimization loss used in TENT (Wang et al., 2020), and the MSE loss can be written as LMSE = âˆ¥Î±âˆ’Î±0âˆ¥2, where Î±0 is the post-trained Î±. We set the learning rate as 1e-2, 2.5e-3, 5e-4, 1e-4, 5e-5, and 2.5e-5 by linearly decreasing according to the test batch size of 200, 64, 16, 4, 2, and 1 (Goyal et al., 2017). The online TTN shows improvements compared to the ofï¬‚ine TTN in all three evaluation scenarios (single, continuously changing, and mixed). Scaled TTN. Following the observation from Table 10, we lowered the interpolating weight by mul- tiplying a constant scale value, ranging (0,1), to the optimized TTN Î±. In Table 11, we empirically set the scale value as 0.4. Dynamic training batch size. We observe that using the dynamic batch size in the post-training stage also improves the performance for small test batch sizes (2 or 1), while slightly deteriorating the performance for large test batch sizes (200 or 64). We randomly sampled training batch size from the range of [4,200] for each iteration. Other hyperparameters are kept as the same. 16Published as a conference paper at ICLR 2023 Table 11: TTN variants. Error rate (â†“) averaged over 15 corruptions of CIFAR-10-C (WRN-40). Test batch size Avg. Method Eval. setting 200 64 16 4 2 1 TTN (ofï¬‚ine, default) Single & Cont.11.67 11.80 12.13 13.93 15.83 17.99 13.89 TTN (ofï¬‚ine, default) Mixed 12.16 12.19 12.34 13.96 15.55 17.83 14.00 TTN (online) Single 11.39 11.64 11.97 13.70 15.41 17.49 13.60 TTN (online) Cont. 11.67 11.96 12.15 13.90 15.67 17.72 13.85 TTN (online) Mixed 12.04 12.04 12.06 13.90 15.46 17.62 13.85 TTN (scaled) Single & Cont.13.20 13.38 13.35 13.88 14.54 15.17 13.92 TTN (scaled) Mixed 13.17 13.05 13.17 13.74 14.36 15.09 13.76 TTN (dynamic train batch size)Single & Cont.11.82 12.04 12.17 13.60 15.13 17.22 13.66 TTN (dynamic train batch size)Mixed 12.12 12.01 11.91 13.43 14.76 17.23 13.58 B.4 S TUDY ON AUGMENTATION TYPE TTN is robust to the data augmentation. We used data augmentation in the post-training phase to simulate domain shifts. The rationale for the simulation is to expose the model to different input domains. Especially when obtaining the prior, we compare the outputs from shifted domains with the clean (original) domain in order to analyze which part of the model is affected by the domain discrepancy. Therefore, changing the domain itself is what matters, not which domain the input is shifted to. We demonstrated this by conducting ablation studies by varying the augmentation type. We analyzed various augmentation types when obtaining prior and when optimizing alpha and the results are shown in Figure 5 (a) and (b), respectively. We use a true corruption, i.e., one of 15 corruption types in the corruption benchmark, as an augmentation type in the post-training phase to analyze how TTN works if the augmentation type and test corruption type are misaligned or perfectly aligned. Speciï¬cally, we used the true corruption when obtaining the prior while keeping the augmentation choices when optimizing alpha as described in the appendix A.1, and vice versa. Expectedly, the diagonal elements, i.e., where the same corruption type is used both for in post- training and in test time, tend to show the lowest error rate in Figure 5(b). The row-wise standard deviation is lower than 1 in most cases and even lower than 0.5 in Figure 5(a), which means the prior is invariant to the augmentation choice. Moreover, we observe that the average error rate over all elements, 11.74% in ablation for obtaining prior and 11.67% in ablation for optimizing alpha, is almost as same as TTN result 11.67% (See Table 14 and Figure 5). Moreover, we conducted an ablation study on the choice of the augmentation type using CIFAR-10-C and WRN-40 (Table12). We observe that obtaining prior and optimizing alpha steps are robust to the augmentation types. Table 12: Ablation study on augmentation choice. From left to right one augmentation type is added at a time. Default setting, which we used in all experiments, is colored in gray. Prioraugmentation type color jitter + grayscale + invert + gaussian blur + horizontal ï¬‚ip 12.03 11.83 11.67 11.59 11.58 OptimizingÎ±augmentation type color jitter + padding + afï¬ne + gaussian blur + horizontal ï¬‚ip 11.78 11.78 11.7 11.70 11.67 B.5 R ESULTS ON IMAGE NET-C Table 13 shows the experimental results using ResNet-50 (He et al., 2016) on ImageNet- C (Hendrycks & Dietterich, 2018) dataset. We emphasized the effectiveness of our proposed method by showing the signiï¬cant improvement in large-scale dataset experiment. Similar to the results in CIFAR-10/100-C, TTN showed the best performance compared to normalization-based methods (TBN (Nado et al., 2020), AdaptiveBN Schneider et al. (2020), and Î±-BN (You et al., 2021)) and improved TTA performance when it is applied to optimization-based methods (TENT (Wang et al., 2020) and SWR (Choi et al., 2022)). During post-training, we used LR of 2.5e-4 and batch size of 64. In test time, we used the learning rate of 2.5e-4 for TENT following the setting from the original paper, and used 5e-6 for TENT+TTN. For SWR and SWR+TTN, we used learning rate of 2.5e-4 for B=64, and 2.5e-6 for B=16, 4, 2, and 1. We had to carefully tune the learning rate especially for SWR, since the method updates the 17Published as a conference paper at ICLR 2023 entire model parameters in an unsupervised manner and hence is very sensitive to the learning rate when the test batch size becomes small. Other hyperparameters and details are kept same (See the appendix A.1 for more details). Moreover, to avoid rapid accumulation, we stored gradients for sufï¬ciently large test samples and then updated the model parameters (for example, we conducted adaptation in every 64 iterations in the case of batch size of 1) in both TENT and SWR. Table 13: Single domain adaptation on ImageNet-C (ResNet-50). Error rate (â†“) averaged over 15 corruption types with severity level 5 is reported for each test batch size. Method Test batch size Avg. Error 64 16 4 2 1 Source (CBN)93.34 93.34 93.34 93.34 93.34 93.34 Norm TBN 74.24 76.81 85.74 95.35 99.86 86.40 AdaptiveBN 77.86 81.47 86.71 90.15 91.11 85.46 Î±-BN 86.06 86.32 87.16 88.33 90.45 87.66 Ours (TTN) 72.21 73.18 76.98 81.52 88.49 78.48 Optim. TENT 66.56 72.61 93.37 99.46 99.90 86.41 +Ours (TTN)71.42 72.45 76.66 81.89 91.00 78.68 SWR 64.41 74.19 84.30 93.05 99.86 83.16 +Ours (TTN)55.68 69.25 78.48 86.37 94.08 76.77 B.6 E RROR RATES OF EACH CORRUPTION In Table 14, we show the error rates of TTN for each corruption of CIFAR-10-C using the WRN-40 backbone. The averaged results over the corruptions are in Table 1. Table 14: Results of each corruption (CIFAR-10-C) batch sizegauss. shot impulse defocus glass motion zoom snow frost fog brightness contrast elastic pixel. jpeg.Avg. 200 14.81 12.78 17.32 7.37 17.87 8.51 7.23 10.29 9.88 11.29 6.06 8.36 13.42 14.89 14.94 11.6764 14.81 12.81 17.42 7.41 18.21 8.66 7.41 10.44 9.93 11.63 6.11 8.35 13.59 15.18 15.02 11.8016 15.23 13.00 17.98 7.71 18.46 8.95 7.68 10.85 10.17 12.21 6.25 8.54 13.95 15.67 15.3412.134 17.03 15.29 19.75 9.26 20.93 10.01 9.62 12.58 11.85 13.65 7.69 9.25 16.21 18.08 17.7013.932 19.21 16.80 21.86 10.70 23.74 11.39 11.92 14.00 13.39 15.38 8.95 10.13 18.92 20.68 20.4015.831 22.03 19.97 25.24 12.41 26.99 12.22 14.39 14.73 14.60 17.27 10.16 9.51 22.10 24.12 24.0917.99 B.7 S EGMENTATION RESULTS For more comparisons, we add segmentation results for TBN and TTN using test batch size (B) of 4 and 8 (Table 15). The results demonstrate that TTN is robust to the test batch sizes. In other words, the performance difference across the test batch size is small when using TTN (TTN with B=2, 4, and 8). The results are averaged over 3 runs (i.e., using 3 independently optimized Î±), and the standard deviation is denoted with a Â±sign. We omitted the standard deviation for TBN, which is 0.0 for every result since no optimization is required. Since the backbone network is trained with a train batch size of 8, we assume that test batch statistics estimated from the test batch with B=8 are sufï¬ciently reliable. Accordingly, TBN with B=8 shows compatible results. However, when B becomes small (i.e., in a more practical scenario), problematic test batch statistics are estimated, and thus TBN suffers from the performance drop while TTN keeps showing robust performance. It is worth noting that TTN outperforms TBN by 3.77% in average accuracy when B=2, i.e., in the most practical evaluation setting, and by 2.54% and 1.99% for B=4 and 8, respectively. Table 15: Adaptation on DG benchmarks in semantic segmentation. mIoU(â†‘) on four unseen domains with test batch size of 2, 4, and 8 using ResNet-50 based DeepLabV3+ as a backbone. Method(Cityscapesâ†’) BDD-100K Mapillary GTA V SYNTHIA Cityscapes Norm TBN (B=2) 43.12 47.61 42.51 25.71 72.94 Ours (TTN)(B=2) 47.40(Â±0.02) 56.88(Â±0.04) 44.69(Â±0.03) 26.68(Â±0.01) 75.09(Â±0.01) TBN (B=4) 45.64 49.17 44.26 25.96 74.29 Ours (TTN)(B=4) 47.72(Â±0.01) 57.11(Â±0.01) 45.08(Â±0.02) 26.52(Â±0.01) 75.56(Â±0.01) TBN (B=8) 46.42 49.38 44.81 25.97 75.42 Ours (TTN)(B=8) 47.25(Â±0.02) 57.28(Â±0.02) 45.13(Â±0.03) 26.45(Â±0.01) 75.82(Â±0.01) 18Published as a conference paper at ICLR 2023 (b)Augmentation type used for optimizing alpha Test corruption type Error rate. Colored by normalized value (across test corruption type)avg.std.14.550.8612.640.6116.770.927.660.3117.750.738.760.297.490.2510.330.279.710.3212.401.466.110.098.260.4913.350.3714.581.2714.730.3911.67 (a)Augmentation type used for obtaining prior Test corruption type Error rate. Colored by normalized value (across test corruption type) avg.std.15.030.3412.790.2217.240.167.410.0617.910.148.480.087.260.0310.440.149.980.1411.550.106.050.048.570.4013.340.0715.211.5914.830.1111.74 Figure 5: True test corruption as augmentation. Each column represents the augmentation type used (a) when obtaining prior or (b) when optimizing Î±. Each row represents the test corruption type. The error rate (â†“) of CIFAR-10-C with severity level 5 and test batch size 200 using WRN-40 is annotated in each element. Element (i,j) represents the error rate when j-th corruption type is used for augmenting the clean train data during post-training and tested on i-th corruption test set. The heatmap is colored by error rate normalized across the test corruption type (row-wise normalization). 19
```

## **Instructions:**
1. Analyze the paper based on the categories listed below.
2. Your response **must be a valid JSON object** that can be directly parsed using `json.loads()`.
3. Do not include any extra text, explanations, or formatting outside of the JSON object.
4. **If a field has no available information, set its value to `"Not mentioned"` instead of leaving it empty.**
5. Ensure that the JSON format is correct, including the use of **double quotes (`"`) for all keys and values.**
## **Output Format (JSON)**:
```json
{
    "main_contributions": "<Concise description of the main research problem and contributions>",
    "methodology": "<Brief explanation of the key techniques, models, or algorithms>",
    "experimental_setup": "<Description of datasets, benchmarks, and validation methods>",
    "limitations": "<Summary of weaknesses, constraints, or assumptions>",
    "future_research_directions": "<Potential areas for extending this research>"
}
```
Output:
{
    "main_contributions": "The paper addresses the performance degradation of deep neural networks in test-time adaptation (TTA) due to domain shifts, particularly highlighting the limitations of Transductive Batch Normalization (TBN) under impractical assumptions like large and i.i.d test batches. It proposes Test-Time Normalization (TTN), a novel domain-shift aware batch normalization strategy that interpolates between Conventional Batch Normalization (CBN) and TBN. TTN uses channel-wise interpolating weights, optimized in a post-training phase, based on the domain-shift sensitivity of each BN layer. This approach improves model robustness across various batch sizes and realistic evaluation scenarios (stationary, continuously changing, and mixed domain adaptation), while also preserving source knowledge and improving the performance of other TTA methods.",
    "methodology": "The proposed TTN layer standardizes features by combining source statistics (mean \"Âµs\" and variance \"ÏƒÂ²s\") and current test mini-batch statistics (mean \"Âµ\" and variance \"ÏƒÂ²\") using a learnable channel-wise interpolating weight \"Î±\". Specifically, the combined statistics are \": \":\n\"ËœÂµ = Î±Âµ + (1âˆ’Î±)Âµs\" and \"ËœÏƒÂ² = Î±ÏƒÂ² + (1âˆ’Î±)ÏƒÂ²s + Î±(1 âˆ’Î±)(Âµâˆ’Âµs)Â². The method introduces a post-training phase (after pre-training, before testing) to optimize these \"Î±\" parameters while freezing all other model weights. This phase consists of two stages: 1) Obtaining a Prior A: By simulating domain shifts through data augmentation and comparing gradients of affine parameters (Î³, Î²) between clean and augmented inputs, the method identifies BN layers/channels sensitive to domain shifts. A gradient distance score is computed and squared to form the prior \"A\". 2) Optimizing \"Î±\": \"Î±\" is initialized with \"A\" and then optimized using a combined loss function: cross-entropy loss (LCE) for consistent predictions on augmented data, and a mean-squared error loss (LMSE = âˆ¥Î±âˆ’Aâˆ¥Â²) to regularize \"Î±\" towards the prior.",
    "experimental_setup": "The method was evaluated on:Image Classification: CIFAR-10/100-C and ImageNet-C corruption benchmark datasets, using WideResNet-40-2 (for CIFAR) and ResNet-50 (for ImageNet) as backbone models.Semantic Segmentation: Cityscapes, BDD-100K, Mapillary, GTA V, and SYNTHIA domain generalization benchmarks, using a ResNet-50-based DeepLabV3+ model.Evaluation Scenarios: Single domain adaptation, continuously changing domain adaptation, and mixed domain adaptation, all with varying test batch sizes (from 200 down to 1). Additional evaluations included source domain adaptation and class-imbalanced target domains.Baselines for comparison included normalization-based methods (TBN, AdaptiveBN, Î±-BN, MixNorm) and optimization-based TTA methods (TENT, SWR, CoTTA), with comparisons also made against the source model (using CBN). Post-training details involved using Adam optimizer, cosine learning rate schedule, and specific data augmentation types for prior calculation and alpha optimization.",
    "limitations": "The proposed method employs a fixed mixing ratio (interpolating weight Î±) during test time. This ratio is optimized before model deployment and does not dynamically adjust based on the varying domain gaps that might be encountered during actual test-time inference.",
    "future_research_directions": "Future work could focus on dynamically determining the optimal mixing ratio for CBN and TBN according to the specific distribution shifts encountered during test time, rather than using a fixed, pre-optimized ratio. This would allow for even greater performance improvements and adaptability."
}
