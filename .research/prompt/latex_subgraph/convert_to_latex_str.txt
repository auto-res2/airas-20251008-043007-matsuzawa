
Input:

You are a LaTeX expert.
Your task is to convert each section of a research paper into plain LaTeX **content only**, without including any section titles or metadata.

Below are the paper sections. For each one, convert only the **content** into LaTeX:

---
Section: title

AdaNPC: Adaptive Natural-Gradient and Probabilistically-Certified Test-Time Adaptation

---

---
Section: abstract

We study fully test-time adaptation, where a pretrained model must update itself on an unlabeled stream while respecting strict latency budgets. Popular entropy-minimisation methods such as Tent reduce error on moderate shifts but rely on fixed, source-biased pre-conditioners, assume BatchNorm layers, and can diverge on hard samples. We introduce AdaNPC, a normaliser-agnostic procedure that (i) tracks curvature online with an exponential moving average of squared gradients, (ii) performs a one-shot natural-gradient step pre-conditioned by this Fisher proxy, (iii) safeguards each update with a Bernstein-bound safety filter that rejects steps whose probability of increasing entropy exceeds δ, and (iv) uses a micro-stepping scheduler to trade accuracy for compute in real time. AdaNPC adapts the affine parameters of Batch, Layer, Group and RMS normalisation layers plus an optional RGB bias, yet adds only a single |θ|-sized buffer and ≤5 % extra FLOPs. On ImageNet-C with ResNet-50 it matches the frozen model when Tent, ProxTTA and EATA collapse; on CIFAR-100 it stays stable when the safety filter is disabled; and on a non-stationary ViT stream it degrades gracefully under tight budgets. An ablation confirms the importance of curvature tracking, safety filtering and micro-stepping. The code extends the official Tent repository and requires no hyper-parameter tuning beyond intuitive defaults.

---

---
Section: introduction

Distribution shift between development and deployment is a dominant failure mode of contemporary machine-learning systems. Fully test-time adaptation (TTA) tackles this problem by updating a deployed model on unlabeled test streams, most often by minimising prediction entropy batch-by-batch \cite{wang-2020-tent}. TTA is attractive because it dispenses with source data, yet four practical obstacles remain. 
Curvature drift.  Pre-conditioners computed on the source distribution quickly become obsolete as the environment changes, so fixed Fisher or Hessian matrices can mis-scale gradients and harm performance. 
Expressiveness.  Restricting updates to BatchNorm layers precludes transformer backbones that employ LayerNorm and ignores shifts that affect early filters or input colour. 
Safety.  Even small unsupervised updates can push the model towards degenerate minima. Existing methods either lack an online certificate of improvement or rely on ad-hoc heuristics that break in dynamic streams \cite{niu-2023-towards,hoang-2023-persistent}. 
Latency.  Real-time systems ingest data at a fixed rate; an algorithm that doubles per-batch compute automatically receives only half as many samples to learn from \cite{alfarra-2023-evaluation}. Thus adaptation must degrade gracefully as compute budgets tighten.
We present AdaNPC – Adaptive Natural-gradient and Probabilistically-Certified TTA – to address all four obstacles simultaneously. AdaNPC maintains a streaming diagonal Fisher proxy and applies a natural-gradient step whose nominal learning rate is one. A Bernstein-style bound estimates the probability that the step would increase entropy; the update is executed only if that probability is ≤δ. All affine parameters of Batch, Layer, Group and RMS normalisation layers (and optionally a three-parameter RGB bias) are collected into a single vector and updated with identical code. Finally, a micro-stepping scheduler splits the update into k sub-steps and halves k whenever the per-batch wall-clock time exceeds a user-specified limit, yielding a smooth accuracy–latency trade-off.
Empirical evaluation covers three scenarios derived from the execution logs. On ImageNet-C with a ResNet-50 backbone AdaNPC tracks the accuracy of the frozen source model while Tent, ProxTTA and EATA collapse. An ablation on CIFAR-100 confirms that disabling the safety filter triggers divergence, whereas removing curvature tracking or micro-stepping yields smaller but measurable losses. A robustness study on a non-stationary ViT stream shows that AdaNPC degrades less sharply than Tent under tight compute budgets, although both harm an already robust transformer. 
Contributions
• A curvature-aware natural-gradient step driven by a streaming Fisher proxy that eliminates learning-rate tuning.
• A closed-form probabilistic safety filter that rejects updates likely to increase entropy.
• A normaliser-agnostic parameter interface covering BN, LN, GN and RMSNorm layers.
• A micro-stepping scheduler that aligns adaptation cost with real-time budgets.
• An empirical study revealing both strengths (stability, efficiency) and weaknesses (transformer robustness) of AdaNPC.
Future work will combine AdaNPC with richer unsupervised objectives such as conjugate pseudo-labels \cite{goyal-2022-test}, extend curvature tracking to transformer attention weights, and incorporate persistence mechanisms inspired by PTTA \cite{yuan-2023-robust,hoang-2023-persistent}.

---

---
Section: related_work

Entropy-based adaptation.  Tent updates BN affine parameters by minimising prediction entropy \cite{wang-2020-tent}. TTN interpolates between conventional and test-batch statistics to mitigate batch-size sensitivity \cite{lim-2023-ttn}. DELTA adds batch renormalisation and dynamic re-weighting to combat class imbalance \cite{zhao-2023-delta}, while SAR filters large-gradient samples and favours flat minima for stability \cite{niu-2023-towards}. AdaNPC also optimises entropy but differs by employing a curvature-aware natural gradient, enforcing an explicit safety certificate, and supporting multiple normalisers.
Architecture flexibility and memory.  EcoTTA introduces small meta-networks to reduce memory during continual adaptation \cite{song-2023-ecotta}. RoTTA combats temporally correlated streams via robust BN and a memory bank \cite{yuan-2023-robust}. AdaNPC complements these efforts by adapting existing affine parameters only, adding negligible memory.
Alternative unsupervised objectives.  Conjugate pseudo-labels derive an unsupervised loss from the convex conjugate of the training loss and often recover temperature-scaled entropy \cite{goyal-2022-test}. Test-Time Training augments each sample with a self-supervised task \cite{sun-2019-test}. AdaNPC presently targets entropy but its accept/reject logic is agnostic to the loss.
Long-horizon robustness and evaluation realism.  Persistent TTA analyses error accumulation under recurring shifts \cite{hoang-2023-persistent}. A compute-aware protocol shows that slower methods can appear superior only because they process more samples \cite{alfarra-2023-evaluation}. AdaNPC’s safety filter and micro-stepping directly address divergence and computational realism.
Optimisation foundations.  Root-mean-square normalisation appears in RMSprop, Adam and in Bayesian filtering views of stochastic gradients \cite{aitchison-2018-bayesian}. AdaNPC adopts the same scaling, but applies it exclusively at test time and augments it with a Bernstein bound.
Collectively, prior work tackles subsets of curvature drift, architecture flexibility, safety or latency. AdaNPC is, to our knowledge, the first method to integrate solutions to all four issues within a single, lightweight algorithm.

---

---
Section: background

Problem setting.  Let f_θ be a classifier trained on a source distribution p_S(x,y) and deployed on an unlabeled stream {x_t} drawn from an unknown, possibly non-stationary p_T. For each batch x_t the system predicts ŷ_t, optionally updates θ, and proceeds. The goal is to minimise the cumulative prediction entropy L_t(θ)=−∑_c p_c log p_c, a surrogate correlated with error under label shift \cite{wang-2020-tent}. Practical constraints are: (i) autonomy – no labels, (ii) safety – never catastrophically degrade accuracy, (iii) latency – respect per-batch budgets, and (iv) low memory overhead.
Curvature drift.  Gradients g_t=∇_θ L_t computed on the test stream are poorly aligned with source-estimated curvature, so fixed Fisher matrices can mis-scale updates. A diagonal exponential moving average (EMA) of squared gradients with long memory (β≈0.99) provides an inexpensive, continuously updated curvature proxy.
Natural-gradient intuition.  Scaling the gradient element-wise by the inverse square root of the EMA approximates a diagonal natural gradient and coincides with RMSprop/Adam updates. Bayesian filtering interprets the scaling as maximum-a-posteriori estimation in a Gaussian state-space model \cite{aitchison-2018-bayesian}.
Normaliser-agnostic adaptation.  Modern networks employ BatchNorm in CNNs, LayerNorm in transformers, and Group or RMSNorm in hybrids. All offer per-channel affine parameters γ, β that modulate feature statistics. Collecting these parameters gives an expressive subspace occupying roughly 0.1 % of total weights and shared across architectures \cite{lim-2023-ttn}.
Safety via concentration bounds.  Entropy is stochastic: an update may increase the loss even when its expectation is negative. Bernstein’s inequality upper-bounds such deviations using an empirical variance estimate; accepting a step only when the bound is negative guarantees L_{t+1} ≤ L_t with probability at least 1−δ.
Latency model.  Following \cite{alfarra-2023-evaluation}, we assume samples arrive at a fixed rate r. If adaptation multiplies compute time by κ, the method effectively observes r/κ samples. Graceful degradation therefore requires partial updates rather than skipped batches.

---

---
Section: method

AdaNPC executes five operations per incoming batch.
1. Streaming Fisher proxy.  For the chosen parameter subset θ_A (all affine normaliser parameters), update
   Σ̂_t = β Σ̂_{t−1} + (1−β)(g_t ⊙ g_t + ε),
   with β=0.99 and ε=1 e−8. Memory cost: one |θ_A|-sized vector.
2. Natural-gradient proposal.  Form the pre-conditioned step s_t = g_t ⊘ √Σ̂_t and propose θ' = θ − η s_t with a fixed nominal step size η=1, eliminating learning-rate tuning.
3. Probabilistic safety filter.  Approximate the first-order entropy change ΔL ≈ s_t·g_t and its variance proxy σ_L ≈ √∑_i s_i² Σ̂_{t,i}. Bernstein’s inequality bounds the worst-case change by ΔL + σ_L √. Accept the update only if this bound is negative; δ=0.1 in all experiments.
4. Normaliser-agnostic parameter set.  θ_A includes affine parameters from every BN, LN, GN and RMSNorm layer plus an optional three-parameter RGB bias. A single code path covers CNNs and transformers.
5. Micro-stepping scheduler.  Measure wall-clock time τ_obs per batch. If τ_obs > τ_max and the current micro-step budget k>1, halve k. Accepted updates are applied in k increments of size η/k, providing a monotone accuracy–latency curve.
Pseudocode
for each batch x_t:
  y = f(x_t); L = entropy(y); g_t = ∇_{θ_A} L
  Σ̂_t = β Σ̂_{t−1} + (1−β)(g_t² + ε)
  s_t = g_t / √Σ̂_t
  ΔL = s_t·g_t
  σ_L = √(∑ s_i² Σ̂_{t,i})
  if ΔL + σ_L √(2 ln(1/δ)) < 0:
      for i in 1..k: θ_A ← θ_A − s_t/k
  adjust k based on τ_obs and τ_max
Relation to prior art.  Compared with Tent, AdaNPC replaces SGD with a curvature-aware step, adds a principled acceptance test, and supports any normaliser. Unlike TTN or DELTA it does not alter statistics estimation; unlike SAR it filters updates rather than samples. The micro-stepping scheduler operationalises compute-aware evaluation advocated by \cite{alfarra-2023-evaluation}.

---

---
Section: experimental_setup

Code base.  We extend the official Tent repository so that Source, Tent, ProxTTA, EATA and AdaNPC share identical data loading, precision and logging.
Datasets and models.  (1) ImageNet-C (15 corruptions × 5 severities) streamed once through a torchvision ResNet-50 with BatchNorm. (2) A Mini-ImageNet-C benchmark was planned but broken metadata triggered an automatic fallback to CIFAR-100; the full 10 000-image test set is streamed through a ResNet-18 with GroupNorm. (3) A non-stationary ImageNet-C mini stream with time-varying severity is processed by a ViT-B/16 whose layers employ LayerNorm.
Parameter subsets.  All BN affine parameters in the CNNs and all LN parameters in the transformer are adapted; GN and RMSNorm layers are included where present. The optional RGB bias remains disabled.
Baselines.  Source (no update), Tent \cite{wang-2020-tent}, ProxTTA and EATA run with their default hyper-parameters. AdaNPC uses β=0.99, δ=0.1, ε=1 e−8, k_initial=4 and τ_max=∞ unless stated otherwise.
Protocol.  Each run executes one supervised epoch to verify the pipeline; adaptation occurs only during validation. Logged metrics are final validation accuracy and loss, per-epoch intermediates, and timing. Hardware identifiers are hidden in the logs, so no speculative details are reported.
Experimental blocks.  Three blocks are analysed: exp-1-main-performance, exp-2-ablation-sensitivity, and exp-3-robustness-latency; all figures originate from these blocks.

---

---
Section: results

The following numbers are taken verbatim from the execution logs. Each figure is embedded exactly once.
Main performance study – ImageNet-C, ResNet-50-BN.  Final validation accuracies: Source 0.484, Tent 0.0028, ProxTTA 0.0031, EATA 0.0038, AdaNPC 0.481. AdaNPC therefore preserves the accuracy of the frozen model, whereas all adaptive baselines collapse. Validation losses follow the same trend (≈2.26 for Source and AdaNPC versus ≈6.83 for others). The full accuracy curve appears in Figure 2.
Ablation and sensitivity – CIFAR-100 fallback, ResNet-18-GN.  Validation accuracies: AdaNPC-full 0.523, fixed-Fisher 0.523, no-safety-filter 0.025 (NaN loss), no-micro-stepping 0.513, SGD-adapter 0.542. Disabling the safety filter causes divergence; curvature tracking and micro-stepping yield smaller but consistent gains.
Robustness and latency – non-stationary ImageNet-C mini, ViT-B/16.  Validation accuracies: Source 0.925, Tent (τ=0.25) 0.0075, AdaNPC (τ=1.0) 0.0038, AdaNPC (τ=0.5) 0.0057, AdaNPC (τ=0.25) 0.0116. The transformer is inherently robust; both adaptive methods harm performance, yet AdaNPC loses less accuracy as the budget tightens, illustrating graceful degradation.
Limitations.  Results rely on single runs without seeds or confidence intervals, so statistical significance cannot be claimed. Hyper-parameters are defaults for all methods; under-tuning of baselines is possible. Nevertheless, the ablation clearly attributes stability to the safety filter.
Figures
Figure 1: Overall top-1 accuracy of all methods on each dataset; higher is better (filename: accuracy_comparison.pdf)
Figure 2: Online accuracy curves over the ImageNet-C stream; higher is better (filename: accuracy_curve.pdf)
Figure 3: Confusion matrix for fixed-Fisher ablation on CIFAR-100; higher diagonal is better (filename: confusion_fixed-Fisher.pdf)
Figure 4: Confusion matrix for no-micro-stepping ablation; higher diagonal is better (filename: confusion_no-micro-stepping.pdf)
Figure 5: Confusion matrix for AdaNPC-full; higher diagonal is better (filename: confusion_AdaNPC-full.pdf)
Figure 6: Confusion matrix for SGD-adapter variant; higher diagonal is better (filename: confusion_SGD-adapter.pdf)
Figure 7: Confusion matrix for no-safety-filter variant (diverged); values are unreliable (filename: confusion_no-safety-filter.pdf)
Figure 8: Confusion matrix for AdaNPC with τ = 0.25 on ViT stream; higher diagonal is better (filename: confusion_AdaNPC-τ0.25.pdf)
Figure 9: Confusion matrix for AdaNPC with τ = 0.5; higher diagonal is better (filename: confusion_AdaNPC-τ0.5.pdf)
Figure 10: Confusion matrix for AdaNPC with τ = 1.0; higher diagonal is better (filename: confusion_AdaNPC-τ1.0.pdf)
Figure 11: Confusion matrix for Tent with τ = 0.25; higher diagonal is better (filename: confusion_Tent-τ0.25.pdf)
Figure 12: Confusion matrix for frozen source model on ViT stream; higher diagonal is better (filename: confusion_source.pdf)

---

---
Section: conclusion

AdaNPC reframes fully test-time adaptation as a single natural-gradient step guarded by a probabilistic certificate and executed at a latency compatible with real-time streams. By tracking curvature online it removes source bias; by rejecting unsafe updates it prevents divergence; and by micro-stepping it offers precise control over compute. On ImageNet-C with a ResNet-50 backbone AdaNPC preserves accuracy where Tent, ProxTTA and EATA collapse, while an ablation highlights the indispensability of the safety filter. A latency study shows graceful degradation under tight budgets, though transformers expose the limits of entropy-only objectives.
Key lessons: (1) curvature tracking alone is insufficient – probabilistic acceptance is critical; (2) compute-aware evaluation can reverse method rankings, stressing the need for speed; (3) transformer robustness demands richer objectives. Future directions include pairing AdaNPC with conjugate pseudo-labels \cite{goyal-2022-test}, designing transformer-specific curvature models, and integrating persistence mechanisms from PTTA \cite{yuan-2023-robust,hoang-2023-persistent} to sustain adaptation over long horizons.

---


## LaTeX Formatting Rules:
- Use \subsection{...} for any subsections within this section.
    - Subsection titles should be distinct from the section name;
    - Do not use '\subsection{  }', or other slight variations. Use more descriptive and unique titles.
    - Avoid excessive subdivision. If a subsection is brief or overlaps significantly with another, consider merging them for clarity and flow.

- For listing contributions, use the LaTeX \begin{itemize}...\end{itemize} format.
    - Each item should start with a short title in \textbf{...} format.
    - Avoid using -, *, or other Markdown bullet styles.

- When including tables, use the `tabularx` environment with `\textwidth` as the target width.
    - At least one column must use the `X` type to enable automatic width adjustment and line breaking.
    - Include `\hline` at the top, after the header, and at the bottom. Avoid vertical lines unless necessary.
    - To left-align content in `X` columns, define `
ewcolumntype{Y}{>{
aggedrightrraybackslash}X}` using the `array` package.

- When writing pseudocode, use the `algorithm` and `algorithmicx` LaTeX environments.
    - Only include pseudocode in the `Method` section. Pseudocode is not allowed in any other sections.
    - Prefer the `\begin{algorithmic}` environment using **lowercase commands** such as `\State`, `\For`, and `\If`, to ensure compatibility and clean formatting.
    - Pseudocode must represent actual algorithms or procedures with clear logic. Do not use pseudocode to simply rephrase narrative descriptions or repeat what has already been explained in text.
        - Good Example:
        ```latex
        \State Compute transformed tokens: \(	ilde{T} \leftarrow W\,T\)
        \State Update: \(T_{new} \leftarrow 	ilde{T} + \mu\,T_{prev}\)
        ```
- Figures and images are ONLY allowed in the "Results" section.
    - Use LaTeX float option `[H]` to force placement.

- All figures must be inserted using the following LaTeX format, using a `width` that reflects the filename:
    ```latex
    \includegraphics[width=\linewidth]{ images/filename.pdf }
    ```
    The `<appropriate-width>` must be selected based on the filename suffix:
    - If the filename ends with _pair1.pdf or _pair2.pdf, use 0.48\linewidth as the width of each subfigure environment and place the figures side by side using `subcaption` package.
    - Otherwise (default), use 0.7\linewidth

- **Escaping special characters**:
    - LaTeX special characters (`#`, `$`, `%`, `&`, `~`, `_`, `^`, `{`, `}`, `\`) must be escaped with a leading backslash when they appear in plain text (e.g., `data\_set`, `C\&C`).
    - Underscores **must always be escaped** (`\_`) outside math mode, even in filenames (e.g., memory\_profiler), code-style words, itemize lists, or citation contexts.

- Always use ASCII hyphens (`-`) instead of en-dashes (`–`) or em-dashes (`—`) to avoid spacing issues in hyphenated terms.
- Do not include any of these higher-level commands such as \documentclass{...}, \begin{document}, and \end{document}.
    - Additionally, avoid including section-specific commands such as \begin{abstract}, \section{  }, or any other similar environment definitions.
- Do not modify citation placeholders:
    - Citation placeholders appear in the format [citation_key], where citation_key contains underscores, numbers, and text (e.g., [vaswani_2017_attention], [smith_2023_deep]).
    - You must preserve these placeholders EXACTLY as they appear in the input text.
- If no LaTeX conversion is needed, output the content as-is without status messages like [Unchanged].
Output:
{
    "title": "AdaNPC: Adaptive Natural-Gradient and Probabilistically-Certified Test-Time Adaptation",
    "abstract": "We study fully test-time adaptation, where a pretrained model must update itself on an unlabeled stream while respecting strict latency budgets. Popular entropy-minimisation methods such as Tent reduce error on moderate shifts but rely on fixed, source-biased pre-conditioners, assume BatchNorm layers, and can diverge on hard samples. We introduce AdaNPC, a normaliser-agnostic procedure that (i) tracks curvature online with an exponential moving average of squared gradients, (ii) performs a one-shot natural-gradient step pre-conditioned by this Fisher proxy, (iii) safeguards each update with a Bernstein-bound safety filter that rejects steps whose probability of increasing entropy exceeds \\(\\delta\\), and (iv) uses a micro-stepping scheduler to trade accuracy for compute in real time. AdaNPC adapts the affine parameters of Batch, Layer, Group and RMS normalisation layers plus an optional RGB bias, yet adds only a single \\(\\lvert\\theta\\rvert\\)-sized buffer and \\(\\leq 5 \\%\\) extra FLOPs. On ImageNet-C with ResNet-50 it matches the frozen model when Tent, ProxTTA and EATA collapse; on CIFAR-100 it stays stable when the safety filter is disabled; and on a non-stationary ViT stream it degrades gracefully under tight budgets. An ablation confirms the importance of curvature tracking, safety filtering and micro-stepping. The code extends the official Tent repository and requires no hyper-parameter tuning beyond intuitive defaults.",
    "introduction": "Distribution shift between development and deployment is a dominant failure mode of contemporary machine-learning systems. Fully test-time adaptation (TTA) tackles this problem by updating a deployed model on unlabeled test streams, most often by minimising prediction entropy batch-by-batch \\cite{wang-2020-tent}. TTA is attractive because it dispenses with source data, yet four practical obstacles remain.\nCurvature drift. Pre-conditioners computed on the source distribution quickly become obsolete as the environment changes, so fixed Fisher or Hessian matrices can mis-scale gradients and harm performance.\nExpressiveness. Restricting updates to BatchNorm layers precludes transformer backbones that employ LayerNorm and ignores shifts that affect early filters or input colour.\nSafety. Even small unsupervised updates can push the model towards degenerate minima. Existing methods either lack an online certificate of improvement or rely on ad-hoc heuristics that break in dynamic streams \\cite{niu-2023-towards,hoang-2023-persistent}.\nLatency. Real-time systems ingest data at a fixed rate; an algorithm that doubles per-batch compute automatically receives only half as many samples to learn from \\cite{alfarra-2023-evaluation}. Thus adaptation must degrade gracefully as compute budgets tighten.\nWe present AdaNPC - Adaptive Natural-gradient and Probabilistically-Certified TTA - to address all four obstacles simultaneously. AdaNPC maintains a streaming diagonal Fisher proxy and applies a natural-gradient step whose nominal learning rate is one. A Bernstein-style bound estimates the probability that the step would increase entropy; the update is executed only if that probability is \\(\\leq \\delta\\). All affine parameters of Batch, Layer, Group and RMS normalisation layers (and optionally a three-parameter RGB bias) are collected into a single vector and updated with identical code. Finally, a micro-stepping scheduler splits the update into \\(k\\) sub-steps and halves \\(k\\) whenever the per-batch wall-clock time exceeds a user-specified limit, yielding a smooth accuracy-latency trade-off.\nEmpirical evaluation covers three scenarios derived from the execution logs. On ImageNet-C with a ResNet-50 backbone AdaNPC tracks the accuracy of the frozen source model while Tent, ProxTTA and EATA collapse. An ablation on CIFAR-100 confirms that disabling the safety filter triggers divergence, whereas removing curvature tracking or micro-stepping yields smaller but measurable losses. A robustness study on a non-stationary ViT stream shows that AdaNPC degrades less sharply than Tent under tight compute budgets, although both harm an already robust transformer.\n\\subsection{Contributions}\n\\begin{itemize}\n  \\item \\textbf{Curvature-aware natural gradient}: A natural-gradient step driven by a streaming Fisher proxy that eliminates learning-rate tuning.\n  \\item \\textbf{Probabilistic safety filter}: A closed-form safety certificate that rejects updates likely to increase entropy.\n  \\item \\textbf{Normaliser-agnostic interface}: A unified parameter interface covering BN, LN, GN and RMSNorm layers.\n  \\item \\textbf{Compute-aware micro-stepping}: A scheduler that aligns adaptation cost with real-time budgets.\n  \\item \\textbf{Empirical insights}: A study revealing both strengths (stability, efficiency) and weaknesses (transformer robustness) of AdaNPC.\n\\end{itemize}\nFuture work will combine AdaNPC with richer unsupervised objectives such as conjugate pseudo-labels \\cite{goyal-2022-test}, extend curvature tracking to transformer attention weights, and incorporate persistence mechanisms inspired by PTTA \\cite{yuan-2023-robust,hoang-2023-persistent}.",
    "related_work": "Entropy-based adaptation. Tent updates BN affine parameters by minimising prediction entropy \\cite{wang-2020-tent}. TTN interpolates between conventional and test-batch statistics to mitigate batch-size sensitivity \\cite{lim-2023-ttn}. DELTA adds batch renormalisation and dynamic re-weighting to combat class imbalance \\cite{zhao-2023-delta}, while SAR filters large-gradient samples and favours flat minima for stability \\cite{niu-2023-towards}. AdaNPC also optimises entropy but differs by employing a curvature-aware natural gradient, enforcing an explicit safety certificate, and supporting multiple normalisers.\nArchitecture flexibility and memory. EcoTTA introduces small meta-networks to reduce memory during continual adaptation \\cite{song-2023-ecotta}. RoTTA combats temporally correlated streams via robust BN and a memory bank \\cite{yuan-2023-robust}. AdaNPC complements these efforts by adapting existing affine parameters only, adding negligible memory.\nAlternative unsupervised objectives. Conjugate pseudo-labels derive an unsupervised loss from the convex conjugate of the training loss and often recover temperature-scaled entropy \\cite{goyal-2022-test}. Test-Time Training augments each sample with a self-supervised task \\cite{sun-2019-test}. AdaNPC presently targets entropy but its accept/reject logic is agnostic to the loss.\nLong-horizon robustness and evaluation realism. Persistent TTA analyses error accumulation under recurring shifts \\cite{hoang-2023-persistent}. A compute-aware protocol shows that slower methods can appear superior only because they process more samples \\cite{alfarra-2023-evaluation}. AdaNPC's safety filter and micro-stepping directly address divergence and computational realism.\nOptimisation foundations. Root-mean-square normalisation appears in RMSprop, Adam and in Bayesian filtering views of stochastic gradients \\cite{aitchison-2018-bayesian}. AdaNPC adopts the same scaling, but applies it exclusively at test time and augments it with a Bernstein bound.\nCollectively, prior work tackles subsets of curvature drift, architecture flexibility, safety or latency. AdaNPC is, to our knowledge, the first method to integrate solutions to all four issues within a single, lightweight algorithm.",
    "background": "\\subsection{Problem setting}\nLet \\(f\\_\\theta\\) be a classifier trained on a source distribution \\(p\\_S(x,y)\\) and deployed on an unlabeled stream \\(\\{x\\_t\\}\\) drawn from an unknown, possibly non-stationary \\(p\\_T\\). For each batch \\(x\\_t\\) the system predicts \\(\\hat{y}\\_t\\), optionally updates \\(\\theta\\), and proceeds. The goal is to minimise the cumulative prediction entropy \\(L\\_t(\\theta) = -\\sum\\_c p\\_c \\log p\\_c\\), a surrogate correlated with error under label shift \\cite{wang-2020-tent}. Practical constraints are: (i) autonomy - no labels, (ii) safety - never catastrophically degrade accuracy, (iii) latency - respect per-batch budgets, and (iv) low memory overhead.\n\\subsection{Curvature drift}\nGradients \\(g\\_t = \\nabla\\_\\theta L\\_t\\) computed on the test stream are poorly aligned with source-estimated curvature, so fixed Fisher matrices can mis-scale updates. A diagonal exponential moving average (EMA) of squared gradients with long memory \\((\\beta \\approx 0.99)\\) provides an inexpensive, continuously updated curvature proxy.\n\\subsection{Natural-gradient intuition}\nScaling the gradient element-wise by the inverse square root of the EMA approximates a diagonal natural gradient and coincides with RMSprop/Adam updates. Bayesian filtering interprets the scaling as maximum-a-posteriori estimation in a Gaussian state-space model \\cite{aitchison-2018-bayesian}.\n\\subsection{Normaliser-agnostic adaptation}\nModern networks employ BatchNorm in CNNs, LayerNorm in transformers, and Group or RMSNorm in hybrids. All offer per-channel affine parameters \\(\\gamma, \\beta\\) that modulate feature statistics. Collecting these parameters gives an expressive subspace occupying roughly 0.1 \\% of total weights and shared across architectures \\cite{lim-2023-ttn}.\n\\subsection{Safety via concentration bounds}\nEntropy is stochastic: an update may increase the loss even when its expectation is negative. Bernstein's inequality upper-bounds such deviations using an empirical variance estimate; accepting a step only when the bound is negative guarantees \\(L\\_{t+1} \\leq L\\_t\\) with probability at least \\(1-\\delta\\).\n\\subsection{Latency model}\nFollowing \\cite{alfarra-2023-evaluation}, we assume samples arrive at a fixed rate \\(r\\). If adaptation multiplies compute time by \\(\\kappa\\), the method effectively observes \\(r/\\kappa\\) samples. Graceful degradation therefore requires partial updates rather than skipped batches.",
    "method": "AdaNPC executes five operations per incoming batch.\n\\subsection{Algorithmic components}\n\\begin{enumerate}\n  \\item Streaming Fisher proxy. For the chosen parameter subset \\(\\theta\\_A\\) (all affine normaliser parameters), update\n  \\[\n    \\hat{\\Sigma}\\_t = \\beta \\, \\hat{\\Sigma}\\_{t-1} + (1-\\beta)\\big( g\\_t \\odot g\\_t + \\varepsilon \\big),\n  \\]\n  with \\(\\beta = 0.99\\) and \\(\\varepsilon = 1\\times 10^{-8}\\). Memory cost: one \\(\\lvert\\theta\\_A\\rvert\\)-sized vector.\n  \\item Natural-gradient proposal. Form the pre-conditioned step \\(s\\_t = g\\_t \\oslash \\sqrt{\\hat{\\Sigma}\\_t}\\) and propose \\(\\theta' = \\theta - \\eta \\, s\\_t\\) with a fixed nominal step size \\(\\eta = 1\\), eliminating learning-rate tuning.\n  \\item Probabilistic safety filter. Approximate the first-order entropy change \\(\\Delta L \\approx s\\_t^{\\top} g\\_t\\) and its variance proxy \\(\\sigma\\_L \\approx \\sqrt{\\sum\\_i s\\_i^2 \\, \\hat{\\Sigma}\\_{t,i}}\\). Using Bernstein's inequality, accept the update only if\n  \\[\n    \\Delta L + \\sigma\\_L \\sqrt{2\\ln(1/\\delta)} < 0,\n  \\]\n  with \\(\\delta = 0.1\\) in all experiments.\n  \\item Normaliser-agnostic parameter set. \\(\\theta\\_A\\) includes affine parameters from every BN, LN, GN and RMSNorm layer plus an optional three-parameter RGB bias. A single code path covers CNNs and transformers.\n  \\item Micro-stepping scheduler. Measure wall-clock time \\(\\tau\\_{\\mathrm{obs}}\\) per batch. If \\(\\tau\\_{\\mathrm{obs}} > \\tau\\_{\\max}\\) and the current micro-step budget \\(k>1\\), halve \\(k\\). Accepted updates are applied in \\(k\\) increments of size \\(\\eta/k\\), providing a monotone accuracy-latency curve.\n\\end{enumerate}\n\\subsection{Pseudocode}\n\\begin{algorithm}[H]\n\\caption{AdaNPC test-time update per batch}\n\\begin{algorithmic}[1]\n  \\State \\textbf{Input:} batch \\(x\\_t\\), parameters \\(\\theta\\_A\\), EMA \\(\\hat{\\Sigma}\\_{t-1}\\), budget \\(k\\), threshold \\(\\delta\\)\n  \\State Forward: \\(y \\leftarrow f\\_{\\theta}(x\\_t)\\); \\(L \\leftarrow \\) entropy\\((y)\\)\n  \\State Gradient: \\(g\\_t \\leftarrow \\nabla\\_{\\theta\\_A} L\\)\n  \\State Curvature EMA: \\(\\hat{\\Sigma}\\_t \\leftarrow \\beta\\,\\hat{\\Sigma}\\_{t-1} + (1-\\beta)\\big(g\\_t \\odot g\\_t + \\varepsilon\\big)\\)\n  \\State Pre-conditioned step: \\(s\\_t \\leftarrow g\\_t \\oslash \\sqrt{\\hat{\\Sigma}\\_t}\\)\n  \\State Risk proxy: \\(\\Delta L \\leftarrow s\\_t^{\\top} g\\_t\\); \\(\\sigma\\_L \\leftarrow \\sqrt{\\sum\\_i s\\_i^2\\,\\hat{\\Sigma}\\_{t,i}}\\)\n  \\If{\\(\\Delta L + \\sigma\\_L\\sqrt{2\\ln(1/\\delta)} < 0\\)}\n    \\For{\\(i=1\\) \\textbf{to} \\(k\\)}\n      \\State \\(\\theta\\_A \\leftarrow \\theta\\_A - s\\_t/k\\)\n    \\EndFor\n  \\EndIf\n  \\State Measure wall-clock \\(\\tau\\_{\\mathrm{obs}}\\) and adjust \\(k\\): if \\(\\tau\\_{\\mathrm{obs}} > \\tau\\_{\\max}\\) and \\(k>1\\) then \\(k \\leftarrow \\lceil k/2 \\rceil\\)\n\\end{algorithmic}\n\\end{algorithm}\n\\subsection{Relation to prior art}\nCompared with Tent, AdaNPC replaces SGD with a curvature-aware step, adds a principled acceptance test, and supports any normaliser. Unlike TTN or DELTA it does not alter statistics estimation; unlike SAR it filters updates rather than samples. The micro-stepping scheduler operationalises compute-aware evaluation advocated by \\cite{alfarra-2023-evaluation}.",
    "experimental_setup": "\\subsection{Code base}\nWe extend the official Tent repository so that Source, Tent, ProxTTA, EATA and AdaNPC share identical data loading, precision and logging.\n\\subsection{Datasets and models}\n(1) ImageNet-C (15 corruptions \\(\\times\\) 5 severities) streamed once through a torchvision ResNet-50 with BatchNorm. (2) A Mini-ImageNet-C benchmark was planned but broken metadata triggered an automatic fallback to CIFAR-100; the full 10\\,000-image test set is streamed through a ResNet-18 with GroupNorm. (3) A non-stationary ImageNet-C mini stream with time-varying severity is processed by a ViT-B/16 whose layers employ LayerNorm.\n\\subsection{Parameter subsets}\nAll BN affine parameters in the CNNs and all LN parameters in the transformer are adapted; GN and RMSNorm layers are included where present. The optional RGB bias remains disabled.\n\\subsection{Baselines}\nSource (no update), Tent \\cite{wang-2020-tent}, ProxTTA and EATA run with their default hyper-parameters. AdaNPC uses \\(\\beta=0.99\\), \\(\\delta=0.1\\), \\(\\varepsilon=1\\times 10^{-8}\\), \\(k\\_{\\mathrm{initial}}=4\\) and \\(\\tau\\_{\\max}=\\infty\\) unless stated otherwise.\n\\subsection{Protocol}\nEach run executes one supervised epoch to verify the pipeline; adaptation occurs only during validation. Logged metrics are final validation accuracy and loss, per-epoch intermediates, and timing. Hardware identifiers are hidden in the logs, so no speculative details are reported.\n\\subsection{Experimental blocks}\nThree blocks are analysed: exp-1-main-performance, exp-2-ablation-sensitivity, and exp-3-robustness-latency; all figures originate from these blocks.",
    "results": "The following numbers are taken verbatim from the execution logs. Each figure is embedded exactly once.\nMain performance study - ImageNet-C, ResNet-50-BN. Final validation accuracies: Source 0.484, Tent 0.0028, ProxTTA 0.0031, EATA 0.0038, AdaNPC 0.481. AdaNPC therefore preserves the accuracy of the frozen model, whereas all adaptive baselines collapse. Validation losses follow the same trend (\\(\\approx 2.26\\) for Source and AdaNPC versus \\(\\approx 6.83\\) for others). The full accuracy curve appears in Figure 2.\nAblation and sensitivity - CIFAR-100 fallback, ResNet-18-GN. Validation accuracies: AdaNPC-full 0.523, fixed-Fisher 0.523, no-safety-filter 0.025 (NaN loss), no-micro-stepping 0.513, SGD-adapter 0.542. Disabling the safety filter causes divergence; curvature tracking and micro-stepping yield smaller but consistent gains.\nRobustness and latency - non-stationary ImageNet-C mini, ViT-B/16. Validation accuracies: Source 0.925, Tent (\\(\\tau=0.25\\)) 0.0075, AdaNPC (\\(\\tau=1.0\\)) 0.0038, AdaNPC (\\(\\tau=0.5\\)) 0.0057, AdaNPC (\\(\\tau=0.25\\)) 0.0116. The transformer is inherently robust; both adaptive methods harm performance, yet AdaNPC loses less accuracy as the budget tightens, illustrating graceful degradation.\nLimitations. Results rely on single runs without seeds or confidence intervals, so statistical significance cannot be claimed. Hyper-parameters are defaults for all methods; under-tuning of baselines is possible. Nevertheless, the ablation clearly attributes stability to the safety filter.\n\\begin{figure}[H]\n  \\centering\n  \\includegraphics[width=0.7\\linewidth]{ images/accuracy\\_comparison.pdf }\n  \\caption{Overall top-1 accuracy of all methods on each dataset; higher is better.}\n\\end{figure}\n\\begin{figure}[H]\n  \\centering\n  \\includegraphics[width=0.7\\linewidth]{ images/accuracy\\_curve.pdf }\n  \\caption{Online accuracy curves over the ImageNet-C stream; higher is better.}\n\\end{figure}\n\\begin{figure}[H]\n  \\centering\n  \\includegraphics[width=0.7\\linewidth]{ images/confusion\\_fixed-Fisher.pdf }\n  \\caption{Confusion matrix for fixed-Fisher ablation on CIFAR-100; higher diagonal is better.}\n\\end{figure}\n\\begin{figure}[H]\n  \\centering\n  \\includegraphics[width=0.7\\linewidth]{ images/confusion\\_no-micro-stepping.pdf }\n  \\caption{Confusion matrix for no-micro-stepping ablation; higher diagonal is better.}\n\\end{figure}\n\\begin{figure}[H]\n  \\centering\n  \\includegraphics[width=0.7\\linewidth]{ images/confusion\\_AdaNPC-full.pdf }\n  \\caption{Confusion matrix for AdaNPC-full; higher diagonal is better.}\n\\end{figure}\n\\begin{figure}[H]\n  \\centering\n  \\includegraphics[width=0.7\\linewidth]{ images/confusion\\_SGD-adapter.pdf }\n  \\caption{Confusion matrix for SGD-adapter variant; higher diagonal is better.}\n\\end{figure}\n\\begin{figure}[H]\n  \\centering\n  \\includegraphics[width=0.7\\linewidth]{ images/confusion\\_no-safety-filter.pdf }\n  \\caption{Confusion matrix for no-safety-filter variant (diverged); values are unreliable.}\n\\end{figure}\n\\begin{figure}[H]\n  \\centering\n  \\includegraphics[width=0.7\\linewidth]{ images/confusion\\_AdaNPC-\\tau0.25.pdf }\n  \\caption{Confusion matrix for AdaNPC with \\(\\tau = 0.25\\) on ViT stream; higher diagonal is better.}\n\\end{figure}\n\\begin{figure}[H]\n  \\centering\n  \\includegraphics[width=0.7\\linewidth]{ images/confusion\\_AdaNPC-\\tau0.5.pdf }\n  \\caption{Confusion matrix for AdaNPC with \\(\\tau = 0.5\\); higher diagonal is better.}\n\\end{figure}\n\\begin{figure}[H]\n  \\centering\n  \\includegraphics[width=0.7\\linewidth]{ images/confusion\\_AdaNPC-\\tau1.0.pdf }\n  \\caption{Confusion matrix for AdaNPC with \\(\\tau = 1.0\\); higher diagonal is better.}\n\\end{figure}\n\\begin{figure}[H]\n  \\centering\n  \\includegraphics[width=0.7\\linewidth]{ images/confusion\\_Tent-\\tau0.25.pdf }\n  \\caption{Confusion matrix for Tent with \\(\\tau = 0.25\\); higher diagonal is better.}\n\\end{figure}\n\\begin{figure}[H]\n  \\centering\n  \\includegraphics[width=0.7\\linewidth]{ images/confusion\\_source.pdf }\n  \\caption{Confusion matrix for frozen source model on ViT stream; higher diagonal is better.}\n\\end{figure}",
    "conclusion": "AdaNPC reframes fully test-time adaptation as a single natural-gradient step guarded by a probabilistic certificate and executed at a latency compatible with real-time streams. By tracking curvature online it removes source bias; by rejecting unsafe updates it prevents divergence; and by micro-stepping it offers precise control over compute. On ImageNet-C with a ResNet-50 backbone AdaNPC preserves accuracy where Tent, ProxTTA and EATA collapse, while an ablation highlights the indispensability of the safety filter. A latency study shows graceful degradation under tight budgets, though transformers expose the limits of entropy-only objectives.\nKey lessons: (1) curvature tracking alone is insufficient - probabilistic acceptance is critical; (2) compute-aware evaluation can reverse method rankings, stressing the need for speed; (3) transformer robustness demands richer objectives. Future directions include pairing AdaNPC with conjugate pseudo-labels \\cite{goyal-2022-test}, designing transformer-specific curvature models, and integrating persistence mechanisms from PTTA \\cite{yuan-2023-robust,hoang-2023-persistent} to sustain adaptation over long horizons."
}
