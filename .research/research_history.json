{
  "research_topic": "Improve Test-Time Adaptation in terms of convergence speed.",
  "queries": [
    "fast test-time adaptation",
    "test-time adaptation convergence",
    "online domain adaptation speed",
    "gradient-based test-time adaptation",
    "adaptive inference optimization",
    "meta-learning test-time adaptation"
  ],
  "research_study_list": [
    {
      "title": "Test Time Adaptation With Regularized Loss for Weakly Supervised Salient Object Detection"
    },
    {
      "title": "Tent: Fully Test-Time Adaptation by Entropy Minimization",
      "abstract": "A model must adapt itself to generalize to new and different data during\ntesting. In this setting of fully test-time adaptation the model has only the\ntest data and its own parameters. We propose to adapt by test entropy\nminimization (tent): we optimize the model for confidence as measured by the\nentropy of its predictions. Our method estimates normalization statistics and\noptimizes channel-wise affine transformations to update online on each batch.\nTent reduces generalization error for image classification on corrupted\nImageNet and CIFAR-10/100 and reaches a new state-of-the-art error on\nImageNet-C. Tent handles source-free domain adaptation on digit recognition\nfrom SVHN to MNIST/MNIST-M/USPS, on semantic segmentation from GTA to\nCityscapes, and on the VisDA-C benchmark. These results are achieved in one\nepoch of test-time optimization without altering training.",
      "full_text": "Published as a conference paper at ICLR 2021 TENT : F ULLY TEST-TIME ADAPTATION BY ENTROPY MINIMIZATION Dequan Wang1∗, Evan Shelhamer2∗†, Shaoteng Liu1, Bruno Olshausen1, Trevor Darrell1 dqwang@cs.berkeley.edu, shelhamer@google.com UC Berkeley1 Adobe Research2 ABSTRACT A model must adapt itself to generalize to new and different data during testing. In this setting of fully test-time adaptation the model has only the test data and its own parameters. We propose to adapt by test entropy minimization (tent 1): we optimize the model for conﬁdence as measured by the entropy of its predictions. Our method estimates normalization statistics and optimizes channel-wise afﬁne transformations to update online on each batch. Tent reduces generalization error for image classiﬁcation on corrupted ImageNet and CIFAR-10/100 and reaches a new state-of-the-art error on ImageNet-C. Tent handles source-free domain adapta- tion on digit recognition from SVHN to MNIST/MNIST-M/USPS, on semantic segmentation from GTA to Cityscapes, and on the VisDA-C benchmark. These results are achieved in one epoch of test-time optimization without altering training. 1 I NTRODUCTION Deep networks can achieve high accuracy on training and testing data from the same distribution, as evidenced by tremendous benchmark progress (Krizhevsky et al., 2012; Simonyan & Zisserman, 2015; He et al., 2016). However, generalization to new and different data is limited (Hendrycks & Dietterich, 2019; Recht et al., 2019; Geirhos et al., 2018). Accuracy suffers when the training (source) data differ from the testing (target) data, a condition known as dataset shift(Quionero-Candela et al., 2009). Models can be sensitive to shifts during testing that were not known during training, whether natural variations or corruptions, such as unexpected weather or sensor degradation. Nevertheless, it can be necessary to deploy a model on different data distributions, so adaptation is needed. During testing, the model must adapt given only its parameters and the target data. Thisfully test-time adaptation setting cannot rely on source data or supervision. Neither is practical when the model ﬁrst encounters new testing data, before it can be collected and annotated, as inference must go on. Real-world usage motivates fully test-time adaptation by data, computation, and task needs: 1. Availability. A model might be distributed without source data for bandwidth, privacy, or proﬁt. 2. Efﬁciency. It might not be computationally practical to (re-)process source data during testing. 3. Accuracy. A model might be too inaccurate without adaptation to serve its purpose. To adapt during testing we minimize the entropy of model predictions. We call this objective the test entropy and name our method tent after it. We choose entropy for its connections to error and shift. Entropy is related to error, as more conﬁdent predictions are all-in-all more correct (Figure 1). Entropy is related to shifts due to corruption, as more corruption results in more entropy, with a strong rank correlation to the loss for image classiﬁcation as the level of corruption increases (Figure 2). To minimize entropy, tent normalizes and transforms inference on target data by estimating statistics and optimizing afﬁne parameters batch-by-batch. This choice of low-dimensional, channel-wise feature modulation is efﬁcient to adapt during testing, even for online updates. Tent does not restrict or alter model training: it is independent of the source data given the model parameters. If the model can be run, it can be adapted. Most importantly, tent effectively reduces not just entropy but error. ∗Equal contribution. †Work done at Adobe Research; the author is now at DeepMind. 1Please see the project page at https://github.com/DequanWang/tent for the code and more. 1 arXiv:2006.10726v3  [cs.LG]  18 Mar 2021Published as a conference paper at ICLR 2021 0.0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1.0 Entropy 0 20 40 60 80Error (%) Figure 1: Predictions with lower entropy have lower error rates on corrupted CIFAR-100-C. Certainty can serve as supervision during testing. 0.2 0.3 0.4 0.5 0.6 Entropy 0.2 0.4 0.6 0.8 1.0 1.2Loss = 0.61 original noise blur digital weather  level level Figure 2: More corruption causes more loss and entropy on CIFAR-100-C. Entropy can estimate the degree of shift without training data or labels. Our results evaluate generalization to corruptions for image classiﬁcation, to domain shift for digit recognition, and to simulation-to-real shift for semantic segmentation. For context with more data and optimization, we evaluate methods for robust training, domain adaptation, and self-supervised learning given the labeled source data. Tent can achieve less error given only the target data, and it improves on the state-of-the-art for the ImageNet-C benchmark. Analysis experiments support our entropy objective, check sensitivity to the amount of data and the choice of parameters for adaptation, and back the generality of tent across architectures. Our contributions • We highlight the setting of fully test-time adaptation with only target data and no source data. To emphasize practical adaptation during inference we benchmark with ofﬂine and online updates. • We examine entropy as an adaptation objective and propose tent: a test-time entropy minimization scheme to reduce generalization error by reducing the entropy of model predictions on test data. • For robustness to corruptions, tent reaches 44.0% error on ImageNet-C, better than the state-of- the-art for robust training (50.2%) and the strong baseline of test-time normalization (49.9%). • For domain adaptation, tent is capable of online and source-free adaptation for digit classiﬁcation and semantic segmentation, and can even rival methods that use source data and more optimization. 2 S ETTING : F ULLY TEST-TIME ADAPTATION Adaptation addresses generalization from source to target. A model fθ(x) with parameters θtrained on source data and labels xs,ys may not generalize when tested on shifted target data xt. Table 1 summarizes adaptation settings, their required data, and types of losses. Our fully test-time adaptation setting uniquely requires only the model fθ and unlabeled target data xt for adaptation during inference. Existing adaptation settings extend training given more data and supervision. Transfer learning by ﬁne-tuning (Donahue et al., 2014; Yosinski et al., 2014) needs target labels to (re-)train with a supervised loss L(xt,yt). Without target labels, our setting denies this supervised training. Domain adaptation (DA) (Quionero-Candela et al., 2009; Saenko et al., 2010; Ganin & Lempitsky, 2015; Tzeng et al., 2015) needs both the source and target data to train with a cross-domain loss L(xs,xt). Test-time training (TTT) (Sun et al., 2019b) adapts during testing but ﬁrst alters training to jointly optimize its supervised loss L(xs,ys) and self-supervised loss L(xs). Without source, our setting denies joint training across domains (DA) or losses (TTT). Existing settings have their purposes, but do not cover all practical cases when source, target, or supervision are not simultaneously available. Unexpected target data during testing requires test-time adaptation. TTT and our setting adapt the model by optimizing an unsupervised loss during testing L(xt). During training, TTT jointly optimizes this same loss on source data L(xs) with a supervised loss L(xs,ys), to ensure the parameters θare shared across losses for compatibility with adaptation by L(xt). Fully test-time adaptation is independent of the training data and training loss given the parameters θ. By not changing training, our setting has the potential to require less data and computation for adaptation. 2Published as a conference paper at ICLR 2021 Table 1: Adaptation settings differ by their data and therefore losses during training and testing. Of the source s and target t data xand labels y, our fully test-time setting only needs the target data xt. setting source data target data train loss test loss ﬁne-tuning - xt,yt L(xt,yt) - domain adaptation xs, ys xt L(xs,ys) + L(xs,xt) - test-time training xs, ys xt L(xs,ys) + L(xs) L(xt) fully test-time adaptation - xt - L(xt)     = f (     ; θ)  Loss (   ,      ) θ (a) training <latexit sha1_base64=\"YDIW6Mi/jc4gnv843zXedRTilJU=\">AAAB63icbVBNS8NAEJ3Ur1q/qh69LBbBU0mkoMeiF48VrS20sWy2m3bpZhN2J2IJ/QlePCiIV/+QN/+N2zYHbX0w8Hhvhpl5QSKFQdf9dgorq2vrG8XN0tb2zu5eef/g3sSpZrzJYhnrdkANl0LxJgqUvJ1oTqNA8lYwupr6rUeujYjVHY4T7kd0oEQoGEUr3T49mF654lbdGcgy8XJSgRyNXvmr249ZGnGFTFJjOp6boJ9RjYJJPil1U8MTykZ0wDuWKhpx42ezUyfkxCp9EsbalkIyU39PZDQyZhwFtjOiODSL3lT8z+ukGF74mVBJilyx+aIwlQRjMv2b9IXmDOXYEsq0sLcSNqSaMrTplGwI3uLLy6R1VvVqVc+7qVXql3keRTiCYzgFD86hDtfQgCYwGMAzvMKbI50X5935mLcWnHzmEP7A+fwBBSaOGg==</latexit> x s <latexit sha1_base64=\"1psCF/1OyjuZ4TwBu21voNG0XaI=\">AAAB63icbVBNS8NAEJ3Ur1q/qh69LBbBU0lE0GPRi8eK1hbaWDbbSbt0swm7GyGE/gQvHhTEq3/Im//GbZuDtj4YeLw3w8y8IBFcG9f9dkorq2vrG+XNytb2zu5edf/gQcepYthisYhVJ6AaBZfYMtwI7CQKaRQIbAfj66nffkKleSzvTZagH9Gh5CFn1FjpLnvU/WrNrbszkGXiFaQGBZr96ldvELM0QmmYoFp3PTcxfk6V4UzgpNJLNSaUjekQu5ZKGqH289mpE3JilQEJY2VLGjJTf0/kNNI6iwLbGVEz0oveVPzP66YmvPRzLpPUoGTzRWEqiInJ9G8y4AqZEZkllClubyVsRBVlxqZTsSF4iy8vk/ZZ3Tuve97tea1xVeRRhiM4hlPw4AIacANNaAGDITzDK7w5wnlx3p2PeWvJKWYO4Q+czx8GrY4b</latexit> y s <latexit sha1_base64=\"XlB1POiMMxMFBsxKqM8k7anLbzY=\">AAAB8XicbVBNS8NAEJ3Ur1q/qh69LBbBU0mkoMeiF48VrFXaWDbbTbt0swm7EyGE/govHhTEq//Gm//GbZuDtj4YeLw3w8y8IJHCoOt+O6WV1bX1jfJmZWt7Z3evun9wZ+JUM95msYz1fUANl0LxNgqU/D7RnEaB5J1gfDX1O09cGxGrW8wS7kd0qEQoGEUrPfRGFPNs8mj61Zpbd2cgy8QrSA0KtPrVr94gZmnEFTJJjel6boJ+TjUKJvmk0ksNTygb0yHvWqpoxI2fzw6ekBOrDEgYa1sKyUz9PZHTyJgsCmxnRHFkFr2p+J/XTTG88HOhkhS5YvNFYSoJxmT6PRkIzRnKzBLKtLC3EjaimjK0GVVsCN7iy8ukc1b3GnXPu2nUmpdFHmU4gmM4BQ/OoQnX0II2MIjgGV7hzdHOi/PufMxbS04xcwh/4Hz+ANhzkOg=</latexit> ˆy s <latexit sha1_base64=\"XlB1POiMMxMFBsxKqM8k7anLbzY=\">AAAB8XicbVBNS8NAEJ3Ur1q/qh69LBbBU0mkoMeiF48VrFXaWDbbTbt0swm7EyGE/govHhTEq//Gm//GbZuDtj4YeLw3w8y8IJHCoOt+O6WV1bX1jfJmZWt7Z3evun9wZ+JUM95msYz1fUANl0LxNgqU/D7RnEaB5J1gfDX1O09cGxGrW8wS7kd0qEQoGEUrPfRGFPNs8mj61Zpbd2cgy8QrSA0KtPrVr94gZmnEFTJJjel6boJ+TjUKJvmk0ksNTygb0yHvWqpoxI2fzw6ekBOrDEgYa1sKyUz9PZHTyJgsCmxnRHFkFr2p+J/XTTG88HOhkhS5YvNFYSoJxmT6PRkIzRnKzBLKtLC3EjaimjK0GVVsCN7iy8ukc1b3GnXPu2nUmpdFHmU4gmM4BQ/OoQnX0II2MIjgGV7hzdHOi/PufMxbS04xcwh/4Hz+ANhzkOg=</latexit> ˆy s <latexit sha1_base64=\"1psCF/1OyjuZ4TwBu21voNG0XaI=\">AAAB63icbVBNS8NAEJ3Ur1q/qh69LBbBU0lE0GPRi8eK1hbaWDbbSbt0swm7GyGE/gQvHhTEq3/Im//GbZuDtj4YeLw3w8y8IBFcG9f9dkorq2vrG+XNytb2zu5edf/gQcepYthisYhVJ6AaBZfYMtwI7CQKaRQIbAfj66nffkKleSzvTZagH9Gh5CFn1FjpLnvU/WrNrbszkGXiFaQGBZr96ldvELM0QmmYoFp3PTcxfk6V4UzgpNJLNSaUjekQu5ZKGqH289mpE3JilQEJY2VLGjJTf0/kNNI6iwLbGVEz0oveVPzP66YmvPRzLpPUoGTzRWEqiInJ9G8y4AqZEZkllClubyVsRBVlxqZTsSF4iy8vk/ZZ3Tuve97tea1xVeRRhiM4hlPw4AIacANNaAGDITzDK7w5wnlx3p2PeWvJKWYO4Q+czx8GrY4b</latexit> y s <latexit sha1_base64=\"XlB1POiMMxMFBsxKqM8k7anLbzY=\">AAAB8XicbVBNS8NAEJ3Ur1q/qh69LBbBU0mkoMeiF48VrFXaWDbbTbt0swm7EyGE/govHhTEq//Gm//GbZuDtj4YeLw3w8y8IJHCoOt+O6WV1bX1jfJmZWt7Z3evun9wZ+JUM95msYz1fUANl0LxNgqU/D7RnEaB5J1gfDX1O09cGxGrW8wS7kd0qEQoGEUrPfRGFPNs8mj61Zpbd2cgy8QrSA0KtPrVr94gZmnEFTJJjel6boJ+TjUKJvmk0ksNTygb0yHvWqpoxI2fzw6ekBOrDEgYa1sKyUz9PZHTyJgsCmxnRHFkFr2p+J/XTTG88HOhkhS5YvNFYSoJxmT6PRkIzRnKzBLKtLC3EjaimjK0GVVsCN7iy8ukc1b3GnXPu2nUmpdFHmU4gmM4BQ/OoQnX0II2MIjgGV7hzdHOi/PufMxbS04xcwh/4Hz+ANhzkOg=</latexit> ˆy s <latexit sha1_base64=\"YDIW6Mi/jc4gnv843zXedRTilJU=\">AAAB63icbVBNS8NAEJ3Ur1q/qh69LBbBU0mkoMeiF48VrS20sWy2m3bpZhN2J2IJ/QlePCiIV/+QN/+N2zYHbX0w8Hhvhpl5QSKFQdf9dgorq2vrG8XN0tb2zu5eef/g3sSpZrzJYhnrdkANl0LxJgqUvJ1oTqNA8lYwupr6rUeujYjVHY4T7kd0oEQoGEUr3T49mF654lbdGcgy8XJSgRyNXvmr249ZGnGFTFJjOp6boJ9RjYJJPil1U8MTykZ0wDuWKhpx42ezUyfkxCp9EsbalkIyU39PZDQyZhwFtjOiODSL3lT8z+ukGF74mVBJilyx+aIwlQRjMv2b9IXmDOXYEsq0sLcSNqSaMrTplGwI3uLLy6R1VvVqVc+7qVXql3keRTiCYzgFD86hDtfQgCYwGMAzvMKbI50X5935mLcWnHzmEP7A+fwBBSaOGg==</latexit> x s (b) fully test-time adaptation θ    = f (    ; θ+Δ)  Entropy (    ) <latexit sha1_base64=\"uzCfvi+otYu2ihjbD7PaMp0JG7Y=\">AAAB8XicbVBNS8NAEJ3Ur1q/qh69LBbBU0mkoMeiF48VrFXaWDbbTbt0swm7EyGE/govHhTEq//Gm//GbZuDtj4YeLw3w8y8IJHCoOt+O6WV1bX1jfJmZWt7Z3evun9wZ+JUM95msYz1fUANl0LxNgqU/D7RnEaB5J1gfDX1O09cGxGrW8wS7kd0qEQoGEUrPfRGFPNs8oj9as2tuzOQZeIVpAYFWv3qV28QszTiCpmkxnQ9N0E/pxoFk3xS6aWGJ5SN6ZB3LVU04sbPZwdPyIlVBiSMtS2FZKb+nshpZEwWBbYzojgyi95U/M/rphhe+LlQSYpcsfmiMJUEYzL9ngyE5gxlZgllWthbCRtRTRnajCo2BG/x5WXSOat7jbrn3TRqzcsijzIcwTGcggfn0IRraEEbGETwDK/w5mjnxXl3PuatJaeYOYQ/cD5/ANn4kOk=</latexit> ˆy t <latexit sha1_base64=\"m/ZzdjACtPk7VVv8qMUMxHkn5f0=\">AAAB63icbVBNS8NAEJ3Ur1q/qh69LBbBU0mkoMeiF48VrS20sWy2m3bpZhN2J2IJ/QlePCiIV/+QN/+N2zYHbX0w8Hhvhpl5QSKFQdf9dgorq2vrG8XN0tb2zu5eef/g3sSpZrzJYhnrdkANl0LxJgqUvJ1oTqNA8lYwupr6rUeujYjVHY4T7kd0oEQoGEUr3T49YK9ccavuDGSZeDmpQI5Gr/zV7ccsjbhCJqkxHc9N0M+oRsEkn5S6qeEJZSM64B1LFY248bPZqRNyYpU+CWNtSyGZqb8nMhoZM44C2xlRHJpFbyr+53VSDC/8TKgkRa7YfFGYSoIxmf5N+kJzhnJsCWVa2FsJG1JNGdp0SjYEb/HlZdI6q3q1qufd1Cr1yzyPIhzBMZyCB+dQh2toQBMYDOAZXuHNkc6L8+58zFsLTj5zCH/gfP4ABquOGw==</latexit> x t <latexit sha1_base64=\"uzCfvi+otYu2ihjbD7PaMp0JG7Y=\">AAAB8XicbVBNS8NAEJ3Ur1q/qh69LBbBU0mkoMeiF48VrFXaWDbbTbt0swm7EyGE/govHhTEq//Gm//GbZuDtj4YeLw3w8y8IJHCoOt+O6WV1bX1jfJmZWt7Z3evun9wZ+JUM95msYz1fUANl0LxNgqU/D7RnEaB5J1gfDX1O09cGxGrW8wS7kd0qEQoGEUrPfRGFPNs8oj9as2tuzOQZeIVpAYFWv3qV28QszTiCpmkxnQ9N0E/pxoFk3xS6aWGJ5SN6ZB3LVU04sbPZwdPyIlVBiSMtS2FZKb+nshpZEwWBbYzojgyi95U/M/rphhe+LlQSYpcsfmiMJUEYzL9ngyE5gxlZgllWthbCRtRTRnajCo2BG/x5WXSOat7jbrn3TRqzcsijzIcwTGcggfn0IRraEEbGETwDK/w5mjnxXl3PuatJaeYOYQ/cD5/ANn4kOk=</latexit> ˆy t <latexit sha1_base64=\"m/ZzdjACtPk7VVv8qMUMxHkn5f0=\">AAAB63icbVBNS8NAEJ3Ur1q/qh69LBbBU0mkoMeiF48VrS20sWy2m3bpZhN2J2IJ/QlePCiIV/+QN/+N2zYHbX0w8Hhvhpl5QSKFQdf9dgorq2vrG8XN0tb2zu5eef/g3sSpZrzJYhnrdkANl0LxJgqUvJ1oTqNA8lYwupr6rUeujYjVHY4T7kd0oEQoGEUr3T49YK9ccavuDGSZeDmpQI5Gr/zV7ccsjbhCJqkxHc9N0M+oRsEkn5S6qeEJZSM64B1LFY248bPZqRNyYpU+CWNtSyGZqb8nMhoZM44C2xlRHJpFbyr+53VSDC/8TKgkRa7YfFGYSoIxmf5N+kJzhnJsCWVa2FsJG1JNGdp0SjYEb/HlZdI6q3q1qufd1Cr1yzyPIhzBMZyCB+dQh2toQBMYDOAZXuHNkc6L8+58zFsLTj5zCH/gfP4ABquOGw==</latexit> x t <latexit sha1_base64=\"uzCfvi+otYu2ihjbD7PaMp0JG7Y=\">AAAB8XicbVBNS8NAEJ3Ur1q/qh69LBbBU0mkoMeiF48VrFXaWDbbTbt0swm7EyGE/govHhTEq//Gm//GbZuDtj4YeLw3w8y8IJHCoOt+O6WV1bX1jfJmZWt7Z3evun9wZ+JUM95msYz1fUANl0LxNgqU/D7RnEaB5J1gfDX1O09cGxGrW8wS7kd0qEQoGEUrPfRGFPNs8oj9as2tuzOQZeIVpAYFWv3qV28QszTiCpmkxnQ9N0E/pxoFk3xS6aWGJ5SN6ZB3LVU04sbPZwdPyIlVBiSMtS2FZKb+nshpZEwWBbYzojgyi95U/M/rphhe+LlQSYpcsfmiMJUEYzL9ngyE5gxlZgllWthbCRtRTRnajCo2BG/x5WXSOat7jbrn3TRqzcsijzIcwTGcggfn0IRraEEbGETwDK/w5mjnxXl3PuatJaeYOYQ/cD5/ANn4kOk=</latexit> ˆy t     = f (     ; θ)  Loss (   ,      ) θ (a) training <latexit sha1_base64=\"YDIW6Mi/jc4gnv843zXedRTilJU=\">AAAB63icbVBNS8NAEJ3Ur1q/qh69LBbBU0mkoMeiF48VrS20sWy2m3bpZhN2J2IJ/QlePCiIV/+QN/+N2zYHbX0w8Hhvhpl5QSKFQdf9dgorq2vrG8XN0tb2zu5eef/g3sSpZrzJYhnrdkANl0LxJgqUvJ1oTqNA8lYwupr6rUeujYjVHY4T7kd0oEQoGEUr3T49mF654lbdGcgy8XJSgRyNXvmr249ZGnGFTFJjOp6boJ9RjYJJPil1U8MTykZ0wDuWKhpx42ezUyfkxCp9EsbalkIyU39PZDQyZhwFtjOiODSL3lT8z+ukGF74mVBJilyx+aIwlQRjMv2b9IXmDOXYEsq0sLcSNqSaMrTplGwI3uLLy6R1VvVqVc+7qVXql3keRTiCYzgFD86hDtfQgCYwGMAzvMKbI50X5935mLcWnHzmEP7A+fwBBSaOGg==</latexit> x s <latexit sha1_base64=\"1psCF/1OyjuZ4TwBu21voNG0XaI=\">AAAB63icbVBNS8NAEJ3Ur1q/qh69LBbBU0lE0GPRi8eK1hbaWDbbSbt0swm7GyGE/gQvHhTEq3/Im//GbZuDtj4YeLw3w8y8IBFcG9f9dkorq2vrG+XNytb2zu5edf/gQcepYthisYhVJ6AaBZfYMtwI7CQKaRQIbAfj66nffkKleSzvTZagH9Gh5CFn1FjpLnvU/WrNrbszkGXiFaQGBZr96ldvELM0QmmYoFp3PTcxfk6V4UzgpNJLNSaUjekQu5ZKGqH289mpE3JilQEJY2VLGjJTf0/kNNI6iwLbGVEz0oveVPzP66YmvPRzLpPUoGTzRWEqiInJ9G8y4AqZEZkllClubyVsRBVlxqZTsSF4iy8vk/ZZ3Tuve97tea1xVeRRhiM4hlPw4AIacANNaAGDITzDK7w5wnlx3p2PeWvJKWYO4Q+czx8GrY4b</latexit> y s <latexit sha1_base64=\"XlB1POiMMxMFBsxKqM8k7anLbzY=\">AAAB8XicbVBNS8NAEJ3Ur1q/qh69LBbBU0mkoMeiF48VrFXaWDbbTbt0swm7EyGE/govHhTEq//Gm//GbZuDtj4YeLw3w8y8IJHCoOt+O6WV1bX1jfJmZWt7Z3evun9wZ+JUM95msYz1fUANl0LxNgqU/D7RnEaB5J1gfDX1O09cGxGrW8wS7kd0qEQoGEUrPfRGFPNs8mj61Zpbd2cgy8QrSA0KtPrVr94gZmnEFTJJjel6boJ+TjUKJvmk0ksNTygb0yHvWqpoxI2fzw6ekBOrDEgYa1sKyUz9PZHTyJgsCmxnRHFkFr2p+J/XTTG88HOhkhS5YvNFYSoJxmT6PRkIzRnKzBLKtLC3EjaimjK0GVVsCN7iy8ukc1b3GnXPu2nUmpdFHmU4gmM4BQ/OoQnX0II2MIjgGV7hzdHOi/PufMxbS04xcwh/4Hz+ANhzkOg=</latexit> ˆy s <latexit sha1_base64=\"XlB1POiMMxMFBsxKqM8k7anLbzY=\">AAAB8XicbVBNS8NAEJ3Ur1q/qh69LBbBU0mkoMeiF48VrFXaWDbbTbt0swm7EyGE/govHhTEq//Gm//GbZuDtj4YeLw3w8y8IJHCoOt+O6WV1bX1jfJmZWt7Z3evun9wZ+JUM95msYz1fUANl0LxNgqU/D7RnEaB5J1gfDX1O09cGxGrW8wS7kd0qEQoGEUrPfRGFPNs8mj61Zpbd2cgy8QrSA0KtPrVr94gZmnEFTJJjel6boJ+TjUKJvmk0ksNTygb0yHvWqpoxI2fzw6ekBOrDEgYa1sKyUz9PZHTyJgsCmxnRHFkFr2p+J/XTTG88HOhkhS5YvNFYSoJxmT6PRkIzRnKzBLKtLC3EjaimjK0GVVsCN7iy8ukc1b3GnXPu2nUmpdFHmU4gmM4BQ/OoQnX0II2MIjgGV7hzdHOi/PufMxbS04xcwh/4Hz+ANhzkOg=</latexit> ˆy s <latexit sha1_base64=\"1psCF/1OyjuZ4TwBu21voNG0XaI=\">AAAB63icbVBNS8NAEJ3Ur1q/qh69LBbBU0lE0GPRi8eK1hbaWDbbSbt0swm7GyGE/gQvHhTEq3/Im//GbZuDtj4YeLw3w8y8IBFcG9f9dkorq2vrG+XNytb2zu5edf/gQcepYthisYhVJ6AaBZfYMtwI7CQKaRQIbAfj66nffkKleSzvTZagH9Gh5CFn1FjpLnvU/WrNrbszkGXiFaQGBZr96ldvELM0QmmYoFp3PTcxfk6V4UzgpNJLNSaUjekQu5ZKGqH289mpE3JilQEJY2VLGjJTf0/kNNI6iwLbGVEz0oveVPzP66YmvPRzLpPUoGTzRWEqiInJ9G8y4AqZEZkllClubyVsRBVlxqZTsSF4iy8vk/ZZ3Tuve97tea1xVeRRhiM4hlPw4AIacANNaAGDITzDK7w5wnlx3p2PeWvJKWYO4Q+czx8GrY4b</latexit> y s <latexit sha1_base64=\"XlB1POiMMxMFBsxKqM8k7anLbzY=\">AAAB8XicbVBNS8NAEJ3Ur1q/qh69LBbBU0mkoMeiF48VrFXaWDbbTbt0swm7EyGE/govHhTEq//Gm//GbZuDtj4YeLw3w8y8IJHCoOt+O6WV1bX1jfJmZWt7Z3evun9wZ+JUM95msYz1fUANl0LxNgqU/D7RnEaB5J1gfDX1O09cGxGrW8wS7kd0qEQoGEUrPfRGFPNs8mj61Zpbd2cgy8QrSA0KtPrVr94gZmnEFTJJjel6boJ+TjUKJvmk0ksNTygb0yHvWqpoxI2fzw6ekBOrDEgYa1sKyUz9PZHTyJgsCmxnRHFkFr2p+J/XTTG88HOhkhS5YvNFYSoJxmT6PRkIzRnKzBLKtLC3EjaimjK0GVVsCN7iy8ukc1b3GnXPu2nUmpdFHmU4gmM4BQ/OoQnX0II2MIjgGV7hzdHOi/PufMxbS04xcwh/4Hz+ANhzkOg=</latexit> ˆy s <latexit sha1_base64=\"YDIW6Mi/jc4gnv843zXedRTilJU=\">AAAB63icbVBNS8NAEJ3Ur1q/qh69LBbBU0mkoMeiF48VrS20sWy2m3bpZhN2J2IJ/QlePCiIV/+QN/+N2zYHbX0w8Hhvhpl5QSKFQdf9dgorq2vrG8XN0tb2zu5eef/g3sSpZrzJYhnrdkANl0LxJgqUvJ1oTqNA8lYwupr6rUeujYjVHY4T7kd0oEQoGEUr3T49mF654lbdGcgy8XJSgRyNXvmr249ZGnGFTFJjOp6boJ9RjYJJPil1U8MTykZ0wDuWKhpx42ezUyfkxCp9EsbalkIyU39PZDQyZhwFtjOiODSL3lT8z+ukGF74mVBJilyx+aIwlQRjMv2b9IXmDOXYEsq0sLcSNqSaMrTplGwI3uLLy6R1VvVqVc+7qVXql3keRTiCYzgFD86hDtfQgCYwGMAzvMKbI50X5935mLcWnHzmEP7A+fwBBSaOGg==</latexit> x s (b) fully test-time adaptation θ    = f (    ; θ+Δ)  Entropy (    ) <latexit sha1_base64=\"uzCfvi+otYu2ihjbD7PaMp0JG7Y=\">AAAB8XicbVBNS8NAEJ3Ur1q/qh69LBbBU0mkoMeiF48VrFXaWDbbTbt0swm7EyGE/govHhTEq//Gm//GbZuDtj4YeLw3w8y8IJHCoOt+O6WV1bX1jfJmZWt7Z3evun9wZ+JUM95msYz1fUANl0LxNgqU/D7RnEaB5J1gfDX1O09cGxGrW8wS7kd0qEQoGEUrPfRGFPNs8oj9as2tuzOQZeIVpAYFWv3qV28QszTiCpmkxnQ9N0E/pxoFk3xS6aWGJ5SN6ZB3LVU04sbPZwdPyIlVBiSMtS2FZKb+nshpZEwWBbYzojgyi95U/M/rphhe+LlQSYpcsfmiMJUEYzL9ngyE5gxlZgllWthbCRtRTRnajCo2BG/x5WXSOat7jbrn3TRqzcsijzIcwTGcggfn0IRraEEbGETwDK/w5mjnxXl3PuatJaeYOYQ/cD5/ANn4kOk=</latexit> ˆy t <latexit sha1_base64=\"m/ZzdjACtPk7VVv8qMUMxHkn5f0=\">AAAB63icbVBNS8NAEJ3Ur1q/qh69LBbBU0mkoMeiF48VrS20sWy2m3bpZhN2J2IJ/QlePCiIV/+QN/+N2zYHbX0w8Hhvhpl5QSKFQdf9dgorq2vrG8XN0tb2zu5eef/g3sSpZrzJYhnrdkANl0LxJgqUvJ1oTqNA8lYwupr6rUeujYjVHY4T7kd0oEQoGEUr3T49YK9ccavuDGSZeDmpQI5Gr/zV7ccsjbhCJqkxHc9N0M+oRsEkn5S6qeEJZSM64B1LFY248bPZqRNyYpU+CWNtSyGZqb8nMhoZM44C2xlRHJpFbyr+53VSDC/8TKgkRa7YfFGYSoIxmf5N+kJzhnJsCWVa2FsJG1JNGdp0SjYEb/HlZdI6q3q1qufd1Cr1yzyPIhzBMZyCB+dQh2toQBMYDOAZXuHNkc6L8+58zFsLTj5zCH/gfP4ABquOGw==</latexit> x t <latexit sha1_base64=\"uzCfvi+otYu2ihjbD7PaMp0JG7Y=\">AAAB8XicbVBNS8NAEJ3Ur1q/qh69LBbBU0mkoMeiF48VrFXaWDbbTbt0swm7EyGE/govHhTEq//Gm//GbZuDtj4YeLw3w8y8IJHCoOt+O6WV1bX1jfJmZWt7Z3evun9wZ+JUM95msYz1fUANl0LxNgqU/D7RnEaB5J1gfDX1O09cGxGrW8wS7kd0qEQoGEUrPfRGFPNs8oj9as2tuzOQZeIVpAYFWv3qV28QszTiCpmkxnQ9N0E/pxoFk3xS6aWGJ5SN6ZB3LVU04sbPZwdPyIlVBiSMtS2FZKb+nshpZEwWBbYzojgyi95U/M/rphhe+LlQSYpcsfmiMJUEYzL9ngyE5gxlZgllWthbCRtRTRnajCo2BG/x5WXSOat7jbrn3TRqzcsijzIcwTGcggfn0IRraEEbGETwDK/w5mjnxXl3PuatJaeYOYQ/cD5/ANn4kOk=</latexit> ˆy t <latexit sha1_base64=\"m/ZzdjACtPk7VVv8qMUMxHkn5f0=\">AAAB63icbVBNS8NAEJ3Ur1q/qh69LBbBU0mkoMeiF48VrS20sWy2m3bpZhN2J2IJ/QlePCiIV/+QN/+N2zYHbX0w8Hhvhpl5QSKFQdf9dgorq2vrG8XN0tb2zu5eef/g3sSpZrzJYhnrdkANl0LxJgqUvJ1oTqNA8lYwupr6rUeujYjVHY4T7kd0oEQoGEUr3T49YK9ccavuDGSZeDmpQI5Gr/zV7ccsjbhCJqkxHc9N0M+oRsEkn5S6qeEJZSM64B1LFY248bPZqRNyYpU+CWNtSyGZqb8nMhoZM44C2xlRHJpFbyr+53VSDC/8TKgkRa7YfFGYSoIxmf5N+kJzhnJsCWVa2FsJG1JNGdp0SjYEb/HlZdI6q3q1qufd1Cr1yzyPIhzBMZyCB+dQh2toQBMYDOAZXuHNkc6L8+58zFsLTj5zCH/gfP4ABquOGw==</latexit> x t <latexit sha1_base64=\"uzCfvi+otYu2ihjbD7PaMp0JG7Y=\">AAAB8XicbVBNS8NAEJ3Ur1q/qh69LBbBU0mkoMeiF48VrFXaWDbbTbt0swm7EyGE/govHhTEq//Gm//GbZuDtj4YeLw3w8y8IJHCoOt+O6WV1bX1jfJmZWt7Z3evun9wZ+JUM95msYz1fUANl0LxNgqU/D7RnEaB5J1gfDX1O09cGxGrW8wS7kd0qEQoGEUrPfRGFPNs8oj9as2tuzOQZeIVpAYFWv3qV28QszTiCpmkxnQ9N0E/pxoFk3xS6aWGJ5SN6ZB3LVU04sbPZwdPyIlVBiSMtS2FZKb+nshpZEwWBbYzojgyi95U/M/rphhe+LlQSYpcsfmiMJUEYzL9ngyE5gxlZgllWthbCRtRTRnajCo2BG/x5WXSOat7jbrn3TRqzcsijzIcwTGcggfn0IRraEEbGETwDK/w5mjnxXl3PuatJaeYOYQ/cD5/ANn4kOk=</latexit> ˆy t Figure 3: Method overview. Tent does not alter training (a), but minimizes the entropy of predictions during testing (b) over a constrained modulation ∆, given the parameters θand target data xt. 3 M ETHOD : T EST ENTROPY MINIMIZATION VIA FEATURE MODULATION We optimize the model during testing to minimize the entropy of its predictions by modulating its features. We call our method tent for test entropy. Tent requires a compatible model, an objective to minimize (Section 3.1), and parameters to optimize over (Section 3.2) to fully deﬁne the algorithm (Section Section 3.3). Figure 3 outlines our method for fully test-time adaptation. The model to be adapted must be trained for the supervised task, probabilistic, and differentiable. No supervision is provided during testing, so the model must already be trained. Measuring the entropy of predictions requires a distribution over predictions, so the model must be probabilistic. Gradients are required for fast iterative optimization, so the model must be differentiable. Typical deep networks for supervised learning satisfy these model requirements. 3.1 E NTROPY OBJECTIVE Our test-time objective L(xt) is to minimize the entropy H(ˆy) of model predictions ˆy= fθ(xt). In particular, we measure the Shannon entropy (Shannon, 1948), H(ˆy) = −∑ cp(ˆyc) logp(ˆyc) for the probability ˆyc of class c. Note that optimizing a single prediction has a trivial solution: assign all probability to the most probable class. We prevent this by jointly optimizing batched predictions over parameters that are shared across the batch. Entropy is an unsupervised objective because it only depends on predictions and not annotations. However, as a measure of the predictions it is directly related to the supervised task and model. In contrast, proxy tasks for self-supervised learning are not directly related to the supervised task. Proxy tasks derive a self-supervised label y′from the input xt without the task label y. Examples of these proxies include rotation prediction (Gidaris et al., 2018), context prediction (Doersch et al., 2015), and cross-channel auto-encoding (Zhang et al., 2017). Too much progress on a proxy task could interfere with performance on the supervised task, and self-supervised adaptation methods have to limit or mix updates accordingly (Sun et al., 2019b;a). As such, care is needed to choose a proxy compatible with the domain and task, to design the architecture for the proxy model, and to balance optimization between the task and proxy objectives. Our entropy objective does not need such efforts. 3.2 M ODULATION PARAMETERS The model parameters θare a natural choice for test-time optimization, and these are the choice of prior work for train-time entropy minimization (Grandvalet & Bengio, 2005; Dhillon et al., 2020; Carlucci et al., 2017). However, θis the only representation of the training/source data in our setting, and altering θcould cause the model to diverge from its training. Furthermore, f can be nonlinear and θcan be high dimensional, making optimization too sensitive and inefﬁcient for test-time usage. 3Published as a conference paper at ICLR 2021 IN OUT+ <latexit sha1_base64=\"FGMSn1olAms3UkJ+mUM6lRBkJrw=\">AAAB6HicbVDLSgNBEOyNryS+oh69DAZBEMKuKHoMevGYgHlgsoTZSW8yZvbBzKwYlnyBFw+K5OoP+C/e/BqdJB40saChqOqmu8uLBVfatj+tzNLyyupaNpdf39jc2i7s7NZVlEiGNRaJSDY9qlDwEGuaa4HNWCINPIENb3A18Rv3KBWPwhs9jNENaC/kPmdUG6l63CkU7ZI9BVkkzg8plnPx+Pb94avSKXy0uxFLAgw1E1SplmPH2k2p1JwJHOXbicKYsgHtYcvQkAao3HR66IgcGqVL/EiaCjWZqr8nUhooNQw80xlQ3Vfz3kT8z2sl2r9wUx7GicaQzRb5iSA6IpOvSZdLZFoMDaFMcnMrYX0qKdMmm7wJwZl/eZHUT0rOaemsatK4hBmysA8HcAQOnEMZrqECNWCA8AjP8GLdWU/WqzWetWasn5k9+APr7RuTUJCF</latexit> \u0000 <latexit sha1_base64=\"8eHH7cr25vA7s0zJYYCDPQNSaT0=\">AAAB7XicbVDLSgNBEOyNrxhfUY+KDAbBU9gVQb0FvXhMwDwgWcLsZDYZM7OzzMwKYcnRuxcPinj1F/Id3vwGf8LJ46CJBQ1FVTfdXUHMmTau++VklpZXVtey67mNza3tnfzuXk3LRBFaJZJL1QiwppxFtGqY4bQRK4pFwGk96N+M/foDVZrJ6M4MYuoL3I1YyAg2Vqq1ulgI3M4X3KI7AVok3owUSoejyvfj0ajczn+2OpIkgkaGcKx103Nj46dYGUY4HeZaiaYxJn3cpU1LIyyo9tPJtUN0YpUOCqWyFRk0UX9PpFhoPRCB7RTY9PS8Nxb/85qJCS/9lEVxYmhEpovChCMj0fh11GGKEsMHlmCimL0VkR5WmBgbUM6G4M2/vEhqZ0XvvHhVsWlcwxRZOIBjOAUPLqAEt1CGKhC4hyd4gVdHOs/Om/M+bc04s5l9+APn4wd3ypLI</latexit> ⇥ <latexit sha1_base64=\"r9CoIRh1LwyAxszWUWZZpZEIYvU=\">AAAB7XicbVA9TwJBEJ3DL8Av1NLmIjGxIndGoyXRxhIT+YhwIXvLHqzs7V5254yE8B9sLDDG1tL/Yuev0QUsFHzJJC/vzWRmXpgIbtDzPp3M0vLK6lo2l1/f2NzaLuzs1oxKNWVVqoTSjZAYJrhkVeQoWCPRjMShYPWwfznx6/dMG67kDQ4SFsSkK3nEKUEr1VrIY2bahaJX8qZwF4n/Q4rlXDK+fX/4qrQLH62OomnMJFJBjGn6XoLBkGjkVLBRvpUalhDaJ13WtFQSuyQYTq8duYdW6biR0rYkulP198SQxMYM4tB2xgR7Zt6biP95zRSj82DIZZIik3S2KEqFi8qdvO52uGYUxcASQjW3t7q0RzShaAPK2xD8+ZcXSe245J+UTq9tGhcwQxb24QCOwIczKMMVVKAKFO7gEcbw7CjnyXlxXmetGednZg/+wHn7Btf2kwo=</latexit> \u0000 <latexit sha1_base64=\"icKTvSnYuWAwxCN4MXaVcPxJrUE=\">AAAB7HicbVBNS8NAEN34WetX1aMiwSJ4KokI6q3oxWMLpi20oWy2k3bpZhN2J0IJPXr24kERr/6G/g5v/gb/hNuPg7Y+GHi8N8PMvCARXKPjfFlLyyura+u5jfzm1vbObmFvv6bjVDHwWCxi1QioBsEleMhRQCNRQKNAQD3o3479+gMozWN5j4ME/Ih2JQ85o2gkrxUA0nah6JScCexF4s5IsXw0qn4/Ho8q7cJnqxOzNAKJTFCtm66ToJ9RhZwJGOZbqYaEsj7tQtNQSSPQfjY5dmifGqVjh7EyJdGeqL8nMhppPYgC0xlR7Ol5byz+5zVTDK/8jMskRZBsuihMhY2xPf7c7nAFDMXAEMoUN7farEcVZWjyyZsQ3PmXF0ntvORelK6rJo0bMkWOHJITckZccknK5I5UiEcY4eSJvJBXS1rP1pv1Pm1dsmYzB+QPrI8ftLWSVw==</latexit> \u0000 <latexit sha1_base64=\"6pSYsGji0D9Bm0vY9by0e43+pZo=\">AAAB6HicbVDLSgNBEOyNrxhfUY9eBoPgxbArAfUW9OIxAfOAZAmzk95kzOzsMjMrhJAv8OJBEa9+kjf/xkmyB00saCiquunuChLBtXHdbye3tr6xuZXfLuzs7u0fFA+PmjpOFcMGi0Ws2gHVKLjEhuFGYDtRSKNAYCsY3c381hMqzWP5YMYJ+hEdSB5yRo2V6he9Ysktu3OQVeJlpAQZar3iV7cfszRCaZigWnc8NzH+hCrDmcBpoZtqTCgb0QF2LJU0Qu1P5odOyZlV+iSMlS1pyFz9PTGhkdbjKLCdETVDvezNxP+8TmrCa3/CZZIalGyxKEwFMTGZfU36XCEzYmwJZYrbWwkbUkWZsdkUbAje8surpHlZ9irlm3qlVL3N4sjDCZzCOXhwBVW4hxo0gAHCM7zCm/PovDjvzseiNedkM8fwB87nD3htjL0=</latexit> ÷ <latexit sha1_base64=\"KLNiQjydwC+UjsLtIanox9T+rq8=\">AAAB63icbVBNS8NAEJ3Ur1q/qh69LBbBU0mkoN6KXjxWsB/QhrLZbNqlu5uwuymU0L/gxYMiXv1D3vw3btoctPXBwOO9GWbmBQln2rjut1Pa2Nza3invVvb2Dw6PqscnHR2nitA2iXmsegHWlDNJ24YZTnuJolgEnHaDyX3ud6dUaRbLJzNLqC/wSLKIEWxyaRCy6bBac+vuAmideAWpQYHWsPo1CGOSCioN4Vjrvucmxs+wMoxwOq8MUk0TTCZ4RPuWSiyo9rPFrXN0YZUQRbGyJQ1aqL8nMiy0nonAdgpsxnrVy8X/vH5qohs/YzJJDZVkuShKOTIxyh9HIVOUGD6zBBPF7K2IjLHCxNh4KjYEb/XlddK5qnuN+u1jo9a8K+IowxmcwyV4cA1NeIAWtIHAGJ7hFd4c4bw4787HsrXkFDOn8AfO5w8aWY5N</latexit> µ <latexit sha1_base64=\"lbHwl5bkUbenc+Yo+u8yNzpxsy0=\">AAAB6nicbVDLSgNBEOyNrxhfUY+KDAbBU9gVQb0FvXhM0DwgWcLsZDYZMjO7zMwKYcnRoxcPinj1I/Id3vwGf8LJ46CJBQ1FVTfdXUHMmTau++VklpZXVtey67mNza3tnfzuXk1HiSK0SiIeqUaANeVM0qphhtNGrCgWAaf1oH8z9usPVGkWyXsziKkvcFeykBFsrHTXEkk7X3CL7gRokXgzUigdjirfj0ejcjv/2epEJBFUGsKx1k3PjY2fYmUY4XSYayWaxpj0cZc2LZVYUO2nk1OH6MQqHRRGypY0aKL+nkix0HogAtspsOnpeW8s/uc1ExNe+imTcWKoJNNFYcKRidD4b9RhihLDB5Zgopi9FZEeVpgYm07OhuDNv7xIamdF77x4VbFpXMMUWTiAYzgFDy6gBLdQhioQ6MITvMCrw51n5815n7ZmnNnMPvyB8/EDTj2RiQ==</latexit> \u0000 <latexit sha1_base64=\"xnrzB72KzfqBMQ17s1zlsxQWR+k=\">AAAB7XicbZDLSgMxFIbP1Fsdb1WXboJFcFVmRFAXYtGNywr2Au1QMmmmjU0yQ5IRytB3cONCETcufBT3bsS3Mb0stPWHwMf/n0POOWHCmTae9+3kFhaXllfyq+7a+sbmVmF7p6bjVBFaJTGPVSPEmnImadUww2kjURSLkNN62L8a5fV7qjSL5a0ZJDQQuCtZxAg21qq1NOsK3C4UvZI3FpoHfwrFiw/3PHn7civtwmerE5NUUGkIx1o3fS8xQYaVYYTTodtKNU0w6eMubVqUWFAdZONph+jAOh0Uxco+adDY/d2RYaH1QIS2UmDT07PZyPwva6YmOg0yJpPUUEkmH0UpRyZGo9VRhylKDB9YwEQxOysiPawwMfZArj2CP7vyPNSOSv5x6ezGK5YvYaI87ME+HIIPJ1CGa6hAFQjcwQM8wbMTO4/Oi/M6Kc05055d+CPn/Qf/xpJs</latexit> <latexit sha1_base64=\"9MzbukliF0G5U4WyINCTJmMNjA8=\">AAACNnicdVBNS8NAFNz4bf2KevSyWAQFLUlR9CiK4EWoYFuhiWWz3dSlu0nYfVFL6K/y4u/w1osHRbz6E9y0PWjVgYVhZh773gSJ4Bocp29NTE5Nz8zOzRcWFpeWV+zVtZqOU0VZlcYiVtcB0UzwiFWBg2DXiWJEBoLVg85p7tfvmNI8jq6gmzBfknbEQ04JGKlpX3gyxZ5gIRCl4nvsSQK3QZCd9RoPTfB3sad5W5Kb8j+h7Xx+D5vszk3Zb9pFp+QMgH8Td0SKaIRK0372WjFNJYuACqJ1w3US8DOigFPBegUv1SwhtEParGFoRCTTfjY4u4e3jNLCYazMiwAP1O8TGZFad2VgkvnCetzLxb+8RgrhkZ/xKEmBRXT4UZgKDDHOO8QtrhgF0TWEUMXNrpjeEkUomKYLpgR3/OTfpFYuuQcl53K/eHwyqmMObaBNtI1cdIiO0TmqoCqi6BH10St6s56sF+vd+hhGJ6zRzDr6AevzC4nRq7w=</latexit> µ  E [ x t ] , \u0000 2  E [( µ \u0000 x t ) 2 ] <latexit sha1_base64=\"5uCFLjsyhVlotMr43Rw1BdZFk0s=\">AAACYXicbZFLS+RAFIUrGZ/tK+Ms3RQ2gqC0iSgzy2bcuHTAVqHTNDfVN21hVRKqbmamCf0nZzcbN/4RKzH4vlBw+O659TiVFEpaCsP/nv9lYXFpeWW1s7a+sbkVfN2+snlpBA5ErnJzk4BFJTMckCSFN4VB0InC6+TurO5f/0ZjZZ5d0qzAkYZpJlMpgBwaB3/jKWgNPFaYEhiT/+EtOeBxAYYkqFgD3UqqzudHL6wxHfI4QXo73YBPh59RbRkH3bAXNsU/iqgVXdbWxTj4F09yUWrMSCiwdhiFBY2qekuhcN6JS4sFiDuY4tDJDDTaUdUkNOd7jkx4mhu3MuINfT1RgbZ2phPnrO9r3/dq+FlvWFL6Y1TJrCgJM/F0UFoqTjmv4+YTaVCQmjkBwkh3Vy5uwYAg9ykdF0L0/skfxdVxLzrthb9Ouv2fbRwrbIftsn0Wse+sz87ZBRswwe69BW/D2/Qe/FU/8LefrL7Xznxjb8rfeQSpH7dZ</latexit> \u0000  \u0000 + @ H / @\u0000 , \u0000  \u0000 + @ H / @\u0000 normalization transformation Figure 4: Tent modulates features during testing by estimating normalization statistics µ,σ and optimizing transformation parameters γ,β. Normalization and transformation apply channel-wise scales and shifts to the features. The statistics and parameters are updated on target data without use of source data. In practice, adapting γ,β is efﬁcient because they make up <1% of model parameters. For stability and efﬁciency, we instead only update feature modulations that are linear (scales and shifts), and low-dimensional (channel-wise). Figure 4 shows the two steps of our modulations: normalization by statistics and transformation by parameters. Normalization centers and standardizes the input xinto ¯x= (x−µ)/σby its mean µand standard deviation σ. Transformation turns ¯xinto the output x′= γ¯x+ βby afﬁne parameters for scale γand shift β. Note that the statistics µ,σ are estimated from the data while the parameters γ,β are optimized by the loss. For implementation, we simply repurpose the normalization layers of the source model. We update their normalization statistics and afﬁne parameters for all layers and channels during testing. 3.3 A LGORITHM Initialization The optimizer collects the afﬁne transformation parameters {γl,k,βl,k}for each normalization layer land channel kin the source model. The remaining parameters θ\\{γl,k,βl,k} are ﬁxed. The normalization statistics {µl,k,σl,k}from the source data are discarded. Iteration Each step updates the normalization statistics and transformation parameters on a batch of data. The normalization statistics are estimated for each layer in turn, during the forward pass. The transformation parameters γ,β are updated by the gradient of the prediction entropy ∇H(ˆy), during the backward pass. Note that the transformation update follows the prediction for the current batch, and so it only affects the next batch (unless forward is repeated). This needs just one gradient per point of additional computation, so we use this scheme by default for efﬁciency. Termination For online adaptation, no termination is necessary, and iteration continues as long as there is test data. For ofﬂine adaptation, the model is ﬁrst updated and then inference is repeated. Adaptation may of course continue by updating for multiple epochs. 4 E XPERIMENTS We evaluate tent for corruption robustness on CIFAR-10/CIFAR-100 and ImageNet, and for domain adaptation on digit adaptation from SVHN to MNIST/MNIST-M/USPS. Our implementation is in PyTorch (Paszke et al., 2019) with the pycls library (Radosavovic et al., 2019). Datasets We run on image classiﬁcation datasets for corruption and domain adaptation conditions. For large-scale experiments we choose ImageNet (Russakovsky et al., 2015), with 1,000 classes, a training set of 1.2 million, and a validation set of 50,000. For experiments at an accessible scale we choose CIFAR-10/CIFAR-100 (Krizhevsky, 2009), with 10/100 classes, a training set of 50,000, and a test set of 10,000. For domain adaptation we choose SVHN (Netzer et al., 2011) as source and MNIST (LeCun et al., 1998)/MNIST-M (Ganin & Lempitsky, 2015)/USPS (Hull, 1994) as targets, with ten classes for the digits 0–9. SVHN has color images of house numbers from street views with a training set of 73,257 and test set of 26,032. MNIST/MNIST-M/USPS have handwritten digits with a training sets of 60,000/60,000/7,291 and test sets of 10,000/10,000/2,007. Models For corruption we use residual networks (He et al., 2016) with 26 layers (R-26) on CIFAR- 10/100 and 50 layers (R-50) on ImageNet. For domain adaptation we use the R-26 architecture. For fair comparison, all methods in each experimental condition share the same architecture. Our networks are equipped with batch normalization (Ioffe & Szegedy, 2015). For the source model without adaptation, the normalization statistics are estimated during training on the source data. For all test-time adaptation methods, we estimate these statistics during testing on the target data, as done in concurrent work on adaptation by normalization (Schneider et al., 2020; Nado et al., 2020). 4Published as a conference paper at ICLR 2021 Table 2: Corruption benchmark on CIFAR-10-C and CIFAR-100-C for the highest severity. Tent has least error, with less optimization than domain adaptation (RG, UDA-SS) and test-time training (TTT), and improves on test-time norm (BN). Method Source Target Error (%) C10-C C100-C Source train 40.8 67.2 RG train train 18.3 38.9 UDA-SS train train 16.7 47.0 TTT train test 17.5 45.0 BN test 17.3 42.6 PL test 15.7 41.2 Tent (ours) test 14.3 37.3 originalgaussshot impulsedefocus glassmotionzoomsnowfrostfog bright contrastelasticpixeljpeg 0 25 50 75Error (%) source 59.5% norm 49.9% tent 44.0% ANT 50.2% Figure 5: Corruption benchmark on ImageNet-C: error for each type averaged over severity levels. Tent improves on the prior state-of-the-art, adver- sarial noise training (Rusak et al., 2020), by fully test-time adaptation without altering training. Optimization We optimize the modulation parameters γ,β following the training hyperparameters for the source model with few changes. On ImageNet we optimize by SGD with momentum; on other datasets we optimize by Adam (Kingma & Ba, 2015). We lower the batch size (BS) to reduce memory usage for inference, then lower the learning rate (LR) by the same factor to compensate (Goyal et al., 2017). On ImageNet, we set BS = 64 and LR = 0.00025, and on other datasets we set BS = 128 and LR = 0.001.We control for ordering by shufﬂing and sharing the order across methods. Baselines We compare to domain adaptation, self-supervision, normalization, and pseudo-labeling: • source applies the trained classiﬁer to the test data without adaptation, • adversarial domain adaptation (RG) reverses the gradients of a domain classiﬁer on source and target to optimize for a domain-invariant representation (Ganin & Lempitsky, 2015), • self-supervised domain adaptation (UDA-SS) jointly trains self-supervised rotation and position tasks on source and target to optimize for a shared representation (Sun et al., 2019a), • test-time training (TTT) jointly trains for supervised and self-supervised tasks on source, then keeps training the self-supervised task on target during testing (Sun et al., 2019b), • test-time normalization (BN) updates batch normalization statistics (Ioffe & Szegedy, 2015) on the target data during testing (Schneider et al., 2020; Nado et al., 2020), • pseudo-labeling (PL) tunes a conﬁdence threshold, assigns predictions over the threshold as labels, and then optimizes the model to these pseudo-labels before testing (Lee, 2013). Only test-time normalization (BN), pseudo-labeling (PL), and tent (ours) are fully test-time adaptation methods. See Section 2 for an explanation and contrast with domain adaptation and test-time training. 4.1 R OBUSTNESS TO CORRUPTIONS To benchmark robustness to corruption, we make use of common image corruptions (see Appendix A for examples). The CIFAR-10/100 and ImageNet datasets are turned into the CIFAR-10/100-C and ImageNet-C corruption benchmarks by duplicating their test/validation sets and applying 15 types of corruptions at ﬁve severity levels (Hendrycks & Dietterich, 2019). Tent improves more with less data and computation.Table 2 reports errors averaged over corrup- tion types at the severest level of corruption. On CIFAR-10/100-C we compare all methods, including those that require joint training across domains or losses, given the convenient sizes of these datasets. Adaptation is ofﬂine for fair comparison with ofﬂine baselines. Tent improves on the fully test-time adaptation baselines (BN, PL) but also the domain adaptation (RG, UDA-SS) and test-time training (TTT) methods that need several epochs of optimization on source and target. Tent consistently improves across corruption types.Figure 5 plots the error for each corruption type averaged over corruption levels on ImageNet-C. We compare the most efﬁcient methods—source, normalization, and tent—given the large scale of the source data (>1 million images) needed by other methods and the 75 target combinations of corruption types and levels. Tent and BN adapt online to rival the efﬁciency of inference without adaptation. Tent reaches the least error for most corruption types without increasing the error on the original data. 5Published as a conference paper at ICLR 2021 Table 3: Digit domain adaptation from SVHN to MNIST/MNIST-M/USPS. Source-free adaptation is not only feasible, but more efﬁcient. Tent always improves on normalization (BN), and in 2/3 cases achieves less error than domain adaptation (RG, UDA-SS) without joint training on source & target. Method Source Target Epochs Error (%) Source + Target MNIST MNIST-M USPS Source train - 18.2 39.7 19.3 RG train train 10 + 10 15.0 33.4 18.9 UDA-SS train train 10 + 10 11.1 22.2 18.4 BN test 0 + 1 15.7 39.7 18.0 Tent (ours) test 0 + 1 10.0 37.0 16.3 Tent (ours) test 0 + 10 8.2 36.8 14.4 Tent reaches a new state-of-the-art without altering training.The state-of-the-art methods for robustness extend training with adversarial noise (ANT) (Rusak et al., 2020) for 50.2% error or mixtures of data augmentations (AugMix) (Hendrycks et al., 2020) for 51.7% error. Combined with stylization from external images (SIN) (Geirhos et al., 2019), ANT+SIN reaches 47.4%. Tent reaches a new state-of-the-art of 44.0% by online adaptation and 42.3% by ofﬂine adaptation. It improves on ANT for all types except noise, on which ANT is trained. This requires just one gradient per test point, without more optimization on the training set (ANT, AugMix) or use of external images (SIN). Among fully test-time adaptation methods, tent reduces the error beyond test-time normalization for 18% relative improvement. In concurrent work, Schneider et al. (2020) report 49.3% error for test-time normalization, for which tent still gives 14% relative improvement. 4.2 S OURCE -FREE DOMAIN ADAPTATION We benchmark digit adaptation (Ganin & Lempitsky, 2015; Tzeng et al., 2015; 2017; Shu et al., 2018) for shifts from SVHN to MNIST/MNIST-M/USPS. Recall that unsupervised domain adaptation makes use the labeled source data and unlabeled target data, while our fully test-time adaptation setting denies use of source data. Adaptation is ofﬂine for fair comparison with ofﬂine baselines. Tent adapts to target without source.Table 3 reports the target errors for domain adaptation and fully test-time adaptation methods. Test-time normalization (BN) marginally improves, while adversarial domain adaptation (RG) and self-supervised domain adaptation (UDA-SS) improve more by joint training on source and target. Tent always has lower error than the source model and BN, and it achieves the lowest error in 2/3 cases, even in just one epoch and without use of source data. While encouraging for fully test-time adaptation, unsupervised domain adaptation remains necessary for the highest accuracy and harder shifts. For SVHN-to-MNIST, DIRT-T (Shu et al., 2018) achieves a remarkable 0.6% error 2. For MNIST-to-SVHN, a difﬁcult shift with source-only error of 71.3%, DIRT-T reaches45.5% and UDA-SS reaches 38.7%. Tent fails on this shift and increases error to 79.8%. In this case success presently requires joint optimization over source and target. Tent needs less computation, but still improves with more.Tent adapts efﬁciently on target data alone with just one gradient per point. RG & UDA-SS also use the source data (SVHN train), which is ∼7×the size of the target data (MNIST test), and optimize for 10 epochs. Tent adapts with ∼80× less computation. With more updates, tent reaches 8.2% error in 10 epochs and 6.5% in 100 epochs. With online updates, tent reaches 12.5% error in one epoch and 8.4% error in 10 epochs. Tent scales to semantic segmentation.To show scalability to large models and inputs, we evaluate semantic segmentation (pixel-wise classiﬁcation) on a domain shift from a simulated source to a real target. The source is GTA (Richter et al., 2017), a video game in an urban environment, and the target is Cityscapes (Cordts et al., 2016), an urban autonomous driving dataset. The model is HRNet-W18, a fully convolutional network (Shelhamer et al., 2017) with high-resolution architecture (Wang et al., 2020). The target intersection-over-union scores (higher is better) are source 28.8%, BN 31.4%, and tent 35.8% with ofﬂine optimization by Adam. For adaptation to a single image, tent reaches 36.4% in 10 iterations with episodic optimization. See the appendix for a qualitative example (Appendix B). 2We exclude DIRT-T from our experiments because of incomparable differences in architecture and model selection. DIRT-T tunes with labeled target data, but we do not. Please refer to Shu et al. (2018) for more detail. 6Published as a conference paper at ICLR 2021 Figure 6: Tent reduces the entropy and loss. We plot changes in entropy∆Hand loss ∆Lfor all of CIFAR-100-C. Change in entropy rank-correlates with change in loss: note the dark diagonal and the rank correlation coefﬁcient of 0.22. (a) Source (b) BN  (c) Tent (d) Oracle  Figure 7: Adapted features on CIFAR-100-C with Gaussian noise (front) and reference features without corruption (back). Corruption shifts fea- tures away from the reference, but BN reduces the shifts. Tent instead shifts features more, and closer to an oracle that optimizes on target labels. Tent scales to the VisDA-C challenge.To show adaptation on a more difﬁcult benchmark, we evaluate on the VisDA-C challenge (Peng et al., 2017). The task is object recognition for 12 classes where the source data is synthesized by rendering 3D models and the target data is collected from real scenes. The validation error for our source model (ResNet-50, pretrained on ImageNet) is 56.1%, while tent reaches 45.6%, and improves to 39.6% by updating all layers except for the ﬁnal classiﬁer as done by Liang et al. (2020). Although ofﬂine source-free adaptation by model adaptation (Li et al., 2020) or SHOT (Liang et al., 2020) can reach lower error with more computation and tuning, tent can adapt online during testing. 4.3 A NALYSIS Tent reduces entropy and error.Figure 6 veriﬁes tent does indeed reduce the entropy and the task loss (softmax cross-entropy). We plot changes in entropy and loss on CIFAR-100-C for all 75 corruption type/level combinations. Both axes are normalized by the maximum entropy of a prediction (log 100) and clipped to ±1. Most points have lower entropy and error after adaptation. Tent needs feature modulation.We ablate the normalization and transformation steps of feature modulation. Not updating normalization increases errors, and can fail to improve over BN and PL. Not updating transformation parameters reduces the method to test-time normalization. Updating only the last layer of the model can improve but then degrades with further optimization. Updating the full model parameters θnever improves over the unadapted source model. Tent generalizes across target data.Adaptation could be limited to the points used for updates. We check that adaptation generalizes across points by adapting on target train and not target test. Test errors drop: CIFAR-100-C error goes from 37.3% to 34.2% and SVHN-to-MNIST error goes from 8.2% to 6.5%. (Train is larger than test; when subsampling to the same size errors differ by <0.1%.) Therefore the adapted modulation is not point speciﬁc but general. Tent modulation differs from normalization.Modulation normalizes and transforms features. We examine the combined effect. Figure 7 contrasts adapted features on corrupted data against reference features on uncorrupted data. We plot features from the source model, normalization, tent, and an oracle that optimizes on the target labels. Normalization makes features more like the reference, but tent does not. Instead, tent makes features more like the oracle. This suggests a different and task-speciﬁc effect. See the appendix for visualizations of more layers (Appendix C). 7Published as a conference paper at ICLR 2021 Tent adapts alternative architectures.Tent is architecture agnostic in principle. To gauge its generality in practice, we evaluate new architectures based on self-attention (SAN) (Zhao et al., 2020) and equilibrium solving (MDEQ) (Bai et al., 2020) for corruption robustness on CIFAR-100-C. Table 4 shows that tent reduces error with the same settings as convolutional residual networks. Table 4: Tent adapts alternative architectures on CIFAR-100-C without tuning. Results are error (%). SAN-10 (pair) SAN-10 (patch) MDEQ (large) Source BN Tent Source BN Tent Source BN Tent 55.3 39.7 36.7 48.0 31.8 29.2 53.3 44.9 41.7 5 R ELATED WORK We relate tent to existing adaptation, entropy minimization, and feature modulation methods. Train-Time AdaptationDomain adaptation jointly optimizes on source and target by cross-domain losses L(xs,xt) to mitigate shift. These losses optimize feature alignment (Gretton et al., 2009; Sun et al., 2017), adversarial invariance (Ganin & Lempitsky, 2015; Tzeng et al., 2017), or shared proxy tasks (Sun et al., 2019a). Transduction (Gammerman et al., 1998; Joachims, 1999; Zhou et al., 2004) jointly optimizes on train and test to better ﬁt speciﬁc test instances. While effective in their settings, neither applies when joint use of source/train and target/test is denied. Tent adapts on target alone. Recent “source-free” methods (Li et al., 2020; Kundu et al., 2020; Liang et al., 2020) also adapt without source data. Li et al. (2020); Kundu et al. (2020) rely on generative modeling and optimize multiple models with multiple losses. Kundu et al. (2020); Liang et al. (2020) also alter training. Tent does not need generative modeling, nor does it alter training, and so it can deployed more generally to adapt online with much more computational efﬁciency. SHOT (Liang et al., 2020) adapts by informa- tion maximization (entropy minimization and diversity regularization), but differs in its other losses and its parameterization. These source-free methods optimize ofﬂine with multiple losses for multiple epochs, which requires more tuning and computation than tent, but may achieve more accuracy with more computation. Tent optimizes online with just one loss and an efﬁcient parameterization of modulation to emphasize fully test-time adaptation during inference. We encourage examination of each of these works on the frontier of adaptation without source data. Chidlovskii et al. (2016) are the ﬁrst to motivate adaptation without source data for legal, commercial, or technical concerns. They adapt predictions by applying denoising auto-encoders while we adapt models by entropy minimization. We share their motivations, but the methods and experiments differ. Test-Time AdaptationTent adapts by test-time optimization and normalization to update the model. Test-time adaptation of predictions, through which harder and uncertain cases are adjusted based on easier and certain cases (Jain & Learned-Miller, 2011), provides inspiration for certainty-based model adaptation schemes like our own. Test-time training (TTT) (Sun et al., 2019b) also optimizes during testing, but differs in its loss and must alter training. TTT relies on a proxy task, such as recognizing rotations of an image, and so its loss depends on the choice of proxy. (Indeed, its authors caution that the proxy must be “both well-deﬁned and non-trivial in the new domain”). TTT alters training to optimize this proxy loss on source before adapting to target. Tent adapts without proxy tasks and without altering training. Normalizing feature statistics is common for domain adaptation (Gretton et al., 2009; Sun et al., 2017). For batch normalization Li et al. (2017); Carlucci et al. (2017) separate source and target statistics during training. Schneider et al. (2020); Nado et al. (2020) estimate target statistics during testing to improve generalization. Tent builds on test-time normalization to further reduce generalization error. Entropy MinimizationEntropy minimization is a key regularizer for domain adaptation (Carlucci et al., 2017; Shu et al., 2018; Saito et al., 2019; Roy et al., 2019), semi-supervised learning (Grandvalet & Bengio, 2005; Lee, 2013; Berthelot et al., 2019), and few-shot learning (Dhillon et al., 2020). Regularizing entropy penalizes decisions at high densities in the data distribution to improve accuracy for distinct classes (Grandvalet & Bengio, 2005). These methods regularize entropy during training in concert with other supervised and unsupervised losses on additional data. Tent is the ﬁrst to minimize 8Published as a conference paper at ICLR 2021 entropy during testing, for adaptation to dataset shifts, without other losses or data. Entropic losses are common; our contribution is to exhibit entropy as the sole lossfor fully test-time adaptation. Feature ModulationModulation makes a model vary with its input. We optimize modulations that are simpler than the full model for stable and efﬁcient adaptation. We modulate channel-wise afﬁne transformations, for their effectiveness in tandem with normalization (Ioffe & Szegedy, 2015; Wu & He, 2018), and for their ﬂexibility in conditioning for different tasks (Perez et al., 2018). These normalization and conditioning methods optimize the modulation during training by a supervised loss, but keep it ﬁxed during testing. We optimize the modulation during testing by an unsupervised loss, so that it can adapt to different target data. 6 D ISCUSSION Tent reduces generalization error on shifted data by test-time entropy minimization. In minimizing entropy, the model adapts itself to feedback from its own predictions. This is truly self-supervised self-improvement. Self-supervision of this sort is totally deﬁned by the supervised task, unlike proxy tasks designed to extract more supervision from the data, and yet it remarkably still reduces error. Nevertheless, errors due to corruption and other shifts remain, and therefore more adaptation is needed. Next steps should pursue test-time adaptation on more and harder types of shift, over more general parameters, and by more effective and efﬁcient losses. Shifts Tent reduces error for a variety of shifts including image corruptions, simple changes in appearance for digits, and simulation-to-real discrepancies. These shifts are popular as standardized benchmarks, but other real-world shifts exist. For instance, the CIFAR 10.1 and ImageNetV2 test sets (Recht et al., 2018; 2019), made by reproducing the dataset collection procedures, entail natural but unknown shifts. Although error is higher on both sets, indicating the presence of shift, tent does not improve generalization. Adversarial shifts (Szegedy et al., 2014) also threaten real-world usage, and attackers keep adapting to defenses. While adversarial training (Madry et al., 2018) makes a difference, test-time adaptation could help counter such test-time attacks. Parameters Tent modulates the model by normalization and transformation, but much of the model stays ﬁxed. Test-time adaptation could update more of the model, but the issue is to identify parameters that are both expressive and reliable, and this may interact with the choice of loss. TTT adapts multiple layers of features shared by supervised and self-supervised models and SHOT adapts all but the last layer(s) of the model. These choices depend on the model architecture, the loss, and tuning. For tent modulation is reliable, but the larger shift on VisDA is better addressed by the SHOT parameterization. Jointly adapting the input could be a more general alternative. If a model can adapt itself on target, then perhaps its input gradients might optimize spatial transformations or image translations to reduce shift without source data. Losses Tent minimizes entropy. For more adaptation, is there an effective loss for general but episodic test-time optimization? Entropy is general across tasks but limited in scope. It needs batches for optimization, and cannot update episodically on one point at a time. TTT can do so, but only with the right proxy task. For less computation, is there an efﬁcient loss for more local optimization? Tent and TTT both require full (re-)computation of the model for updates because they depend on its predictions. If the loss were instead deﬁned on the representation, then updates would require less forward and backward computation. Returning to entropy speciﬁcally, this loss may interact with calibration (Guo et al., 2017), as better uncertainty estimation could drive better adaptation. We hope that the fully test-time adaptation setting can promote new methods for equipping a model to adapt itself, just as tent yields a new model with every update. ACKNOWLEDGMENTS We thank Eric Tzeng for discussions on domain adaptation, Bill Freeman for comments on the experiments, Yu Sun for consultations on test-time training, and Kelsey Allen for feedback on the exposition. We thank the anonymous reviewers of ICLR 2021 for their feedback, which certainly improved the latest adaptation of the paper. 9Published as a conference paper at ICLR 2021 REFERENCES Shaojie Bai, Vladlen Koltun, and J Zico Kolter. Multiscale deep equilibrium models. arXiv preprint arXiv:2006.08656, 2020. David Berthelot, Nicholas Carlini, Ian Goodfellow, Nicolas Papernot, Avital Oliver, and Colin A Raffel. Mixmatch: A holistic approach to semi-supervised learning. In NeurIPS, 2019. Fabio Maria Carlucci, Lorenzo Porzi, Barbara Caputo, Elisa Ricci, and Samuel Rota Bulo. Autodial: Automatic domain alignment layers. In 2017 IEEE International Conference on Computer Vision (ICCV), pp. 5077–5085. IEEE, 2017. Boris Chidlovskii, Stephane Clinchant, and Gabriela Csurka. Domain adaptation in the absence of source domain data. In SIGKDD, pp. 451–460, 2016. Marius Cordts, Mohamed Omran, Sebastian Ramos, Timo Rehfeld, Markus Enzweiler, Rodrigo Benenson, Uwe Franke, Stefan Roth, and Bernt Schiele. The cityscapes dataset for semantic urban scene understanding. In CVPR, 2016. Guneet Singh Dhillon, Pratik Chaudhari, Avinash Ravichandran, and Stefano Soatto. A baseline for few-shot image classiﬁcation. In ICLR, 2020. Carl Doersch, Abhinav Gupta, and Alexei A Efros. Unsupervised visual representation learning by context prediction. In ICCV, 2015. J. Donahue, Y . Jia, O. Vinyals, J. Hoffman, N. Zhang, E. Tzeng, and T. Darrell. Decaf: A deep convolutional activation feature for generic visual recognition. In ICML, 2014. A Gammerman, V V ovk, and V Vapnik. Learning by transduction. InUAI, 1998. Yaroslav Ganin and Victor Lempitsky. Unsupervised domain adaptation by backpropagation. In ICML, 2015. Robert Geirhos, Carlos RM Temme, Jonas Rauber, Heiko H Schütt, Matthias Bethge, and Felix A Wichmann. Generalisation in humans and deep neural networks. In NeurIPS, 2018. Robert Geirhos, Patricia Rubisch, Claudio Michaelis, Matthias Bethge, Felix A. Wichmann, and Wieland Brendel. Imagenet-trained CNNs are biased towards texture; increasing shape bias improves accuracy and robustness. In International Conference on Learning Representations, 2019. Spyros Gidaris, Praveer Singh, and Nikos Komodakis. Unsupervised representation learning by predicting image rotations. In ICLR, 2018. Priya Goyal, Piotr Dollár, Ross Girshick, Pieter Noordhuis, Lukasz Wesolowski, Aapo Kyrola, Andrew Tulloch, Yangqing Jia, and Kaiming He. Accurate, large minibatch sgd: training imagenet in 1 hour. arXiv preprint arXiv:1706.02677, 2017. Yves Grandvalet and Yoshua Bengio. Semi-supervised learning by entropy minimization. In NeurIPS, 2005. A. Gretton, AJ. Smola, J. Huang, M. Schmittfull, KM. Borgwardt, and B. Schölkopf. Covariate shift and local learning by distribution matching. In Dataset Shift in Machine Learning, pp. 131–160. MIT Press, Cambridge, MA, USA, 2009. Chuan Guo, Geoff Pleiss, Yu Sun, and Kilian Q Weinberger. On calibration of modern neural networks. In ICML, 2017. Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image recognition. In CVPR, June 2016. Dan Hendrycks and Thomas Dietterich. Benchmarking neural network robustness to common corruptions and perturbations. In ICLR, 2019. Dan Hendrycks, Norman Mu, Ekin D Cubuk, Barret Zoph, Justin Gilmer, and Balaji Lakshminarayanan. Augmix: A simple data processing method to improve robustness and uncertainty. In ICLR, 2020. Jonathan J. Hull. A database for handwritten text recognition research. TPAMI, 1994. Sergey Ioffe and Christian Szegedy. Batch normalization: Accelerating deep network training by reducing internal covariate shift. In ICML, 2015. 10Published as a conference paper at ICLR 2021 Vidit Jain and Erik Learned-Miller. Online domain adaptation of a pre-trained cascade of classiﬁers. In CVPR, 2011. Thorsten Joachims. Transductive inference for text classiﬁcation using support vector machines. In ICML, 1999. Diederik Kingma and Jimmy Ba. Adam: A method for stochastic optimization. In ICLR, 2015. A. Krizhevsky, I. Sutskever, and G. Hinton. Imagenet classiﬁcation with deep convolutional neural networks. NeurIPS, 25, 2012. Alex Krizhevsky. Learning multiple layers of features from tiny images. Technical report, University of Toronto, 2009. Jogendra Nath Kundu, Naveen Venkat, R Venkatesh Babu, et al. Universal source-free domain adaptation. In CVPR, pp. 4544–4553, 2020. Y . LeCun, L. Bottou, Y . Bengio, and P. Haffner. Gradient-based learning applied to document recognition. Proceedings of the IEEE, 86(11):2278–2324, 1998. Dong-Hyun Lee. Pseudo-label: The simple and efﬁcient semi-supervised learning method for deep neural networks. In ICML Workshop on challenges in representation learning, 2013. Rui Li, Qianfen Jiao, Wenming Cao, Hau-San Wong, and Si Wu. Model adaptation: Unsupervised domain adaptation without source data. In CVPR, June 2020. Yanghao Li, Naiyan Wang, Jianping Shi, Jiaying Liu, and Xiaodi Hou. Revisiting batch normalization for practical domain adaptation. In ICLRW, 2017. Jian Liang, Dapeng Hu, and Jiashi Feng. Do we really need to access the source data? source hypothesis transfer for unsupervised domain adaptation. In ICML, 2020. Aleksander Madry, Aleksandar Makelov, Ludwig Schmidt, Dimitris Tsipras, and Adrian Vladu. Towards deep learning models resistant to adversarial attacks. In International Conference on Learning Representations, 2018. Zachary Nado, Shreyas Padhy, D Sculley, Alexander D’Amour, Balaji Lakshminarayanan, and Jasper Snoek. Evaluating prediction-time batch normalization for robustness under covariate shift. arXiv preprint arXiv:2006.10963, 2020. Yuval Netzer, Tao Wang, Adam Coates, Alessandro Bissacco, Bo Wu, and Andrew Y Ng. Reading digits in natural images with unsupervised feature learning. NeurIPS Workshop on Deep Learning and Unsupervised Feature Learning, 2011. Adam Paszke, Sam Gross, Francisco Massa, Adam Lerer, James Bradbury, Gregory Chanan, Trevor Killeen, Zeming Lin, Natalia Gimelshein, Luca Antiga, et al. Pytorch: An imperative style, high-performance deep learning library. In NeurIPS, 2019. Xingchao Peng, Ben Usman, Neela Kaushik, Judy Hoffman, Dequan Wang, and Kate Saenko. VisDA: The visual domain adaptation challenge. arXiv preprint arXiv:1710.06924, 2017. Ethan Perez, Florian Strub, Harm De Vries, Vincent Dumoulin, and Aaron Courville. Film: Visual reasoning with a general conditioning layer. In AAAI, 2018. Joaquin Quionero-Candela, Masashi Sugiyama, Anton Schwaighofer, and Neil D Lawrence. Dataset shift in machine learning. MIT Press, Cambridge, MA, USA, 2009. Ilija Radosavovic, Justin Johnson, Saining Xie, Wan-Yen Lo, and Piotr Dollár. On network design spaces for visual recognition. In ICCV, 2019. Benjamin Recht, Rebecca Roelofs, Ludwig Schmidt, and Vaishaal Shankar. Do cifar-10 classiﬁers generalize to cifar-10? arXiv preprint arXiv:1806.00451, 2018. Benjamin Recht, Rebecca Roelofs, Ludwig Schmidt, and Vaishaal Shankar. Do ImageNet classiﬁers generalize to ImageNet? In ICML, 2019. Stephan R Richter, Zeeshan Hayder, and Vladlen Koltun. Playing for benchmarks. In ICCV, 2017. Subhankar Roy, Aliaksandr Siarohin, Enver Sangineto, Samuel Rota Bulo, Nicu Sebe, and Elisa Ricci. Unsuper- vised domain adaptation using feature-whitening and consensus loss. In CVPR, 2019. 11Published as a conference paper at ICLR 2021 Evgenia Rusak, Lukas Schott, Roland S Zimmermann, Julian Bitterwolf, Oliver Bringmann, Matthias Bethge, and Wieland Brendel. A simple way to make neural networks robust against diverse image corruptions. In ECCV, 2020. Olga Russakovsky, Jia Deng, Hao Su, Jonathan Krause, Sanjeev Satheesh, Sean Ma, Zhiheng Huang, Andrej Karpathy, Aditya Khosla, Michael Bernstein, et al. ImageNet large scale visual recognition challenge. IJCV, 2015. Kate Saenko, Brian Kulis, Mario Fritz, and Trevor Darrell. Adapting visual category models to new domains. In European conference on computer vision, pp. 213–226. Springer, 2010. Kuniaki Saito, Donghyun Kim, Stan Sclaroff, Trevor Darrell, and Kate Saenko. Semi-supervised domain adaptation via minimax entropy. In ICCV, 2019. Steffen Schneider, Evgenia Rusak, Luisa Eck, Oliver Bringmann, Wieland Brendel, and Matthias Bethge. Improv- ing robustness against common corruptions by covariate shift adaptation. arXiv preprint arXiv:2006.16971, 2020. C.E. Shannon. A mathematical theory of communication. Bell system technical journal, 27, 1948. Evan Shelhamer, Jonathan Long, and Trevor Darrell. Fully convolutional networks for semantic segmentation. PAMI, 2017. Rui Shu, Hung H Bui, Hirokazu Narui, and Stefano Ermon. A dirt-t approach to unsupervised domain adaptation. In ICLR, 2018. Karen Simonyan and Andrew Zisserman. Very deep convolutional networks for large-scale image recognition. In ICLR, 2015. Baochen Sun, Jiashi Feng, and Kate Saenko. Correlation alignment for unsupervised domain adaptation. In Domain Adaptation in Computer Vision Applications, pp. 153–171. Springer, 2017. Yu Sun, Eric Tzeng, Trevor Darrell, and Alexei A Efros. Unsupervised domain adaptation through self- supervision. arXiv preprint arXiv:1909.11825, 2019a. Yu Sun, Xiaolong Wang, Zhuang Liu, John Miller, Alexei A Efros, and Moritz Hardt. Test-time training for out-of-distribution generalization. arXiv preprint arXiv:1909.13231, 2019b. Christian Szegedy, Wojciech Zaremba, Ilya Sutskever, Joan Bruna, Dumitru Erhan, Ian Goodfellow, and Rob Fergus. Intriguing properties of neural networks. 2014. Eric Tzeng, Judy Hoffman, Trevor Darrell, and Kate Saenko. Simultaneous deep transfer across domains and tasks. In ICCV, 2015. Eric Tzeng, Judy Hoffman, Kate Saenko, and Trevor Darrell. Adversarial discriminative domain adaptation. In CVPR, 2017. Jingdong Wang, Ke Sun, Tianheng Cheng, Borui Jiang, Chaorui Deng, Yang Zhao, Dong Liu, Yadong Mu, Mingkui Tan, Xinggang Wang, et al. Deep high-resolution representation learning for visual recognition. PAMI, 2020. Yuxin Wu and Kaiming He. Group normalization. In ECCV, 2018. Jason Yosinski, Jeff Clune, Yoshua Bengio, and Hod Lipson. How transferable are features in deep neural networks? In NeurIPS, 2014. Richard Zhang, Phillip Isola, and Alexei A Efros. Split-brain autoencoders: Unsupervised learning by cross- channel prediction. In CVPR, 2017. Hengshuang Zhao, Jiaya Jia, and Vladlen Koltun. Exploring self-attention for image recognition. In CVPR, 2020. Dengyong Zhou, Olivier Bousquet, Thomas Navin Lal, Jason Weston, and Bernhard Schölkopf. Learning with local and global consistency. NeurIPS, 2004. 12Published as a conference paper at ICLR 2021 APPENDIX This supplement summarizes the image corruptions used in our experiments, highlights a qualitative example of instance-wise adaptation for semantic segmentation, and visualizes feature shifts across more layers. A R OBUSTNESS TO CORRUPTIONS In Section 4.1 we evaluate methods on a common image corruptions benchmark. Table 2 reports errors on the most severe level of corruption, level 5, and Figure 5 reports errors for each corruption type averaged across each of the levels 1–5. We summarize these corruptions types by example in Figure 8. Gaussian Noise  Shot Noise  Impulse Noise  Defocus Blur  Frosted Glass Blur Motion Blur  Zoom Blur  Snow  Frost  Fog Brightness  Contrast  Elastic  Pixelate  JPEG Figure 8: Examples of each corruption type in the image corruptions benchmark. While synthetic, this set of corruptions aims to represent natural factors of variation like noise, blur, weather, and digital imaging effects. This ﬁgure is reproduced from Hendrycks & Dietterich (2019). B S OURCE -FREE ADAPTATION FOR SEMANTIC SEGMENTATION Figure 9 shows a qualitative result on source-free adaptation for semantic segmentation (pixel-wise classiﬁcation) with simulation-to-real (sim-to-real) shift. For this sim-to-real condition, the source data is simulated while the target data is real. Our source data is GTA Richter et al. (2017), a visually-sophisticated video game set in an urban environment, and our target data is Cityscapes Cordts et al. (2016), an urban autonomous driving dataset. The supervised model is HRnet-W18, a fully convolutional network Shelhamer et al. (2017) in the high-resolution network family Wang et al. (2020). For this qualitative example, we run tent on a single image for multiple iterations, because an image is in effect a batch of pixels. This demonstrates adaptation to a target instance, without any further access to the target domain through usage of multiple images from the target distribution. 13Published as a conference paper at ICLR 2021 image label source-only tent, iteration 1 tent, iteration 5 tent, iteration 10 Figure 9: Adaptation for semantic segmentation with simulation-to-real shift from GTA Richter et al. (2017) to Cityscapes Cordts et al. (2016). Tent only uses the target data, and optimizes over a single image as a dataset of pixel-wise predictions. This episodic optimization in effect ﬁts a custom model to each image of the target domain. In only 10 iterations our method suppresses noise (see the completion of the street segment, in purple) and recovers missing classes (see the motorcycle and rider, center). 14Published as a conference paper at ICLR 2021 C F EATURE SHIFTS ACROSS LAYERS AND METHODS (a) Source (b) BN (c) Tent (d) Oracle Layer 2 Layer 5 Layer 8 Layer 11 Layer 14 Layer 18 Layer 20 Layer 23 Layer 26 Figure 10: Adapted features on CIFAR-100-C with Gaussian noise (front) and reference features without corruption (back). Corruption shifts the source features from the reference. BN shifts the features back to be more like the reference. Tent shifts features to be less like the reference, and more like an oracle that optimizes on target labels. 15",
      "meta_data": {
        "arxiv_id": "2006.10726v3",
        "authors": [
          "Dequan Wang",
          "Evan Shelhamer",
          "Shaoteng Liu",
          "Bruno Olshausen",
          "Trevor Darrell"
        ],
        "published_date": "2020-06-18T17:55:28Z",
        "pdf_url": "https://arxiv.org/pdf/2006.10726v3.pdf"
      },
      "llm_extracted_info": {
        "main_contributions": "The paper highlights the novel setting of fully test-time adaptation, where a model adapts to new data during inference using only the target data and its own parameters, without access to source data or supervision. It proposes Tent (Test Entropy Minimization), a scheme to reduce generalization error by optimizing for confidence, measured by minimizing the entropy of model predictions on test data. Tent achieves a new state-of-the-art error of 44.0% on ImageNet-C for robustness to corruptions and demonstrates effective source-free adaptation for digit classification and semantic segmentation, even rivaling methods that use source data and more optimization.",
        "methodology": "Tent adapts deep networks by minimizing the Shannon entropy of model predictions H(ˆy) during testing. This is achieved by modulating features through two main steps: 1) estimating normalization statistics (mean µ and standard deviation σ) and 2) optimizing channel-wise affine transformation parameters (scale γ and shift β). These modulations are applied to the normalization layers of a pre-trained source model. The normalization statistics are estimated on each batch during the forward pass, while the affine parameters γ and β are updated via gradient descent (SGD with momentum or Adam) on the test entropy, without altering the training process or the majority of model parameters. This process can be applied online (continuous iteration) or offline (fixed number of updates before inference).",
        "experimental_setup": "The method was evaluated on: 1) Corruption Robustness: CIFAR-10/100-C and ImageNet-C, using 15 corruption types at five severity levels. 2) Domain Adaptation: Digit recognition from SVHN (source) to MNIST, MNIST-M, and USPS (targets); Semantic segmentation from GTA (simulated source) to Cityscapes (real target); Object recognition on VisDA-C challenge (synthetic source to real target). Models used include Residual Networks (R-26 for CIFAR/digits, R-50 for ImageNet), HRNet-W18 for semantic segmentation, and ResNet-50 for VisDA-C. Baselines included 'source' (no adaptation), adversarial domain adaptation (RG), self-supervised domain adaptation (UDA-SS), test-time training (TTT), test-time normalization (BN), and pseudo-labeling (PL). Optimization hyperparameters (batch size, learning rate) were adapted from source model training. Analysis included examining entropy/loss changes, ablating modulation components, and testing generalization across target data and alternative architectures (SAN, MDEQ).",
        "limitations": "Tent does not improve generalization on all types of dataset shifts, specifically failing to show improvement on natural, unknown shifts like CIFAR 10.1 and ImageNetV2. It can also fail on particularly difficult domain shifts, such as MNIST-to-SVHN, where its error increased, suggesting that joint optimization with source data might be necessary for such challenging cases. The method primarily modulates only normalization and transformation parameters, leaving the majority of the model fixed, which limits its expressiveness for adaptation. The entropy objective requires batches for optimization and cannot update episodically on a single point at a time. Furthermore, the method requires full re-computation of the model for updates, which can be computationally intensive if more local optimization is desired.",
        "future_research_directions": "Future work could focus on extending test-time adaptation to handle more diverse and challenging types of shifts, including adversarial shifts. Research into adapting a broader set of model parameters, beyond just normalization and transformation layers, is suggested, with emphasis on identifying parameters that are both expressive and reliable. Exploring more effective and efficient loss functions for general but episodic test-time optimization is another direction, potentially including representation-defined losses to reduce computational overhead. Investigating how the entropy loss interacts with calibration for improved uncertainty estimation and adaptation is also proposed. Finally, adapting the input itself through spatial transformations or image translations using input gradients could offer a more general approach to reducing shift without source data."
      }
    },
    {
      "title": "What How and When Should Object Detectors Update in Continually Changing Test Domains?",
      "abstract": "It is a well-known fact that the performance of deep learning models\ndeteriorates when they encounter a distribution shift at test time. Test-time\nadaptation (TTA) algorithms have been proposed to adapt the model online while\ninferring test data. However, existing research predominantly focuses on\nclassification tasks through the optimization of batch normalization layers or\nclassification heads, but this approach limits its applicability to various\nmodel architectures like Transformers and makes it challenging to apply to\nother tasks, such as object detection. In this paper, we propose a novel online\nadaption approach for object detection in continually changing test domains,\nconsidering which part of the model to update, how to update it, and when to\nperform the update. By introducing architecture-agnostic and lightweight\nadaptor modules and only updating these while leaving the pre-trained backbone\nunchanged, we can rapidly adapt to new test domains in an efficient way and\nprevent catastrophic forgetting. Furthermore, we present a practical and\nstraightforward class-wise feature aligning method for object detection to\nresolve domain shifts. Additionally, we enhance efficiency by determining when\nthe model is sufficiently adapted or when additional adaptation is needed due\nto changes in the test distribution. Our approach surpasses baselines on widely\nused benchmarks, achieving improvements of up to 4.9\\%p and 7.9\\%p in mAP for\nCOCO $\\rightarrow$ COCO-corrupted and SHIFT, respectively, while maintaining\nabout 20 FPS or higher.",
      "full_text": "What, How, and When Should Object Detectors Update in Continually Changing Test Domains? Jayeon Yoo1 Dongkwan Lee1 Inseop Chung1 Donghyun Kim2∗ Nojun Kwak1∗ 1Seoul National University 2Korea University 1{jayeon.yoo, biancco, jis3613, nojunk}@snu.ac.kr 2d kim@korea.ac.kr Abstract It is a well-known fact that the performance of deep learning models deteriorates when they encounter a dis- tribution shift at test time. Test-time adaptation (TTA) al- gorithms have been proposed to adapt the model online while inferring test data. However, existing research pre- dominantly focuses on classification tasks through the op- timization of batch normalization layers or classification heads, but this approach limits its applicability to various model architectures like Transformers and makes it chal- lenging to apply to other tasks, such as object detection. In this paper, we propose a novel online adaption approach for object detection in continually changing test domains, considering which part of the model to update, how to up- date it, and when to perform the update. By introducing architecture-agnostic and lightweight adaptor modules and only updating these while leaving the pre-trained backbone unchanged, we can rapidly adapt to new test domains in an efficient way and prevent catastrophic forgetting. Fur- thermore, we present a practical and straightforward class- wise feature aligning method for object detection to resolve domain shifts. Additionally, we enhance efficiency by deter- mining when the model is sufficiently adapted or when ad- ditional adaptation is needed due to changes in the test dis- tribution. Our approach surpasses baselines on widely used benchmarks, achieving improvements of up to 4.9%p and 7.9%p in mAP for COCO → COCO-corrupted and SHIFT, respectively, while maintaining about 20 FPS or higher. 1. Introduction Although deep learning models have demonstrated remark- able success in numerous vision-related tasks, they remain susceptible to domain shifts where the test data distribu- tion differs from that of the training data [3, 25, 40]. In real-world applications, domain shifts frequently occur at test-time due to natural variations, corruptions, changes in weather conditions (e.g., fog, rain) , camera sensor differ- Figure 1. We propose an online adaptation method for object detection in continually changing test domains. Object detectors trained with clean images suffer from performance degradation due to various corruption, such as camera sensor degradation or environmental changes (Direct-Test). Updating full parameters for online adaptation require a large number of test samples and vul- nerable to drastic domain changes (Full-Finetuning), while using only our lightweight adaptor is robust and quickly adapts within a few time steps (Ours). We can further improve efficiency by skip- ping unnecessary adaptation steps (Ours-Skip). ences (e.g., pixelate, defocus blur) , and various other fac- tors. Test-Time Adaptation (TTA) [3, 25, 30, 40, 43, 47] has been proposed to solve the domain shifts in test-time by adapting models to a specific target (test) distribution in an online manner. Furthermore, it is essential to take into account continuously changing test distributions, as the test distribution has the potential to undergo changes and devel- opments as time progresses (i.e., Continual Test-time Adap- tation (CTA)). For instance, autonomous driving systems may experience transitions from clear and sunny conditions to rainy or from daytime to nighttime, which causes contin- ually changing domain shifts [39]. While it is an important research topic, continual test-time adaptation for object de- tection has not been well explored. Recently, several TTA methods [6, 29, 36] tailored for 1 arXiv:2312.08875v1  [cs.CV]  12 Dec 2023object detection have been proposed. ActMAD [29] aligns all the output feature maps ( RC×H×W ) after Batch Nor- malization (BN) layers [14] to adapt the test domain to be similar to that of the training domain. However, this ap- proach requires significant memory during adaptation and does not explicitly consider the objects present in the image. TeST [36] and STFAR [6] adapt to a test domain by utiliz- ing weak and strong augmented test samples with a teacher- student network [37], but they significantly increase infer- ence costs since they require additional forward passes and update steps. Also, these methods update all network pa- rameters, making them highly inefficient in online adapta- tion and vulnerable to losing task-specific knowledge when the test domain experiences continual or drastic changes. In this paper, we aim to develop an efficient continual test-time adaptation (CTA) method for object detection. We investigate the following three key aspects to improve ef- ficiency; what to update: while previous TTA methods for object detection [6, 29, 36] use full fine-tuning, updating all parameters at test time, they are inefficient and prone to losing task-specific knowledge in relatively complex object detection tasks. Updating BN layers, as done in many TTA methods for classification [17, 30, 43, 47], is not as effective for object detection, given its smaller batch size compared to classification and the limitation in applying various back- bones, such as Transformer [26, 41].how to update: several previous TTA methods for object detection [6, 36] adapt the model by using teacher-student networks, resulting in a significant decrease in inference speed, which is detri- mental during test time. While another existing method [29] aligns feature distributions for adaptation, it does not con- sider each object individually, focusing only on image fea- tures, making it less effective for object detection. when to update: most TTA or CTA methods update models using all incoming test samples. However, it is inefficient to update continuously the model if it is already sufficiently adapted when the change of the test domain is not significant. To this end, (1) we propose an efficient continual test- time adaptation method for object detectors to adapt to continually changing test domains through the use of lightweight adaptors which require only 0.54%∼0.89% ad- ditional parameters compared to the full model. It exhibits efficiency in parameters, memory usage, and adaptation time, along with robustness to continuous domain shifts without catastrophic forgetting. Additionally, it demon- strates wide applicability to various backbone types com- pared to BN-based TTA methods [17, 22, 30, 43, 47, 48]. (2) To enhance the adaptation effectiveness in the object detec- tion task, we align the feature distribution of the test domain with that of the training domain at both the image-level and object-level using only the mean and variance of features. For estimating the mean of the test domain features, we employ Exponentially Moving Average (EMA) as we can leverage only the current incoming test samples, not the en- tire test domain data. Due to the unavailability of training data access, we utilize only the mean and variance of the features from a few training samples. (3) We also introduce two novel criteria that do not require additional resources to determine when the model needs adaptation to enhance efficiency in a continually changing test domain environ- ment. As illustrated in Fig. 1, our approach Ours, employ- ing adaptors, tends to adapt much faster to domain changes compared to full parameter updates. This enables efficient TTA by using only a few test samples to update the adaptor and skipping the rest of the updates as shown in Ours-Skip. Our main contributions are summarized as follows: • We introduce an architecture-agnostic lightweight adap- tor, constituting only a maximum of 0.89% of the total model parameters, into the backbone of the object de- tector to adapt the model in a continually changing test domain. This approach ensures efficiency in parameters, memory usage, and adaptation speed, demonstrating the robust preservation of task-specific knowledge owing to its inherent structural characteristics. • We propose a straightforward and effective adaptation loss for CTA in object detection tasks. This is achieved by aligning the distribution of training and test domain fea- tures at both the image and object levels, utilizing only the mean and variance of a few training samples and EMA- updated mean features of the test domain. • We also propose two criteria to determine when the model requires adaptation, enabling dynamic skipping or resum- ing adaptation as needed. This enhancement significantly boosts inference speed by up to about 2 times while main- taining adaptation performance. • Our adaptation method proves effective for diverse types of domain shifts, including weather changes and sensor variations, regardless of whether the domain shift is dras- tic or continuous. In particular, our approach consistently improves the mAP by up to 7.9% in COCO →COCO-C and SHIFT-Discrete/Continuous with higher than 20 FPS. 2. Related Work Test-time adaptation. Recently, there has been a surge of interest in research that adapts models online using unla- beled test samples while simultaneously inferring the test sample to address the domain shift problem, where the test data distribution differs from that of the training data. There are two lines for online adaptation to the test do- main, Test-time Training (TTT) and Test-time Adaptation (TTA). TTT [1, 2, 25, 40] involves modifying the model architecture during training to train it with self-supervised loss, allowing adaptation to the test domain in the test time by applying this self-supervised loss to the unlabeled test samples. On the other hand, TTA aims to adapt the trained model directly to the test domain without specifically tai- 2lored model architectures or losses during training time. NORM [35] and DUA [28] address the domain shifts by adjusting the statistics of batch normalization (BN) layers using the current test samples, without updating other pa- rameters, inspired by [21]. Following this, [22, 30, 43, 48] and [17] update the affine parameters of BN layers using unsupervised loss, entropy minimization loss to enhance the confidence of test data predictions, and feature distribution alignments loss, respectively. Several studies [15, 16] up- date the classifier head using the pseudo-prototypes from the test domain. However, these methods limit their appli- cability to architectures without BN layers or to object de- tection tasks that involve multiple objects in a single im- age. Others [29, 38, 47] update full parameters for online adaptation to the test domain in an online manner, but this approach is inefficient and susceptible to the noisy signal from the unsupervised loss. While existing TTA methods are oriented towards classification tasks, we aim to propose an effective and efficient method for online adaptation in the object detection task. Continual test-time adaptation. Recent studies [31, 44] point out that existing TTA methods have primarily focused on adapting to test domains following an i.i.d assumption and may not perform well when the test data distribution deviates from this assumption. [44] introduces a Contin- ual TTA (CTA) method designed for scenarios where the test domain continuously changes over time. This poses challenges in preventing the model from over-adapting to a particular domain shift and preserving the knowledge of the pre-trained model to avoid catastrophic forgetting. In the field of CTA, the self-training strategy adopting an Exponentially Moving Average (EMA) teacher-student structure is attracting interest as an effective algorithm en- abling robust representation to be learned through self- knowledge distillation. In many studies, the EMA teacher- student structure and catastrophic restoration of source model weights have been proposed as a solution to achieve the goal of CTA [4, 44, 45]. Approaches using source re- play [32], and anti-forgetting regularization [30] have also achieved good performances in robust continuous adapta- tion. Furthermore, there is growing attention on methods that mitigate the computational and memory challenges as- sociated with CTA, such as [12], which relies on updates to batch normalization statistics. Test-time adaptive object detection. Research on TTA for Object Detection (TTAOD) is progressively emerging [6, 29, 36, 42]. Most existing TTAOD methods [6, 36, 42] exploit a teacher-student network to adapt to the test do- main, following the self-training approach commonly em- ployed in Unsupervised Domain Adaptation for object de- tection [7, 18, 19, 34]. However, it is inefficient for TTA due to data augmentation requirements and additional for- ward and backward steps, resulting in slower inference speeds and higher memory usage. Another approach, Act- MAD [29], aligns the distributions of output feature maps after all BN layers along the height, width, and channel axes to adapt to the test domain. However, this location-aware feature alignment is limited to datasets with fixed location priors, such as driving datasets, and is less effective for nat- ural images like COCO. Additionally, CTA for Object De- tection (CTAOD)have not been thoroughly explored. There- fore, there is a need for an effective CTAOD method con- sidering memory and time efficiency. 3. Method To enable the efficient and effective Continual Test-time Adaptation of Object Detectors (CTAOD), we introduce an approach that specifies which part of the model should be updated, describes how to update those using unlabeled test data, and determines whether we perform model updates or not to improve efficiency. 3.1. Preliminary Assume that we have an object detector h ◦ gΘ, here h and g are the RoI head and the backbone, respectively with their parameters being Θ. The training dataset is denoted as Dtrain = {(xi, yi)}N i=1, where xi ∼ Ptrain(x) and yi = ( bboxi, ci), containing information on the bounding box (bbox) and class label ci ∈ C. Consider deploying the detector to the test environments where the test data at pe- riod T is denoted as xT j ∼ PT test(x), PT test ̸= Ptrain and PT test deviates from the i.i.d. assumption. In addition, the domain of PT test continually changes according to T (i.e., PT test ̸= PT−1 test ). Our goal is to adapt the detector h ◦ g to PT test using only test data xT j while making predictions. 3.2. What to update: Adaptation via an adaptor Previous methods [6, 29, 36, 42] adapt the model to the test domain by updating all parameters Θ, leading to in- efficiency at test time and a high risk of losing task knowl- edge from the training data. In contrast, we adapt the model by introducing an adaptor with an extremely small set of parameters and updating only this module while freezing Θ. We introduce a shallow adaptor in parallel for each block, inspired by [5, 13], where transformer-based mod- els are fine-tuned for downstream tasks through parameter- efficient adaptors, as shown in Fig. 2. Each adaptor consists of down-projection layers Wdown ∈ Rd×d r , up-projection layers Wup ∈ R d r ×d and ReLUs, where d denotes the in- put channel dimension and r is the channel reduction ratio set to 32 for all adaptors. We use MLP layers for the Trans- former block (Fig. 2a) and 1×1 convolutional layers for the ResNet block (Fig. 2b) to introduce architecture-agnostic adaptors. The up-projection layer is initialized to 0 values so that the adaptor does not modify the output of the block, 3(a) A block of Transformer  (b) A block of ResNet Figure 2. We attach an adaptor, which is a shallow and low-rank MLP or CNN, to every N block in parallel. We update only these adaptors while other parameters are frozen. Our approach can be applied to diverse architectures including CNNs and Transformers. but as the adaptor is gradually updated, it adjusts the output of the block to adapt to the test domain. Even as the adaptor is updated in the test domain, the original backbone param- eter Θ remains frozen and fully preserved. This structural preservation, as evident in Ours in Fig. 1, enables robust and efficient adaptation to domain changes by maintaining relatively complex task knowledge in object detection and updating very few parameters. 3.3. How to update: EMA feature alignment To adapt the object detector to the test domain, we align the feature distribution of the test domain with that of the training data, inspired by [17, 29, 38]. In contrast to these methods that solely align image feature distribution, we ad- ditionally align object-level features in a class-wise manner, considering class frequency, to enhance its effectiveness for object detection. As the training data is not accessible dur- ing test time, we pre-compute the first and second-order statistics, denoted as µtr = E[Ftr] and Σtr = Var[Ftr], where the operators E and Var represent the mean and vari- ance respectively. The features Ftr = {gΘ(xtr)} are com- puted using only 2,000 training samples, a small subset of the training data. Since a sufficient amount of test domain data is not available at once, and only the current incoming test data, whose features are denoted as Ft te, is accessible at time step t, we estimate the mean of test data features using an exponentially moving average (EMA) as follows: µt te = (1 − α) · µt−1 te + α · E[Ft te], s.t. µ0 te = µtr. (1) Considering the typically small batch size in object detec- tion compared to classification, we approximate the vari- ance of the test features as Σte ≃ Σtr to reduce instability. Image-level feature alignment. We estimate the training and test feature distributions as normal distributions and minimize the KL divergence between them as follows: Limg = DKL(N(µtr, Σtr), N(µt te, Σtr)). (2) Region-level class-wise feature alignment. In object de- tection, we deal with multiple objects within a single image, making it challenging to apply the class-wise feature align- ment proposed in [38], a TTA method for classification. To handle region-level features that correspond to an object, we use ground truth bounding boxes for the training data and utilize the class predictions of RoI pooled features, ft te, for unlabeled test data. In object detection, domain shifts often result in lower recall rates, as a significant number of proposals are predicted as background [20]. To mitigate this issue, we filter out features with background scores exceed- ing a specific threshold. Subsequently, we assign them to the foreground class with the highest probability, as follows: Fk,t te = {ft te|argmax c pfg = k, pbg < 0.5}, where hcls(ft te) = [pfg , pbg] = [p0, ..., pC−1, pbg]. (3) We estimate the class-wise feature distribution of the test domain by exploiting Fk,t te and Eq.1. Furthermore, we in- troduce a weighting scheme for aligning features of less frequently appearing classes, taking into account the severe class imbalance where specific instance ( e.g., person) may appear multiple times within a single image, as follows: Nk,t = Nk,t−1 + ||Fk,t te ||, s.t. Nk,0 = 0 wk,t = log \u0012maxi Ni,t Nk,t \u0013 + 0.01 Lobj = X k wk,t · DKL(N(µk tr, Σk tr), N(µk,t te , Σk tr)). (4) Here, the class-wise mean µk and variance Σk of the train- ing and test data are obtained in the same way as the image- level features. We can effectively adapt the object detector by updating the model to align the feature distribution at both the image and object levels as L = Limg + Lobj. 3.4. When to update: Adaptation on demand As shown in Fig. 1, Ours, which only updates the adaptor proposed in Sec. 3.2, efficiently adapts to changes in the test domain, even with a small subset of early test samples. We leverage its rapid adaptation characteristics to reduce com- putational costs by skipping model updates ( i.e., skipping backward passes) when the model has already sufficiently adapted to the current test domain and resuming model up- dates when confronted with a new test domain. Therefore, we introduce two criteria to determine when to update the model or not as follows: (Criterion 1) When the distribution gap exceeds the in- domain distribution gap. Recall that Limg (Eq. 2) mea- sures the distribution gap between the test and train distri- butions. We assume a model is well-adapted to the current test domain when Limg is closer to the in-domain distri- bution gap. We measure the in-domain distribution gap by 4(a) The ratio of Limg to Din KL (b) The ratio of Limg to Lt ema Figure 3. The test domain undergoes a shift every 4,000 time steps, and each metric reaches its peak at the same intervals. sampling two disjoint subsets, xi and xj, of training fea- tures Ftr from Sec. 3.3 as follows: Din KL = DKL(N(µi tr, Σi tr), N(µj tr, Σj tr)), (5) where µi tr, Σi tr are obtained from xi ∼ Ptrain(x) and µj tr, Σj tr from xj ∼ Ptrain(x). In other words, if Limg is noticeably higher than the in-domain distribution gapDin KL, we consider a model encountering a test domain whose dis- tribution differs from Ptrain(x) and needs to be updated. Based on this, we introduce a new index Limg Din KL . Fig. 3a plots the trend of this index during the model adaptation to a con- tinually changing test domain. It shows that the index has a large value in the early stages of a domain change, decreases rapidly, and then maintains a value close to 1. This index exhibits a similar trend regardless of the backbone type and dataset, as included in the appendix. Therefore, we establish the criterion that model updates are necessary when this in- dex exceeds a certain threshold, τ1, as Limg Din KL > τ1. (Criterion 2 ) When the distribution gap suddenly in- creases. Additionally, we can determine when the test dis- tribution changes and model updates are necessary by ob- serving the trend of the distribution gap ( i.e., Limg). The convergence of Limg indicates that a model is well-adapted to the current test domain. To put it differently, Limg will exhibit a sudden increase when the model encounters a new test domain. We introduce an additional index, denoted as Limg Ltema , representing the ratio of the currentLimg to its expo- nentially moving averageLt ema at time t. We calculate it us- ing the following formula:Lt ema = 0.99·Lt−1 ema+0.01·Limg. Fig. 3b illustrates the trend of the ratio of Limg over the timesteps. It tends to reach a value of 1 as the loss stabilizes at a specific level. Nevertheless, when the model encounters shifts in the test distribution, the ratio experiences a sharp increase, indicating the necessity of a model update when it exceeds a specific threshold, τ2, as Limg Ltema > τ2. If at least one of the two criteria is satisfied, we conclude that the model requires adaptation and proceed to update it. 4. Experiments Sec. 4.1 presents the two object detection benchmark datasets with test distributions that change continuously, ei- ther in a drastic or gradual manner, and our implementation detail is in 4.2. Sec. 4.4 compares our method with other TTA baselines described in Secs. 4.3.. We present detailed ablation studies of our method analyzing the effectiveness and efficiency of our method in terms of what, how, and when to update the models for CTAOD in Sec. 4.5. 4.1. Datasets We experiment with the following three scenarios. COCO → COCO-C simulates continuous and drastic real- istic test domain changes over a long sequence. MS-COCO [23] collects 80 classes of common objects in their natural context with 118k training images and 5k validation images. COCO-C is created by employing 15 types of realistic cor- ruptions [27], such as image distortion and various weather conditions, to simulate test domain changes. In the experi- ments, the model is only trained on the COCO train set and sequentially evaluated on each corruption in the COCO-C validation set during test-time for reproducing continually changing test domains. Finally, the model is evaluated on the original COCO validation set to assess how well it pre- serves knowledge of the original domain (denoted as Org.). SHIFT-(Discrete / Continuous) [39] is a synthetic driving image dataset with 6 classes under different conditions us- ing five weather attributes (clear, cloudy, overcast, fog, rain) and three time-of-day attributes ( daytime, dawn, night ). In SHIFT-Discrete, there are image sets for each attribute, and the model is sequentially evaluated on these attributes, cloudy → overcast → foggy → rainy → dawn → night → clear which contains 2.4k, 1.6k, 2.7k, 3.2k, 1.2k, 1.4k, and 2.8k validation images, respectively. This simulates scenar- ios where the domain undergoes drastic changes. InSHIFT- Continuous, the model is evaluated on four sequences, each consisting of 4k frames, continuously transitioning from clear to foggy (or rainy) and back to clear. 4.2. Implementation Detail We experiment with Faster-RCNN [33] models using ResNet50 [10] and Swin-Tiny [26] as a backbone with FPN [24]. For the COCO → COCO-C adaptation, we em- ploy the publicity available models trained on COCO re- leased in [46] and [26] for ResNet5- and Swin-Tiny-based Faster-RCNN, respectively. For SHIFT experiments, mod- els are trained on the training domain using the detectron2 framework following [33] and [26]. For test-time adapta- tion, we always set the learning to 0.001 for the SGD opti- mizer, and α of Eq. 1 to 0.01, while τ1 and τ2 are set to 1.1 5Table 1. Comparison of mAP, the number of backward and forward passes, and FPS between baselines and our model on COCO→ COCO- C. Our model consistently outperforms baselines on the two different backbones. Furthermore, Ours-Skip with ResNet notably reduces backward passes by as much as 90.5%, leading to a significantly improved frames per second (FPS) rate by up to 109.9%. Noise Blur Weather Digital # step Backbone Method Gau Sht Imp Def Gls Mtn Zm Snw Frs Fog Brt Cnt Els Px Jpg Org. Avg. For. Back. FPS Swin-T [26] Direct-Test 9.7 11.4 10.0 13.4 7.5 12.1 5.2 20.7 24.8 36.1 36.0 12.9 19.1 4.9 15.8 43.0 17.7 80K 0 21.5 ActMAD 10.7 12.0 9.4 12.3 5.7 9.5 4.5 15.3 17.5 27.6 28.2 1.1 16.7 2.6 8.7 36.3 13.9 80K 80K 8.3 Mean-Teacher 10.0 12.1 11.2 12.8 8.1 12.1 4.9 19.6 23.7 34.9 34.0 8.0 18.9 6.1 17.6 41.0 17.2 160K 80K 6.9 Ours 13.6 16.6 16.1 14.0 13.6 14.2 8.3 23.7 27.2 37.4 36.4 27.2 27.2 22.2 22.3 42.3 22.6 80K 80K 9.5 Ours-Skip 13.3 15.3 15.1 14.0 12.8 13.9 6.5 22.0 25.4 35.5 34.9 26.5 25.9 23.4 20.2 41.2 21.6 80K 9.7K 17.7 ResNet50 [10] Direct-Test 9.1 11.0 9.8 12.6 4.5 8.8 4.6 19.1 23.1 38.4 38.0 21.4 15.6 5.3 11.9 44.2 17.3 80K 0 25.8 NORM 9.9 11.9 11.0 12.6 5.2 9.1 5.1 19.4 23.5 38.2 37.6 22.4 17.2 5.7 10.3 43.4 17.5 80K 0 25.8 DUA 9.8 11.7 10.8 12.8 5.2 8.9 5.1 19.3 23.7 38.4 37.8 22.3 17.2 5.4 10.1 44.1 17.1 80K 0 25.8 ActMAD 9.1 9.6 7.0 11.0 3.2 6.1 3.3 12.8 14.0 27.7 27.8 3.9 12.9 2.3 7.2 34.3 10.5 80K 80K 9.6 Mean-Teacher 9.6 12.5 12.0 4.0 2.9 4.8 3.1 16.2 23.5 35.1 34.0 21.8 16.6 8.2 12.7 40.3 14.5 160K 80K 8.1 Ours 12.7 17.8 17.5 12.4 11.5 11.3 6.6 22.8 26.9 38.6 38.5 28.0 25.1 21.2 22.2 41.8 22.2 80K 80K 10.1 Ours-Skip 14.4 17.1 16.0 13.9 11.7 12.2 6.3 22.1 25.5 37.7 37.1 25.5 24.1 23.1 21.1 42.8 21.9 80K 7.6K 21.2 and 1.05, respectively. We use the same hyper-parameters across all backbones and datasets. All experiments are con- ducted with a batch size of 4. 4.3. Baselines Direct-Test evaluates the model trained in the training do- main without adaptation to the test domain. ActMAD [29] is a TTA method aligning the distribution of output features across all BN layers. To apply ActMAD to the Swin Trans- former-based model, we align the output features of the LN layers. We implement Mean-Teacher using a teacher- student network framework to reproduce as close as possi- ble to TeST [36], as its implementation is not publicly avail- able. We follow the FixMatch [37] augmentation method and report results after tuning all hyper-parameters in our scenario. NORM [35] and DUA [28], TTA methods ini- tially designed for classification, are directly applicable to detection tasks by either mixing a certain amount of current batch statistics or updating batch statistics via EMA. How- ever, these are only compatible with architectures contain- ing BN layers. Additional details are provided in Appendix. 4.4. Main Results We compare the performance of each method using mAP and efficiency metrics, including the number of forward and backward passes, as well as FPS during test-time adapta- tion. Results of COCO and SHIFT are in Tab. 1 and 2, re- spectively. COCO → COCO-C. Tab. 1 demonstrates the effective adaptation performance of Ours in the challenging COCO benchmark with 80 classes due to object-level class-wise feature alignment. ActMAD also aligns feature distribution for TTA, but is not effective since it only aligns whole fea- ture maps without considering specific classes in the im- age. NORM and DUA, applicable only to ResNet [10], show minimal performance improvement by adaptation as they are not specifically tailored for object detection and only modify batch statistics across the entire feature map. Ad- ditionally, ActMAD and Mean-Teacher, updating full pa- rameters, gradually lose task knowledge in the continually changing test distributions, resulting in much lower perfor- mance on Org. , the domain identical to the training data, than that of Direct-Test. In contrast, Ours effectively pre- vents catastrophic forgetting by freezing the original param- eters of the models and updating only the adaptor, obtain- ing performance on par with Direct-Test on the Org. do- main and consistently high performance across corrupted domains, with an average mAP improvement of 4.9%p compared to that of Direct-Test. Furthermore, leveraging the rapid adaptation ability of the adaptor,Ours-Skip, which skips unnecessary adaptation, allows using only a maxi- mum of about 12% of the total samples for adaptation with- out significant performance loss. This leads to a substantial improvement in inference speed, more than doubling com- pared to other TTA methods, reaching over 17.7 FPS. SHIFT-Discrete. Ours is also effective in SHIFT, which simulates continuous changes in weather and time in driv- ing scenarios according to the left section of Tab. 2. Espe- cially, Ours shows significant improvements in mAP by 7- 9%p, particularly for the foggy and dawn attributes where Direct-Test obtains lower performance due to severe do- main shift. In contrast, with ActMAD, catastrophic forget- ting takes place when adapting to the cloudy and overcast weather. This is due to the updating of the full parame- ters, despite that Direct-Test already shows proficient per- formance in these conditions. As a result, the performance in the later domains is worse than that of the Direct-Test. DUA, which updates batch statistics using EMA, shows a gradual decrease in performance as the domain contin- uously changes, resulting in much lower performance in the original clear domain ( i.e., clear ). On the other hand, NORM, which utilizes the statistics of the current batch samples, exhibits no catastrophic forgetting and relatively good adaptation, as SHIFT is a relatively easier task com- pared to COCO due to having only 6 classes. Compared to NORM, Ours shows better adaptation performance, and is 6Table 2. Comparison of mAP, the number of backward and forward passes, and FPS between baselines and our model on SHIFT-Discrete and SHIFT-Continuous. Baselines perform effectively in a particular setting but lack generalizability across various settings. Our method consistently achieves results that are either better or on par with the best model in all settings, demonstrating its strong stability. Ours-Skip also effectively reduces the number of backward passes without compromising mAP performance, resulting in a higher FPS. SHIFT-Discrete SHIFT-Continuous mAP # step mAP # Avg. step Backbone Method cloudy overc. fog rain dawn night clear Avg. For. Back. FPS clear↔fog clear ↔rain For. Back. FPS Swin-T [26] Direct-Test 50.0 38.9 23.1 45.1 26.9 39.5 45.9 38.5 15.3K 0 27.5 18.1 21.1 4K 0 28.3 ActMAD 49.8 38.4 21.4 43.1 19.0 32.0 44.8 35.5 15.3K 15.3K 9.3 15.6 16.3 4K 4K 9.8 Mean-Teacher 50.0 39.2 25.7 45.4 26.0 37.5 42.2 38.0 15.3K 15.3K 7.8 20.4 24.3 8K 4K 6.5 Ours 50.3 39.2 32.2 46.7 30.4 39.9 44.3 40.4 15.3K 15.3K 11.2 23.9 22.6 4K 4K 11.6 Ours-Skip 50.3 39.7 29.1 47.1 30.2 41.5 45.9 40.6 15.3K 6.1K 20.0 25.1 23.8 4K 0.83K 19.2 ResNet50 [10] Direct-Test 49.4 37.9 19.7 43.1 20.1 35.3 45.6 35.9 15.3K 0 30.1 12.1 15.4 4K 0 30.0 NORM 49.7 38.6 22.9 44.7 25.1 37.4 45.5 37.7 15.3K 0 30.1 16.9 19.4 4K 0 30.0 DUA 45.2 31.5 27.7 31.9 15.2 18.6 21.1 27.3 15.3K 0 30.1 22.5 22.4 4K 0 30.0 ActMAD 49.2 37.7 18.0 40.6 16.0 32.9 44.3 34.1 15.3K 15.3K 11.3 12.7 16.3 4K 4K 11.2 Mean-Teacher 49.6 38.4 26.8 43.4 26.6 33.1 41.6 37.1 15.3K 15.3K 9.9 16.0 20.8 8K 4K 9.8 Ours 49.7 38.7 27.4 46.3 27.4 37.6 43.8 38.7 15.3K 15.3K 12.9 20.9 21.9 4K 4K 13.9 Ours-Skip 49.7 38.8 26.9 46.2 27.6 38.8 45.0 39.0 15.3K 8.9K 21.5 20.0 22.5 4K 0.75K 21.3 Table 3. Comparison of adaptation performance (mAP), the num- ber of trainable parameters (# Params), and memory usage (Cache) according to which part of the backbone is updated. SD / SC de- notes SHIFT-Discrete/Continuous, respectively. mAP # Params Cache Backbone Trainable Params SD SC Num Ratio Avg. Max Swin-T Full-params 38.4 20.6 27.7M 100% 0.86 11.0 LayerNorm 38.5 20.0 0.03M 0.1% 0.65 7.49 adaptor (Ours) 40.4 23.2 0.15M 0.5% 0.65 6.96 ResNet50 Full-params 37.6 20.4 23.7M 100% 1.65 9.29 BatchNorm 37.9 20.2 0.05M 0.2% 1.47 9.11 adaptor (Ours) 38.7 21.7 0.21M 0.9% 1.48 5.41 also applicable to BN-layer-free Swin Transformers. SHIFT-Continuous. In scenarios where the test domain gradually changes across the entire sequence, Ours also demonstrates effectiveness, improving mAP by up to 7%p, as shown in the right section of Tab. 2. WhileDUA performs well in the clear to foggy transition, it is prone to catas- trophic forgetting in situations where the sequence becomes longer, and the test domain changes more diversely, as seen in the left section. Our strategy for determining when model adaptation is necessary is particularly effective in SHIFT. It improves FPS by about 9, reaching about 20 FPS, while en- hancing mAP. This is likely due to avoiding overfitting that can occur when adapting to all repetitive frames in SHIFT, which consists of continuous frames, leading to improve- ments in both inference speed and adaptation performance. 4.5. Additional Analyses We aim to demonstrate the effectiveness and detailed anal- ysis of our proposed model in terms of 1) which parts of, 2) how, and 3) when the model should be updated. Which part to update? Tab. 3 shows how updating dif- ferent parts of the backbone model affects the performance and the memory usage during continual test-time adapta- Table 4. Ablation on each component of our loss. SHIFT-D / C denotes SHIFT-Discrete / Continuous, respectively. The left and right value in each cell corresponds to the mAP for the Swin-T and ResNet50 backbone, respectively. Limg Lobj COCO SHIFT-D. SHIFT-C. - - 17.7/ 17.3 38.5/ 35.9 19.6/ 13.8 ✔ - 16.7/ 18.1 36.6/ 37.0 19.1/ 16.0 ✔ no class weight 17.8/ 18.9 39.7/ 38.0 25.1/ 23.4 ✔ class weight wk,t 22.6/ 22.2 40.4/ 38.7 23.2/ 21.7 tion. We compare (1) updating full parameters, (2) affine parameters of the normalization layer, and (3) our proposed adaptor for each backbone on the SHIFT dataset. Although our adaptor has fewer parameters, about 0.9% or less of the full parameters, it demonstrates the best adaptation perfor- mance. Updating only the affine parameters of the normal- ization layer, while having fewer parameters, seems less ef- fective for adaptation in object detection compared to clas- sification [30, 43]. Additionally, our adaptor requires only about 60% of the memory compared to updating the full parameters, making it memory-efficient. Ablation study on each component in our loss. Tab. 4 presents the effects of image-level feature alignment,Limg, object-level feature class-wise alignment Lobj, and class frequency weighting wk,t proposed to address class im- balance. Aligning only the image-level feature distribu- tion with Limg (first row) leads to modest adaptation in the ResNet50 backbone, while performance in the Swin- T backbone is even lower than without adaptation. No- tably, aligning object-level features with Lobj leads to a substantial improvement, with the mAP increasing by approximately 10%p compared to the no-adaptation sce- nario. Introducing class-specific frequency-based weighting wk,t, despite a slight performance decrease in the SHIFT- Continuous setting, proves highly effective, particularly in scenarios with significant class imbalance, such as COCO 7(a) Swin Transformer backbone  (b) ResNet50 backbone Figure 4. Comparison of mAP and FPS fromOurs-Skip with vary- ing values of τ1 (♦) and τ2 (▲) against Evenly-Skip (×), adapting every N-th instances, on COCO→COCO-C using both (a) Swin- T and (b) ResNet50. The upward and rightward movement indi- cates a better strategy with higher mAP and faster inference speed, showing that Ours-Skip is consistently better than Evenly-Skip. (a) Accumulated number of backward steps (b) Number of backward steps and mAP of Direct-Test in each domain Figure 5. Analysis of the adaptation of Ours-Skip. with 80 classes, where it enhances the mAP by around 5%p. Trade-off between adaptation performance and effi- ciency according to different skipping strategies. Fig. 4 presents mAP and FPS depending on the values ofτ1 and τ2 in the Sec. 3.4 on COCO → COCO-C, which are used for two criteria to determine when the adaptation is needed. We also show the simple baselineEvenly-Skip, which adapts ev- ery N-th step and skips the rest. In Fig. 4, the blue lines (▲) show the results when τ1 is changing from 1.0 to infinity, where only criterion 2 is used, while τ2 is fixed at 1.05. As τ1 decreases, more adaptation is required, leading to slower FPS but higher mAP. The green lines (♦) show the results of changing τ2, where ‘τ2 = inf’ denotes using only criterion 1, without criterion 2. For all main experiments, we set τ1 and τ2 as 1.1 and 1.05, respectively, considering the balance between mAP and FPS. Additionally, our skipping strategy consistently outperforms Evenly-Skip, achieving higher val- ues in both mAP and FPS. This indicates that our criterion for deciding when to bypass model updates provides an ef- fective balance between accuracy and speed. When do models actually update? We analyze when the model actually skips adaptation and only performs infer- ence or actively utilizes test samples for model adaptation based on the two criteria we propose. This analysis is con- ducted in COCO to COCO-C with 15 corruption domains and 1 original domain. Fig. 5a plots the number of back- ward passes, i.e., the number of batches of test samples used for adaptation, with different values of τ1 for the two backbones. The horizontal and vertical axes represent se- quentially incoming test domains and the cumulative back- ward numbers, respectively. A steep slope in a region in- dicates frequent adaptation, while a gentle slope indicates skipping adaptation, performing only inference. Notably, even without explicit information about when the test do- main changes, the model actively performs adaptation, es- pecially right after the test domain changes. This trend is consistent regardless of changes in τ value or backbone type. Furthermore, it is evident that the number of backward passes is primarily determined by the value ofτ1 rather than the type of backbone, suggesting that a consistent τ1 value can be used irrespective of the backbone. Fig. 5b visually represents the adaptation tendencies by dividing backward steps for each domain in the case of Swin-T backbone with τ1 = 1.1. More clearly, it shows that adaptation occurs ac- tively around the points where each domain changes, and af- terward, adaptation happens intermittently or almost not at all. The light pink bars represent the performance ofDirect- Test, showing that domains with initially high model per- formance tend to have less adaptation, while domains with lower performance initially need more adaptation. In other words, the amount of skipping adaptation is proportional to the amount of the domain shift. Interestingly, the second do- main, ’Shot Noise’, shows almost no adaptation despite the lower performance of the Direct-Test. We conjecture that the preceding domain, ’Gaussian Noise’, shares a similar nature of noise, leading the model to decide that additional adaptation steps may not be necessary. As a result, our skip- ping strategy enables the model to efficiently adapt, consid- ering both the original domain the model is trained on and the previous domain the model has been adapted to. 5. Conclusion We introduce an efficient Continual Test-time Adaptation (CTA) method for object detection in the continually chang- ing domain. Our approach involves 1) lightweight adap- tors, 2) class-wise object-level feature alignment, and 3) skipping unnecessary adaptation. These contributions col- lectively yield a highly efficient and effective adaptation method, showcasing robustness to diverse domain shifts, and achieving notable improvements in mAP performance across various CTA scenarios without serious slowdown in the inference speed. 8What, How, and When Should Object Detectors Update in Continually Changing Test Domains? Supplementary Material 6. Additional Details for Baselines We provide additional implementation details for each base- line model. Our framework incorporates all baseline models using the official code except Mean-Teacher. The results of the experiments are reported based on the optimal hyperpa- rameters that yield the best results in our scenario. ActMAD [29] As ActMAD exclusively conducts experi- ments on the KITTI dataset, where all images have a con- stant height and width (e.g., 370 x 1224), ensuring consis- tent feature map sizes for all samples. ActMAD can easily align them along the spatial axis. However, in the general setting of object detection tasks, such as the COCO bench- mark set, where image sizes and width-to-height ratios vary, aligning feature maps along the spatial axis becomes chal- lenging due to different sizes. To adapt ActMAD to our COCO → COCO-C scenario, we perform center cropping on the feature maps to match the size of training domain fea- ture maps and the current test sample feature maps. We em- ploy a learning rate of 1e-5 for COCO and 1e-4 for SHIFT, respectively. Mean-Teacher As the official code of TeST [36] is not available, we implement the EMA-updated Teacher and Student models following TeST [36], to conduct experi- ments in our scenarios. TeST involves three forward steps for a batch: forwarding weakly augmented samples through the student network, strong augmented samples through the teacher network, and original samples through the teacher network for outputs. However, for a fair comparison, we perform two forward steps, forwarding the original sample through the teacher network and strong augmented sam- ples through the student network, to make predictions be- fore adaptation for every samples. We utilize a learning rate of 1e-5 and set the EMA update rate for the teacher network to 0.999. NORM [35] We set the hyperparameter N that controls the trade-off between training statistics and estimated tar- get statistics as 128. DUA [28] We set the momentum decay as 0.94, minimum momentum constant as 1e-4, and the initial momentum de- cay as 1e-3. 7. The effect of Bottleneck Reduction Ratio in the Adaptor Table 5 shows the results for COCO → COCO-C, SHIFT- Discrete, and SHIFT-Continuous based on the dimension reduction ratio ( r) discussed in Section 3.2, representing Table 5. Comparison of adaptation performance (mAP), the num- ber of trainable parameters (# Params), and memory usage (Cache) according to r of Sec. 3.2, the bottleneck reduction ratio in the adaptor. We set r as 32 for all our experiments in the main paper. SD / SC denotes SHIFT-Discrete / Continuous, respectively. mAP # Params Cache Backbone r COCO SD SC Num Ratio Avg. Max Swin-T 1 22.6 40.0 21.3 4.33M 15.7% 0.75 7.51 2 22.6 40.3 23.2 2.17M 7.85% 0.73 7.27 4 22.6 40.4 23.2 1.09M 3.95% 0.70 7.06 8 22.6 40.4 23.2 0.55M 2.00% 0.69 7.00 16 22.6 40.4 23.2 0.28M 1.02% 0.67 6.98 32 22.6 40.4 23.2 0.15M 0.54% 0.65 6.96 64 22.6 40.4 23.2 0.08M 0.29% 0.65 6.95 ResNet50 1 22.5 38.7 20.8 6.31M 26.7% 1.55 5.89 2 22.4 38.7 20.9 3.16M 13.4% 1.51 5.64 4 22.3 38.6 21.3 1.59M 6.71% 1.49 5.52 8 22.3 38.6 21.4 0.80M 3.39% 1.48 5.46 16 22.2 38.6 21.4 0.41M 1.73% 1.48 5.43 32 22.2 38.7 21.4 0.21M 0.89% 1.48 5.41 64 22.1 38.7 21.3 0.11M 0.48% 1.48 5.40 the ratio of bottleneck size compared to the input size in the adaptor. The adaptation performance remains consistent across different r values. However, in the case of r = 1 in SHIFT experiments, mAP decreases, potentially due to catastrophic forgetting resulting from a large number of adaptable parameters. Since increasing the value of r sig- nificantly reduces the number of learnable parameters and memory usage, we set r to 32 in all other experiments. 8. Results on the KITTI Dataset We conduct additional experiments on the KITTI [8] dataset, the commonly used object detection dataset consist- ing of driving scenes with 8 classes (car, van, truck, person, person sitting, cyclist, tram, misc). To simulate the continu- ally changing domains, we use the following scenario ( Fog → Rain → Snow → Clear) as done in [29]. We use the physics-based rendered dataset [9] forfog and rain and sim- ulate snow using the corruption library from [11]. We use the same split of [29], which divides the 7,441 training sam- ples into 3,740 training and 3,741 test samples. We train the Faster-RCNN using 3,741 training samples representing the Clear attribute with Swin-Transformer and ResNet50 back- bones, and evaluate it sequentially on Fog, Rain, Snow, and Clear test samples. We conduct all experiments with a batch size of 16 on 1 RTX A6000 GPU. Table 6 shows the mAP@50, the num- 1Table 6. Comparison of mAP, the number of backward and forward passes, FPS, and memory usage between baselines and our models on the continually changing KITTI datasets ( Fog → Rain → Snow → Clear). Our models improve mAP@50 by 15.1 and 11.3 for Swin-T and ResNet50 backbone, respectively, compared to Direct-Test while maintaining comparable FPS. All experiments are conducted with a batch size of 16. mAP@50 # For. Steps # Backward Steps FPS Cache Backbone Method Fog Rain Snow Clear Avg. All Fog Rain Snow Clear All Avg. Avg. Max Swin-T Direct-Test 46.9 69.5 28.7 89.6 58.7 936 0 0 0 0 0 24.7 0.4 5.5 ActMAD 53.3 78.1 41.2 90.7 65.8 936 234 234 234 234 936 16.8 0.8 21.9 Mean-Teacher 54.5 80.2 43.2 92.4 67.6 936 234 234 234 234 936 10.0 1.0 22.6 Ours 56.7 82.1 64.6 91.8 73.8 936 234 234 234 234 936 17.1 0.4 11.8 Ours-Skip 57.4 81.5 64.3 91.3 73.6 936 234 65 224 36 559 22.9 0.4 11.8 ResNet50 Direct-Test 33.4 63.5 29.8 88.6 53.8 936 0 0 0 0 0 27.7 0.8 4.3 NORM 38.4 66.4 35.9 87.3 57.0 936 0 0 0 0 0 27.7 0.8 4.3 DUA 34.8 67.7 30.9 89.0 55.6 936 0 0 0 0 0 27.7 0.8 4.3 ActMAD 40.4 66.5 42.7 84.5 58.5 936 234 234 234 234 936 18.5 1.6 22.6 Mean-Teacher 39.6 71.3 43.5 88.2 60.6 936 234 234 234 234 936 11.1 1.8 31.1 Ours 45.6 71.4 52.5 88.3 64.5 936 234 234 234 234 936 18.8 0.8 9.4 Ours-Skip 45.8 71.3 50.9 88.4 64.1 936 234 111 98 45 488 24.5 0.8 9.4 (a) GT bounding boxes. (b) Prediction results of Direct-Test. (c) Prediction results of Ours. Figure 6. Results of COCO images corrupted by Shot-Noise. In the analysis of Sec. 4.5, we conjecture that Ours largely skips adaptation in Shot-Noise domain, despite the low mAP of Direct-Test, because the model has already adapted to a similar domain, Gaussian-Noise. In (c), at the first step before adaptation to the Shot-Noise, our model already predicts ’Oven’ and ’Refrigerator’ which Direct-Test fails to detect. This results in a much faster adaptation, and Ours successfully detects various objects, including rare ones such as ’Fire Hydrants’, in the remaining images of the Shot-Noise domain. ber of forward and backward steps, FPS, and memory usage (Cache). Ours improves the mAP@50 by 15.1 and 10.7 for Swin-T and ResNet50 backbones, respectively, compared to Direct-Test. Compared to ActMAD and Mean-Teacher, our model not only improves the adaptation performance but also reduces memory usage, as we update only an ex- tremely small number of parameters of the adaptor. Further- more, using our skipping criteria of Sec. 3.4 with τ = 1.1 and β = 1.05, we can improve FPS by more than 5.8 with- out sacrificing mAP@50, resulting in much faster inference 2(a) GT bounding boxes. (b) Prediction results of Direct-Test. (c) Prediction results of Ours. Figure 7. Results for COCO images corrupted by Pixelate. In the Pixelate domain, where the model has already experienced various corruptions in a long sequence, Ours initially incorrectly detects objects. In (c), it misidentifies a bed as a couch in the first step. However, it rapidly adapts to the Pixelate domain and effectively detects various objects. Notably, even in cases whereDirect-Testcorrectly identifies objects but with low confidence, Ours detects them with much higher confidence. (a) GT bounding boxes. (b) Prediction results of Direct-Test. (c) Prediction results of Ours. Figure 8. Results for SHIFT-Discrete with continually changing attributes, foggy → rainy → dawn → night. speed compared to other TTA baselines. 39. Qualitative Results Fig. 6 and 7 and Fig. 8 show the qualitative results of Ours and Direct-Test which predict the samples without adapta- tion for COCO → COCO-C and SHIFT, respectively. 9.1. COCO → COCO-C Fig. 6 and 7 compare the prediction results for COCO im- ages corrupted. When the model encounters test images with various corruptions sequentially ( Gaussian-Noise → Shot-Noise → Impulse-Noise → Defocus-Blur → Glass- Blur → Motion-Blur → Zoom-Blur → Snow → Frost → Fog → Brightness → Contrast → Elastic-Transform → Pixelate → JPEG-Compression → Original), Fig. 6 and 7 shows the results when the test images are corrupted by Shot-Noise and Pixelate, respectively. Compared to Direct- Test, our model adapts to the current domain within a few steps, such as 100 iterations, and detects various objects very well in the remaining incoming images. 9.2. SHIFT-Discrete Fig. 8 shows the qualitative results for SHIFT-Discrete. In the SHIFT-Discrete scenario, the model encounters environ- ments sequentially, transitioning from cloudy → overcast → foggy → rainy → dawn → night → clear. Figure. 8 se- lectively shows the foggy → rainy → dawn → night se- quence, where the domain gap from the original clear envi- ronments is relatively large. Compared to Direct-Test, Ours detects various objects such as ’cars’ and ’pedestrians’ re- gardless of distribution changes. References [1] Alexander Bartler, Florian Bender, Felix Wiewel, and Bin Yang. Ttaps: Test-time adaption by aligning prototypes using self-supervision. In 2022 International Joint Conference on Neural Networks (IJCNN), pages 1–8. IEEE, 2022. 2 [2] Alexander Bartler, Andre B ¨uhler, Felix Wiewel, Mario D¨obler, and Bin Yang. Mt3: Meta test-time training for self- supervised test-time adaption. In International Conference on Artificial Intelligence and Statistics , pages 3080–3090. PMLR, 2022. 2 [3] Malik Boudiaf, Romain Mueller, Ismail Ben Ayed, and Luca Bertinetto. Parameter-free online test-time adaptation. In Proceedings of the IEEE/CVF Conference on Computer Vi- sion and Pattern Recognition, pages 8344–8353, 2022. 1 [4] Dhanajit Brahma and Piyush Rai. A probabilistic frame- work for lifelong test-time adaptation. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 3582–3591, 2023. 3 [5] Shoufa Chen, Chongjian Ge, Zhan Tong, Jiangliu Wang, Yib- ing Song, Jue Wang, and Ping Luo. Adaptformer: Adapting vision transformers for scalable visual recognition.Advances in Neural Information Processing Systems, 35:16664–16678, 2022. 3 [6] Yijin Chen, Xun Xu, Yongyi Su, and Kui Jia. Stfar: Im- proving object detection robustness at test-time by self- training with feature alignment regularization.arXiv preprint arXiv:2303.17937, 2023. 1, 2, 3 [7] Jinhong Deng, Wen Li, Yuhua Chen, and Lixin Duan. Un- biased mean teacher for cross-domain object detection. In Proceedings of the IEEE/CVF Conference on Computer Vi- sion and Pattern Recognition, pages 4091–4101, 2021. 3 [8] Andreas Geiger, Philip Lenz, Christoph Stiller, and Raquel Urtasun. Vision meets robotics: The kitti dataset. The Inter- national Journal of Robotics Research , 32(11):1231–1237, 2013. 1 [9] Shirsendu Sukanta Halder, Jean-Franc ¸ois Lalonde, and Raoul de Charette. Physics-based rendering for improving robustness to rain. In Proceedings of the IEEE/CVF Interna- tional Conference on Computer Vision, pages 10203–10212, 2019. 1 [10] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image recognition. In Proceed- ings of the IEEE conference on computer vision and pattern recognition, pages 770–778, 2016. 5, 6, 7 [11] Dan Hendrycks and Thomas Dietterich. Benchmarking neu- ral network robustness to common corruptions and perturba- tions. arXiv preprint arXiv:1903.12261, 2019. 1 [12] Junyuan Hong, Lingjuan Lyu, Jiayu Zhou, and Michael Spranger. Mecta: Memory-economic continual test-time model adaptation. In The Eleventh International Conference on Learning Representations, 2022. 3 [13] Edward J Hu, Yelong Shen, Phillip Wallis, Zeyuan Allen- Zhu, Yuanzhi Li, Shean Wang, Lu Wang, and Weizhu Chen. Lora: Low-rank adaptation of large language models. arXiv preprint arXiv:2106.09685, 2021. 3 [14] Sergey Ioffe and Christian Szegedy. Batch normalization: Accelerating deep network training by reducing internal co- variate shift. In International conference on machine learn- ing, pages 448–456. pmlr, 2015. 2 [15] Yusuke Iwasawa and Yutaka Matsuo. Test-time classifier adjustment module for modelagnostic domain generaliza- tion. In Advances in Neural Information Processing Systems (NeurIPS), 2021. 3 [16] Minguk Jang, Sae-Young Chung, and Hye Won Chung. Test- time adaptation via self-training with nearest neighbor infor- mation. In International Conference on Learning Represen- tations (ICLR), 2023. 3 [17] Sanghun Jung, Jungsoo Lee, Nanhee Kim, Amirreza Sha- ban, Byron Boots, and Jaegul Choo. Cafa: Class-aware fea- ture alignment for test-time adaptation. In Proceedings of the IEEE/CVF International Conference on Computer Vi- sion, 2023. 2, 3, 4 [18] Mehran Khodabandeh, Arash Vahdat, Mani Ranjbar, and William G Macready. A robust learning approach to domain adaptive object detection. In Proceedings of the IEEE/CVF International Conference on Computer Vision , pages 480– 490, 2019. 3 [19] Seunghyeon Kim, Jaehoon Choi, Taekyung Kim, and Chang- ick Kim. Self-training and adversarial background regular- ization for unsupervised domain adaptive one-stage object 4detection. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 6092–6101, 2019. 3 [20] Xianfeng Li, Weijie Chen, Di Xie, Shicai Yang, Peng Yuan, Shiliang Pu, and Yueting Zhuang. A free lunch for unsuper- vised domain adaptive object detection without source data. In Proceedings of the AAAI Conference on Artificial Intelli- gence, pages 8474–8481, 2021. 4 [21] Yanghao Li, Naiyan Wang, Jianping Shi, Jiaying Liu, and Xiaodi Hou. Revisiting batch normalization for practical do- main adaptation, 2017. 3 [22] Hyesu Lim, Byeonggeun Kim, Jaegul Choo, and Sungha Choi. Ttn: A domain-shift aware batch normalization in test- time adaptation, 2023. 2, 3 [23] Tsung-Yi Lin, Michael Maire, Serge Belongie, James Hays, Pietro Perona, Deva Ramanan, Piotr Doll´ar, and C Lawrence Zitnick. Microsoft coco: Common objects in context. In Computer Vision–ECCV 2014: 13th European Conference, Zurich, Switzerland, September 6-12, 2014, Proceedings, Part V 13, pages 740–755. Springer, 2014. 5 [24] Tsung-Yi Lin, Piotr Doll ´ar, Ross Girshick, Kaiming He, Bharath Hariharan, and Serge Belongie. Feature pyra- mid networks for object detection. In Proceedings of the IEEE conference on computer vision and pattern recogni- tion, pages 2117–2125, 2017. 5 [25] Yuejiang Liu, Parth Kothari, Bastien Van Delft, Baptiste Bellot-Gurlet, Taylor Mordan, and Alexandre Alahi. Ttt++: When does self-supervised test-time training fail or thrive? Advances in Neural Information Processing Systems , 34: 21808–21820, 2021. 1, 2 [26] Ze Liu, Yutong Lin, Yue Cao, Han Hu, Yixuan Wei, Zheng Zhang, Stephen Lin, and Baining Guo. Swin transformer: Hierarchical vision transformer using shifted windows. In Proceedings of the IEEE/CVF international conference on computer vision, pages 10012–10022, 2021. 2, 5, 6, 7 [27] Claudio Michaelis, Benjamin Mitzkus, Robert Geirhos, Evgenia Rusak, Oliver Bringmann, Alexander S Ecker, Matthias Bethge, and Wieland Brendel. Benchmarking ro- bustness in object detection: Autonomous driving when win- ter is coming. arXiv preprint arXiv:1907.07484, 2019. 5 [28] M Jehanzeb Mirza, Jakub Micorek, Horst Possegger, and Horst Bischof. The norm must go on: Dynamic unsuper- vised domain adaptation by normalization. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pat- tern Recognition, pages 14765–14775, 2022. 3, 6, 1 [29] Muhammad Jehanzeb Mirza, Pol Jan ´e Soneira, Wei Lin, Ma- teusz Kozinski, Horst Possegger, and Horst Bischof. Act- mad: Activation matching to align distributions for test-time- training. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition , pages 24152– 24161, 2023. 1, 2, 3, 4, 6 [30] Shuaicheng Niu, Jiaxiang Wu, Yifan Zhang, Yaofo Chen, Shijian Zheng, Peilin Zhao, and Mingkui Tan. Efficient test-time model adaptation without forgetting. In Interna- tional conference on machine learning, pages 16888–16905. PMLR, 2022. 1, 2, 3, 7 [31] Shuaicheng Niu, Jiaxiang Wu, Yifan Zhang, Zhiquan Wen, Yaofo Chen, Peilin Zhao, and Mingkui Tan. Towards stable test-time adaptation in dynamic wild world. arXiv preprint arXiv:2302.12400, 2023. 3 [32] Mario obler, Robert A Marsden, and Bin Yang. Robust mean teacher for continual and gradual test-time adaptation. In Proceedings of the IEEE/CVF Conference on Computer Vi- sion and Pattern Recognition, pages 7704–7714, 2023. 3 [33] Shaoqing Ren, Kaiming He, Ross Girshick, and Jian Sun. Faster r-cnn: Towards real-time object detection with region proposal networks. 2016. 5 [34] Aruni RoyChowdhury, Prithvijit Chakrabarty, Ashish Singh, SouYoung Jin, Huaizu Jiang, Liangliang Cao, and Erik Learned-Miller. Automatic adaptation of object detectors to new domains using self-training. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), 2019. 3 [35] Steffen Schneider, Evgenia Rusak, Luisa Eck, Oliver Bring- mann, Wieland Brendel, and Matthias Bethge. Improving robustness against common corruptions by covariate shift adaptation. Advances in neural information processing sys- tems, 33:11539–11551, 2020. 3, 6, 1 [36] Samarth Sinha, Peter Gehler, Francesco Locatello, and Bernt Schiele. Test: Test-time self-training under distribution shift. In Proceedings of the IEEE/CVF Winter Conference on Ap- plications of Computer Vision, pages 2759–2769, 2023. 1, 2, 3, 6 [37] Kihyuk Sohn, David Berthelot, Nicholas Carlini, Zizhao Zhang, Han Zhang, Colin A Raffel, Ekin Dogus Cubuk, Alexey Kurakin, and Chun-Liang Li. Fixmatch: Simplifying semi-supervised learning with consistency and confidence. Advances in neural information processing systems, 33:596– 608, 2020. 2, 6 [38] Yongyi Su, Xun Xu, and Kui Jia. Revisiting realistic test- time training: Sequential inference and adaptation by an- chored clustering. Advances in Neural Information Process- ing Systems, 35:17543–17555, 2022. 3, 4 [39] Tao Sun, Mattia Segu, Janis Postels, Yuxuan Wang, Luc Van Gool, Bernt Schiele, Federico Tombari, and Fisher Yu. Shift: a synthetic driving dataset for continuous multi-task domain adaptation. In Proceedings of the IEEE/CVF Con- ference on Computer Vision and Pattern Recognition, pages 21371–21382, 2022. 1, 5 [40] Yu Sun, Xiaolong Wang, Zhuang Liu, John Miller, Alexei Efros, and Moritz Hardt. Test-time training with self- supervision for generalization under distribution shifts. In International conference on machine learning, pages 9229– 9248. PMLR, 2020. 1, 2 [41] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszko- reit, Llion Jones, Aidan N Gomez, Łukasz Kaiser, and Illia Polosukhin. Attention is all you need. Advances in neural information processing systems, 30, 2017. 2 [42] Vibashan VS, Poojan Oza, and Vishal M Patel. Towards on- line domain adaptive object detection. In Proceedings of the IEEE/CVF Winter Conference on Applications of Computer Vision, pages 478–488, 2023. 3 [43] Dequan Wang, Evan Shelhamer, Shaoteng Liu, Bruno Ol- shausen, and Trevor Darrell. Tent: Fully test-time adaptation by entropy minimization. arXiv preprint arXiv:2006.10726, 2020. 1, 2, 3, 7 5[44] Qin Wang, Olga Fink, Luc Van Gool, and Dengxin Dai. Continual test-time domain adaptation. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 7201–7211, 2022. 3 [45] Zehao Xiao, Xiantong Zhen, Shengcai Liao, and Cees GM Snoek. Energy-based test sample adaptation for domain gen- eralization. arXiv preprint arXiv:2302.11215, 2023. 3 [46] Mengde Xu, Zheng Zhang, Han Hu, Jianfeng Wang, Lijuan Wang, Fangyun Wei, Xiang Bai, and Zicheng Liu. End- to-end semi-supervised object detection with soft teacher. Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV), 2021. 5 [47] Marvin Zhang, Sergey Levine, and Chelsea Finn. Memo: Test time robustness via adaptation and augmentation. Advances in Neural Information Processing Systems , 35: 38629–38642, 2022. 1, 2, 3 [48] Bowen Zhao, Chen Chen, and Shu-Tao Xia1. Delta: Degradation-free fully test-time adaptation. In International Conference on Learning Representations (ICLR), 2023. 2, 3 6",
      "meta_data": {
        "arxiv_id": "2312.08875v1",
        "authors": [
          "Jayeon Yoo",
          "Dongkwan Lee",
          "Inseop Chung",
          "Donghyun Kim",
          "Nojun Kwak"
        ],
        "published_date": "2023-12-12T07:13:08Z",
        "pdf_url": "https://arxiv.org/pdf/2312.08875v1.pdf"
      },
      "llm_extracted_info": {
        "main_contributions": "The paper proposes an efficient Continual Test-time Adaptation (CTA) method for object detection in continually changing test domains. Its key contributions include the introduction of architecture-agnostic lightweight adaptor modules (0.54%~0.89% of total parameters) that enable rapid and robust adaptation without catastrophic forgetting, applicable to diverse backbones (CNNs, Transformers). It also presents a practical class-wise feature alignment method for object detection, operating at both image and object levels using Exponentially Moving Average (EMA) updated test features and pre-computed training statistics. Furthermore, the research introduces two novel criteria to dynamically determine when model adaptation is needed, thereby enhancing efficiency by skipping unnecessary updates and boosting inference speed by up to approximately 2 times while maintaining adaptation performance. The approach consistently improves mAP by up to 4.9%p and 7.9%p on COCO \n\n COCO-corrupted and SHIFT benchmarks, respectively, while maintaining over 20 FPS.",
        "methodology": "The proposed methodology addresses \"what, how, and when\" to update object detectors in continually changing test domains. For \"what to update\", it utilizes lightweight, architecture-agnostic adaptor modules (MLP layers for Transformers and 1x1 convolutional layers for ResNet) attached in parallel to backbone blocks. Only these adaptors are updated, while the pre-trained backbone parameters remain frozen. For \"how to update\", it employs an EMA feature alignment strategy to align test domain feature distributions with training data. This includes image-level feature alignment (Limg) by minimizing KL divergence between approximated normal distributions (using EMA-updated test means and pre-computed training means/variances), and region-level class-wise feature alignment (Lobj) for object-level features, incorporating a frequency-based weighting scheme to address class imbalance. The total adaptation loss is a sum of Limg and Lobj. For \"when to update\", an \"adaptation on demand\" mechanism is introduced with two criteria: (1) update if the image-level distribution gap (Limg) significantly exceeds the in-domain distribution gap by a threshold (τ1); (2) update if Limg suddenly increases compared to its Exponentially Moving Average (Ltema) by a threshold (τ2). Adaptation (backward pass) is performed if at least one criterion is met.",
        "experimental_setup": "The method was evaluated on Faster-RCNN models using ResNet50 and Swin-Tiny backbones with FPN. Three primary scenarios were used for evaluation: COCO \n\n COCO-C, which simulates continuous and drastic domain changes by sequentially evaluating on 15 types of realistic corruptions applied to MS-COCO validation images, followed by evaluation on the original COCO validation set. SHIFT-Discrete/Continuous, a synthetic driving image dataset with 6 classes, simulating discrete (e.g., cloudy \n\n overcast) and continuous (e.g., clear \n\n foggy \n\n clear) changes in weather and time-of-day attributes. An additional experiment was conducted on the KITTI dataset (Fog \n\n Rain \n\n Snow \n\n Clear sequence) for further validation. Baselines included Direct-Test, ActMAD, Mean-Teacher (reproducing TeST), NORM, and DUA. Performance was measured by mAP (mAP@50 for KITTI), number of forward/backward passes, and Frames Per Second (FPS). Standard hyperparameters included a learning rate of 0.001 for the SGD optimizer, EMA update rate α=0.01, and adaptation criteria thresholds τ1=1.1 and τ2=1.05, with a batch size of 4 (16 for KITTI). The adaptor bottleneck reduction ratio (r) was set to 32.",
        "limitations": "One limitation is that the frequency-based class weighting scheme (wk,t), while generally effective in class-imbalanced scenarios like COCO, shows a slight performance decrease in the SHIFT-Continuous setting. Another constraint or assumption of the method is the approximation of the test feature variance (Σte) as equivalent to the pre-computed training variance (Σtr) to enhance stability, which might not universally hold true. Furthermore, the \"adaptation on demand\" strategy relies on empirically determined hyperparameters (τ1 and τ2) that necessitate tuning to balance the trade-off between mAP performance and inference speed (FPS).",
        "future_research_directions": "Not mentioned"
      }
    },
    {
      "title": "DELTA: DEGRADATION-FREE FULLY TEST-TIME ADAPTATION",
      "abstract": "Fully test-time adaptation aims at adapting a pre-trained model to the test\nstream during real-time inference, which is urgently required when the test\ndistribution differs from the training distribution. Several efforts have been\ndevoted to improving adaptation performance. However, we find that two\nunfavorable defects are concealed in the prevalent adaptation methodologies\nlike test-time batch normalization (BN) and self-learning. First, we reveal\nthat the normalization statistics in test-time BN are completely affected by\nthe currently received test samples, resulting in inaccurate estimates. Second,\nwe show that during test-time adaptation, the parameter update is biased\ntowards some dominant classes. In addition to the extensively studied test\nstream with independent and class-balanced samples, we further observe that the\ndefects can be exacerbated in more complicated test environments, such as\n(time) dependent or class-imbalanced data. We observe that previous approaches\nwork well in certain scenarios while show performance degradation in others due\nto their faults. In this paper, we provide a plug-in solution called DELTA for\nDegradation-freE fuLly Test-time Adaptation, which consists of two components:\n(i) Test-time Batch Renormalization (TBR), introduced to improve the estimated\nnormalization statistics. (ii) Dynamic Online re-weighTing (DOT), designed to\naddress the class bias within optimization. We investigate various test-time\nadaptation methods on three commonly used datasets with four scenarios, and a\nnewly introduced real-world dataset. DELTA can help them deal with all\nscenarios simultaneously, leading to SOTA performance.",
      "meta_data": {
        "arxiv_id": "2301.13018v1",
        "authors": [
          "Bowen Zhao",
          "Chen Chen",
          "Shu-Tao Xia"
        ],
        "published_date": "2023-01-30T15:54:00Z",
        "pdf_url": "https://arxiv.org/pdf/2301.13018v1.pdf"
      }
    },
    {
      "title": "Test-Time Training with Self-Supervision for Generalization under Distribution Shifts",
      "abstract": "In this paper, we propose Test-Time Training, a general approach for\nimproving the performance of predictive models when training and test data come\nfrom different distributions. We turn a single unlabeled test sample into a\nself-supervised learning problem, on which we update the model parameters\nbefore making a prediction. This also extends naturally to data in an online\nstream. Our simple approach leads to improvements on diverse image\nclassification benchmarks aimed at evaluating robustness to distribution\nshifts.",
      "full_text": "Test-Time Training with Self-Supervision for Generalization under Distribution Shifts Yu Sun1 Xiaolong Wang1 2 Zhuang Liu1 John Miller1 Alexei A. Efros1 Moritz Hardt1 Abstract In this paper, we propose Test-Time Training, a general approach for improving the performance of predictive models when training and test data come from different distributions. We turn a sin- gle unlabeled test sample into a self-supervised learning problem, on which we update the model parameters before making a prediction. This also extends naturally to data in an online stream. Our simple approach leads to improvements on di- verse image classiﬁcation benchmarks aimed at evaluating robustness to distribution shifts. 1. Introduction Supervised learning remains notoriously weak at generaliza- tion under distribution shifts. Unless training and test data are drawn from the same distribution, even seemingly minor differences turn out to defeat state-of-the-art models (Recht et al., 2018). Adversarial robustness and domain adapta- tion are but a few existing paradigms that try to anticipate differences between the training and test distribution with either topological structure or data from the test distribution available during training. We explore a new take on gener- alization that does not anticipate the distribution shifts, but instead learns from them at test time. We start from a simple observation. The unlabeled test sample xpresented at test time gives us a hint about the distribution from which it was drawn. We propose to take advantage of this hint on the test distribution by allowing the model parameters θto depend on the test sample x, but not its unknown label y. The concept of a variable decision boundary θ(x) is powerful in theory since it breaks away from the limitation of ﬁxed model capacity (see additional discussion in Section A1), but the design of a feedback mechanism from xto θ(x) raises new challenges in practice that we only begin to address here. 1University of California, Berkeley 2University of California, San Diego. Correspondence to: Yu Sun <yusun@berkeley.edu>. Proceedings of the 37 th International Conference on Machine Learning, Vienna, Austria, PMLR 119, 2020. Copyright 2020 by the author(s). Our proposed test-time training method creates a self- supervised learning problem based on this single test sample x, updating θat test time before making a prediction. Self- supervised learning uses an auxiliary task that automatically creates labels from unlabeled inputs. In our experiments, we use the task of rotating each input image by a multiple of 90 degrees and predicting its angle (Gidaris et al., 2018). This approach can also be easily modiﬁed to work outside the standard supervised learning setting. If several test samples arrive in a batch, we can use the entire batch for test-time training. If samples arrive in an online stream, we obtain further improvements by keeping the state of the parameters. After all, prediction is rarely a single event. The online version can be the natural mode of deployment under the additional assumption that test samples are produced by the same or smoothly changing distribution shifts. We experimentally validate our method in the context of object recognition on several standard benchmarks. These include images with diverse types of corruption at various levels (Hendrycks & Dietterich, 2019), video frames of moving objects (Shankar et al., 2019), and a new test set of unknown shifts collected by (Recht et al., 2018). Our algorithm makes substantial improvements under distribu- tion shifts, while maintaining the same performance on the original distribution. In our experiments, we compare with a strong baseline (labeled joint training) that uses both supervised and self- supervised learning at training-time, but keeps the model ﬁxed at test time. Recent work shows that training-time self- supervision improves robustness (Hendrycks et al., 2019a); our joint training baseline corresponds to an improved imple- mentation of this work. A comprehensive review of related work follows in Section 5. We complement the empirical results with theoretical inves- tigations in Section 4, and establish an intuitive sufﬁcient condition on a convex model of when Test-Time Training helps; this condition, roughly speaking, is to have correlated gradients between the loss functions of the two tasks. Project website: https://test-time-training.github.io/. arXiv:1909.13231v3  [cs.LG]  1 Jul 2020Test-Time Training with Self-Supervision for Generalization under Distribution Shifts 2. Method This section describes the algorithmic details of our method. To set up notation, consider a standard K-layer neural net- work with parameters θk for layer k. The stacked parameter vector θ = ( θ1,...,θ K) speciﬁes the entire model for a classiﬁcation task with loss function lm(x,y; θ) on the test sample (x,y). We call this the main task, as indicated by the subscript of the loss function. We assume to have training data (x1,y1),..., (xn,yn) drawn i.i.d. from a distribution P. Standard empirical risk minimization solves the optimization problem: min θ 1 n n∑ i=1 lm(xi,yi; θ). (1) Our method requires a self-supervised auxiliary task with loss function ls(x). In this paper, we choose the rotation prediction task (Gidaris et al., 2018), which has been demon- strated to be simple and effective at feature learning for convolutional neural networks. The task simply rotates x in the image plane by one of 0, 90, 180 and 270 degrees and have the model predict the angle of rotation as a four- way classiﬁcation problem. Other self-supervised tasks in Section 5 might also be used for our method. The auxiliary task shares some of the model parameters θe = ( θ1,...,θ κ) up to a certain κ ∈ {1,...,K }. We designate those κlayers as a shared feature extractor. The auxiliary task uses its own task-speciﬁc parameters θs = (θ′ κ+1,...,θ ′ K). We call the unshared parameters θs the self-supervised task branch, and θm = (θκ+1,...,θ K) the main task branch . Pictorially, the joint architecture is a Y-structure with a shared bottom and two branches. For our experiments, the self-supervised task branch has the same architecture as the main branch, except for the output dimensionality of the last layer due to the different number of classes in the two tasks. Training is done in the fashion of multi-task learning (Caru- ana, 1997); the model is trained on both tasks on the same data drawn fromP. Losses for both tasks are added together, and gradients are taken for the collection of all parameters. The joint training problem is therefore min θe,θm,θs 1 n n∑ i=1 lm(xi,yi; θm,θe) + ls(xi; θs,θe). (2) Now we describe the standard version of Test-Time Training on a single test sample x. Simply put, Test-Time Training ﬁne-tunes the shared feature extractor θe by minimizing the auxiliary task loss on x. This can be formulated as min θe ls(x; θs,θe). (3) Denote θ∗ e the (approximate) minimizer of Equation 3. The model then makes a prediction using the updated parameters θ(x) = (θ∗ e,θm). Empirically, the difference is negligible between minimizing Equation 3 over θe versus over both θe and θs. Theoretically, the difference exists only when optimization is done with more than one gradient step. Test-Time Training naturally beneﬁts from standard data augmentation techniques. On each test sample x, we per- form the exact same set of random transformations as for data augmentation during training, to form a batch only con- taining these augmented copies of xfor Test-Time Training. Online Test-Time Training. In the standard version of our method, the optimization problem in Equation 3 is al- ways initialized with parameters θ= (θe,θs) obtained by minimizing Equation 2. After making a prediction on x, θ∗ e is discarded. Outside of the standard supervised learning setting, when the test samples arrive online sequentially, the online version solves the same optimization problem as in Equation 3 to update the shared feature extractor θe. How- ever, on test sample xt, θis instead initialized with θ(xt−1) updated on the previous sample xt−1. This allows θ(xt) to take advantage of the distributional information available in x1,...,x t−1 as well as xt. 3. Empirical Results We experiment with both versions of our method (standard and online) on three kinds of benchmarks for distribution shifts, presented here in the order of visually low to high- level. Our code is available at the project website. Network details. Our architecture and hyper-parameters are consistent across all experiments. We use ResNets (He et al., 2016b), which are constructed differently for CIFAR-10 (Krizhevsky & Hinton, 2009) (26-layer) and Ima- geNet (Russakovsky et al., 2015) (18-layer). The CIFAR-10 dataset contains 50K images for training, and 10K images for testing. The ImageNet contains 1.2M images for train- ing and the 50K validation images are used as the test set. ResNets on CIFAR-10 have three groups, each containing convolutional layers with the same number of channels and size of feature maps; our splitting point is the end of the second group. ResNets on ImageNet have four groups; our splitting point is the end of the third group. We use Group Normalization (GN) instead of Batch Nor- malization (BN) in our architecture, since BN has been shown to be ineffective when training with small batches, for which the estimated batch statistics are not accurate (Ioffe & Szegedy, 2015). This technicality hurts Test-Time Training since each batch only contains (augmented) copies of a single image. Different from BN, GN is not dependent on batch size and achieves similar results on our baselines.Test-Time Training with Self-Supervision for Generalization under Distribution Shifts originalgauss shot impulsedefocus glass motion zoom snow frost fog bright contrastelasticpixelate jpeg 0 10 20 30 40 50Error (%) Object recognition task only Joint training (Hendrycks et al. 2019) TTT TTT-Online Figure 1.Test error (%) on CIFAR-10-C with level 5 corruptions.We compare our approaches, Test-Time Training (TTT) and its online version (TTT-Online), with two baselines: object recognition without self-supervision, and joint training with self-supervision but keeping the model ﬁxed at test time. TTT improves over the baselines and TTT-Online improves even further. We report results with BN in Section A4 of the appendix for completeness. We directly compare our architecture to that of Hendrycks et al. (2018) in subsection A4.5. Optimization details. For joint training (Equation 2), we use stochastic gradient descent with standard hyper- parameters as (Huang et al., 2016; He et al., 2016a). For Test-Time Training (Equation 3), we use stochastic gradient descent with the learning rate set to that of the last epoch during training, which is 0.001 in all our experiments. We set weight decay and momentum to zero during Test-Time Training, inspired by practice in (He et al., 2018; Liu et al., 2018). For the standard version of Test-Time Training, we take ten gradient steps, using batches independently gener- ated by the same image. For online version of Test-Time Training, we take only one gradient step given each new im- age. We use random crop and random horizontal ﬂip for data augmentation. See Section A2 of the appendix for computa- tional aspects of our method. In all the tables and ﬁgures, object recognition task onlyrefers to the plain ResNet model (using GN, unless otherwise speciﬁed); joint training refers to the model jointly trained on both the main task and the self-supervised task, ﬁxed at test time; this has been pro- posed as the method in Hendrycks et al. (2019a); Test-Time Training (TTT) refers to the standard version described sec- tion 2; and online Test-Time Training (TTT-Online)refers to the online version that does not discardθ(xt) for xt arriving sequentially from the same distribution. Performance for TTT-Online is calculated as the average over the entire test set; we always shufﬂe the test set before TTT-Online to avoid ordering artifacts. 3.1. Object Recognition on Corrupted Images Hendrycks & Dietterich (2019) propose to benchmark ro- bustness of object recognition with 15 types of corruptions from four broad categories: noise, blur, weather and digital. Each corruption type comes in ﬁve levels of severity, with level 5 the most severe (details and sample images in the ap- pendix). The corruptions are simulated to mimic real-world corruptions as much as possible on copies of the test set for both CIFAR-10 and ImageNet. The new test sets are named as CIFAR-10-C and ImageNet-C, respectively. In the pro- posed benchmark, training should be done on the original training set, and the diversity of corruption types should make it difﬁcult for any methods to work well across the board if it relies too much on corruption speciﬁc knowledge. For online Test-Time Training, we take the entire test set as a stream of incoming images, and update and test on each image in an online manner as it arrives. CIFAR-10-C. Our results on the level 5 corruptions (most severe) are shown in Figure 1. The results on levels 1-4 are shown in Section A4 in appendix. Across all ﬁve levels and 15 corruption types, both standard and online versions of Test-Time Training improve over the object recognition task only baseline by a large margin. The standard version always improves over joint training, and the online version often improves signiﬁcantly (>10%) over joint training and never hurts by more than 0.2%. Speciﬁcally, TTT-Online contributes >24% on the three noise types and 38% on pix- elation. For a learning problem with the seemingly unstable setup that abuses a single image, this kind of consistency is rather surprising. The baseline ResNet-26 with object recognition task only has error 8.9% on the original test set of CIFAR-10. The joint training baseline actually improves performance on the original to 8.1%. More surprisingly, unlike many other methods that trade off original performance for robustness, Test-Time Training further improves on the original test set by 0.2% consistently over multiple independent trials. This suggests that our method does not choose between speciﬁcity and generality.Test-Time Training with Self-Supervision for Generalization under Distribution Shifts originalgauss shot impulsedefocus glass motion zoom snow frost fog bright contrastelasticpixelate jpeg 0 20 40 60Accuracy (%) Object recognition task only Joint training (Hendrycks et al. 2019) TTT TTT-Online 0 20000 40000 Number of samples 60 62 64 66 68 70 72 74 76Accuracy (%) Original Sliding window average 0 20000 40000 Number of samples 12 15 18 21 24 27 30 33Accuracy (%) Gaussian Noise Sliding window average 0 20000 40000 Number of samples 16 18 20 22 24 26 28 30 32Accuracy (%) Defocus Blur Sliding window average 0 20000 40000 Number of samples 28 30 32 34 36 38Accuracy (%) Zoom Blur Sliding window average 0 20000 40000 Number of samples 33 36 39 42 45 48 51 54Accuracy (%) Fog Sliding window average 0 20000 40000 Number of samples 30 33 36 39 42 45 48 51Accuracy (%) Elastic Transform Sliding window average Figure 2.Test accuracy (%) on ImageNet-C with level 5 corruptions.Upper panel: Our approaches, TTT and TTT-Online, show signiﬁcant improvements in all corruption types over the two baselines. Lower panel: We show the accuracy of TTT-Online as the average over a sliding window of 100 samples; TTT-Online generalizes better as more samples are evaluated (x-axis), without hurting on the original distribution. We use accuracy instead of error here because the baseline performance is very low for most corruptions. Separate from our method, it is interesting to note that joint training consistently improves over the single-task baseline, as discovered by Hendrycks et al. (2019a). Hendrycks & Dietterich (2019) have also experimented with various other training methods on this benchmark, and point to Adversar- ial Logit Pairing (ALP) (Kannan et al., 2018) as the most effective approach. Results of this additional baseline on all levels of CIFAR-10-C are shown in the appendix, along with its implementation details. While surprisingly robust under some of the most severe corruptions (especially the three noise types), ALP incurs a much larger error (by a factor of two) on the original distribution and some corruptions (e.g. all levels of contrast and fog), and hurts performance signiﬁcantly when the corruptions are not as severe (espe- cially on levels 1-3); this kind of tradeoff is to be expected for methods based on adversarial training. ImageNet-C. Our results on the level 5 corruptions (most severe) are shown in Figure 2. We use accuracy instead of error for this dataset because the baseline performance is very low for most corruptions. The general trend is roughly the same as on CIFAR-10-C. The standard version of TTT always improves over the baseline and joint training, while the online version only hurts on the original by 0.1% over the baseline, but signiﬁcantly improves (by a factor of more than three) on many of the corruption types. In the lower panel of Figure 2, we visualize how the accu- racy (averaged over a sliding window) of the online version changes as more images are tested. Due to space constraints, we show this plot on the original test set, as well as every third corruption type, following the same order as in the original paper. On the original test set, there is no visible trend in performance change after updating on the 50,000 samples. With corruptions, accuracy has already risen sig- niﬁcantly after 10,000 samples, but is still rising towards the end of the 50,000 samples, indicating room for additional improvements if more samples were available. Without seeing a single label, TTT-Online behaves as if we were training on the test set from the appearance of the plots.Test-Time Training with Self-Supervision for Generalization under Distribution Shifts orig gauss shot impul defoc glass motn zoom snow frost fog brit contr elast pixel jpeg TTT-Online 8.2 25.8 22.6 30.6 14.6 34.4 18.3 17.1 20.0 18.0 16.9 11.2 15.6 21.6 18.1 21.2 UDA-SS 9.0 28.2 26.5 20.8 15.6 43.7 24.5 23.8 25.0 24.9 17.2 12.7 11.6 22.1 20.3 22.6 Table 1.Test error (%) on CIFAR-10-C with level 5 corruption.Comparison between online Test-Time Training (TTT-Online) and unsupervised domain adaptation by self-supervision (UDA-SS) (Sun et al., 2019) with access to the entire (unlabeled) test set during training. We highlight the lower error in bold. We have abbreviated the names of the corruptions, in order: original test set, Gaussian noise, shot noise, impulse noise, defocus blur, glass blue, motion blur, zoom blur, snow, frost, fog, brightness, contrast, elastic transformation, pixelation, and JPEG compression. The reported numbers for TTT-Online are the same as in Figure 1. See complete table in Table A2. 0 2000 4000 6000 8000 Number of samples 12 16 20 24 28 32 36 40 44 48Error (%) Gaussian Noise Joint training TTT TTT-Online UDA-SS 0 2000 4000 6000 8000 Number of samples 9 12 15 18 21 24 27 30 33 36Error (%) Shot Noise Joint training TTT TTT-Online UDA-SS 0 2000 4000 6000 8000 Number of samples 15 20 25 30 35 40 45 50Error (%) Impulse Noise Joint training TTT TTT-Online UDA-SS Figure 3.Test error (%) on CIFAR-10-C, for the three noise types, with gradually changing distribution.The distribution shifts are created by increasing the standard deviation of each noise type from small to large, the further we go on the x-axis. As the samples get noisier, all methods suffer greater errors the more we evaluate into the test set, but online Test-Time Training (TTT-Online) achieves gentler slopes than joint training. For the ﬁrst two noise types, TTT-Online also achieves better results over unsupervised domain adaptation by self-supervision (UDA-SS) (Sun et al., 2019). Comparison with unsupervised domain adaptation. Table 1 empirically compares online Test-Time Training (TTT-Online) with unsupervised domain adaptation through self-supervision (UDA-SS) (Sun et al., 2019), which is sim- ilar to our method in spirit but is designed for the setting of unsupervised domain adaptation (Section 5 provides a sur- vey of other related work in this setting). Given labeled data from the training distribution and unlabeled data from the test distribution, UDA-SS hopes to ﬁnd an invariant repre- sentation that extracts useful features for both distributions by learning to perform a self-supervised task, speciﬁcally rotation prediction, simultaneously on data from both. It then learns a labeling function on top of the invariant rep- resentation using the labeled data. In our experiments, the unlabeled data given to UDA-SS is the entire test set itself without the labels. Because TTT-Online can only learn from the unlabeled test samples that have already been evaluated on, it is given less information than UDA-SS at all times. In this sense, UDA- SS should be regarded as an oracle rather than a baseline. Surprisingly, TTT-Online outperforms UDA-SS on 13 out of the 15 corruptions as well as the original distribution. Our explanation is that UDA-SS has to ﬁnd an invariant representation for both distributions, while TTT-Online only adapts the representation to be good for the current test distribution. That is, TTT-Online has the ﬂexibility to forget the training distribution representation, which is no longer relevant. This suggests that in our setting, forgetting is not harmful and perhaps should even be taken advantage of. Gradually changing distribution shifts.In our previous experiments, we have been evaluating the online version under the assumption that the test inputs xt for t= 1...nare all sampled from the same test distribution Q, which can be different from the training distribution P. This assumption is indeed satisﬁed for i.i.d. samples from a shufﬂed test set. But here we show that this assumption can in fact be relaxed to allow xt ∼Qt, where Qt is close to Qt+1 (in the sense of distributional distance). We call this the assumption of gradually changing distribution shifts. We perform experiments by simulating such distribution shifts on the three noise types of CIFAR-10-C. For each noise type, xt is corrupted with standard deviation σt, and σ1,...,σ n interpolate between the standard deviation of level 1 and level 5. So xt is more severely corrupted as we evaluate further into the test set and t grows larger. As shown in Figure 3, TTT-Online still improves upon joint training (and our standard version) with this relaxed assumption, and even upon UDA-SS for the ﬁrst two noise types.Test-Time Training with Self-Supervision for Generalization under Distribution Shifts Accuracy (%) Airplane Bird Car Dog Cat Horse Ship Average Object recognition task only 67.9 35.8 42.6 14.7 52.0 42.0 66.7 41.4 Joint training (Hendrycks et al., 2019a) 70.2 36.7 42.6 15.5 52.0 44.0 66.7 42.4 TTT (standard version) 70.2 39.2 42.6 21.6 54.7 46.0 77.8 45.2 TTT-Online 70.2 39.2 42.6 22.4 54.7 46.0 77.8 45.4 Table 2.Class-wise and average classiﬁcation accuracy (%) on CIFAR classes in VID-Robust, adapted from (Shankar et al., 2019). Test-Time Training (TTT) and online Test-Time Training (TTT-Online) improve over the two baselines on average, and by a large margin on “ship” and “dog” classes where the rotation task is more meaningful than in classes like “airplane” (sample images in Figure A7). 3.2. Object Recognition on Video Frames The Robust ImageNet Video Classiﬁcation (VID-Robust) dataset was developed by Shankar et al. (2019) from the Ima- geNet Video detection dataset (Russakovsky et al., 2015), to demonstrate how deep models for object recognition trained on ImageNet (still images) fail to adapt well to video frames. The VID-Robust dataset contains 1109 sets of video frames in 30 classes; each set is a short video clip of frames that are similar to an anchor frame. Our results are reported on the anchor frames. To map the 1000 ImageNet classes to the 30 VID-Robust classes, we use the max-conversion function in Shankar et al. (2019). Without any modiﬁcations for videos, we apply our method to VID-Robust on top of the same ImageNet model as in the previous subsection. Our classiﬁcation accuracy is reported in Table 3. In addition, we take the seven classes in VID-Robust that overlap with CIFAR-10, and re-scale those video frames to the size of CIFAR-10 images, as a new test set for the model trained on CIFAR-10 in the previous subsection. Again, we apply our method to this dataset without any modiﬁcations. Our results are shown in Table 2, with a breakdown for each class. Noticing that Test-Time Training does not improve on the airplane class, we inspect some airplane samples (Figure A7), and observe black margins on two sides of most images, which provide a trivial hint for rotation prediction. In addition, given an image of airplanes in the sky, it is often impossible even for humans to tell if it is rotated. This shows that our method requires the self-supervised task to be both well deﬁned and non-trivial. 3.3. CIFAR-10.1: Unknown Distribution Shifts CIFAR-10.1 (Recht et al., 2018) is a new test set of size 2000 modeled after CIFAR-10, with the exact same classes and image dimensionality, following the dataset creation process documented by the original CIFAR-10 paper as closely as possible. The purpose is to investigate the distribution shifts present between the two test sets, and the effect on object recognition. All models tested by the authors suffer a large performance drop on CIFAR-10.1 comparing to CIFAR-10, even though there is no human noticeable difference, and Method Accuracy (%) Object recognition task only 62.7 Joint training (Hendrycks et al., 2019a) 63.5 TTT (standard version) 63.8 TTT-Online 64.3 Table 3.Test accuracy (%) on VID-Robust dataset (Shankar et al., 2019). TTT and TTT-Online improve over the baselines. Method Error (%) Object recognition task only 17.4 Joint training (Hendrycks et al., 2019a) 16.7 TTT (standard version) 15.9 Table 4.Test error (%) on CIFAR-10.1 (Recht et al., 2018). TTT is the ﬁrst method to improve the performance of an existing model on this new test set. both have the same human accuracy. This demonstrates how insidious and ubiquitous distribution shifts are, even when researchers strive to minimize them. The distribution shifts from CIFAR-10 to CIFAR-10.1 pose an extremely difﬁcult problem, and no prior work has been able to improve the performance of an existing model on this new test set, probably because: 1) researchers cannot even identify the distribution shifts, let alone describe them mathematically; 2) the samples in CIFAR-10.1 are only revealed at test time; and even if they were revealed during training, the distribution shifts are too subtle, and the sample size is too small, for domain adaptation (Recht et al., 2018). On the original CIFAR-10 test set, the baseline with only object recognition has error 8.9%, and with joint training has 8.1%; comparing to the ﬁrst two rows of Table 4, both suffer the typical performance drop (by a factor of two). TTT yields an improvement of 0.8% (relative improvement of 4.8%) over joint training. We recognize that this improve- ment is small relative to the performance drop, but see it as an encouraging ﬁrst step for this very difﬁcult problem.Test-Time Training with Self-Supervision for Generalization under Distribution Shifts 0 10 20 30 40 50 60 Gradient inner product 0 1 2 3 4 5Improvement (%) Level 5 Level 4 Level 3 Level 2 Level 1 0 10 20 30 40 50 60 Gradient inner product 0 5 10 15 20 25 30 35Improvement (%) Level 5 Level 4 Level 3 Level 2 Level 1 Figure 4.Scatter plot of the inner product between the gradients (on the shared feature extractor θe) of the main task lm and the self- supervised task le, and the improvement in test error (%) from Test-Time Training, for the standard (left) and online (right) version. Each point is the average over a test set, and each scatter plot has 75 test sets, from all 15 types of corruptions over ﬁve levels as described in subsection 3.1. The blue lines and bands are the best linear ﬁts and the 99% conﬁdence intervals. The linear correlation coefﬁcients are 0.93 and 0.89 respectively, indicating strong positive correlation between the two quantities, as suggested by Theorem 1. 4. Theoretical Results This section contains our preliminary study of when and why Test-Time Training is expected to work. For convex models, we prove that positive gradient correlation between the loss functions leads to better performance on the main task after Test-Time Training. Equipped with this insight, we then empirically demonstrate that gradient correlation governs the success of Test-Time Training on the deep learning model discussed in Section 3. Before stating our main theoretical result, we ﬁrst illustrate the general intuition with a toy model. Consider a regression problem where x∈Rd denotes the input, y1 ∈R denotes the label, and the objective is the square loss (ˆy−y1)2/2 for a prediction ˆy. Consider a two layer linear network parametrized by A∈Rh×d and v ∈Rh (where hstands for the hidden dimension). The prediction according to this model is ˆy= v⊤Ax, and the main task loss is lm(x,y1; A,v) = 1 2 ( y1 −v⊤Ax )2 . (4) In addition, consider a self-supervised regression task that also uses the square loss and automatically generates a label ys for x. Let the self-supervised head be parametrized by w∈Rh. Then the self-supervised task loss is ls(x,y2; A,w) = 1 2 ( y2 −w⊤Ax )2 . (5) Now we apply Test-Time Training to update the shared feature extractor Aby one step of gradient descent on ls, which we can compute with y2 known. This gives us A′←A−η ( y2 −w⊤Ax )( −wx⊤) , (6) where A′is the updated matrix and ηis the learning rate. If we set η= η∗where η∗= y1 −v⊤Ax (y2 −w⊤Ax) v⊤wx⊤x, (7) then with some simple algebra, it is easy to see that the main task loss lm(x,y1; A′,v) = 0. Concretely, Test-Time Training drives the main task loss down to zero with a single gradient step for a carefully chosen learning rate. In prac- tice, this learning rate is unknown since it depends on the unknown y1. However, since our model is convex, as long as η∗is positive, it sufﬁces to set η to be a small positive constant (see details in the appendix). If x̸= 0, one sufﬁ- cient condition for η∗to be positive (when neither loss is zero) is to have sign ( y1 −v⊤Ax ) = sign ( y2 −w⊤Ax ) (8) and v⊤w>0 . (9) For our toy model, both parts of the condition above have an intuition interpretation. The ﬁrst part says that the mistakes should be correlated, in the sense that predictions from both tasks are mistaken in the same direction. The second part, v⊤w>0, says that the decision boundaries on the feature space should be correlated. In fact, these two parts hold iff. ⟨∇lm(A),∇ls(A)⟩>0 (see a simple proof of this fact in the appendix). To summarize, if the gradients have positive correlation, Test-Time Training is guaranteed to reduce the main task loss. Our main theoretical result extends this to general smooth and convex loss functions.Test-Time Training with Self-Supervision for Generalization under Distribution Shifts Theorem 1. Let lm(x,y; θ) denote the main task loss on test instance x,y with parameters θ, and ls(x; θ) the self- supervised task loss that only depends onx. Assume that for all x,y, lm(x,y; θ) is differentiable, convex andβ-smooth in θ, and both ∥∇lm(x,y; θ)∥,∥∇ls(x,θ)∥≤ Gfor all θ. With a ﬁxed learning rate η= ϵ βG2 , for every x,y such that ⟨∇lm(x,y; θ),∇ls(x; θ)⟩>ϵ, (10) we have lm(x,y; θ) >lm(x,y; θ(x)), (11) where θ(x) = θ−η∇ls(x; θ) i.e. Test-Time Training with one step of gradient descent. The proof uses standard techniques in optimization, and is left for the appendix. Theorem 1 reveals gradient correlation as a determining factor of the success of Test-Time Training in the smooth and convex case. In Figure 4, we empirically show that our insight also holds for non-convex loss func- tions, on the deep learning model and across the diverse set of corruptions considered in Section 3; stronger gradient cor- relation clearly indicates more performance improvement over the baseline. 5. Related Work Learning on test instances. Shocher et al. (2018) pro- vide a key inspiration for our work by showing that image super-resolution could be learned at test time simply by try- ing to upsample a downsampled version of the input image. More recently, Bau et al. (2019) improve photo manipula- tion by adapting a pre-trained GAN to the statistics of the input image. One of the earlier examples of this idea comes from Jain & Learned-Miller (2011), who improve Viola- Jones face detection (Viola et al., 2001) by bootstrapping the more difﬁcult faces in an image from the more easily detected faces in that same image. The online version of our algorithm is inspired by the work of Mullapudi et al. (2018), which makes video segmentation more efﬁcient by using a student model that learns online from a teacher model. The idea of online updates has also been used in Kalal et al. (2011) for tracking and detection. A recent work in echocardiography (Zhu et al., 2019) improves the deep learning model that tracks myocardial motion and cardiac blood ﬂow with sequential updates. Lastly, we share the philosophy of transductive learning (Vapnik, 2013; Gam- merman et al., 1998), but have little in common with their classical algorithms; recent work by Tripuraneni & Mackey (2019) theoretically explores this for linear prediction, in the context of debiasing the LASSO estimator. Self-supervised learning studies how to create labels from the data, by designing various pretext tasks that can learn semantic information without human annotations, such as context prediction (Doersch et al., 2015), solving jig- saw puzzles (Noroozi & Favaro, 2016), colorization (Lars- son et al., 2017; Zhang et al., 2016), noise prediction (Bo- janowski & Joulin, 2017), feature clustering (Caron et al., 2018). Our paper uses rotation prediction (Gidaris et al., 2018). Asano et al. (2019) show that self-supervised learn- ing on only a single image, surprisingly, can produce low- level features that generalize well. Closely related to our work, Hendrycks et al. (2019a) propose that jointly training a main task and a self-supervised task (our joint training baseline in Section 3) can improve robustness on the main task. The same idea is used in few-shot learning (Su et al., 2019), domain generalization (Carlucci et al., 2019), and unsupervised domain adaptation (Sun et al., 2019). Adversarial robustness studies the robust risk RP,∆(θ) = Ex,y∼P maxδ∈∆ l(x + δ,y; θ), where l is some loss function, and ∆ is the set of perturbations; ∆ is often chosen as the Lp ball, for p ∈{1,2,∞}. Many popular algorithms formulate and solve this as a robust optimization problem (Goodfellow et al., 2014; Madry et al., 2017; Sinha et al., 2017; Raghunathan et al., 2018; Wong & Kolter, 2017; Croce et al., 2018), and the most well known technique is adversarial training. Another line of work is based on randomized smoothing (Cohen et al., 2019; Salman et al., 2019), while some other approaches, such as input transformations (Guo et al., 2017; Song et al., 2017), are shown to be less effective (Athalye et al., 2018). There are two main problems with the approaches above. First, all of them can be seen as smoothing the decision boundary. This establishes a theoretical tradeoff between accuracy and robustness (Tsipras et al., 2018; Zhang et al., 2019), which we also observe empirically with our adversarial training baseline in Section 3. Intuitively, the more diverse ∆ is, the less effective this one-boundary-ﬁts-all approach can be for a particular element of ∆. Second, adversarial methods rely heavily on the mathematical structure of ∆, which might not accurately model perturbations in the real world. Therefore, generalization remains hard outside of the ∆ we know in advance or can mathematically model, especially for non-adversarial distribution shifts. Empirically, Kang et al. (2019) shows that robustness for one ∆ might not transfer to another, and training on the L∞ball actually hurts robustness on the L1 ball. Non-adversarial robustness studies the effect of corrup- tions, perturbations, out-of-distribution examples, and real- world distribution shifts (Hendrycks et al., 2019b;a; 2018; Hendrycks & Gimpel, 2016). Geirhos et al. (2018) show that training on images corrupted by Gaussian noise makes deep learning models robust to this particular noise type, but does not improve performance on images corrupted by another noise type e.g. salt-and-pepper noise.Test-Time Training with Self-Supervision for Generalization under Distribution Shifts Unsupervised domain adaptation (a.k.a. transfer learn- ing) studies the problem of distribution shifts, when an unlabeled dataset from the test distribution (target domain) is available at training time, in addition to a labeled dataset from the training distribution (source domain) (Chen et al., 2011; Gong et al., 2012; Long et al., 2015; Ganin et al., 2016; Long et al., 2016; Tzeng et al., 2017; Hoffman et al., 2017; Csurka, 2017; Chen et al., 2018). The limitation of the problem setting, however, is that generalization might only be improved for this speciﬁc test distribution, which can be difﬁcult to anticipate in advance. Prior work try to anticipate broader distributions by using multiple and evolv- ing domains (Hoffman et al., 2018; 2012; 2014). Test-Time Training does not anticipate any test distribution, by chang- ing the setting of unsupervised domain adaptation, while taking inspiration from its algorithms. Our paper is a follow- up to Sun et al. (2019), which we explain and empirically compare with in Section 3. Our update rule can be viewed as performing one-sample unsupervised domain adaptation on the ﬂy, with the caveat that standard domain adaptation techniques might become ill-deﬁned when there is only one sample from the target domain. Domain generalization studies the setting where a meta distribution generates multiple environment distributions, some of which are available during training (source), while others are used for testing (target) (Li et al., 2018; Shankar et al., 2018; Muandet et al., 2013; Balaji et al., 2018; Ghifary et al., 2015; Motiian et al., 2017; Li et al., 2017a; Gan et al., 2016). With only a few environments, information on the meta distribution is often too scarce to be helpful, and with many environments, we are back to the i.i.d. setting where each environment can be seen as a sample, and a strong baseline is to simply train on all the environments (Li et al., 2019). The setting of domain generalization is limited by the inherent tradeoff between speciﬁcity and generality of a ﬁxed decision boundary, and the fact that generalization is again elusive outside of the meta distribution i.e. the actual P learned by the algorithm. One (few)-shot learning studies how to learn a new task or a new classiﬁcation category using only one (or a few) sample(s), on top of a general representation that has been learned on diverse samples (Snell et al., 2017; Vinyals et al., 2016; Fei-Fei et al., 2006; Ravi & Larochelle, 2016; Li et al., 2017b; Finn et al., 2017; Gidaris & Komodakis, 2018). Our update rule can be viewed as performing one-shot self- supervised learning and can potentially be improved by progress in one-shot learning. Continual learning (a.k.a. learning without forgetting) studies the setting where a model is made to learn a sequence of tasks, and not forget about the earlier ones while training for the later (Li & Hoiem, 2017; Lopez-Paz & Ranzato, 2017; Kirkpatrick et al., 2017; Santoro et al., 2016). In contrast, with Test-Time Training, we are not concerned about forgetting the past test samples since they have already been evaluated on; and if a past sample comes up by any chance, it would go through Test-Time Training again. In addition, the impact of forgetting the training set is minimal, because both tasks have already been jointly trained. Online learning (a.k.a. online optimization) is a well- studied area of learning theory (Shalev-Shwartz et al., 2012; Hazan et al., 2016). The basic setting repeats the following: receive xt, predict ˆyt, receive yt from a worst-case oracle, and learn. Final performance is evaluated using the regret, which colloquially translates to how much worse the online learning algorithm performs in comparison to the best ﬁxed model in hindsight. In contrast, our setting never reveals any yt during testing even for the online version, so we do not need to invoke the concept of the worst-case oracle or the regret. Also, due to the lack of feedback from the envi- ronment after predicting, our algorithm is motivated to learn (with self-supervision) before predicting ˆyt instead of after. Note that some of the previously covered papers (Hoffman et al., 2014; Jain & Learned-Miller, 2011; Mullapudi et al., 2018) use the term “online learning” outside of the learning theory setting, so the term can be overloaded. 6. Discussion The idea of test-time training also makes sense for other tasks, such as segmentation and detection, and in other ﬁelds, such as speech recognition and natural language process- ing. For machine learning practitioners with prior domain knowledge in their respective ﬁelds, their expertise can be leveraged to design better special-purpose self-supervised tasks for test-time training. Researchers for general-purpose self-supervised tasks can also use test-time training as an evaluation benchmark, in addition to the currently prevalent benchmark of pre-training and ﬁne-tuning. More generally, we hope this paper can encourage re- searchers to abandon the self-imposed constraint of a ﬁxed decision boundary for testing, or even the artiﬁcial division between training and testing altogether. Our work is but a small step toward a new paradigm where much of the learning happens after a model is deployed. Acknowledgements. This work is supported by NSF grant 1764033, DARPA and Berkeley DeepDrive. This paper took a long time to develop, and beneﬁted from con- versations with many of our colleagues, including Ben Recht and his students Ludwig Schmidt, Vaishaal Shanker and Becca Roelofs; Ravi Teja Mullapudi, Achal Dave and Deva Ramanan; and Armin Askari, Allan Jabri, Ashish Kumar, Angjoo Kanazawa and Jitendra Malik.Test-Time Training with Self-Supervision for Generalization under Distribution Shifts References Asano, Y . M., Rupprecht, C., and Vedaldi, A. Surprising effectiveness of few-image unsupervised feature learning. arXiv preprint arXiv:1904.13132, 2019. Athalye, A., Carlini, N., and Wagner, D. Obfuscated gradients give a false sense of security: Circumvent- ing defenses to adversarial examples. arXiv preprint arXiv:1802.00420, 2018. Balaji, Y ., Sankaranarayanan, S., and Chellappa, R. Metareg: Towards domain generalization using meta-regularization. In Advances in Neural Information Processing Systems, pp. 998–1008, 2018. Bau, D., Strobelt, H., Peebles, W., Wulff, J., Zhou, B., Zhu, J.-Y ., and Torralba, A. Semantic photo manipulation with a generative image prior. ACM Transactions on Graphics (TOG), 38(4):59, 2019. Bojanowski, P. and Joulin, A. Unsupervised learning by predicting noise. In Proceedings of the 34th International Conference on Machine Learning-Volume 70, pp. 517– 526. JMLR. org, 2017. Carlucci, F. M., D’Innocente, A., Bucci, S., Caputo, B., and Tommasi, T. Domain generalization by solving jigsaw puzzles. In Proceedings of the IEEE Conference on Com- puter Vision and Pattern Recognition , pp. 2229–2238, 2019. Caron, M., Bojanowski, P., Joulin, A., and Douze, M. Deep clustering for unsupervised learning of visual features. In Proceedings of the European Conference on Computer Vision (ECCV), pp. 132–149, 2018. Caruana, R. Multitask learning. Machine learning, 28(1): 41–75, 1997. Chen, M., Weinberger, K. Q., and Blitzer, J. Co-training for domain adaptation. In Advances in neural information processing systems, pp. 2456–2464, 2011. Chen, X., Sun, Y ., Athiwaratkun, B., Cardie, C., and Wein- berger, K. Adversarial deep averaging networks for cross- lingual sentiment classiﬁcation. Transactions of the Asso- ciation for Computational Linguistics, 6:557–570, 2018. Cohen, J. M., Rosenfeld, E., and Kolter, J. Z. Certiﬁed adversarial robustness via randomized smoothing. arXiv preprint arXiv:1902.02918, 2019. Croce, F., Andriushchenko, M., and Hein, M. Provable robustness of relu networks via maximization of linear regions. arXiv preprint arXiv:1810.07481, 2018. Csurka, G. Domain adaptation for visual applications: A comprehensive survey. arXiv preprint arXiv:1702.05374, 2017. Ding, G. W., Wang, L., and Jin, X. AdverTorch v0.1: An adversarial robustness toolbox based on pytorch. arXiv preprint arXiv:1902.07623, 2019. Doersch, C., Gupta, A., and Efros, A. A. Unsupervised visual representation learning by context prediction. In Proceedings of the IEEE International Conference on Computer Vision, pp. 1422–1430, 2015. Fei-Fei, L., Fergus, R., and Perona, P. One-shot learning of object categories. IEEE transactions on pattern analysis and machine intelligence, 28(4):594–611, 2006. Finn, C., Abbeel, P., and Levine, S. Model-agnostic meta- learning for fast adaptation of deep networks. In Proceed- ings of the 34th International Conference on Machine Learning-Volume 70, pp. 1126–1135. JMLR. org, 2017. Gammerman, A., V ovk, V ., and Vapnik, V . Learning by transduction. In Proceedings of the Fourteenth conference on Uncertainty in artiﬁcial intelligence , pp. 148–155. Morgan Kaufmann Publishers Inc., 1998. Gan, C., Yang, T., and Gong, B. Learning attributes equals multi-source domain generalization. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pp. 87–97, 2016. Ganin, Y ., Ustinova, E., Ajakan, H., Germain, P., Larochelle, H., Laviolette, F., Marchand, M., and Lempitsky, V . Domain-adversarial training of neural networks. The Journal of Machine Learning Research, 17(1):2096–2030, 2016. Geirhos, R., Temme, C. R., Rauber, J., Sch¨utt, H. H., Bethge, M., and Wichmann, F. A. Generalisation in humans and deep neural networks. In Advances in Neural Information Processing Systems, pp. 7538–7550, 2018. Ghifary, M., Bastiaan Kleijn, W., Zhang, M., and Balduzzi, D. Domain generalization for object recognition with multi-task autoencoders. In Proceedings of the IEEE international conference on computer vision, pp. 2551– 2559, 2015. Gidaris, S. and Komodakis, N. Dynamic few-shot visual learning without forgetting. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pp. 4367–4375, 2018. Gidaris, S., Singh, P., and Komodakis, N. Unsupervised rep- resentation learning by predicting image rotations. arXiv preprint arXiv:1803.07728, 2018. Gong, B., Shi, Y ., Sha, F., and Grauman, K. Geodesic ﬂow kernel for unsupervised domain adaptation. In2012 IEEE Conference on Computer Vision and Pattern Recognition, pp. 2066–2073. IEEE, 2012.Test-Time Training with Self-Supervision for Generalization under Distribution Shifts Goodfellow, I. J., Shlens, J., and Szegedy, C. Explain- ing and harnessing adversarial examples. arXiv preprint arXiv:1412.6572, 2014. Guo, C., Rana, M., Cisse, M., and van der Maaten, L. Coun- tering adversarial images using input transformations. arXiv preprint arXiv:1711.00117, 2017. Hazan, E. et al. Introduction to online convex optimization. Foundations and Trends® in Optimization, 2(3-4):157– 325, 2016. He, K., Zhang, X., Ren, S., and Sun, J. Deep residual learn- ing for image recognition. In Proceedings of the IEEE conference on computer vision and pattern recognition, pp. 770–778, 2016a. He, K., Zhang, X., Ren, S., and Sun, J. Identity mappings in deep residual networks. In European conference on computer vision, pp. 630–645. Springer, 2016b. He, K., Girshick, R., and Doll ´ar, P. Rethinking imagenet pre-training. arXiv preprint arXiv:1811.08883, 2018. Hendrycks, D. and Dietterich, T. Benchmarking neural network robustness to common corruptions and perturba- tions. arXiv preprint arXiv:1903.12261, 2019. Hendrycks, D. and Gimpel, K. A baseline for detecting misclassiﬁed and out-of-distribution examples in neural networks. arXiv preprint arXiv:1610.02136, 2016. Hendrycks, D., Mazeika, M., Wilson, D., and Gimpel, K. Using trusted data to train deep networks on labels cor- rupted by severe noise. InAdvances in neural information processing systems, pp. 10456–10465, 2018. Hendrycks, D., Lee, K., and Mazeika, M. Using pre-training can improve model robustness and uncertainty. arXiv preprint arXiv:1901.09960, 2019a. Hendrycks, D., Mazeika, M., Kadavath, S., and Song, D. Improving model robustness and uncertainty estimates with self-supervised learning. arXiv preprint, 2019b. Hoffman, J., Kulis, B., Darrell, T., and Saenko, K. Discover- ing latent domains for multisource domain adaptation. In European Conference on Computer Vision, pp. 702–715. Springer, 2012. Hoffman, J., Darrell, T., and Saenko, K. Continuous man- ifold based adaptation for evolving visual domains. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pp. 867–874, 2014. Hoffman, J., Tzeng, E., Park, T., Zhu, J.-Y ., Isola, P., Saenko, K., Efros, A. A., and Darrell, T. Cycada: Cycle- consistent adversarial domain adaptation. arXiv preprint arXiv:1711.03213, 2017. Hoffman, J., Mohri, M., and Zhang, N. Algorithms and theory for multiple-source adaptation. In Advances in Neural Information Processing Systems, pp. 8246–8256, 2018. Huang, G., Sun, Y ., Liu, Z., Sedra, D., and Weinberger, K. Q. Deep networks with stochastic depth. In European conference on computer vision, pp. 646–661. Springer, 2016. Ioffe, S. and Szegedy, C. Batch normalization: Accelerating deep network training by reducing internal covariate shift. arXiv preprint arXiv:1502.03167, 2015. Jain, V . and Learned-Miller, E. Online domain adaptation of a pre-trained cascade of classiﬁers. In CVPR 2011, pp. 577–584. IEEE, 2011. Kalal, Z., Mikolajczyk, K., and Matas, J. Tracking-learning- detection. IEEE transactions on pattern analysis and machine intelligence, 34(7):1409–1422, 2011. Kang, D., Sun, Y ., Brown, T., Hendrycks, D., and Steinhardt, J. Transfer of adversarial robustness between perturbation types. arXiv preprint arXiv:1905.01034, 2019. Kannan, H., Kurakin, A., and Goodfellow, I. Adversarial logit pairing. arXiv preprint arXiv:1803.06373, 2018. Kirkpatrick, J., Pascanu, R., Rabinowitz, N., Veness, J., Des- jardins, G., Rusu, A. A., Milan, K., Quan, J., Ramalho, T., Grabska-Barwinska, A., et al. Overcoming catastrophic forgetting in neural networks. Proceedings of the national academy of sciences, 114(13):3521–3526, 2017. Krizhevsky, A. and Hinton, G. Learning multiple layers of features from tiny images. Technical report, Citeseer, 2009. Larsson, G., Maire, M., and Shakhnarovich, G. Colorization as a proxy task for visual understanding. In CVPR, 2017. Li, D., Yang, Y ., Song, Y .-Z., and Hospedales, T. M. Deeper, broader and artier domain generalization. In Proceed- ings of the IEEE International Conference on Computer Vision, pp. 5542–5550, 2017a. Li, D., Zhang, J., Yang, Y ., Liu, C., Song, Y .-Z., and Hospedales, T. M. Episodic training for domain gen- eralization. arXiv preprint arXiv:1902.00113, 2019. Li, Y ., Tian, X., Gong, M., Liu, Y ., Liu, T., Zhang, K., and Tao, D. Deep domain generalization via conditional invariant adversarial networks. In Proceedings of the European Conference on Computer Vision (ECCV), pp. 624–639, 2018.Test-Time Training with Self-Supervision for Generalization under Distribution Shifts Li, Z. and Hoiem, D. Learning without forgetting. IEEE transactions on pattern analysis and machine intelligence, 40(12):2935–2947, 2017. Li, Z., Zhou, F., Chen, F., and Li, H. Meta-sgd: Learning to learn quickly for few-shot learning. arXiv preprint arXiv:1707.09835, 2017b. Liu, Z., Sun, M., Zhou, T., Huang, G., and Darrell, T. Re- thinking the value of network pruning. arXiv preprint arXiv:1810.05270, 2018. Long, M., Cao, Y ., Wang, J., and Jordan, M. I. Learn- ing transferable features with deep adaptation networks. arXiv preprint arXiv:1502.02791, 2015. Long, M., Zhu, H., Wang, J., and Jordan, M. I. Unsupervised domain adaptation with residual transfer networks. In Advances in Neural Information Processing Systems, pp. 136–144, 2016. Lopez-Paz, D. and Ranzato, M. Gradient episodic memory for continual learning. In Advances in Neural Information Processing Systems, pp. 6467–6476, 2017. Madry, A., Makelov, A., Schmidt, L., Tsipras, D., and Vladu, A. Towards deep learning models resistant to adversarial attacks. arXiv preprint arXiv:1706.06083 , 2017. Motiian, S., Piccirilli, M., Adjeroh, D. A., and Doretto, G. Uniﬁed deep supervised domain adaptation and gen- eralization. In Proceedings of the IEEE International Conference on Computer Vision, pp. 5715–5725, 2017. Muandet, K., Balduzzi, D., and Sch ¨olkopf, B. Domain generalization via invariant feature representation. In International Conference on Machine Learning, pp. 10– 18, 2013. Mullapudi, R. T., Chen, S., Zhang, K., Ramanan, D., and Fatahalian, K. Online model distillation for efﬁcient video inference. arXiv preprint arXiv:1812.02699, 2018. Noroozi, M. and Favaro, P. Unsupervised learning of visual representations by solving jigsaw puzzles. In European Conference on Computer Vision , pp. 69–84. Springer, 2016. Raghunathan, A., Steinhardt, J., and Liang, P. Certiﬁed defenses against adversarial examples. arXiv preprint arXiv:1801.09344, 2018. Ravi, S. and Larochelle, H. Optimization as a model for few-shot learning. IEEE transactions on pattern analysis and machine intelligence, 2016. Recht, B., Roelofs, R., Schmidt, L., and Shankar, V . Do cifar-10 classiﬁers generalize to cifar-10? arXiv preprint arXiv:1806.00451, 2018. Russakovsky, O., Deng, J., Su, H., Krause, J., Satheesh, S., Ma, S., Huang, Z., Karpathy, A., Khosla, A., Bernstein, M., Berg, A. C., and Fei-Fei, L. ImageNet Large Scale Visual Recognition Challenge. International Journal of Computer Vision (IJCV) , 115(3):211–252, 2015. doi: 10.1007/s11263-015-0816-y. Salman, H., Yang, G., Li, J., Zhang, P., Zhang, H., Razen- shteyn, I., and Bubeck, S. Provably robust deep learn- ing via adversarially trained smoothed classiﬁers. arXiv preprint arXiv:1906.04584, 2019. Santoro, A., Bartunov, S., Botvinick, M., Wierstra, D., and Lillicrap, T. Meta-learning with memory-augmented neu- ral networks. In International conference on machine learning, pp. 1842–1850, 2016. Shalev-Shwartz, S. et al. Online learning and online con- vex optimization. Foundations and Trends® in Machine Learning, 4(2):107–194, 2012. Shankar, S., Piratla, V ., Chakrabarti, S., Chaudhuri, S., Jyothi, P., and Sarawagi, S. Generalizing across domains via cross-gradient training. arXiv preprint arXiv:1804.10745, 2018. Shankar, V ., Dave, A., Roelofs, R., Ramanan, D., Recht, B., and Schmidt, L. Do image classiﬁers generalize across time? arXiv, 2019. Shocher, A., Cohen, N., and Irani, M. zero-shot super- resolution using deep internal learning. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pp. 3118–3126, 2018. Sinha, A., Namkoong, H., and Duchi, J. Certifying some dis- tributional robustness with principled adversarial training. arXiv preprint arXiv:1710.10571, 2017. Snell, J., Swersky, K., and Zemel, R. Prototypical networks for few-shot learning. In Advances in Neural Information Processing Systems, pp. 4077–4087, 2017. Song, Y ., Kim, T., Nowozin, S., Ermon, S., and Kushman, N. Pixeldefend: Leveraging generative models to understand and defend against adversarial examples. arXiv preprint arXiv:1710.10766, 2017. Su, J.-C., Maji, S., and Hariharan, B. Boosting supervi- sion with self-supervision for few-shot learning. arXiv preprint arXiv:1906.07079, 2019. Sun, Y ., Tzeng, E., Darrell, T., and Efros, A. A. Unsuper- vised domain adaptation through self-supervision. arXiv preprint, 2019.Test-Time Training with Self-Supervision for Generalization under Distribution Shifts Tripuraneni, N. and Mackey, L. Debiasing linear prediction. arXiv preprint arXiv:1908.02341, 2019. Tsipras, D., Santurkar, S., Engstrom, L., Turner, A., and Madry, A. Robustness may be at odds with accuracy. arXiv preprint arXiv:1805.12152, 2018. Tzeng, E., Hoffman, J., Saenko, K., and Darrell, T. Adver- sarial discriminative domain adaptation. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pp. 7167–7176, 2017. Vapnik, V .The nature of statistical learning theory. Springer science & business media, 2013. Vinyals, O., Blundell, C., Lillicrap, T., Wierstra, D., et al. Matching networks for one shot learning. In Advances in neural information processing systems, pp. 3630–3638, 2016. Viola, P., Jones, M., et al. Rapid object detection using a boosted cascade of simple features. CVPR (1), 1(511- 518):3, 2001. Wong, E. and Kolter, J. Z. Provable defenses against adver- sarial examples via the convex outer adversarial polytope. arXiv preprint arXiv:1711.00851, 2017. Zhang, H., Yu, Y ., Jiao, J., Xing, E. P., Ghaoui, L. E., and Jor- dan, M. I. Theoretically principled trade-off between ro- bustness and accuracy. arXiv preprint arXiv:1901.08573, 2019. Zhang, R., Isola, P., and Efros, A. A. Colorful image col- orization. In European conference on computer vision, pp. 649–666. Springer, 2016. Zhu, W., Huang, Y ., Vannan, M. A., Liu, S., Xu, D., Fan, W., Qian, Z., and Xie, X. Neural multi-scale self-supervised registration for echocardiogram dense tracking. arXiv preprint arXiv:1906.07357, 2019.Appendix: Test-Time Training with Self-Supervision for Generalization under Distribution Shifts A1. Informal Discussion on Our Variable Decision Boundary In the introduction, we claim that in traditional supervised learning θgives a ﬁxed decision boundary, while ourθgives a variable decision boundary. Here we informally discuss this claim. Denote the input space Xand output space Y. A decision boundary is simply a mapping f : X →Y. Let Θ be a model class e.g Rd. Now consider a family of parametrized functions gθ : X→Y , where θ∈Θ. In the context of deep learning, gis the neural network architecture and θcontains the parameters. We say that f is a ﬁxed decision boundary w.r.t. g and Θ if there exists θ ∈Θ s.t. f(x) = gθ(x) for every x ∈X , and a variable decision boundary if for every x∈X, there exists θ∈Θ s.t. f(x) = gθ(x). Note how selection of θcan depend on xfor a variable decision boundary, and cannot for a ﬁxed one. It is then trivial to verify that our claim is true under those deﬁnitions. A critical reader might say that with an arbitrarily large model class, can’t every decision boundary be ﬁxed? Yes, but this is not the end of the story. Let d = dim( X) × dim(Y), and consider the enormous model class Θ′= Rd which is capable of representing all possible mappings be- tween Xand Y. Let g′ θ′ simply be the mapping represented by θ′ ∈Θ′. A variable decision boundary w.r.t. g and Θ then indeed must be a ﬁxed decision boundary w.r.t. g′and Θ′, but we would like to note two things. First, without any prior knowledge, generalization in Θ′is impossible with any ﬁnite amount of training data; reasoning about g′and Θ′is most likely not productive from an algorithmic point of view, and the concept of a variable decision boundary is to avoid such reasoning. Second, selecting θbased on xfor a variable decision boundary can be thought of as “training” on all points x ∈Rd; however, “training” only happens when necessary, for the xthat it actually encounters. Altogether, the concept of a variable decision boundary is different from what can be described by traditional learning theory. A formal discussion is beyond the scope of this paper and might be of interest to future work. A2. Computational Aspects of Our Method At test time, our method is 2 × batch size × number of iterations times slower than regular test- ing, which only performs a single forward pass for each sample. As the ﬁrst work on Test-Time Training, this paper is not as concerned about computational efﬁciency as improving robustness, but here we provide two poten- tial solutions that might be useful, but have not been thor- oughly veriﬁed. The ﬁrst is to use the thresholding trick on ls, introduced as a solution for the small batches prob- lem in the method section. For the models considered in our experiments, roughly 80% of the test instances fall below the threshold, so Test-Time Training can only be performed on the other 20% without much effect on per- formance, because those 20% contain most of the sam- ples with wrong predictions. The second is to reduce the number of iterations of test-time updates. For the online version, the number of iterations is al- ready 1, so there is nothing to do. For the standard ver- sion, we have done some preliminary experiments setting number of iterations to 1 (instead of 10) and learn- ing rate to 0.01 (instead of 0.001), and observing results almost as good as the standard hyper-parameter setting. A more in depth discussion on efﬁciency is left for future works, which might, during training, explicitly make the model amenable to fast updates. A3. Proofs Here we prove the theoretical results in the main paper. A3.1. The Toy Problem The following setting applies to the two lemmas; this is simply the setting of our toy problem, reproduced here for ease of reference.Test-Time Training with Self-Supervision for Generalization under Distribution Shifts Consider a two layer linear network parametrized by A∈ Rh×d (shared) and v,w ∈Rh (ﬁxed) for the two heads, respectively. Denote x∈Rd the input and y1,y2 ∈R the labels for the two tasks, respectively. For the main task loss lm(A; v) = 1 2 ( y1 −v⊤Ax )2 , (12) and the self-supervised task loss ls(A; w) = 1 2 ( y2 −w⊤Ax )2 , (13) Test-Time Training yields an updated matrix A′←A−η ( y2 −w⊤Ax )( −wx⊤) , (14) where ηis the learning rate. Lemma 1. Following the exposition of the main paper, let η∗= (y1 −v⊤Ax) (y2 −w⊤Ax)v⊤wx⊤x. (15) Assume η∗∈[ϵ,∞) for some ϵ> 0. Then for any η∈(0,ϵ], we are guaranteed an improvement on the main loss i.e. lm(A′) <lm(A). Proof. From the exposition of the main paper, we know that lm(A−η∗∇lsA)) = 0, which can also be derived from simple algebra. Then by convexity, we have lm(A−η∇ls(A)) (16) = lm (( 1 − η η∗ ) A+ η η∗(A−η∗∇ls(A)) ) (17) ≤ ( 1 − η η∗ ) lm(A) + 0 (18) ≤ ( 1 −η ϵ ) lm(A) (19) <lm(A), (20) where the last inequality uses the assumption that lm(A) > 0, which holds because η∗>0. Lemma 2. Deﬁne ⟨U,V⟩= vec (U)⊤vec (V) i.e. the Frobenious inner product, then sign (η∗) = sign (⟨∇lm(A),∇ls(A)⟩) . (21) Proof. By simple algebra, ⟨∇lm(A),∇ls(A)⟩ = ⟨ ( y1 −v⊤Ax )( −vx⊤) , ( y2 −w⊤Ax )( −wx⊤) ⟩ = ( y1 −v⊤Ax )( y2 −w⊤Ax ) Tr ( xv⊤wx⊤) = ( y1 −v⊤Ax )( y2 −w⊤Ax ) v⊤wx⊤x, which has the same sign as η∗. A3.2. Proof of Theorem 1 For any η, by smoothness and convexity, lm(x,y; θ(x)) = lm(x,y; θ−η∇ls(x; θ)) ≤lm(x,y; θ) + η⟨∇lm(x,y; θ),∇ls(x,θ)⟩ + η2β 2 ∥∇ls(x; θ)∥2 . Denote η∗= ⟨∇lm(x,y; θ),∇ls(x,θ)⟩ β∥∇ls(x; θ)∥2 . Then Equation 22 becomes lm(x,y; θ−η∗∇ls(x; θ)) (22) ≤lm(x,y; θ) −⟨∇lm(x,y; θ),∇ls(x,θ)⟩2 2β∥∇ls(x; θ)∥2 . (23) And by our assumptions on the gradient norm and gradient inner product, lm(x,y; θ) −lm(x,y; θ−η∗∇ls(x; θ)) ≥ ϵ2 2βG2 . (24) Because we cannot observe η∗in practice, we instead use a ﬁxed learning rate η = ϵ βG2 , as stated in Theorem 1. Now we argue that this ﬁxed learning rate still improves performance on the main task. By our assumptions, η∗ ≥ ϵ βG2 , so η ∈(0,η∗]. Denote g= ∇ls(x; θ), then by convexity of lm, lm(x,y; θ(x)) = lm(x,y; θ−ηg) (25) = lm ( x,y; ( 1 − η η∗ ) θ+ η η∗(θ−η∗g) ) (26) ≤ ( 1 − η η∗ ) lm(x,y; θ) + η η∗lm(x,y; θ−η∗g) (27) Combining with Equation 24, we have lm(x,y; θ(x)) ≤ ( 1 − η η∗ ) lm(x,y; θ) + η η∗ ( lm(x,y; θ) − ϵ2 2βG2 ) = lm(x,y; θ) − η η∗ ϵ2 2βG2 Since η/η∗>0, we have shown that lm(x,y; θ) −lm(x,y; θ(x)) >0. (28)Test-Time Training with Self-Supervision for Generalization under Distribution Shifts A4. Additional Results on the Common Corruptions Dataset For table aethetics, we use the following abbreviations: B for baseline, JT for joint training, TTT for Test-Time Train- ing standard version, and TTT-Online for online Test-Time Training i.e. the online version. We have abbreviated the names of the corruptions, in order: original test set, Gaussian noise, shot noise, impulse noise, defocus blur, glass blue, motion blur, zoom blur, snow, frost, fog, brightness, contrast, elastic transformation, pixelation, and JPEG compression. A4.1. Results Using Batch Normalization As discussed in the results section, Batch Normalization (BN) is ineffective for small batches, which are the inputs for Test-Time Training (both standard and online version) since there is only one sample available when forming each batch; therefore, our main results are based on a ResNet using Group Normalization (GN). Figure A2 and Table A1 show results of our method on CIFAR-10-C level 5, with a ResNet using Batch Normalization (BN). These results are only meant to be a point of reference for the curious readers. In the early stage of this project, we have experimented with two potential solutions to the small batches problem with BN. The naive solution is to ﬁx the BN layers during Test-Time Training. but this diminishes the performance gains since there are fewer shared parameters. The better solution, adopted for the results below, is hard example mining: instead of updating on all inputs, we only update on inputs that incur large self-supervised task loss ls, where the large improvements might counter the negative effects of inaccurate statistics. Test-Time Training (standard version) is still very effective with BN. In fact, some of the improvements are quite dra- matic, such as on contrast (34%), defocus blue (18%) and Gaussian noise (22% comparing to joint-training, and 16% comparing to the baseline). Performance on the original distribution is still almost the same, and the original error with BN is in fact slightly lower than with GN, and takes half as many epochs to converge. We did not further experiment with BN because of two rea- sons: 1) The online version does not work with BN, because the problem with inaccurate batch statistics is exacerbated when training online for many (e.g. 10000) steps. 2) The baseline error for almost every corruption type is signiﬁ- cantly higher with BN than with GN. Although unrelated to the main idea of our paper, we make the interesting note that GN signiﬁcantly improves model robustness. A4.2. Additional Baseline: Adversarial Logit Pairing As discussed in the results section, Hendrycks & Dietterich (2019) point to Adversarial Logit Pairing (ALP) (Kannan et al., 2018) as an effective method for improving model robustness to corruptions and perturbations, even though it was designed to defend against adversarial attacks. We take ALP as an additional baseline on all benchmarks based on CIFAR-10 (using GN), following the training proce- dure in Kannan et al. (2018) and their recommended hyper- parameters. The implementation of the adversarial attack comes from the codebase of Ding et al. (2019). We did not run ALP on ImageNet because the two papers we reference for this method, Kannan et al. (2018) and Hendrycks & Di- etterich (2019), did not run on ImageNet or make any claim or recommendation. A4.3. Results on CIFAR-10-C and ImageNet-C, Level 5 Table A2 and Table A3 correspond to the bar plots in the results section. Two rows of Table A2 have been presented as Table 1 in the main text. A4.4. Results on CIFAR-10-C, Levels 1-4 The following bar plots and tables are on levels 1-4 of CIFAR-10-C. The original distribution is the same for all levels, so are our results on the original distribution. A4.5. Direct Comparison with Hendrycks et al. (2019a) The following comparison has been requested by an anony- mous reviewer for our ﬁnal version. Our joint training baseline is based on Hendrycks et al. (2019a), but also incor- porates some architectural changes (see below). We found these changes improved the robustness of our method, and felt that it was important to give the baseline the same ben- eﬁt. Note that our joint training baseline overall performs better than Hendrycks: Compare Table S2 to Figure 3 of Hendrycks et al. (2019a) (provided by the authors), our baseline has average error of 22.8% across all corruptions and levels, while their average error is 28.6%. Summary of architectural changes: 1) Group Normalization (GN) instead of Batch Normalization (BN). For complete- ness, the results with BN are provided in Table S1; c.f. GN results in Table S2 which signiﬁcantly improves robustness, with or without self-supervision. 2) We split after the sec- ond residual group, while they split after the third residual group right before the linear layer. This consistently gives about 0.5% - 1% improvement. 3) We use a ResNet-26, while they use a 40-2 Wide ResNet. But our baseline still performs better than their method even though our network is 4x smaller, due to the two tricks above.Test-Time Training with Self-Supervision for Generalization under Distribution Shifts Gaussian Noise  Shot Noise  Impulse Noise  Defocus Blur  Frosted Glass Blur Motion Blur  Zoom Blur  Snow  Frost  Fog Brightness  Contrast  Elastic  Pixelate  JPEG Figure A1.Sample images from the Common Corruptions Benchmark, taken from the original paper by Hendrycks & Dietterich (2019). originalgauss shot impulsedefocus glass motion zoom snow frost fog bright contrastelasticpixelate jpeg 0 20 40 60Error (%) Object recognition task only Joint training (Hendrycks et al. 2019) TTT Figure A2.Test error (%) on CIFAR-10-C, level 5, ResNet-26 with Batch Normalization. orig gauss shot impul defoc glass motn zoom snow frost fog brit contr elast pixel jpeg B 7.9 63.9 58.8 64.3 46.3 54.6 41.6 45.9 31.9 44.0 37.5 13.0 69.2 33.8 61.4 31.7 JT 7.5 70.7 65.6 67.2 43.1 55.4 40.9 42.7 30.3 44.5 42.5 12.7 58.6 30.7 62.6 31.9 TTT 7.9 47.9 45.2 54.8 27.6 50.4 31.5 30.9 28.7 34.3 26.9 12.6 35.2 30.6 51.2 31.3 Table A1.Test error (%) on CIFAR-10-C, level 5, ResNet-26 with Batch Normalization. orig gauss shot impul defoc glass motn zoom snow frost fog brit contr elast pixel jpeg B 8.9 50.5 47.2 56.1 23.7 51.7 24.3 26.3 25.6 34.4 28.1 13.5 25.0 27.4 55.8 29.8 JT 8.1 49.4 45.3 53.4 24.2 48.5 24.8 26.4 25.0 32.5 27.5 12.6 25.3 24.0 51.6 28.7 TTT 7.9 45.6 41.8 50.0 21.8 46.1 23.0 23.9 23.9 30.0 25.1 12.2 23.9 22.6 47.2 27.2 TTT-Online 8.2 25.8 22.6 30.6 14.6 34.4 18.3 17.1 20.0 18.0 16.9 11.2 15.6 21.6 18.1 21.2 UDA-SS 9.0 28.2 26.5 20.8 15.6 43.7 24.5 23.8 25.0 24.9 17.2 12.7 11.6 22.1 20.3 22.6 ALP 16.5 22.7 22.9 28.3 25.0 25.6 27.4 23.1 25.2 27.2 64.8 21.7 73.6 23.0 20.2 18.9 Table A2.Test error (%) on CIFAR-10-C, level 5, ResNet-26. orig gauss shot impul defoc glass motn zoom snow frost fog brit contr elast pixel jpeg B 68.9 1.3 2.0 1.3 7.5 6.6 11.8 16.2 15.7 14.9 15.3 43.9 9.7 16.5 15.3 23.4 JT 69.1 2.1 3.1 2.1 8.7 6.7 12.3 16.0 15.3 15.8 17.0 45.3 11.0 18.4 19.7 22.9 TTT 69.0 3.1 4.5 3.5 10.1 6.8 13.5 18.5 17.1 17.9 20.0 47.0 14.4 20.9 22.8 25.3 TTT-Online 68.8 26.3 28.6 26.9 23.7 6.6 28.7 33.4 35.6 18.7 47.6 58.3 35.3 44.3 47.8 44.3 Table A3.Test accuracy (%) on ImageNet-C, level 5, ResNet-18.Test-Time Training with Self-Supervision for Generalization under Distribution Shifts originalgauss shot impulsedefocus glass motion zoom snow frost fog bright contrastelasticpixelate jpeg 0 10 20 30 40 50Error (%) Object recognition task only Joint training (Hendrycks et al. 2019) TTT TTT-Online Figure A3.Test error (%) on CIFAR-10-C, level 4. See the results section for details. orig gauss shot impul defoc glass motn zoom snow frost fog brit contr elast pixel jpeg B 8.9 46.4 39.2 44.8 15.3 52.5 19.1 20.5 21.3 26.9 13.3 10.5 13.7 20.8 35.3 26.9 JT 8.1 45.0 38.3 42.2 16.4 50.2 20.7 20.5 21.1 25.4 14.1 10.0 14.7 19.0 33.2 25.1 TTT 7.9 41.5 35.4 39.8 15.0 47.8 19.1 18.4 20.1 24.0 13.5 10.0 14.1 17.7 29.4 24.5 TTT-Online 8.2 22.9 20.0 23.9 11.2 35.1 15.6 13.8 18.6 15.9 12.3 9.7 11.9 16.7 13.6 19.8 ALP 16.5 21.3 20.5 24.5 20.7 25.9 23.7 21.4 24.2 23.9 42.2 17.5 53.7 22.1 19.1 18.5 Table A4.Test error (%) on CIFAR-10-C, level 4, ResNet-26. originalgauss shot impulsedefocus glass motion zoom snow frost fog bright contrastelasticpixelate jpeg 0 10 20 30 40Error (%) Object recognition task only Joint training (Hendrycks et al. 2019) TTT TTT-Online Figure A4.Test error (%) on CIFAR-10-C, level 3. See the results section for details. orig gauss shot impul defoc glass motn zoom snow frost fog brit contr elast pixel jpeg B 8.9 42.2 35.1 30.7 12.2 41.7 18.6 17.5 19.0 25.3 10.8 9.7 11.6 15.3 21.7 24.6 JT 8.1 40.2 34.4 29.9 12.2 37.9 20.8 17.3 18.4 25.0 11.4 9.2 12.0 15.2 20.8 22.8 TTT 7.9 37.2 31.6 28.6 11.5 35.8 19.1 15.8 17.8 23.3 11.0 9.1 11.6 14.3 18.9 22.3 TTT-Online 8.2 21.3 17.7 17.9 9.0 23.4 15.3 12.5 16.4 15.8 10.9 9.0 10.7 12.8 12.2 18.7 ALP 16.5 20.0 19.3 20.5 19.2 21.2 24.0 20.5 20.9 24.2 30.1 16.6 39.6 20.9 17.8 18.0 Table A5.Test error (%) on CIFAR-10-C, level 3, ResNet-26.Test-Time Training with Self-Supervision for Generalization under Distribution Shifts originalgauss shot impulsedefocus glass motion zoom snow frost fog bright contrastelasticpixelate jpeg 0 10 20 30 40Error (%) Object recognition task only Joint training (Hendrycks et al. 2019) TTT TTT-Online Figure A5.Test error (%) on CIFAR-10-C, level 2. See the results section for details. orig gauss shot impul defoc glass motn zoom snow frost fog brit contr elast pixel jpeg B 8.9 31.7 22.6 24.3 9.9 42.6 14.9 14.7 21.7 18.4 9.8 9.1 10.0 13.1 17.1 22.4 JT 8.1 31.0 22.6 23.4 9.1 39.2 16.4 14.2 21.2 17.5 9.4 8.3 10.6 12.8 15.9 20.5 TTT 7.9 28.8 20.7 23.0 9.0 36.6 15.4 13.1 20.2 16.9 9.2 8.3 10.2 12.5 14.8 19.7 TTT-Online 8.2 16.8 13.8 15.5 8.5 23.4 13.3 11.5 16.8 12.7 9.4 8.4 9.7 12.4 11.5 17.0 ALP 16.5 18.0 17.2 19.0 17.8 20.7 21.2 19.3 19.0 20.1 22.4 16.3 29.2 20.3 17.4 17.8 Table A6.Test error (%) on CIFAR-10-C, level 2, ResNet-26. originalgauss shot impulsedefocus glass motion zoom snow frost fog bright contrastelasticpixelate jpeg 0 10 20 30 40Error (%) Object recognition task only Joint training (Hendrycks et al. 2019) TTT TTT-Online Figure A6.Test error (%) on CIFAR-10-C, level 1. See the results section for details. orig gauss shot impul defoc glass motn zoom snow frost fog brit contr elast pixel jpeg B 8.9 21.7 17.1 17.0 9.0 44.0 12.1 13.9 14.3 13.4 9.2 8.9 9.0 13.2 12.0 17.3 JT 8.1 20.4 16.6 16.9 8.2 40.5 12.2 13.0 13.1 12.3 8.4 8.1 8.5 12.9 11.3 15.9 TTT 7.9 19.1 15.8 16.5 8.0 37.9 11.7 12.2 12.8 11.9 8.2 8.0 8.3 12.6 11.1 15.5 TTT-Online 8.2 13.8 11.9 12.2 8.5 24.4 10.5 11.5 12.4 10.7 8.5 8.3 8.6 12.4 10.7 14.4 ALP 17.0 16.8 17.6 16.8 20.9 18.7 19.0 17.3 17.5 17.4 16.1 18.4 20.4 17.0 17.2 17.5 Table A7.Test error (%) on CIFAR-10-C, level 1, ResNet-26.Test-Time Training with Self-Supervision for Generalization under Distribution Shifts Figure A7.Sample Images from the VID-Robust dataset (Shankar et al., 2019) in the results section adapted to CIFAR-10. Each row shows eight sample images from one class. The seven classes shown are, in order: airplane, bird, car, dog, cat, horse, ship.",
      "meta_data": {
        "arxiv_id": "1909.13231v3",
        "authors": [
          "Yu Sun",
          "Xiaolong Wang",
          "Zhuang Liu",
          "John Miller",
          "Alexei A. Efros",
          "Moritz Hardt"
        ],
        "published_date": "2019-09-29T08:09:15Z",
        "pdf_url": "https://arxiv.org/pdf/1909.13231v3.pdf"
      },
      "llm_extracted_info": {
        "main_contributions": "The paper proposes Test-Time Training (TTT), a novel approach to improve predictive models' generalization under distribution shifts. Instead of a fixed decision boundary, TTT updates model parameters on a single unlabeled test sample using a self-supervised learning problem before making a prediction. Key contributions include demonstrating substantial performance improvements on diverse image classification benchmarks (corrupted images, video frames, unknown shifts like CIFAR-10.1) without compromising performance on the original distribution. An online version of TTT further enhances performance under sequential, smoothly changing distribution shifts. Theoretically, the paper establishes that positive gradient correlation between the main and self-supervised task losses is a sufficient condition for TTT to reduce the main task loss in convex models, which is empirically validated for deep learning.",
        "methodology": "The core methodology involves training a neural network with a Y-shaped architecture, comprising a shared feature extractor (θe) and two task-specific branches: one for the main classification task (θm) and another for a self-supervised auxiliary task (θs). The initial training is multi-task learning, minimizing combined losses on labeled training data. At test time, TTT fine-tunes only the shared feature extractor (θe) by minimizing the self-supervised task's loss on the current unlabeled test sample. The self-supervised task used is image rotation prediction (classifying 0, 90, 180, or 270-degree rotations). The standard TTT performs ten gradient steps on each test sample, initialized from the jointly trained model. Online TTT takes only one gradient step per new test sample, initializing its parameters from the state updated on the previous test sample. Group Normalization (GN) is used instead of Batch Normalization (BN) due to the small batch size (single test image plus augmentations) during test-time updates.",
        "experimental_setup": "The method was evaluated on object recognition using ResNet architectures (ResNet-26 for CIFAR-10, ResNet-18 for ImageNet). Datasets included: 1) CIFAR-10-C and ImageNet-C, benchmarking robustness to 15 types of corruptions across 5 severity levels; 2) VID-Robust, a video frame dataset for evaluating adaptation to video data; and 3) CIFAR-10.1, a new test set designed to reveal subtle, unknown distribution shifts. Baselines compared were: 'object recognition task only' (plain ResNet), 'joint training' (supervised + self-supervised at training, fixed at test time), Adversarial Logit Pairing (ALP) for CIFAR-10-C, and Unsupervised Domain Adaptation by Self-Supervision (UDA-SS) for CIFAR-10-C (where UDA-SS had access to the entire unlabeled test set). Optimization for joint training used SGD, while test-time training used SGD with a fixed learning rate of 0.001 and zero weight decay/momentum. Data augmentation (random crop, horizontal flip) was applied during test-time updates.",
        "limitations": "The computational cost of Test-Time Training is significantly higher than standard inference, being proportional to `2 × batch_size × number_of_iterations` slower. The effectiveness of TTT is contingent on the self-supervised task being both 'well defined and non-trivial'; for instance, rotation prediction on certain airplane images with black margins or ambiguous rotations for humans did not yield improvements. While TTT improved performance on CIFAR-10.1, the gain was modest, highlighting the challenge with extremely subtle and unidentifiable distribution shifts. Online TTT relies on the assumption that test samples are from the same or smoothly changing distributions. The method's reliance on Group Normalization indicates a weakness with Batch Normalization for single-sample updates. The theoretical results on gradient correlation are primarily proven for smooth and convex loss functions, while practical applications involve non-convex deep learning models.",
        "future_research_directions": "Future work can extend Test-Time Training to other machine learning tasks, such as segmentation and detection, and other fields like speech recognition and natural language processing. Researchers can leverage domain knowledge to design more effective and specialized self-supervised tasks. The authors suggest using test-time training as a new evaluation benchmark for general-purpose self-supervised learning algorithms. Further research is needed to improve the computational efficiency of test-time training, potentially through techniques like thresholding the self-supervised loss or making models amenable to faster updates during training. A formal theoretical discussion on the concept of a 'variable decision boundary' is also suggested. Additionally, the approach could potentially benefit from advancements in one-shot learning, as the update rule can be viewed as performing one-shot self-supervised learning. More broadly, the paper encourages a paradigm shift away from fixed decision boundaries and the strict separation of training and testing, advocating for more learning to occur post-deployment."
      }
    },
    {
      "title": "Robust Test-Time Adaptation in Dynamic Scenarios",
      "abstract": "Test-time adaptation (TTA) intends to adapt the pretrained model to test\ndistributions with only unlabeled test data streams. Most of the previous TTA\nmethods have achieved great success on simple test data streams such as\nindependently sampled data from single or multiple distributions. However,\nthese attempts may fail in dynamic scenarios of real-world applications like\nautonomous driving, where the environments gradually change and the test data\nis sampled correlatively over time. In this work, we explore such practical\ntest data streams to deploy the model on the fly, namely practical test-time\nadaptation (PTTA). To do so, we elaborate a Robust Test-Time Adaptation (RoTTA)\nmethod against the complex data stream in PTTA. More specifically, we present a\nrobust batch normalization scheme to estimate the normalization statistics.\nMeanwhile, a memory bank is utilized to sample category-balanced data with\nconsideration of timeliness and uncertainty. Further, to stabilize the training\nprocedure, we develop a time-aware reweighting strategy with a teacher-student\nmodel. Extensive experiments prove that RoTTA enables continual testtime\nadaptation on the correlatively sampled data streams. Our method is easy to\nimplement, making it a good choice for rapid deployment. The code is publicly\navailable at https://github.com/BIT-DA/RoTTA",
      "full_text": "Robust Test-Time Adaptation in Dynamic Scenarios Longhui Yuan Binhui Xie Shuang Li \f School of Computer Science and Technology, Beijing Institute of Technology {longhuiyuan,binhuixie,shuangli}@bit.edu.cn Abstract Test-time adaptation (TTA) intends to adapt the pre- trained model to test distributions with only unlabeled test data streams. Most of the previous TTA methods have achieved great success on simple test data streams such as independently sampled data from single or multiple distri- butions. However, these attempts may fail in dynamic sce- narios of real-world applications like autonomous driving, where the environments gradually change and the test data is sampled correlatively over time. In this work, we ex- plore such practical test data streams to deploy the model on the fly, namely practical test-time adaptation (PTTA). To do so, we elaborate a Robust Test-Time Adaptation (RoTTA) method against the complex data stream in PTTA. More specifically, we present a robust batch normalization scheme to estimate the normalization statistics. Meanwhile, a memory bank is utilized to sample category-balanced data with consideration of timeliness and uncertainty. Further, to stabilize the training procedure, we develop a time-aware reweighting strategy with a teacher-student model. Exten- sive experiments prove that RoTTA enables continual test- time adaptation on the correlatively sampled data streams. Our method is easy to implement, making it a good choice for rapid deployment. The code is publicly available at https://github.com/BIT-DA/RoTTA 1. Introduction In recent years, many machine learning problems have made considerable headway with the success of deep neu- ral networks [13, 22, 33, 38]. Unfortunately, the perfor- mance of deep models drops significantly when training data and testing data come from different distributions [59], which limits their utility in real-world applications. To re- duce the distribution shift, a handful of works focus on transfer learning field [56], in particular, domain adapta- tion (DA) [17, 42, 45, 48, 69, 72] or domain generalization (DG) [40, 41, 52, 71, 83], in which one or more different but \fCorresponding author Test data stream Continual TTANon-i.i.d.TTAPractical  TTACategoryDistribution Fully TTA Correlation samplingDistributionchanging Figure 1. We consider the practical test-time adaptation (TTA) setup and compare it with related ones. First, Fully TTA [70] adapts models on a fixed test distribution with an independently sampled test stream. Then, on this basis, Continual TTA [73] takes the continually changing distributions into consideration. Next, Non-i.i.d. TTA [19] tries to tackle the correlatively sampled test streams on a single test distribution, where the label distribution among a batch of data deviates from that of the test distribution. To be more practical, Practical TTA strives to connect both worlds: distribution changing and correlation sampling. related labeled datasets (a.k.a. source domain) are collected to help the model generalize well to unlabeled or unseen samples in new datasets (a.k.a. target domain). While both DA and DG have extensively studied the problem of distribution shifts, they typically assume acces- sibility to the raw source data. However, in many practical scenarios like personal consumption records, the raw data should not be publicly available due to data protection reg- ulations. Further, existing methods have to perform heavy backward computation, resulting in unbearable training costs. Test-time adaptation (TTA) [3,11,16,24,26,54,65,81] attempts to address the distribution shift online at test time with only unlabeled test data streams. Unequivocally, TTA has drawn widespread attention in a variety of applications, e.g., 2D/3D visual recognition [2, 29, 49, 65, 82], multi- modality [63, 64] and document understanding [15]. Prior TTA studies [7, 20, 70, 73] mostly concentrate on a simple adaptation scenario, where test samples are inde- pendently sampled from a fixed target domain. To name a few, Sun et al. [65] adapt to online test samples drawn from a constant or smoothly changing distribution with an auxil- iary self-supervised task. Wang et al. [70] adapt to a fixed arXiv:2303.13899v1  [cs.CV]  24 Mar 2023Table 1. Comparison between our proposed practical test-time adaptation (PTTA) and related adaptation settings. Setting Adaptation StageAvailable Data Test Data Stream Train Test Source Target Distribution Sampling Protocol Domain Adaptation ! % ! ! - - Domain Generalization ! % ! % - - Test-Time Training [65] ! ! ! ! stationary independently Fully Test-Time Adaptation [70] % ! % ! stationary independently Continual Test-Time Adaptation [73]% ! % ! continually changing independently Non-i.i.d. Test-Time Adaptation [5, 19]% ! % ! stationary correlatively Practical Test-Time Adaptation (Ours)% ! % ! continually changing correlatively target distribution by performing entropy minimization on- line. However, such an assumption is violated when the test environments change frequently [73]. Later on, Boudiaf et al. [5] and Gonget al. [19] consider the temporal correlation ship within test samples. For example, in autonomous driv- ing, test samples are highly correlated over time as the car will follow more vehicles on the highway or will encounter more pedestrians in the streets. More realistically, the data distribution changes as the surrounding environment alerts in weather, location, or other factors. In a word, distribution change and data correlation occur simultaneously in reality. Confronting continually changing distributions, tradi- tional algorithms like pseudo labeling or entropy minimiza- tion become more unreliable as the error gradients cumu- late. Moreover, the high correlation among test samples re- sults in the erroneous estimation of statistics for batch nor- malization and collapse of the model. Driven by this analy- sis, adapting to such data streams will encounter two major obstacles: 1) incorrect estimation in the batch normaliza- tion statistics leads to erroneous predictions of test samples, consequently resulting in invalid adaptation; 2) the model will easily or quickly overfit to the distribution caused by the correlative sampling. Thus, such dynamic scenarios are pressing for a new TTA paradigm to realize robust adapta- tion. In this work, we launch a more realistic TTA setting, where distribution changing and correlative sampling oc- cur simultaneously at the test phase. We call this Practical Test-Time Adaptation, or briefly,PTTA. To understand more clearly the similarities and differences between PTTA and the previous setups, we visualize them in Figure 1 and sum- marize them in Table 1. To conquer this challenging prob- lem, we propose a Robust Test-Time Adaptation (RoTTA) method, which consists of three parts: 1) robust statistics es- timation, 2) category-balanced sampling considering time- liness and uncertainty and 3) time-aware robust training. More concretely, we first replace the erroneous statistics of the current batch with global ones maintained by the expo- nential moving average. It is a more stable manner to esti- mate the statistics in BatchNorm layers. Then, we simulate a batch of independent-like data in memory with category- balanced sampling while considering the timeliness and un- certainty of the buffered samples. That is, samples that are newer and less uncertain are kept in memory with higher priority. With this batch of category-balanced, timely and confident samples, we can obtain a snapshot of the current distribution. Finally, we introduce a time-aware reweight- ing strategy that considers the timeliness of the samples in the memory bank, with a teacher-student model to perform robust adaptation. With extensive experiments, we demon- strate that RoTTA can robustly adapt in the practical setup, i.e., PTTA. In a nutshell, our contributions can be summarized as: • We propose a new test-time adaptation setup that is more suitable for real-world applications, namely practical test-time adaptation (PTTA). PTTA considers both distribution changing and correlation sampling. • We benchmark the performance of prior methods in PTTA and uncover that they only consider one aspect of the problem, resulting in ineffective adaptation. • We propose a robust test-time adaptation method (RoTTA), which has a more comprehensive considera- tion of PTTA challenges. Ease of implementation and effectiveness make it a practical deployment option. • We extensively demonstrate the practicality of PTTA and the effectiveness of RoTTA on common TTA benchmarks [23], i.e., CIFAR-10-C and CIFAR-100- C and a large-scale DomainNet [58] dataset. RoTTA obtains state-of-the-art results, outperforming the best baseline by a large margin (reducing the averaged classification error by over 5.9%, 5.5% and 2.2% on CIFAR-10-C, CIFAR-100-C and DomainNet, respec- tively). 2. Related Work Domain adaptation (DA) studies the problem of transfer- ring the knowledge learned from a labeled source dataset to an unlabeled target dataset [8, 17, 43, 51, 67, 68]. Represen- tative techniques include latent distribution alignment [48, 77], adversarial training [17, 62], or self-training [75, 85]. The limitation of this setting, however, is that an unlabeled test dataset (target domain) is needed at training time, in addition to a labeled training dataset (source domain). Ac- cordingly, it might fail to handle more practical scenariosFeature 𝐹Robust batch normalization (RBN)Update𝜇௚, 𝜎௚ଶNormalizeFeature𝐹′Update bank with current sample  Training lossℒ௥in Eq. (7) Teacher StudentAdaptation with RBNMemorybankEMA 𝑡A stream of online dataUpdateTest timeCorrelationsamplingStrong & weakaugmentation flowDistributionsCategoryTeacherMajor classhas highest ℋin majorRemoveAddWhen ℋ>ℋSamples to beadded& removed Figure 2. Framework overview. Firstly, we replace the batch normalization layer with RBN which robustly normalizes the feature map. During the inference of the online test stream of PTTA, we utilize the predictions of samples to maintain a memory bank by category- balanced sampling with timeliness and uncertainty. Finally, we use the category-balanced, timely and confident data in the memory bank combined with a robust loss to adapt the model at test time. like test-time adaptation. Our practical test-time adaptation setting can be viewed as performing correlatively sample adaptation on the fly. It is worth noting that standard domain adaptation techniques might collapse when only continual data streams from multiple target domains are accessible. Domain generalization (DG) assumes that multiple source domains are available for model training and tries to learn models that can generalize well to any unseen domains [4, 26,40,41,52,84]. A broad spectrum of methodologies based on data augmentation [78, 84], meta-learning [14, 40], or domain alignment [50,52] has made great progress. In con- trast, this work instead aims to improve the performance of source pre-trained models at the test time by using unla- beled online data streams from multiple continually chang- ing target domains. Continual learning (CL) (also known as incremental learning, life-long learning) addresses the problem of learn- ing a model for many tasks sequentially without forgetting knowledge obtained from the preceding tasks. [1, 6, 31, 37, 60]. CL methods can often be categorized into replay- based [60, 66] and regularization-based [31, 44] methods. Ideas from continual learning are also adopted for continu- ous domain adaptation approaches [34, 74] In our work, we share the same motivation as CL and point out that prac- tical test-time adaptation (PTTA) also suffers catastrophic forgetting (i.e., performance degradation on new test sam- ples due to correlation sampling), which makes test-time adaptation approaches are unstable to deploy. Test-time adaptation (TTA) focus on more challenging settings where only source model and unlabeled target data are available [9, 18, 27, 28, 35, 46, 61]. A similar paradigm is source-free domain adaptation (SFDA) [10, 36, 47, 79], which also requires no access to the training (source) data. To name a few, Liang et al . [45] fit the source hypoth- esis by exploiting the information maximization and self- supervised pseudo-labeling. Kundu et al. [35] formalize a unified solution that explores SFDA without any category- gap knowledge. To fully utilize any arbitrary pre-trained model, Sun et al. [65] propose conducting adaptation on the fly with an auxiliary self-supervised task. Later on, Wanget al. [70] take a source pre-trained model and adapt it to the test data by updating a few trainable parameters in Batch- Norm layers [25] using entropy minimization [21]. While standard TTA has been widely studied in many tasks [2, 20, 63, 64, 70, 82], the fact remains that both dis- tribution changing [73] and data correlation sampling [19] has only been considered in isolation. For example, Gong et al. [19] propose instance-aware batch normalization and prediction-balanced reservoir sampling to address the chal- lenges of correlatively sampled test streams, however, it does not consider unstable adaptation resulting from long- term adaptation on continually changing distributions. On the other hand, Wang et al. [73] assume that the target test data is streamed from a continually changing environment and continually adapt an off-the-shelf source pre-trained model to the current test data. In this work, we launch PTTA, a more practical TTA setting to connect both worlds: distribution changing and correlation sampling. 3. Method 3.1. Problem Definition and Motivation Given a model fθ0 with parameter θ0 pre-trained on source domain DS = {(xS, yS)}, the proposed practical test-time adaptation (PTTA) aims to adapt fθ0 to a stream of online unlabeled samples X0, X1, ...,XT , where Xt is a batch of highly correlated samples from the distribution Ptest that changes with time t continually. More specifi- cally, at test time, with time going on, the test distribution Ptest changes continually as P0, P1, ...,P∞. At time step t, we will receive a batch of unlabeled and correlated samplesmotion distribution changing snow time  Distributions and Labels of PTTA T est Stream uniform 10 1 0.1 0.01 0.001 Dirichlet Parameter  Figure 3. Illustration of the labels and distributions of the test stream of CIFAR10-C under the setup PTTA. And we adopt Dirichlet distribution to simulate the process of correlative sam- pling. It is clear that as the concentration parameter δ decreases, the correlation among sampled data increases, which is reflected in the increasing aggregation of categories. Xt from Ptest. Next, Xt is fed into the model fθt and the model needs to adapt itself to the current test data streams and make predictions fθt (Xt) on the fly. As a matter of fact, this setup is largely driven the prac- tical demands of deploying models in dynamic scenarios. Taking for example the case of autonomous driving men- tioned in § 1, test samples are highly correlated and the data distribution changes continually with the weather or loca- tion. Another example is the situation of intelligent moni- toring, the camera will continuously capture more people at certain times, such as after work, but fewer of them during work time. Meanwhile, the light condition changes con- tinually from day to night. The deployed model should be robustly adapted in such dynamic scenarios. In a word, dis- tribution change and data correlation often happen simul- taneously in the real world. For this reason, existing TTA methods [7,9,19,28,70,73,81] might become unstable when the test stream is sampled from such dynamic scenarios. To obtain the test stream of PTTA, we adopt Dirich- let Distribution with parameter δ to simulate the correla- tion among test samples. We present the test data streams corresponding to different values of δ on the CIFAR10-C dataset in Figure 3. We can observe that the smaller δ is, the higher the correlation will be. For the sake of unity, we set δ = 0.1 as the default for all experiments. In the follow- ing, we present a robust test-time adaptation framework for the practical test-time adaptation setup defined above. An overview of our RoTTA is illustrated in Figure 2. 3.2. Robust Test-Time Adaptation Motivated by the fact that the statistics of current batch data, which are commonly used in previous TTA meth- ods [7, 20, 65, 70, 73], become unreliable when they en- counter correlative test data streams, we first turn to the global robust statistics for normalization. Then, to effec- tively adapt to the current distribution, we maintain a mem- ory bank by category-balanced sampling with considering timeliness and uncertainty, which captures a more stable snapshot of the distribution. Finally, we utilize the teacher- student model and design a timeliness-based reweighting strategy to train the model robustly. Robust batch normalization (RBN). Batch Normaliza- tion (BN) [25] is a widely-used training technique as it can accelerate the training and convergence speed of networks and stabilize the training process by reducing the risk of gradient explosion and vanishing. Given the feature map F ∈ RB×C×H×W as the input for a BN layer when train- ing, the channel-wise mean µ ∈ RC and variance σ2 ∈ RC are calculated as follows: µc = 1 BHW BX b=1 HX h=1 WX w=1 F(b,c,h,w) , (1) σ2 c = 1 BHW BX b=1 HX h=1 WX w=1 (F(b,c,h,w) − µc)2 . (2) Then the feature map is normalized and refined in a channel-wise manner as BN (F(b,c,h,w); µ, σ2) =γc F(b,c,h,w) − µc √σ2c + ϵ + βc , (3) where γ, β∈ RC are learnable parameters in the layer and ϵ > 0 is a constant for numerical stability. Meanwhile, during training, the BN layer maintains a group of global running mean and running variance (µs, σ2 s) for inference. Due to the domain shift at test time, the global statis- tics (µs, σ2 s) normalize test features inaccurately, causing significant performance degradation. To tackle the prob- lem above, some methods [55, 70, 73] use the statistics of the current batch to perform normalization. Unfortunately, when the test samples have a high correlation under PTTA setup, the statistics of the current batch also fail to correctly normalize the feature map, as demonstrated in Figure 4c. Specifically, the performance of BN [53] decreases rapidly as the data correlation increases. Based on the analysis above, we propose a robust batch normalization (RBN) module, which maintains a group of global statistics (µg, σ2 g) to normalize the feature map ro- bustly. Before the whole test-time adaptation, (µg, σ2 g) is initialized as the running mean and variance (µs, σ2 s) of the pre-trained model. When adapting the model, we update the global statistics first by exponential moving average as µg = (1− α)µg + αµ , (4) σ2 g = (1− α)σ2 g + ασ2 , (5) where (µ, σ2) is the statistics of the buffered samples in the memory bank. Then we normalize and affine the feature as Eq. (3) with (µg, σ2 g). When inferring for test samples, we directly utilize (µg, σ2 g) to calculate the output as Eq (3). Al- though simple, RBN is effective enough to tackle the prob- lem of normalization on test streams of PTTA.Category-balanced sampling with timeliness and uncer- tainty (CSTU). In the PTTA setup, the correlation among test samples Xt at time t leads to a deviation between the observed distribution bPtest and the test distribution Ptest. Specifically, the marginal label distribution p(y|t) tends to differ from p(y). Continuously learning with Xt over time t can lead to model adaptation to an unreliable distribution bPtest, resulting in ineffective adaptation and an increased risk of model collapse. To address this issue, we propose a category-balanced memory bank M with a capacity of N, which takes into account the timeliness and uncertainty of samples when up- dating. In particular, we adopt the predictions of test sam- ples as pseudo labels to guide the update ofM. Meanwhile, to guarantee the balance among categories, we distribute the capacity of M equally to each category, and samples of the major categories will be replaced first (refer to lines 5-9 in Algorithm 1). Furthermore, due to the continually changing test distribution, old samples in M are limited in value, and could even impair the ability of the model to adapt to the current distribution. Additionally, samples of high uncer- tainty always produce erroneous gradient information that can hinder model adaptation, as suggested by [55]. With this in mind, we attach each sample in M with a group of heuristics (A, U), where A, initialized as 0 and in- creasing with time t, is the age of the sample, and U the un- certainty calculated as the entropy of the prediction. Next, we combine the timeliness and uncertainty to calculate a heuristic score, i.e., category-balanced sampling with time- liness and uncertainty (CSTU), as follows: H = λt 1 1 + exp(−A/N) + λu U log C , (6) where λt and λu make the trade-off between timeliness and uncertainty, and for simplicity, λt and λu are set to 1.0 for all experiments, andC is the number of categories. We sum- marize our sampling algorithm in Algorithm 1. With CSTU, we can obtain a robust snapshot of the current test distribu- tion Ptest, and effectively adapt the model to it. Robust training with timeliness. Actually, after replacing BN layers with our RBN and obtaining the memory bank selected via CSTU, we can directly adopt the widely used techniques like pseudo labeling or entropy minimization to perform test-time adaptation. However, we notice that too old or unreliable instances still have the opportunity to stay in M since keeping the category balance is assigned the top priority. In addition, too aggressive updates of the model will make the category balance ofM unreliable, resulting in unstable adaptation. Meanwhile, error accumulation caused by the distribution change also makes the aforementioned approaches unworkable. To further reduce the risk of error gradients information from old and unreliable instances and stabilize the adapta- tion, we turn to the robust unsupervised learning method Algorithm 1: CSTU for one test sample. 1 Input: a test sample x and the teacher model fθT . 2 Define: memory bank M and its capacity N, number of classes C, per class occupation O ∈RC, total occupation Ω, classes to pop instance D. 3 Infer as p(y|x) =Softmax(fθT (x)). 4 Calculate the predicted category of x as ˆy = arg maxc p(c|x), the uncertainty as Ux = −PC c=1 p(c|x) log(p(c|x)), the age as Ax = 0, and the heuristic score Hx of x with Eq (6) 5 if Oˆy < N C then 6 if Ω <N: Search range D = ∅. 7 else: Search range D = {j|j = arg maxc Oc} 8 else 9 Search range D = {ˆy} 10 if D is ∅ then 11 Add (x, ˆy, Hx, Ux) into M. 12 else 13 Find the instance (ˆx, yˆx, Aˆx, Uˆx) with the highest value in Eq (6) Hˆx among D. 14 if Hx < Hˆx then 15 Remove (ˆx, yˆx, Aˆx, Uˆx) from M. 16 Add (x, ˆy, Hx, Ux) into M. 17 else 18 Discard x. 19 Increase the age of all instances in M. teacher-student model and propose a timeliness reweight- ing strategy. In addition, for the sake of time efficiency and stability, only affine parameters in RBN are trained during adaptation. At time step t, after inferring for the correlated data Xt with the teacher model fθT t and updating the memory bank M with Xt, we begin updating the student model fθS t and the teacher model fθT t . Firstly, we update parameters of stu- dent model θS t → θS t+1 by minimizing the following loss: Lr = 1 Ω ΩX i=1 L(xM i , Ai; θT t , θS t ) , (7) where Ω = |M| is the total occupation of the memory bank, and xM i and Ai(i = 1, ..., Ω) are instances in the memory bank and their age respectively. Subsequently, the teacher model is updated by exponential moving average as θT t+1 = (1− ν)θT t + νθS t+1 . (8) To calculate the loss value of an instancexM i from the mem- ory bank, the timeliness reweighting term is computed as E(Ai) = exp(−Ai/N) 1 + exp(−Ai/N) , (9)where Ai is the age of xM i , and N is the capacity of the bank. And then we calculate the cross entropy between the soft-max prediction pS(y|x′′ i ) of the strong-augmented view x′′ i from the student model and that pT (y|x′ i) of the weak- augmented view 1 x′ i from the teacher model as follows: ℓ(x′ i, x′′ i ) =−1 C CX c=1 pT (c|x′ i) logpS(c|x′′ i ) . (10) Finally, equipped with Eq. (9) and Eq. (10), the right-hand side of Eq. (7) reduces to L(xM i , Ai; θT t , θS t ) =E(Ai)ℓ(x′ i, x′′ i ) . (11) To sum up, equipped with RBN, CSTU, and robust training with timeliness, our RoTTA is capable of effectively adapt- ing any pre-trained models in dynamic scenarios. 4. Experiments 4.1. Setup Datasets. CIFAR10-C and CIFAR100-C [23] are the com- monly used TTA benchmarks to testify the robustness un- der corruptions. Both of them are obtained by applying 15 kinds of corruption with 5 different degrees of severity on their clean test images of original datasets CIFAR10 and CIFAR100 respectively. CIFAR10/CIFAR100 [32] have 50,000/10,000 training/test images, all of which fall into 10/100 categories. DomainNet [58] is the largest and hard- est dataset to date for domain adaptation and consists of about 0.6 million images with 345 classes. It consists of six different domains including Clipart (clp), Infograph (inf), Painting (pnt), Quickdraw (qdr), Real (rel), and Sketch (skt). We first pre-train a source model on the train set in one of six domains and testify all baseline methods on the test set of the remaining five domains. Implementation details. All experiments are conducted with PyTorch [57] framework. In the case of robustness to corruption, following the previous methods [55, 70, 73], we obtain the pre-trained model from RobustBench bench- mark [12], including the WildResNet-28 [80] for CIFAR10 → CIFAR10-C, and the ResNeXt-29 [76] for CIFAR100 → CIFAR100-C. Then, we change the test corruption at the highest severity 5 one by one to simulate that the test distri- bution continually changes with time in PTTA. And in the case of generalization under the huge domain gap, we train a ResNet-101 [22] by standard classification loss for each domain in DomainNet and adapt them continually to differ- ent domains except the source domain. Meanwhile, we uti- lize the Dirichlet distribution to simulate the correlatively sampled test stream for all datasets. For optimization, we adopt Adam [30] optimizer with learning rate 1.0 × 10−3, 1Weak augmentation is ReSize+CenterCrop. Strong augmentation is a combination nine operations like Clip, ColorJitter, and RandomAffine. β = 0.9. For a fair comparison, we set the batch size for all methods as 64 and the capacity of the memory bank of RoTTA as N = 64. Concerning the hyperparameters, we adopt a unified set of values for RoTTA across all experi- ments including α = 0.05, ν = 0.001, λt = 1.0, λu = 1.0, and δ = 0.1. More details are provided in the appendix. 4.2. Comparisons with the State-of-the-arts Robustness under corruptions. The classification error on CIFAR10→CIFAR10-C and CIFAR100→CIFAR100-C are shown in Table 2 and Table 3 respectively. We change the type of the current corruption at the highest severity 5 as time goes on, and sample data correlatively for infer- ence and adaptation simultaneously. The same test stream is shared across all compared methods. From Table 2 and Table 3, we can see that RoTTA achieves the best performance compared to previous meth- ods. Moreover, RoTTA has a significant performance gain to the second-best method that 5.9% improvement on CIFAR10 →CIFAR10-C and 5.5% improvement on CIFAR100→CIFAR100-C respectively, verifying the effec- tiveness of RoTTA to adapt the model under PTTA. In more detail, we can observe that BN [53], PL [39], TENT [70] and CoTTA [73] negatively adapt the model to the test streams of both datasets compared to Source (−6.5 ∼ −46.4%). This is attributed to the fact that these methods overlook the issues posed by correlation sampling, which can result in highly correlated data within a batch. As a consequence, traditional normalization statistics may be ineffective in appropriately normalizing the feature maps. Equipped with RBN and CSTU, RoTTA no longer suffers from this issue. Meanwhile, in Table 3, if focus on the adaptation procedure, we can see that the performance of PL [39], TENT [70] and NOTE [19] becomes worse and worse, and eventually, the model even collapses (error rate > 97%). This reveals that the impact of error accumula- tion on long-term adaptation can be catastrophic. To tackle this problem, RoTTA turns to robustly adapt the model with timeliness reweighting and confident samples in the mem- ory bank, and superior performance throughout the adapta- tion process demonstrates its effectiveness. In addition, we find that although LAME [5] never tunes the parameters of the model, it is still a competi- tive baseline for example it achieves the second-best result on CIFAR100→CIFAR100-C. However, its performance is very dependent on the performance of the pre-trained model e.g. negligible improvement on difficult corruptions (shot, gaussian, pixelate). On the contrary, our RoTTA is more flexible and achieves better and more robust results. Generalization under domain shift. We also evalu- ate RoTTA under a more challenging dataset DomainNet, where we continually adapt a source pre-trained model to correlatively sampled test streams of the rest domains. AsTable 2. Average classification error of the task CIFAR10→ CIFAR10-C while continually adapting to different corruptions at the highest severity 5 with correlatively sampled test stream under the proposed setup PTTA. Time t− − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − → Method motionsnow fog shot defocuscontrastzoom brightnessfrost elasticglass gaussianpixelatejpeg impulse Avg. Source 34.8 25.1 26.0 65.7 46.9 46.7 42.0 9.3 41.3 26.6 54.3 72.3 58.5 30.3 72.9 43.5BN [53] 73.2 73.4 72.7 77.2 73.7 72.5 72.9 71.0 74.1 77.7 80.0 76.9 75.5 78.3 79.0 75.2PL [39] 73.9 75.0 75.6 81.0 79.9 80.6 82.0 83.2 85.3 87.3 88.3 87.5 87.5 87.5 88.2 82.9TENT [70] 74.3 77.4 80.1 86.2 86.7 87.3 87.9 87.4 88.2 89.0 89.2 89.0 88.3 89.7 89.2 86.0LAME [5] 29.5 19.0 20.3 65.3 42.4 43.4 36.8 5.4 37.2 18.6 51.2 73.2 57.0 22.6 71.3 39.5CoTTA [73]77.1 80.6 83.1 84.4 83.9 84.2 83.1 82.6 84.4 84.2 84.5 84.6 82.7 83.8 84.9 83.2NOTE [19] 18.0 22.1 20.6 35.6 26.9 13.6 26.5 17.3 27.2 37.0 48.3 38.8 42.6 41.9 49.7 31.1 RoTTA 18.1 21.3 18.8 33.6 23.6 16.5 15.1 11.2 21.9 30.7 39.6 26.8 33.7 27.8 39.5 25.2(+5.9) Table 3. Average classification error of the task CIFAR100 → CIFAR100-C while continually adapting to different corruptions at the highest severity 5 with correlatively sampled test stream under the proposed setup PTTA. Time t− − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − → Method motionsnow fog shot defocuscontrastzoom brightnessfrost elasticglass gaussianpixelatejpeg impulse Avg. Source 30.8 39.5 50.3 68.0 29.3 55.1 28.8 29.5 45.8 37.2 54.1 73.0 74.7 41.2 39.4 46.4BN [53] 48.5 54.0 58.9 56.2 46.4 48.0 47.0 45.4 52.9 53.4 57.1 58.2 51.7 57.1 58.8 52.9PL [39] 50.6 62.1 73.9 87.8 90.8 96.0 94.8 96.4 97.4 97.2 97.4 97.4 97.3 97.4 97.4 88.9TENT [70] 53.3 77.6 93.0 96.5 96.7 97.5 97.1 97.5 97.3 97.2 97.1 97.7 97.6 98.0 98.3 92.8LAME [5] 22.4 30.4 43.9 66.3 21.3 51.7 20.6 21.8 39.6 28.0 48.7 72.8 74.6 33.1 32.3 40.5CoTTA [73]49.2 52.7 56.8 53.0 48.7 51.7 49.4 48.7 52.5 52.2 54.3 54.9 49.6 53.4 56.2 52.2NOTE [19] 45.7 53.0 58.2 65.6 54.2 52.0 59.8 63.5 74.8 91.8 98.1 98.3 96.8 97.0 98.2 73.8 RoTTA 31.8 36.7 40.9 42.1 30.0 33.6 27.9 25.4 32.3 34.0 38.8 38.7 31.3 38.0 42.9 35.0(+5.5) Table 4. Average classification error of DomainNet while continually adapting to different domains with correlatively sampled test stream. Timet− − − − − − − − − − − − − − − − − − →Timet− − − − − − − − − − − − − − − − − − →Timet− − − − − − − − − − − − − − − − − − →Timet− − − − − − − − − − − − − − − − − − →Sourceclp inf pnt qdr rel sktAvg. BN clp inf pnt qdr rel sktAvg. PL clp inf pnt qdr rel sktAvg.TENTclp inf pnt qdr rel sktAvg. clp N/A 83.9 65.4 88.6 48.0 59.1 69.0clp N/A 88.6 70.7 90.5 65.4 67.0 76.5clp N/A 94.5 98.9 99.5 99.7 99.7 98.5clp N/A 87.5 71.9 94.2 96.2 98.9 89.7inf 61.8 N/A 66.9 96.0 50.0 70.6 69.1inf 68.6 N/A 74.2 96.2 69.9 76.8 77.1inf 82.6 N/A 99.2 99.6 99.7 99.3 96.1inf 68.6 N/A 75.0 97.3 95.9 98.7 87.1pnt 56.5 83.7 N/A 94.2 42.6 63.4 68.1pnt 60.8 87.9 N/A 94.3 62.3 68.7 74.8pnt 78.6 99.4 N/A 99.7 99.6 99.7 95.4pnt 61.7 87.1 N/A 96.4 95.3 98.8 87.8qdr 89.2 99.0 98.6 N/A 95.0 92.3 94.8qdr 80.3 97.7 92.6 N/A 88.7 88.1 89.5qdr 81.7 99.5 99.6 N/A 99.7 99.8 96.1qdr 78.9 97.1 91.6 N/A 89.2 88.7 89.1rel 49.4 80.4 51.5 93.4 N/A 63.3 67.6rel 57.9 87.1 63.1 94.3 N/A 70.8 74.6rel 73.5 99.4 99.2 99.6 N/A 99.7 94.3rel 57.8 86.4 68.1 96.9 N/A 96.7 81.2skt 47.5 88.2 62.9 87.1 51.8 N/A 67.5skt 50.4 87.6 64.6 89.6 63.1 N/A 71.1skt 64.8 99.2 99.4 99.7 99.7 N/A 92.6skt 51.9 87.2 69.1 95.3 97.3 N/A 80.1Avg.60.9 87.0 69.1 91.9 57.5 69.7 72.7Avg.63.6 89.8 73.0 93.0 69.9 74.3 77.3Avg.76.2 98.4 99.3 99.6 99.7 99.6 95.5Avg.63.8 89.0 75.1 96.0 94.8 96.4 85.8 Timet− − − − − − − − − − − − − − − − − − →Timet− − − − − − − − − − − − − − − − − − →Timet− − − − − − − − − − − − − − − − − − →Timet− − − − − − − − − − − − − − − − − − →LAMEclp inf pnt qdr rel sktAvg.COTTAclp inf pnt qdr rel sktAvg.NOTEclp inf pnt qdr rel sktAvg.RoTTAclp inf pnt qdr rel sktAvg. clp N/A 82.2 64.5 87.7 46.9 58.9 68.0clp N/A 90.6 77.9 89.3 76.3 72.7 81.4clp N/A 89.2 73.0 94.8 98.4 99.4 91.0clp N/A 85.5 62.0 82.0 49.3 59.8 67.7inf 60.1 N/A 65.7 95.4 48.5 69.4 67.8inf 74.5 N/A 82.0 95.7 80.2 81.5 82.8inf 75.4 N/A 78.7 98.7 98.1 99.5 90.1inf 61.8 N/A 63.7 91.5 52.5 67.6 67.4pnt 55.8 81.5 N/A 93.3 41.3 62.1 66.8pnt 66.3 89.8 N/A 93.4 74.0 75.4 79.8pnt 64.7 89.8 N/A 97.8 98.4 99.2 90.0pnt 53.3 84.1 N/A 89.1 47.3 61.4 67.0qdr 88.3 99.1 99.0 N/A 94.9 92.2 94.7qdr 82.3 98.2 94.6 N/A 92.5 90.1 91.5qdr 74.7 97.2 92.2 N/A 93.5 99.6 91.4qdr 77.5 97.0 89.8 N/A 80.3 82.2 85.3rel 48.0 79.3 50.1 91.6 N/A 60.2 65.8rel 64.0 90.3 73.2 93.5 N/A 77.6 79.7rel 61.3 89.2 68.9 98.8 N/A 99.2 83.5rel 49.1 82.3 50.3 88.0 N/A 61.1 66.2skt 45.6 87.1 59.5 83.9 49.9 N/A 65.2skt 56.1 89.2 71.9 89.2 73.5 N/A 76.0skt 55.2 89.7 70.1 96.9 98.3 N/A 82.0skt 42.6 83.7 54.4 80.9 47.5 N/A 61.8Avg.59.6 85.8 67.8 90.4 56.3 68.6 71.4Avg.68.6 91.6 79.9 92.2 79.3 79.5 81.9Avg.66.3 91.0 76.6 97.4 97.3 99.4 88.0Avg.56.8 86.5 64.0 86.3 55.4 66.469.2(+2.2) shown in Table 4, consistent with the previous analysis, most of the methods include BN [53], PL [39], TENT [70], CoTTA [73] and NOTE [19] even perform worse than the Source model ( −4.6 ∼ −22.8%). RoTTA consistently achieves the best performance and has 2.2% gain than the second method LAME [5], demonstrating RoTTA’s effec- tiveness again. 4.3. Ablation Study Effect of each component. To further investigate the effi- cacy of each component, we replace each part with the nor- mally used solutions to obtain three variants: (1) RoTTA w/o RBN, replace RBN with test-time BN in TENT [70]; (2) RoTTA w/o CSTU, directly adapt the model on test stream; (3) RoTTA w/o robust training (RT), directly adapt the model only with entropy minimization. As shown in Table 5, we can observe that significant performance degra- dation occurs for all variants, proving that every part of our proposed method is valid for PTTA. Take one com- ponent for a detailed example, without RBN robustly nor- malizing feature maps, the performance of RoTTA drops 50.2% and 16.3% on CIFAR10-C and CIFAR100-C respec- tively, proving that RBN is robust enough to tackle the prob- lem of normalization of correlatively sampled data streams. CSTU enables RoTTA to adapt to a more stable distribu- tion by maintaining a timely and confident snapshot of the test distribution. Meanwhile, robust training with timeliness greatly reduces the accumulation of errors. Every compo- nent behaves significantly to enable effective adaptation un- der PTTA. Effect of the distribution changing order. To exclude the effect of a fixed order of distribution changing, we con- ducted experiments on ten different sequences of changes on CIFAR10-C and CIFAR100-C with independently andBN PL TENT LAME CoTTA NOTE RoTTA0 10 20 30 40 50 60 70 80Classification error (%) Source CIFAR-10  CIFAR-10-C Independent Correlative (a) CIFAR10-C. BN PL TENT LAME CoTTA NOTE RoTTA0 20 40 60 80Classification error (%) Source CIFAR-100  CIFAR-100-C Independent Correlative (b) CIFAR100-C. uniform 10 1 0.1 0.01 0.001 30 40 50 60 70 80 90 100Classification error (%) Source BN PL TENT LAME CoTTA NOTE RoTTA (c) δ. 16 32 64 128 256 512 40 50 60 70 80 90 100Classification error (%) Source BN PL TENT LAME CoTTA NOTE RoTTA (d) Batch size. Figure 4. (a) & (b) we adapt the model continually to different corruptions of 10 different orders with independently and correlatively sampled test streams on CIFAR10-C and CFAR100-C respectively and report their average classification error. (c) & (d) we verify the effect of δ and batch size to different methods on CIFAR100-C respectively. Table 5. Classification error of different variants of our RoTTA. Variant CIFAR10-C CIFAR100-C Avg. RoTTA w/o RBN 75.4 51.3 63.4 RoTTA w/o CSTU 47.1 46.3 46.7 RoTTA w/o RT 78.2 95.0 81.6 RoTTA 25.2 35.0 30.1 correlatively sampled test streams respectively. As shown in Figure 4a and 4b, no matter what kind of setup, RoTTA can achieve excellent results. The detailed results on the correlatively sampled test streams are shown in Table 6, RoTTA achieves 4.3% and 4.7% progress on CIFAR10- C and CIFAR100-C respectively. This shows that RoTTA can adapt the model robustly and effectively in long-term scenarios where distribution continually changes and test streams are sampled either independently or correlatively, making it a good choice for model deployment. Effect of Dirichlet concentration parameter δ. We vary the value of δ on CIFAR100-C and compare RoTTA with other approaches in Figure 4c. As the value of δ increases, the performance of BN [53], PL [39], TENT [70] and CoTTA [73] drops quickly, because they never consider the increasing correlation among test samples. NOTE [19] is stable to correlatively sampled test streams but does not consider the distribution changing, causing ineffective adaptation. Meanwhile, the higher correlation between test samples will make the propagation of labels more accurate, which is why the result of LAME [5] slightly improves. Fi- nally, excellent and stable results once again prove the sta- bility and effectiveness of RoTTA. Effect of batch size. In real scenarios, considering deploy- ment environments may use different test batch sizes, we conduct experiments with different values of test batch sizes and results are shown in Figure 4d. For a fair comparison, we control the frequency of updating the model of RoTTA so that the number of samples involved in back-propagation is the same. As the batch size increases, we can see that all of the compared methods have a significant improvement except for lame which has a slight decrease. This is be- cause the number of categories in a batch increases with the Table 6. Average classification error of tasks CIFAR10 → CIFAR10-C and CIFAR100 → CIFAR100-C while continually adapting to different corruptions of 10 different orders at the high- est severity 5 with correlatively sampled test stream. Method CIFAR10-C CIFAR100-C Avg. Source 43.5 46.4 46.9 BN [53] 75.2 52.9 64.1 PL [39] 75.2 52.9 60.1 TENT [70] 82.3 93.2 87.8 LAME [5] 39.5 40.6 40.1 NOTE [19] 30.5 76.1 53.3 CoTTA [73] 83.1 52.8 67.9 RoTTA 26.2(+4.3) 35.9(+4.7) 31.1(+9.0) increasing batch size, causing the overall correlation to be- come lower but the propagation of labels to become more difficult. Most significantly, RoTTA achieves the best re- sults across different batch sizes, demonstrating its robust- ness in dynamic scenarios once again. 5. Conclusion This work proposes a more realistic TTA setting where distribution changing and correlative sampling occur si- multaneously at the test phase, namely Practical Test-Time Adaptation (PTTA). To tackle the problems of PTTA, we propose Robust Test-Time Adaptation (RoTTA) method against the complex data stream. More specifically, a group of robust statistics for the normalization of feature maps is estimated by robust batch normalization. Meanwhile, a memory bank is adopted to capture a snapshot of the test distribution by category-balanced sampling with consider- ing timeliness and uncertainty. Further, we develop a time- aware reweighting strategy with a teacher-student model to stabilize the adaptation process. Extensive experiments and ablation studies are conducted to verify the robustness and effectiveness of the proposed method. We believe this work will pave the way for thinking about adapting models into real-world applications by test-time adaptation algorithm. Acknowledgements. This paper was supported by National Key R&D Program of China (No. 2021YFB3301503), and also supported by the National Natural Science Foundation of China under Grant No. 61902028.References [1] Rahaf Aljundi, Min Lin, Baptiste Goujaud, and Yoshua Ben- gio. Gradient based sample selection for online continual learning. In NeurIPS, pages 11816–11825, 2019. 3 [2] Fatemeh Azimi, Sebastian Palacio, Federico Raue, J ¨orn Hees, Luca Bertinetto, and Andreas Dengel. Self-supervised test-time adaptation on video data. In WACV, pages 2603– 2612, 2022. 1, 3 [3] Mathilde Bateson, Herve Lombaert, and Ismail Ben Ayed. Test-time adaptation with shape moments for image segmen- tation. In MICCAI, pages 736–745, 2022. 1 [4] Gilles Blanchard, Gyemin Lee, and Clayton Scott. General- izing from several related classification tasks to a new unla- beled sample. In NeurIPS, pages 2178–2186, 2011. 3 [5] Malik Boudiaf, Romain Mueller, Ismail Ben Ayed, and Luca Bertinetto. Parameter-free online test-time adaptation. In CVPR, pages 8344–8353, 2022. 2, 6, 7, 8, 13, 14, 15, 16, 17 [6] Francisco M Castro, Manuel J Mar ´ın-Jim´enez, Nicol´as Guil, Cordelia Schmid, and Karteek Alahari. End-to-end incre- mental learning. In ECCV, pages 233–248, 2018. 3 [7] Dian Chen, Dequan Wang, Trevor Darrell, and Sayna Ebrahimi. Contrastive test-time adaptation. In CVPR, pages 295–305, 2022. 1, 4 [8] Yuhua Chen, Wen Li, Christos Sakaridis, Dengxin Dai, and Luc Van Gool. Domain adaptive faster r-cnn for object de- tection in the wild. In CVPR, pages 3339–3348, 2018. 2 [9] Zhixiang Chi, Yang Wang, Yuanhao Yu, and Jin Tang. Test- time fast adaptation for dynamic scene deblurring via meta- auxiliary learning. In CVPR, pages 9137–9146, 2021. 3, 4 [10] Boris Chidlovskii, St ´ephane Clinchant, and Gabriela Csurka. Domain adaptation in the absence of source domain data. In KDD, pages 451–460, 2016. 3 [11] Sungha Choi, Seunghan Yang, Seokeon Choi, and Sun- grack Yun. Improving test-time adaptation via shift-agnostic weight regularization and nearest source prototypes. In ECCV, pages 440–458, 2022. 1 [12] Francesco Croce, Maksym Andriushchenko, Vikash Se- hwag, Edoardo Debenedetti, Nicolas Flammarion, Mung Chiang, Prateek Mittal, and Matthias Hein. Robustbench: a standardized adversarial robustness benchmark. In Neurips, 2021. 6 [13] Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner, Mostafa Dehghani, Matthias Minderer, Georg Heigold, Syl- vain Gelly, Jakob Uszkoreit, and Neil Houlsby. An image is worth 16x16 words: Transformers for image recognition at scale. In ICLR, 2021. 1 [14] Ying-Jun Du, Jun Xu, Huan Xiong, Qiang Qiu, Xiantong Zhen, Cees G. M. Snoek, and Ling Shao. Learning to learn with variational information bottleneck for domain general- ization. In ECCV, pages 200–216, 2020. 3 [15] Sayna Ebrahimi, Sercan ¨O. Arik, and Tomas Pfister. Test- time adaptation for visual document understanding. CoRR, abs/2206.07240, 2022. 1 [16] Yossi Gandelsman, Yu Sun, Xinlei Chen, and Alexei A Efros. Test-time training with masked autoencoders. In NeurIPS, 2022. 1 [17] Yaroslav Ganin, Evgeniya Ustinova, Hana Ajakan, Pas- cal Germain, Hugo Larochelle, Franc ¸ois Laviolette, Mario Marchand, and Victor S. Lempitsky. Domain-adversarial training of neural networks. J. Mach. Learn. Res., 17:59:1– 59:35, 2016. 1, 2 [18] Yunhe Gao, Xingjian Shi, Yi Zhu, Hao Wang, Zhiqiang Tang, Xiong Zhou, Mu Li, and Dimitris N. Metaxas. Vi- sual prompt tuning for test-time domain adaptation. CoRR, abs/2210.04831, 2022. 3 [19] Taesik Gong, Jongheon Jeong, Taewon Kim, Yewon Kim, Jinwoo Shin, and Sung-Ju Lee. Robust continual test- time adaptation: Instance-aware BN and prediction-balanced memory. In NeurIPS, 2022. 1, 2, 3, 4, 6, 7, 8, 13, 14, 15, 16, 17 [20] Sachin Goyal, Mingjie Sun, Aditi Raghunathan, and J Zico Kolter. Test time adaptation via conjugate pseudo-labels. In NeurIPS, 2022. 1, 3, 4 [21] Yves Grandvalet and Yoshua Bengio. Semi-supervised learning by entropy minimization. In NeurIPS, pages 529– 536, 2004. 3 [22] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image recognition. In CVPR, pages 770–778, 2016. 1, 6 [23] Dan Hendrycks and Thomas G. Dietterich. Benchmarking neural network robustness to common corruptions and per- turbations. In ICLR, 2019. 2, 6 [24] Hengguan Huang, Xiangming Gu, Hao Wang, Chang Xiao, Hongfu Liu, and Ye Wang. Extrapolative continuous-time bayesian neural network for fast training-free test-time adap- tation. In NeurIPS, 2022. 1 [25] Sergey Ioffe and Christian Szegedy. Batch normalization: Accelerating deep network training by reducing internal co- variate shift. In ICML, pages 448–456, 2015. 3, 4 [26] Yusuke Iwasawa and Yutaka Matsuo. Test-time classifier ad- justment module for model-agnostic domain generalization. In NeurIPS, pages 2427–2440, 2021. 1, 3 [27] Vidit Jain and Erik Learned-Miller. Online domain adapta- tion of a pre-trained cascade of classifiers. In CVPR, pages 577–584, 2011. 3 [28] Minguk Jang and Sae-Young Chung. Test-time adaptation via self-training with nearest neighbor information. CoRR, abs/2207.10792, 2022. 3, 4 [29] Junho Kim, Inwoo Hwang, and Young Min Kim. Ev-tta: Test-time adaptation for event-based object recognition. In CVPR, pages 17724–17733, 2022. 1 [30] Diederik P. Kingma and Jimmy Ba. Adam: A method for stochastic optimization. In ICLR, 2015. 6 [31] James Kirkpatrick, Razvan Pascanu, Neil C. Rabinowitz, Joel Veness, Guillaume Desjardins, Andrei A. Rusu, Kieran Milan, John Quan, Tiago Ramalho, Agnieszka Grabska- Barwinska, Demis Hassabis, Claudia Clopath, Dharshan Ku- maran, and Raia Hadsell. Overcoming catastrophic forget- ting in neural networks. CoRR, abs/1612.00796, 2016. 3 [32] Alex Krizhevsky, Geoffrey Hinton, et al. Learning multiple layers of features from tiny images. 2009. 6[33] Alex Krizhevsky, Ilya Sutskever, and Geoffrey E. Hinton. Imagenet classification with deep convolutional neural net- works. In NeurIPS, pages 1097–1105, 2012. 1 [34] Ananya Kumar, Tengyu Ma, and Percy Liang. Understand- ing self-training for gradual domain adaptation. In ICML, pages 5468–5479, 2020. 3 [35] Jogendra Nath Kundu, Naveen Venkat, Rahul M. V ., and R. Venkatesh Babu. Universal source-free domain adapta- tion. In CVPR, pages 4543–4552, 2020. 3 [36] Vinod K Kurmi, Venkatesh K Subramanian, and Vinay P Namboodiri. Domain impression: A source data free do- main adaptation method. In WACV, pages 615–625, 2021. 3 [37] Matthias De Lange, Rahaf Aljundi, Marc Masana, Sarah Parisot, Xu Jia, Ales Leonardis, Gregory G. Slabaugh, and Tinne Tuytelaars. A continual learning survey: Defying for- getting in classification tasks. IEEE Trans. Pattern Anal. Mach. Intell., 44(7):3366–3385, 2022. 3 [38] Yann LeCun, Yoshua Bengio, and Geoffrey E. Hinton. Deep learning. Nat., 521(7553):436–444, 2015. 1 [39] Dong-Hyun Lee et al. Pseudo-label: The simple and effi- cient semi-supervised learning method for deep neural net- works. In Workshop on challenges in representation learn- ing, ICML, volume 3, page 896, 2013. 6, 7, 8, 12, 14, 15, 16, 17 [40] Da Li, Yongxin Yang, Yi-Zhe Song, and Timothy M. Hospedales. Learning to generalize: Meta-learning for do- main generalization. In AAAI, pages 3490–3497, 2018. 1, 3 [41] Haoliang Li, Sinno Jialin Pan, Shiqi Wang, and Alex C. Kot. Domain generalization with adversarial feature learning. In CVPR, pages 5400–5409, 2018. 1, 3 [42] Shuang Li, Binhui Xie, Qiuxia Lin, Chi Harold Liu, Gao Huang, and Guoren Wang. Generalized domain conditioned adaptation network. IEEE Trans. Pattern Anal. Mach. Intell., 44(8):4093–4109, 2022. 1 [43] Shuang Li, Mixue Xie, Kaixiong Gong, Chi Harold Liu, Yulin Wang, and Wei Li. Transferable semantic augmen- tation for domain adaptation. In CVPR, pages 11516–11525, 2021. 2 [44] Zhizhong Li and Derek Hoiem. Learning without forgetting. IEEE Trans. Pattern Anal. Mach. Intell., 40(12):2935–2947, 2018. 3 [45] Jian Liang, Dapeng Hu, and Jiashi Feng. Do we really need to access the source data? source hypothesis transfer for un- supervised domain adaptation. In ICML, pages 6028–6039, 2020. 1, 3 [46] Yuejiang Liu, Parth Kothari, Bastien van Delft, Baptiste Bellot-Gurlet, Taylor Mordan, and Alexandre Alahi. TTT++: when does self-supervised test-time training fail or thrive? In NeurIPS, pages 21808–21820, 2021. 3 [47] Yuang Liu, Wei Zhang, and Jun Wang. Source-free do- main adaptation for semantic segmentation. In CVPR, pages 1215–1224, 2021. 3 [48] Mingsheng Long, Yue Cao, Zhangjie Cao, Jianmin Wang, and Michael I. Jordan. Transferable representation learning with deep adaptation networks. IEEE Trans. Pattern Anal. Mach. Intell., 41(12):3071–3085, 2019. 1, 2 [49] Wenao Ma, Cheng Chen, Shuang Zheng, Jing Qin, Huimao Zhang, and Qi Dou. Test-time adaptation with calibration of medical image classification nets for label distribution shift. In MICCAI, pages 313–323, 2022. 1 [50] Divyat Mahajan, Shruti Tople, and Amit Sharma. Domain generalization using causal matching. In ICML, pages 7313– 7324, 2021. 3 [51] Yishay Mansour, Mehryar Mohri, and Afshin Rostamizadeh. Domain adaptation: Learning bounds and algorithms. In COLT, 2009. 2 [52] Krikamol Muandet, David Balduzzi, and Bernhard Sch¨olkopf. Domain generalization via invariant fea- ture representation. In ICML, pages 10–18, 2013. 1, 3 [53] Zachary Nado, Shreyas Padhy, D. Sculley, Alexander D’Amour, Balaji Lakshminarayanan, and Jasper Snoek. Evaluating prediction-time batch normalization for robust- ness under covariate shift. CoRR, abs/2006.10963, 2020. 4, 6, 7, 8, 12, 14, 15, 16, 17 [54] Shuaicheng Niu, Jiaxiang Wu, Yifan Zhang, Yaofo Chen, Shijian Zheng, Peilin Zhao, and Mingkui Tan. Efficient test- time model adaptation without forgetting. In ICML, pages 16888–16905, 2022. 1 [55] Shuaicheng Niu, Jiaxiang Wu, Yifan Zhang, Yaofo Chen, Shijian Zheng, Peilin Zhao, and Mingkui Tan. Efficient test- time model adaptation without forgetting. In ICML, volume 162, pages 16888–16905, 2022. 4, 5, 6 [56] Sinno Jialin Pan and Qiang Yang. A survey on transfer learning. IEEE Trans. Knowl. Data Eng., 22(10):1345–1359, 2010. 1 [57] Adam Paszke, Sam Gross, Francisco Massa, Adam Lerer, James Bradbury, Gregory Chanan, Trevor Killeen, Zeming Lin, Natalia Gimelshein, Luca Antiga, et al. Pytorch: An imperative style, high-performance deep learning library. In NeurIPS, pages 8024–8035, 2019. 6 [58] Xingchao Peng, Qinxun Bai, Xide Xia, Zijun Huang, Kate Saenko, and Bo Wang. Moment matching for multi-source domain adaptation. In ICCV, pages 1406–1415, 2019. 2, 6 [59] Joaquin Quinonero-Candela, Masashi Sugiyama, Anton Schwaighofer, and Neil D Lawrence. Dataset shift in ma- chine learning. 2008. 1 [60] Sylvestre-Alvise Rebuffi, Alexander Kolesnikov, Georg Sperl, and Christoph H. Lampert. icarl: Incremental classi- fier and representation learning. InCVPR, pages 5533–5542, 2017. 3 [61] Amelie Royer and Christoph H Lampert. Classifier adapta- tion at prediction time. In CVPR, pages 1401–1409, 2015. 3 [62] Kuniaki Saito, Kohei Watanabe, Yoshitaka Ushiku, and Tat- suya Harada. Maximum classifier discrepancy for unsuper- vised domain adaptation. In CVPR, pages 3723–3732, 2018. 2 [63] Inkyu Shin, Yi-Hsuan Tsai, Bingbing Zhuang, Samuel Schulter, Buyu Liu, Sparsh Garg, In So Kweon, and Kuk- Jin Yoon. MM-TTA: multi-modal test-time adaptation for 3d semantic segmentation. In CVPR, pages 16907–16916, 2022. 1, 3[64] Manli Shu, Weili Nie, De-An Huang, Zhiding Yu, Tom Goldstein, Anima Anandkumar, and Chaowei Xiao. Test- time prompt tuning for zero-shot generalization in vision- language models. In NeurIPS, 2022. 1, 3 [65] Yu Sun, Xiaolong Wang, Zhuang Liu, John Miller, Alexei Efros, and Moritz Hardt. Test-time training with self- supervision for generalization under distribution shifts. In ICML, pages 9229–9248, 2020. 1, 2, 3, 4 [66] Rishabh Tiwari, KrishnaTeja Killamsetty, Rishabh K. Iyer, and Pradeep Shenoy. GCR: gradient coreset based replay buffer selection for continual learning. In CVPR, pages 99– 108, 2022. 3 [67] Yi-Hsuan Tsai, Wei-Chih Hung, Samuel Schulter, Ki- hyuk Sohn, Ming-Hsuan Yang, and Manmohan Chandraker. Learning to adapt structured output space for semantic seg- mentation. In CVPR, pages 7472–7481, 2018. 2 [68] Eric Tzeng, Judy Hoffman, Trevor Darrell, and Kate Saenko. Simultaneous deep transfer across domains and tasks. In ICCV, pages 4068–4076, 2015. 2 [69] Eric Tzeng, Judy Hoffman, Kate Saenko, and Trevor Darrell. Adversarial discriminative domain adaptation. In CVPR, pages 2962–2971, 2017. 1 [70] Dequan Wang, Evan Shelhamer, Shaoteng Liu, Bruno A. Ol- shausen, and Trevor Darrell. Tent: Fully test-time adaptation by entropy minimization. In ICLR, 2021. 1, 2, 3, 4, 6, 7, 8, 12, 13, 14, 15, 16, 17 [71] Jindong Wang, Cuiling Lan, Chang Liu, Yidong Ouyang, Tao Qin, Wang Lu, Yiqiang Chen, Wenjun Zeng, and Philip Yu. Generalizing to unseen domains: A survey on domain generalization. IEEE Trans. Knowl. Data Eng., 2022. 1 [72] Mei Wang and Weihong Deng. Deep visual domain adapta- tion: A survey. Neurocomputing, 312:135–153, 2018. 1 [73] Qin Wang, Olga Fink, Luc Van Gool, and Dengxin Dai. Con- tinual test-time domain adaptation. In CVPR, pages 7191– 7201, 2022. 1, 2, 3, 4, 6, 7, 8, 13, 14, 15, 16, 17 [74] Markus Wulfmeier, Alex Bewley, and Ingmar Posner. Incre- mental adversarial domain adaptation for continually chang- ing environments. In ICRA, pages 4489–4495, 2018. 3 [75] Binhui Xie, Shuang Li, Mingjia Li, Chi Harold Liu, Gao Huang, and Guoren Wang. Sepico: Semantic-guided pixel contrast for domain adaptive semantic segmentation. IEEE Trans. Pattern Anal. Mach. Intell., pages 1–17, 2023. 2 [76] Saining Xie, Ross Girshick, Piotr Doll ´ar, Zhuowen Tu, and Kaiming He. Aggregated residual transformations for deep neural networks. In CVPR, pages 5987–5995, 2017. 6 [77] Ruijia Xu, Guanbin Li, Jihan Yang, and Liang Lin. Larger norm more transferable: An adaptive feature norm approach for unsupervised domain adaptation. In ICCV, pages 1426– 1435, 2019. 2 [78] Zhenlin Xu, Deyi Liu, Junlin Yang, Colin Raffel, and Marc Niethammer. Robust and generalizable visual representation learning via random convolutions. In ICLR, 2021. 3 [79] Shiqi Yang, Yaxing Wang, Joost van de Weijer, Luis Herranz, and Shangling Jui. Generalized source-free domain adapta- tion. In ICCV, pages 8978–8987, 2021. 3 [80] Sergey Zagoruyko and Nikos Komodakis. Wide residual net- works. In BMVC, 2016. 6 [81] Marvin Mengxin Zhang, Sergey Levine, and Chelsea Finn. MEMO: Test time robustness via adaptation and augmenta- tion. In NeurIPS, 2022. 1, 4 [82] Yizhe Zhang, Shubhankar Borse, Hong Cai, and Fatih Porikli. Auxadapt: Stable and efficient test-time adaptation for temporally consistent video semantic segmentation. In WACV, pages 2633–2642, 2022. 1, 3 [83] Kaiyang Zhou, Ziwei Liu, Yu Qiao, Tao Xiang, and Chen Change Loy. Domain generalization: A survey. IEEE Trans. Pattern Anal. Mach. Intell., 2022. 1 [84] Kaiyang Zhou, Yongxin Yang, Yu Qiao, and Tao Xiang. Do- main generalization with mixstyle. In ICLR, 2021. 3 [85] Yang Zou, Zhiding Yu, BVK Vijaya Kumar, and Jinsong Wang. Unsupervised domain adaptation for semantic seg- mentation via class-balanced self-training. In ECCV, pages 289–305, 2018. 26. Appendix 6.1. Discussion Societal impact. RoTTA enables adapting pre-trained models on continually changing distributions with correl- atively sampled test streams without any more raw data or label requirements. Thus, our work may have a positive im- pact on communities to effectively deploy and adapt models in various real-world scenarios, which is economically and environmentally friendly. And since no training data is re- quired, this protects data privacy and has potential commer- cial value. We carry out experiments on benchmark datasets and do not notice any societal issues. It does not involve sensitive attributes. Future work. Our work suggests a few promising direc- tions for future work. Firstly, the proposed RoTTA is a preliminary attempt to perform test-time adaptation for the more realistic test stream under the setup PTTA. One could experiment to improve the algorithm by replacing some parts of RoTTA. More importantly, we hope that with this work, we can open a path to the original goal of test-time adaptation, which is performing test-time adaptation in real- world scenarios. Thus, one could improve PTTA to make it more realistic. Limitations. RoTTA achieves excellent performance on various tasks under the setup PTTA as demonstrated in Sec- tion 4 in the main paper, but we still find some limitations of it. Firstly, the adopted robust batch normalization (RBN) is a naive solution to the normalization of the correlatively sampled batch of data. This requires careful design of the value of α in RBN. Secondly, we observe that during the adaptation procedure of some methods like PL [39] and TENT [70], the model collapse finally. Although we de- sign many strategies to stabilize the adaptation and model collapse never occurs in the experiments of RoTTA, we are still missing a way to recover the model from the collapse state as a remedy. Thirdly, category similarity is only one kind of correlation. Although we conduct experiments on different datasets with Dirichlet distribution to simulate cor- relatively sampled test streams, we still need to validate our approach in some real-world scenarios. 6.2. Sensitivity to different hyper-parameters In this section, we conduct a detailed sensitivity analy- sis of the hyperparameters involved in RoTTA. All experi- ments are conducted on CIFAR100→CIFAR100-C, and the corruptions changes as motion, snow, fog, shot, defocus, contrast, zoom, brightness, frost, elastic, glass, gaussian, pixelate, jpeg, and impulse, and test streams are sampled correlatively with the Dirichlet parameter δ = 0.1. When we investigate the sensitivity to a specific hyperparameter, other hyperparameters are fixed to the default values, i.e., λt = 1.0, λu = 1.0, α = 0.05, and ν = 0.001, for all experiments. Table 7. Classification error with different value of λt/λu. λt/λu 0.0/2.0 0.5/1.5 1.0/1.0 1.5/ 0.5 2.0/ 0.0 CIFAR100-C 57.5 36.9 35.0 35.9 38.9 Trade-off between timeliness and uncertainty. When updating the memory bank, we take the timeliness and uncertainty of samples into account simultaneously, and λt and λu will make a trade-off between them. In Table 7, we show the results of RoTTA with varying λt/λu, i.e., λt/λu ∈ {0.0/2.0, 0.5/1.5, 1.0/1.0, 1.5/0.5, 2.0/0.0}. When we consider both of them, the results are relatively stable (35.0-36.9%). When we only think about one side, the performance drops significantly. For example, when we set λt/λu = 0.0/2.0 which means only considering uncer- tainty, the performance drops 22.5%. That’s because some confident samples get stuck in the memory bank, making it not work the way we design it. Table 8. Classification error with varying α α 0.5 0.1 0.05 0.01 0.005 0.001 CIFAR100-C 39.0 36.0 35.0 36.0 38.1 41.5 Sensitivity to α. We show the results of RoTTA with vary- ing α, i.e., α ∈ {0.5, 0.1, 0.05, 0.01, 0.005, 0.001} in Ta- ble 8. A larger value of α means updating the global statis- tics faster and vice versa. We can see that RoTTA achieves competitive results (35.0 − 36.0%) at appropriate values of α, i.e., α ∈ {0.1, 0.05, 0.01}. Updating too aggressively or too gently can lead to unreliable estimates of statistics. Table 9. Classification error with varying ν ν 0.05 0.01 0.005 0.001 0.0005 0.0001 CIFAR100-C 44.8 39.1 37.1 35.0 37.6 43.6 Sensitivity to ν. We show the results of RoTTA with vary- ing ν, i.e., ν ∈ {0.05, 0.01, 0.005, 0.001, 0.0005, 0.0001} in Table 9. As we can see, the best performance is achieved at ν = 0.001. Updating the teacher model too quickly or too slowly can cause performance degradation. 6.3. Additional experiment details and results 6.3.1 Compared methods BN [53] utilizes statistics of the current batch of data to nor- malize their feature maps without tuning any parameters. PL [39] is based on BN [53], and adopts pseudo labels to train the affine parameters in BN layers.TENT [70] is the first to propose fully test-time adaptation. It adopts test-time batch normalization and utilizes entropy minimization to train the affine parameters of BN layers. We reimplement it following the released code https:// github.com/DequanWang/tent. LAME [5] adapts the output of the pre-trained model by optimizing a group of latent variables without tuning any in- ner parts of the model. We reimplement it following the re- leased code https://github.com/fiveai/LAME. CoTTA [73] considers performing test-time adapta- tion on continually changing distributions and pro- pose augmentation-averaged pseudo-labels and stochastic restoration to address error accumulation and catastrophic forgetting. We reimplement it following the released code https://github.com/qinenergy/cotta. NOTE [19] proposes instance-aware normalization and prediction-balanced reservoir sampling to stable the adapta- tion on temporally correlated test streams. We reimplement it following the released code https://github.com/ TaesikGong/NOTE. 6.3.2 Simulate correlatively sampling As we described in the scenarios of autonomous driving that the car will follow more vehicles on the highway or will en- counter more pedestrians on the sidewalk, so we use the same category to simulate correlation. From a macro point of view, the test distribution Ptest changes continually as P0, P1, ...,P∞. During the period when Ptest = Pt, we adopt Dirichlet distribution to simulate correlatively sam- pled test stream. More specifically, we consider dividing samples of C classes into T slots. Firstly, we utilize Dirich- let distribution with parameter γ to generate the partition criterion q ∈ RC×T . Then for each class c, we split samples into T parts according to qc and assign each part to each slot respectively. Finally, we concatenate all slots to sim- ulate the correlatively sampled test stream for Ptest = Pt. And as Ptest changes, we use the above method again to generate the test stream. 6.3.3 Detailed results of different orders We report the average classification error of ten different distribution changing orders in Table 6 of the main pa- per. And then we present the specific results here, includ- ing Table 10, 11, 12, 13, 14, 15, 16, 17, 18, and 19 for CIFAR10→CIFAR10-C and Table 20, 21, 22, 23, 24, 25, 26, 27, 28, and 29 for CIFAR100 →CIFAR100-C. We can see consistently superior performance of RoTTA. One thing to mention is that on DomainNet we use alphabetical order to determine the order of domain changes.Table 10. Average classification error of the task CIFAR10→ CIFAR10-C while continually adapting to different corruptions at the highest severity 5 with correlatively sampled test stream under the proposed setup PTTA. Time t− − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − → Method brightnesspixelategaussianmotionzoom glass impulsejpeg defocuselasticshot frost snow fog contrast Avg. Source 9.3 58.5 72.3 34.8 42.0 54.3 72.9 30.3 46.9 26.6 65.7 41.3 25.1 26.0 46.7 43.5BN [53] 71.1 75.2 76.8 74.2 73.7 80.1 79.3 77.5 73.8 77.7 77.2 73.3 73.8 72.7 71.7 75.2PL [39] 71.7 75.9 80.2 78.4 80.2 85.2 85.3 85.4 85.1 86.7 87.9 87.9 88.1 88.3 87.9 83.6TENT [70] 71.6 75.9 81.3 80.5 82.3 85.6 87.1 87.0 87.1 88.1 88.2 87.8 87.9 88.3 88.2 84.4LAME [5] 5.4 56.8 73.1 29.1 37.0 50.5 71.4 22.3 42.8 18.6 65.5 37.3 18.8 20.4 43.6 39.5CoTTA [73] 75.0 79.8 83.1 83.4 83.2 84.0 84.5 83.2 83.5 83.3 83.6 83.0 83.0 83.4 83.7 82.6NOTE [19] 10.1 29.9 47.1 23.4 28.4 48.4 46.1 41.8 26.9 36.1 37.5 25.0 25.0 23.2 14.2 30.9 RoTTA 10.4 26.6 37.5 23.9 17.0 40.9 39.7 30.1 18.0 29.9 30.1 23.6 21.7 17.6 19.0 25.7(+5.2) Table 11. Average classification error of the task CIFAR10→ CIFAR10-C while continually adapting to different corruptions at the highest severity 5 with correlatively sampled test stream under the proposed setup PTTA. Time t− − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − → Method jpeg shot zoom frost contrastfog defocuselasticgaussianbrightnessglass impulsepixelatesnow motion Avg. Source 30.3 65.7 42.0 41.3 46.7 26.0 46.9 26.6 72.3 9.3 54.3 72.9 58.5 25.1 34.8 43.5BN [53] 77.6 75.8 73.4 74.1 73.1 72.5 72.9 77.1 77.2 72.2 79.9 79.9 75.5 74.6 72.9 75.2PL [39] 77.6 77.1 76.6 78.3 77.5 79.8 82.0 84.8 86.1 83.5 87.8 87.1 86.5 85.6 85.7 82.4TENT [70] 78.5 78.2 79.2 81.8 84.8 84.8 86.4 87.3 87.9 86.7 87.3 87.8 87.2 87.5 87.1 84.8LAME [5] 22.5 65.2 37.0 37.1 44.0 20.3 41.7 18.7 72.8 5.2 51.2 71.5 57.0 19.0 29.4 39.5CoTTA [73]78.5 81.0 82.8 84.1 84.9 83.4 83.5 83.5 84.5 83.3 84.7 84.6 83.0 84.4 83.4 83.3NOTE [19]35.4 36.1 22.1 21.3 11.6 24.8 24.5 36.0 37.7 18.4 49.0 47.4 43.9 30.4 29.2 31.2 RoTTA 33.2 33.3 19.8 24.1 24.9 20.5 16.2 31.7 28.4 11.8 43.1 36.9 32.5 20.7 20.6 26.5(+4.7) Table 12. Average classification error of the task CIFAR10→ CIFAR10-C while continually adapting to different corruptions at the highest severity 5 with correlatively sampled test stream under the proposed setup PTTA. Time t− − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − → Method contrastdefocusgaussianshot snow frost glass zoom elasticjpeg pixelatebrightnessimpulsemotion fog Avg. Source 46.7 46.9 72.3 65.7 25.1 41.3 54.3 42.0 26.6 30.3 58.5 9.3 72.9 34.8 26.0 43.5BN [53] 72.3 72.6 76.9 77.1 74.8 73.5 80.0 73.2 77.4 78.6 76.4 71.0 79.1 73.9 71.5 75.2PL [39] 72.4 75.3 80.7 82.6 83.3 83.5 86.6 85.7 86.6 88.4 87.5 86.6 88.3 88.2 86.8 84.1TENT [70] 73.5 77.9 85.5 86.9 87.6 87.8 88.3 87.7 88.6 89.2 88.5 88.5 89.3 88.6 88.6 86.4LAME [5] 43.5 42.3 73.1 65.3 19.2 37.3 51.1 36.8 18.5 22.5 56.9 5.5 71.1 29.1 20.5 39.5CoTTA [73]79.4 80.3 83.8 83.9 83.9 83.4 85.0 83.2 85.1 84.3 83.9 83.3 84.7 83.9 82.5 83.4NOTE [19] 9.6 21.8 40.1 31.0 25.5 22.6 44.8 22.8 33.2 39.4 33.2 18.1 50.0 28.3 29.8 30.0 RoTTA 18.4 17.9 38.4 31.9 23.3 19.8 40.7 17.4 31.4 29.8 27.8 11.3 43.8 19.7 18.8 26.0(+4.0) Table 13. Average classification error of the task CIFAR10→ CIFAR10-C while continually adapting to different corruptions at the highest severity 5 with correlatively sampled test stream under the proposed setup PTTA. Time t− − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − → Method shot fog glass pixelatesnow elasticbrightnessimpulsedefocusfrost contrastgaussianmotionjpeg zoom Avg. Source 65.7 26.0 54.3 58.5 25.1 26.6 9.3 72.9 46.9 41.3 46.7 72.3 34.8 30.3 42.0 43.5BN [53] 76.4 72.0 80.4 76.2 74.8 77.0 71.1 79.6 73.8 74.4 73.0 77.0 72.5 78.3 72.5 75.3PL [39] 77.0 73.3 82.4 79.8 81.0 82.3 79.5 84.4 82.7 83.5 83.5 85.5 84.8 87.0 84.5 82.1TENT [70]76.9 74.6 82.3 81.7 82.0 84.9 84.8 87.3 86.6 87.3 87.6 89.2 88.3 88.9 87.3 84.6LAME [5] 65.3 20.6 50.9 56.7 19.2 18.8 5.4 71.8 42.8 37.2 43.3 73.2 29.4 22.6 36.9 39.6CoTTA [73]77.4 77.6 83.8 81.9 82.2 82.6 80.4 83.3 82.3 81.5 82.7 82.6 81.1 82.9 81.0 81.6NOTE [19]34.0 20.9 43.1 36.6 24.0 36.4 12.1 48.0 25.9 23.9 13.4 38.1 25.0 43.2 24.2 29.9 RoTTA 35.0 21.1 43.9 29.2 22.1 29.7 10.8 44.6 25.3 22.7 24.6 29.4 26.9 34.4 16.1 27.7(+2.2) Table 14. Average classification error of the task CIFAR10→ CIFAR10-C while continually adapting to different corruptions at the highest severity 5 with correlatively sampled test stream under the proposed setup PTTA. Time t− − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − → Method pixelateglass zoomsnow fog impulsebrightnessmotionfrost jpeg gaussianshot contrastdefocus elastic Avg. Source 58.5 54.3 42.0 25.1 26.0 72.9 9.3 34.8 41.3 30.3 72.3 65.7 46.7 46.9 26.6 43.5BN [53] 76.0 79.6 73.3 75.2 72.9 79.8 71.1 73.5 74.1 78.6 77.4 76.1 72.0 73.8 76.4 75.3PL [39] 76.7 81.3 77.4 80.3 81.2 86.3 83.3 85.9 86.2 87.7 88.1 88.4 87.4 87.6 87.7 84.4TENT [70] 76.4 80.2 77.8 81.2 83.0 87.1 85.6 87.2 87.6 88.7 88.6 88.9 88.5 88.6 88.2 85.2LAME [5] 56.9 50.7 37.0 19.0 20.3 71.5 5.4 29.2 37.2 22.5 73.0 65.3 43.8 42.4 18.7 39.5CoTTA [73]77.1 83.6 84.1 84.8 84.4 85.2 84.0 84.3 84.9 84.9 85.0 84.7 85.3 84.4 84.3 84.1NOTE [19] 27.8 52.2 24.5 22.3 21.6 44.5 14.5 21.3 25.9 42.5 38.8 36.0 16.7 28.1 40.6 30.5 RoTTA 25.9 43.3 17.7 22.1 20.2 41.5 12.2 22.9 22.5 31.2 33.8 26.0 31.4 17.7 27.6 26.4(+4.1)Table 15. Average classification error of the task CIFAR10→ CIFAR10-C while continually adapting to different corruptions at the highest severity 5 with correlatively sampled test stream under the proposed setup PTTA. Time t− − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − → Method motionsnow fog shot defocuscontrastzoom brightnessfrost elasticglass gaussianpixelatejpeg impulse Avg. Source 34.8 25.1 26.0 65.7 46.9 46.7 42.0 9.3 41.3 26.6 54.3 72.3 58.5 30.3 72.9 43.5BN [53] 73.2 73.4 72.7 77.2 73.7 72.5 72.9 71.0 74.1 77.7 80.0 76.9 75.5 78.3 79.0 75.2PL [39] 73.9 75.0 75.6 81.0 79.9 80.6 82.0 83.2 85.3 87.3 88.3 87.5 87.5 87.5 88.2 82.9TENT [70] 74.3 77.4 80.1 86.2 86.7 87.3 87.9 87.4 88.2 89.0 89.2 89.0 88.3 89.7 89.2 86.0LAME [5] 29.5 19.0 20.3 65.3 42.4 43.4 36.8 5.4 37.2 18.6 51.2 73.2 57.0 22.6 71.3 39.5CoTTA [73]77.1 80.6 83.1 84.4 83.9 84.2 83.1 82.6 84.4 84.2 84.5 84.6 82.7 83.8 84.9 83.2NOTE [19] 18.0 22.1 20.6 35.6 26.9 13.6 26.5 17.3 27.2 37.0 48.3 38.8 42.6 41.9 49.7 31.1 RoTTA 18.1 21.3 18.8 33.6 23.6 16.5 15.1 11.2 21.9 30.7 39.6 26.8 33.7 27.8 39.5 25.2(+5.9) Table 16. Average classification error of the task CIFAR10→ CIFAR10-C while continually adapting to different corruptions at the highest severity 5 with correlatively sampled test stream under the proposed setup PTTA. Time t− − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − → Method frost impulsejpeg contrastzoom glass pixelatesnow defocusmotionbrightnesselasticshot fog gaussian Avg. Source 41.3 72.9 30.3 46.7 42.0 54.3 58.5 25.1 46.9 34.8 9.3 26.6 65.7 26.0 72.3 43.5BN [53] 73.8 79.1 77.9 73.0 73.7 80.1 75.7 74.4 73.7 74.0 71.7 77.0 75.9 72.8 76.2 75.3PL [39] 74.2 80.9 80.4 79.5 81.8 85.9 83.9 85.1 84.7 85.9 85.9 86.7 87.2 87.0 87.8 83.8TENT [70]73.9 80.3 81.8 81.6 83.6 86.3 85.6 85.7 86.4 87.7 87.4 88.8 88.8 88.5 88.4 85.0LAME [5] 37.4 71.8 22.4 43.5 37.0 50.5 57.0 19.0 42.8 29.1 5.4 18.7 65.2 20.4 72.9 39.5CoTTA [73]76.5 82.2 82.8 85.0 82.9 85.0 83.0 82.9 83.5 83.4 82.6 83.7 83.2 83.3 83.6 82.9NOTE [19]21.1 41.4 36.3 10.2 21.7 46.7 37.5 26.4 26.1 21.4 14.3 37.9 38.5 24.4 40.7 29.6 RoTTA 22.2 44.9 35.2 18.8 19.7 41.5 28.5 23.2 21.2 18.6 12.4 30.0 27.4 20.0 31.2 26.3(+3.3) Table 17. Average classification error of the task CIFAR10→ CIFAR10-C while continually adapting to different corruptions at the highest severity 5 with correlatively sampled test stream under the proposed setup PTTA. Time t− − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − → Method defocusmotionzoom shot gaussianglass jpeg fog contrastpixelatefrost snow brightnesselastic impulse Avg. Source 46.9 34.8 42.0 65.7 72.3 54.3 30.3 26.0 46.7 58.5 41.3 25.1 9.3 26.6 72.9 43.5BN [53] 72.8 72.7 73.3 77.2 77.3 80.0 77.6 72.6 73.3 76.6 73.8 74.1 70.3 77.5 79.0 75.2PL [39] 73.2 74.6 76.5 81.7 82.8 84.6 85.1 84.6 86.2 86.4 86.1 87.1 86.8 88.4 88.1 83.5TENT [70] 73.7 74.3 77.1 82.5 84.3 86.9 87.4 86.6 88.0 88.5 88.1 88.5 88.4 89.4 88.9 84.8LAME [5] 42.5 29.3 37.0 65.3 73.2 50.5 22.5 20.5 43.5 56.9 37.1 18.9 5.4 18.5 71.3 39.5CoTTA [73]76.3 79.8 82.4 83.3 83.8 84.5 83.1 82.7 84.7 82.9 83.0 83.3 81.4 83.8 83.8 82.6NOTE [19] 18.5 18.8 23.6 36.5 33.7 47.8 38.6 22.8 13.0 40.0 29.2 26.3 17.5 44.0 52.9 30.9 RoTTA 17.0 17.5 16.5 33.8 33.3 42.7 29.4 18.0 19.6 29.5 20.7 22.1 11.5 29.5 38.1 25.3(+5.6) Table 18. Average classification error of the task CIFAR10→ CIFAR10-C while continually adapting to different corruptions at the highest severity 5 with correlatively sampled test stream under the proposed setup PTTA. Time t− − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − → Method glass zoom impulsefog snow jpeg gaussianfrost shot brightnesscontrastmotionpixelatedefocus elastic Avg. Source 54.3 42.0 72.9 26.0 25.1 30.3 72.3 41.3 65.7 9.3 46.7 34.8 58.5 46.9 26.6 43.5BN [53] 79.7 72.3 79.8 73.2 74.7 77.7 76.6 73.2 77.1 72.2 73.0 73.3 75.5 73.8 76.4 75.2PL [39] 79.6 73.2 81.3 77.3 79.1 83.0 83.2 83.0 85.5 84.3 87.0 86.9 86.4 86.5 87.6 82.9TENT [70] 79.5 74.1 84.2 82.2 84.5 86.5 86.7 85.9 87.2 86.6 86.8 87.3 86.9 87.4 87.3 84.9LAME [5] 50.8 36.9 71.3 20.6 19.2 22.4 72.5 37.2 65.4 5.2 43.3 29.1 57.0 42.4 18.7 39.5CoTTA [73]81.5 79.4 85.2 84.1 84.5 84.2 84.8 84.0 84.8 83.2 85.2 83.8 83.2 84.6 83.6 83.7NOTE [19]45.0 21.2 42.3 21.0 21.6 38.4 36.4 21.4 33.1 16.7 14.6 25.4 43.5 29.1 38.5 29.9 RoTTA 42.6 17.6 48.1 23.9 21.9 32.6 32.1 20.7 30.2 12.0 21.9 20.0 33.7 16.4 28.1 26.8(+3.1) Table 19. Average classification error of the task CIFAR10→ CIFAR10-C while continually adapting to different corruptions at the highest severity 5 with correlatively sampled test stream under the proposed setup PTTA. Time t− − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − → Method contrastgaussiandefocuszoom frost glass jpeg fog pixelateelasticshot impulsesnow motion brightness Avg. Source 46.7 72.3 46.9 42.0 41.3 54.3 30.3 26.0 58.5 26.6 65.7 72.9 25.1 34.8 9.3 43.5BN [53] 72.4 76.2 73.2 73.7 73.6 80.0 77.6 72.6 76.4 77.7 77.2 79.9 73.8 73.9 70.0 75.2PL [39] 73.0 78.2 76.7 79.7 81.6 85.6 86.0 85.3 87.2 88.2 88.3 88.9 88.5 89.2 88.2 84.3TENT [70] 73.6 80.9 83.1 85.6 87.1 88.5 88.8 88.4 89.2 89.3 89.0 89.0 89.3 89.9 89.1 86.7LAME [5] 43.5 73.2 42.3 37.0 37.2 50.5 22.5 20.5 57.0 18.6 65.5 71.5 18.8 29.1 5.6 39.5CoTTA [73]79.5 81.4 83.4 83.6 83.9 85.0 84.0 82.8 84.8 84.8 84.5 84.7 84.1 84.4 82.8 83.6NOTE [19] 9.6 43.6 26.5 24.8 23.9 46.9 38.0 23.4 34.0 41.2 41.5 45.0 27.6 25.8 19.0 31.4 RoTTA 18.4 36.0 21.1 15.6 23.0 41.7 30.8 19.1 34.1 31.1 31.3 39.9 26.0 18.8 12.8 26.6(+4.8)Table 20. Average classification error of the task CIFAR100 → CIFAR100-C while continually adapting to different corruptions at the highest severity 5 with correlatively sampled test stream under the proposed setup PTTA. Time t− − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − → Method brightnesspixelategaussianmotionzoom glass impulsejpeg defocuselasticshot frost snow fog contrast Avg. Source 29.5 74.7 73.0 30.8 28.8 54.1 39.4 41.2 29.3 37.2 68.0 45.8 39.5 50.3 55.1 46.4BN [53] 46.5 52.0 58.6 47.4 47.4 57.6 58.2 56.9 47.0 53.4 56.0 52.5 53.1 57.7 49.1 52.9PL [39] 48.5 60.7 77.1 85.9 91.5 95.5 95.8 96.6 96.8 96.9 97.3 97.5 97.6 97.7 97.9 88.9TENT [70] 49.8 69.4 92.2 96.0 96.7 97.3 97.5 97.9 97.5 97.9 98.0 98.2 98.2 98.2 98.2 92.2LAME [5] 21.7 75.1 72.7 22.9 20.6 49.0 32.1 33.3 21.2 28.0 66.8 40.0 30.6 43.9 51.3 40.6CoTTA [73] 46.8 48.4 54.7 48.7 48.6 53.5 55.4 52.8 49.8 51.8 53.5 52.9 54.1 56.7 53.6 52.1NOTE [19] 42.6 53.0 69.9 52.1 53.3 70.4 73.1 76.7 80.8 96.0 97.7 97.1 96.6 97.2 95.8 76.8 RoTTA 28.4 37.3 44.6 31.9 28.3 41.8 43.6 39.9 28.0 35.2 38.2 33.7 33.0 39.5 31.0 35.6(+5.0) Table 21. Average classification error of the task CIFAR100 → CIFAR100-C while continually adapting to different corruptions at the highest severity 5 with correlatively sampled test stream under the proposed setup PTTA. Time t− − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − → Method jpeg shot zoom frost contrastfog defocuselasticgaussianbrightnessglass impulsepixelatesnow motion Avg. Source 41.2 68.0 28.8 45.8 55.1 50.3 29.3 37.2 73.0 29.5 54.1 39.4 74.7 39.5 30.8 46.4BN [53] 58.3 56.8 47.8 51.8 48.9 57.3 46.8 53.5 57.8 45.5 57.1 58.5 51.7 53.3 48.8 52.9PL [39] 59.4 66.3 74.9 87.5 94.2 95.5 96.2 97.1 97.4 97.2 97.5 97.7 98.0 98.2 98.2 90.4TENT [70] 62.0 79.3 91.7 95.8 96.9 97.0 97.4 97.7 97.6 97.7 97.9 97.9 98.0 97.9 97.9 93.5LAME [5] 33.6 66.7 21.1 39.9 50.6 43.9 21.0 28.6 72.5 21.6 48.6 32.5 74.5 30.6 22.5 40.6CoTTA [73]54.6 54.1 49.6 52.1 52.7 58.0 50.3 53.3 55.0 49.1 55.4 55.7 51.0 54.6 52.1 53.2NOTE [19]60.4 63.0 49.9 55.7 47.0 65.2 59.4 76.6 90.9 87.2 96.8 97.0 97.3 96.7 96.8 76.0 RoTTA 43.9 45.3 31.0 37.3 35.7 41.2 27.7 34.8 39.7 26.6 39.5 41.9 32.0 33.0 30.5 36.0(+4.6) Table 22. Average classification error of the task CIFAR100 → CIFAR100-C while continually adapting to different corruptions at the highest severity 5 with correlatively sampled test stream under the proposed setup PTTA. Time t− − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − → Method contrastdefocusgaussianshot snow frost glass zoom elasticjpeg pixelatebrightnessimpulsemotion fog Avg. Source 55.1 29.3 73.0 68.0 39.5 45.8 54.1 28.8 37.2 41.2 74.7 29.5 39.4 30.8 50.3 46.4BN [53] 49.4 47.2 58.6 56.2 52.7 52.0 57.9 46.1 54.4 57.7 50.5 46.2 58.2 47.6 58.5 52.9PL [39] 54.8 64.2 83.3 92.4 95.5 96.5 96.9 96.4 97.2 97.4 97.8 97.8 97.9 97.7 98.0 90.9TENT [70] 60.2 83.1 95.2 96.5 96.9 97.3 97.0 97.3 97.8 97.8 97.6 97.9 97.8 97.9 98.1 93.9LAME [5] 51.3 21.3 72.7 66.3 30.2 40.0 48.6 20.9 27.7 33.3 75.0 21.5 32.2 22.5 43.8 40.5CoTTA [73]52.1 48.6 55.1 52.7 53.4 51.9 55.9 49.2 53.2 52.8 49.2 49.7 56.2 50.7 58.1 52.6NOTE [19] 39.5 45.9 68.8 61.8 57.4 58.5 71.4 66.5 80.8 90.9 94.2 94.9 97.0 95.5 96.6 74.6 RoTTA 41.7 30.5 44.9 40.5 35.4 34.1 40.5 28.2 34.5 39.5 31.1 26.7 43.3 31.4 38.8 36.1(+4.4) Table 23. Average classification error of the task CIFAR100 → CIFAR100-C while continually adapting to different corruptions at the highest severity 5 with correlatively sampled test stream under the proposed setup PTTA. Time t− − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − → Method shot fog glass pixelatesnow elasticbrightnessimpulsedefocusfrost contrastgaussianmotionjpeg zoom Avg. Source 68.0 50.3 54.1 74.7 39.5 37.2 29.5 39.4 29.3 45.8 55.1 73.0 30.8 41.2 28.8 46.4BN [53] 57.5 58.6 58.5 50.5 52.7 53.1 45.9 57.9 47.0 51.5 47.8 58.2 48.2 57.1 47.7 52.8PL [39] 59.5 72.9 85.1 89.6 94.5 96.8 97.1 97.9 97.8 98.0 98.3 98.2 98.0 98.0 98.2 92.0TENT [70]60.3 81.4 95.0 96.6 97.0 97.3 97.3 97.7 97.7 97.7 97.8 97.7 97.6 97.6 97.9 93.8LAME [5] 66.4 43.2 49.0 75.2 30.2 28.5 21.6 32.5 21.2 39.5 52.0 72.8 22.3 33.1 20.5 40.5CoTTA [73]54.5 58.4 55.6 50.0 53.9 53.4 50.3 56.7 51.3 53.2 53.7 56.1 52.0 54.5 51.5 53.7NOTE [19]61.8 60.2 63.4 55.6 59.8 65.9 58.6 75.1 77.8 93.8 94.2 97.0 95.0 95.5 94.4 76.5 RoTTA 45.5 44.5 43.5 35.6 35.1 35.7 26.2 44.0 29.7 34.2 32.0 40.7 31.4 39.4 27.7 36.3(+4.2) Table 24. Average classification error of the task CIFAR100 → CIFAR100-C while continually adapting to different corruptions at the highest severity 5 with correlatively sampled test stream under the proposed setup PTTA. Time t− − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − → Method pixelateglass zoomsnow fog impulsebrightnessmotionfrost jpeg gaussianshot contrastdefocus elastic Avg. Source 74.7 54.1 28.8 39.5 50.3 39.4 29.5 30.8 45.8 41.2 73.0 68.0 55.1 29.3 37.2 46.4BN [53] 51.7 58.6 47.8 52.9 57.1 58.2 45.9 47.6 52.9 57.8 57.5 56.7 49.5 46.1 54.0 52.9PL [39] 52.4 68.0 73.4 87.9 93.7 96.1 95.7 96.0 96.5 96.7 97.5 97.7 97.7 97.3 97.7 89.6TENT [70] 53.5 77.8 91.1 96.0 97.0 97.6 97.4 97.6 97.9 98.1 98.1 98.0 98.1 97.9 98.1 92.9LAME [5] 74.8 48.2 21.1 30.6 43.4 32.5 21.6 23.0 39.6 33.3 72.7 66.5 51.5 20.7 27.5 40.5CoTTA [73]49.3 55.1 49.1 52.9 56.8 55.7 49.5 50.0 53.6 53.4 54.9 53.9 53.8 50.1 53.5 52.8NOTE [19] 52.2 64.9 47.5 57.0 61.9 67.3 60.4 67.8 77.4 90.6 97.1 96.8 92.8 95.9 96.6 75.1 RoTTA 36.4 44.4 29.7 36.5 41.0 44.1 26.8 29.5 33.0 40.3 40.3 38.2 33.9 28.5 34.9 35.8(+4.7)Table 25. Average classification error of the task CIFAR100 → CIFAR100-C while continually adapting to different corruptions at the highest severity 5 with correlatively sampled test stream under the proposed setup PTTA. Time t− − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − → Method motionsnow fog shot defocuscontrastzoom brightnessfrost elasticglass gaussianpixelatejpeg impulse Avg. Source 30.8 39.5 50.3 68.0 29.3 55.1 28.8 29.5 45.8 37.2 54.1 73.0 74.7 41.2 39.4 46.4BN [53] 48.5 54.0 58.9 56.2 46.4 48.0 47.0 45.4 52.9 53.4 57.1 58.2 51.7 57.1 58.8 52.9PL [39] 50.6 62.1 73.9 87.8 90.8 96.0 94.8 96.4 97.4 97.2 97.4 97.4 97.3 97.4 97.4 88.9TENT [70] 53.3 77.6 93.0 96.5 96.7 97.5 97.1 97.5 97.3 97.2 97.1 97.7 97.6 98.0 98.3 92.8LAME [5] 22.4 30.4 43.9 66.3 21.3 51.7 20.6 21.8 39.6 28.0 48.7 72.8 74.6 33.1 32.3 40.5CoTTA [73]49.2 52.7 56.8 53.0 48.7 51.7 49.4 48.7 52.5 52.2 54.3 54.9 49.6 53.4 56.2 52.2NOTE [19] 45.7 53.0 58.2 65.6 54.2 52.0 59.8 63.5 74.8 91.8 98.1 98.3 96.8 97.0 98.2 73.8 RoTTA 31.8 36.7 40.9 42.1 30.0 33.6 27.9 25.4 32.3 34.0 38.8 38.7 31.3 38.0 42.9 35.0(+5.5) Table 26. Average classification error of the task CIFAR100 → CIFAR100-C while continually adapting to different corruptions at the highest severity 5 with correlatively sampled test stream under the proposed setup PTTA. Time t− − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − → Method frost impulsejpeg contrastzoom glass pixelatesnow defocusmotionbrightnesselasticshot fog gaussian Avg. Source 45.8 39.4 41.2 55.1 28.8 54.1 74.7 39.5 29.3 30.8 29.5 37.2 68.0 50.3 73.0 46.4BN [53] 52.9 58.8 57.6 48.2 47.4 57.6 50.9 52.4 47.0 47.2 45.1 54.0 56.4 57.7 58.2 52.8PL [39] 56.9 73.3 86.7 94.4 95.8 97.3 97.2 97.4 97.6 97.4 97.7 97.6 97.8 98.3 98.1 92.2TENT [70]60.1 84.2 95.7 97.2 97.4 97.9 97.8 98.0 98.1 98.2 98.3 98.4 98.4 98.4 98.4 94.4LAME [5] 39.9 32.4 33.4 51.4 20.6 49.0 74.4 31.3 21.2 22.6 21.9 28.1 66.9 43.9 72.5 40.6CoTTA [73]51.5 55.3 54.3 51.8 49.4 55.3 50.7 54.2 51.4 50.6 49.5 53.6 55.0 57.1 55.8 53.0NOTE [19]51.6 60.9 60.3 45.4 54.3 70.8 68.8 75.0 75.7 87.1 94.7 95.6 96.7 96.4 97.2 75.4 RoTTA 40.0 46.3 42.8 36.4 29.2 42.3 33.2 34.4 28.4 29.2 26.4 34.5 38.5 39.8 39.3 36.0(+4.6) Table 27. Average classification error of the task CIFAR100 → CIFAR100-C while continually adapting to different corruptions at the highest severity 5 with correlatively sampled test stream under the proposed setup PTTA. Time t− − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − → Method defocusmotionzoom shot gaussianglass jpeg fog contrastpixelatefrost snow brightnesselastic impulse Avg. Source 29.3 30.8 28.8 68.0 73.0 54.1 41.2 50.3 55.1 74.7 45.8 39.5 29.5 37.2 39.4 46.4BN [53] 47.1 48.6 47.8 56.2 57.6 57.6 57.6 57.5 48.7 50.6 51.8 53.2 46.9 53.5 58.8 52.9PL [39] 48.8 58.7 69.9 88.0 95.1 96.6 96.7 96.9 97.4 97.4 98.2 98.2 98.2 98.3 98.5 89.1TENT [70] 51.0 67.6 85.8 95.9 97.2 97.5 97.2 97.7 98.1 97.9 97.7 97.7 98.0 98.0 98.2 91.7LAME [5] 21.2 22.8 21.1 66.3 72.8 49.0 33.3 44.8 51.7 74.9 39.8 31.2 21.3 27.3 32.3 40.6CoTTA [73]48.4 48.8 48.2 52.9 54.0 53.8 52.7 57.2 52.6 48.6 51.8 53.9 49.4 52.3 56.0 52.0NOTE [19] 45.1 46.7 49.1 67.3 65.5 69.4 75.5 80.3 83.8 96.0 97.6 97.1 96.1 97.9 98.7 77.7 RoTTA 29.6 31.3 28.8 43.9 41.5 41.3 40.9 39.8 32.1 32.6 33.1 33.0 26.5 34.5 42.9 35.4(+5.2) Table 28. Average classification error of the task CIFAR100 → CIFAR100-C while continually adapting to different corruptions at the highest severity 5 with correlatively sampled test stream under the proposed setup PTTA. Time t− − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − → Method glass zoom impulsefog snow jpeg gaussianfrost shot brightnesscontrastmotionpixelatedefocus elastic Avg. Source 54.1 28.8 39.4 50.3 39.5 41.2 73.0 45.8 68.0 29.5 55.1 30.8 74.7 29.3 37.2 46.4BN [53] 58.8 47.7 59.2 57.6 52.7 56.9 58.2 52.0 56.7 45.5 47.8 48.2 51.7 46.1 54.0 52.9PL [39] 60.1 59.5 75.1 85.7 91.5 94.6 96.5 97.1 97.4 97.3 98.0 97.7 97.9 97.8 97.7 89.6TENT [70] 61.6 71.5 91.0 95.9 96.6 97.1 96.9 97.3 97.4 97.2 97.9 98.0 98.1 97.9 97.8 92.8LAME [5] 48.6 20.6 32.3 44.4 30.2 33.6 72.4 40.0 66.3 21.6 52.0 22.8 74.6 20.7 27.5 40.5CoTTA [73]56.4 48.9 56.1 57.8 54.1 54.2 56.2 53.6 55.4 50.0 53.6 51.6 51.2 50.7 54.4 53.6NOTE [19]62.5 46.3 61.5 61.1 58.6 68.4 76.1 78.3 92.0 93.4 96.1 95.4 96.2 95.8 96.4 78.5 RoTTA 45.5 30.0 45.9 42.6 35.3 41.8 42.2 34.5 40.2 27.3 31.3 30.2 32.7 28.1 34.9 36.2(+4.3) Table 29. Average classification error of the task CIFAR100 → CIFAR100-C while continually adapting to different corruptions at the highest severity 5 with correlatively sampled test stream under the proposed setup PTTA. Time t− − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − → Method contrastgaussiandefocuszoom frost glass jpeg fog pixelateelasticshot impulsesnow motion brightness Avg. Source 55.1 73.0 29.3 28.8 45.8 54.1 41.2 50.3 74.7 37.2 68.0 39.4 39.5 30.8 29.5 46.4BN [53] 49.5 58.8 47.0 46.5 52.2 57.6 57.6 57.6 51.7 53.5 56.0 58.5 53.1 47.6 46.3 52.9PL [39] 53.6 70.4 76.0 85.1 91.2 95.2 96.0 97.0 96.9 97.3 97.3 97.6 97.5 97.6 97.7 89.8TENT [70] 60.2 89.1 95.0 96.2 96.9 97.0 96.5 97.0 97.0 97.2 97.6 97.8 97.5 97.9 97.7 94.0LAME [5] 51.3 72.5 21.5 21.0 39.6 49.0 33.3 44.8 74.8 28.0 66.8 32.5 30.6 22.5 21.4 40.6CoTTA [73]52.3 55.3 49.5 48.1 52.1 54.8 52.7 56.9 50.6 52.6 53.7 55.8 54.6 50.6 50.5 52.7NOTE [19] 39.1 64.7 48.9 50.6 59.1 70.1 71.7 75.0 85.2 95.7 96.9 98.4 96.0 95.9 94.9 76.1 RoTTA 41.4 46.2 30.5 28.5 36.0 40.9 40.5 39.6 33.0 35.0 38.2 43.1 33.9 30.7 27.1 36.3(+4.3)",
      "meta_data": {
        "arxiv_id": "2303.13899v1",
        "authors": [
          "Longhui Yuan",
          "Binhui Xie",
          "Shuang Li"
        ],
        "published_date": "2023-03-24T10:19:14Z",
        "pdf_url": "https://arxiv.org/pdf/2303.13899v1.pdf"
      },
      "llm_extracted_info": {
        "main_contributions": "This paper introduces Practical Test-Time Adaptation (PTTA), a new and more realistic test-time adaptation (TTA) setup that simultaneously accounts for continually changing test data distributions and correlative sampling. The proposed Robust Test-Time Adaptation (RoTTA) method effectively addresses the challenges of PTTA by providing robust statistics estimation, category-balanced sampling with timeliness and uncertainty, and time-aware robust training. Extensive experiments demonstrate that RoTTA achieves state-of-the-art results on CIFAR-10-C, CIFAR-100-C, and DomainNet, outperforming baselines by a significant margin and proving its suitability for rapid deployment in dynamic real-world applications.",
        "methodology": "RoTTA consists of three main components: (1) Robust Batch Normalization (RBN) replaces traditional batch statistics with global ones maintained by an Exponential Moving Average (EMA) to robustly normalize feature maps, initialized from the pre-trained model's running statistics and updated using buffered samples. (2) Category-Balanced Sampling with Timeliness and Uncertainty (CSTU) manages a memory bank by prioritizing samples that are newer and less uncertain. It uses pseudo-labels for category balancing and assigns a heuristic score based on sample age and prediction entropy to guide replacement. (3) Robust Training with Timeliness utilizes a teacher-student model architecture. The student model's affine parameters in RBN are updated by minimizing a loss function over the memory bank samples, incorporating a timeliness reweighting term (E(Ai)) that reduces the influence of older samples. The teacher model's parameters are updated via EMA of the student's parameters. The training loss is a cross-entropy between the student's prediction on strong-augmented views and the teacher's prediction on weak-augmented views.",
        "experimental_setup": "Experiments were conducted on CIFAR-10-C, CIFAR-100-C for robustness to corruptions, and DomainNet for generalization under domain shift. Pre-trained models included WildResNet-28 for CIFAR10-C, ResNeXt-29 for CIFAR100-C (from RobustBench), and ResNet-101 for DomainNet. The PTTA setup was simulated by changing corruption types (severity 5) one by one and using a Dirichlet distribution (δ=0.1 by default) to model correlatively sampled test streams. All experiments used PyTorch, Adam optimizer (learning rate 1.0 × 10^-3), batch size 64, and a memory bank capacity N=64. RoTTA's hyperparameters were fixed across experiments (α=0.05, ν=0.001, λt=1.0, λu=1.0). Validation involved comparing average classification error with state-of-the-art TTA methods, ablation studies for each component, and sensitivity analyses for distribution changing order, Dirichlet parameter δ, and batch size.",
        "limitations": "The Robust Batch Normalization (RBN) component is considered a 'naive solution' for handling correlatively sampled data and requires careful tuning of the parameter α. While RoTTA is designed to prevent model collapse, the paper notes a lack of a explicit recovery mechanism for cases where other methods (like PL, TENT) fail. Furthermore, the simulation of data correlation primarily focuses on category similarity using Dirichlet distribution, and the approach needs further validation in more diverse real-world scenarios to confirm its broader applicability.",
        "future_research_directions": "Future work could focus on improving the RoTTA algorithm by replacing or refining its existing components. More importantly, the authors hope this work will pave the way for closer integration of TTA algorithms with real-world applications by further enhancing the PTTA setup to make it even more realistic and comprehensive."
      }
    },
    {
      "title": "Evaluation of Test-Time Adaptation Under Computational Time Constraints",
      "abstract": "This paper proposes a novel online evaluation protocol for Test Time\nAdaptation (TTA) methods, which penalizes slower methods by providing them with\nfewer samples for adaptation. TTA methods leverage unlabeled data at test time\nto adapt to distribution shifts. Although many effective methods have been\nproposed, their impressive performance usually comes at the cost of\nsignificantly increased computation budgets. Current evaluation protocols\noverlook the effect of this extra computation cost, affecting their real-world\napplicability. To address this issue, we propose a more realistic evaluation\nprotocol for TTA methods, where data is received in an online fashion from a\nconstant-speed data stream, thereby accounting for the method's adaptation\nspeed. We apply our proposed protocol to benchmark several TTA methods on\nmultiple datasets and scenarios. Extensive experiments show that, when\naccounting for inference speed, simple and fast approaches can outperform more\nsophisticated but slower methods. For example, SHOT from 2020, outperforms the\nstate-of-the-art method SAR from 2023 in this setting. Our results reveal the\nimportance of developing practical TTA methods that are both accurate and\nefficient.",
      "full_text": "Evaluation of Test-Time Adaptation Under Computational Time Constraints Motasem Alfarra 1 2 Hani Itani 1 Alejandro Pardo 1 Shyma Alhuwaider 1 Merey Ramazanova 1 Juan C. P´erez 1 Zhipeng Cai 2 Matthias M¨uller 2 Bernard Ghanem 1 Abstract This paper proposes a novel online evaluation protocol for Test Time Adaptation (TTA) meth- ods, which penalizes slower methods by provid- ing them with fewer samples for adaptation. TTA methods leverage unlabeled data at test time to adapt to distribution shifts. Although many effec- tive methods have been proposed, their impressive performance usually comes at the cost of signif- icantly increased computation budgets. Current evaluation protocols overlook the effect of this extra computation cost, affecting their real-world applicability. To address this issue, we propose a more realistic evaluation protocol for TTA meth- ods, where data is received in an online fashion from a constant-speed data stream, thereby ac- counting for the method’s adaptation speed. We apply our proposed protocol to benchmark sev- eral TTA methods on multiple datasets and sce- narios. Extensive experiments show that, when accounting for inference speed, simple and fast approaches can outperform more sophisticated but slower methods. For example, SHOT from 2020, outperforms the state-of-the-art method SAR from 2023 in this setting. Our results re- veal the importance of developing practical TTA methods that are both accurate and efficient1. 1. Introduction In recent years, Deep Neural Networks (DNNs) have demon- strated remarkable success in various tasks (He et al., 2016) thanks to their ability to learn from large datasets (Deng et al., 2009). However, a significant limitation of DNNs is their poor performance when tested on out-of-distribution 1King Abdullah University of Science and Technol- ogy (KAUST), Thuwal, Saudi Arabia 2Intel Labs, Munich, Germany. Correspondence to: Motasem Alfarra <mo- tasem.alfarra@kaust.edu.sa>. Proceedings of the 41 st International Conference on Machine Learning, Vienna, Austria. PMLR 235, 2024. Copyright 2024 by the author(s). 1Code: github/MotasemAlfarra/Online-Test-Time-Adaptation Current Evaluation Realistic Evaluation40 45 50 55 60 65 70 75Error Rate (%)  AdaBN 17  AdaBN 17  SHOT 20  SHOT 20  TENT 21  TENT 21  SAR 23  SAR 23 Figure 1: The trend of average error rate using offline evaluation vs our proposed online evaluation. In the offline setup, TTA methods demonstrate progress across time with a decreasing average error rate, e.g. from 68.5% using AdaBN to 56.2% using SAR. We propose a realistic evaluation protocol that accounts for the adaptation speed of TTA methods. Under this protocol, fast methods ( e.g. AdaBN) are unaffected, while slower (but more recent and sophisticated) methods (e.g. SAR) are penalized. data, which violates the i.i.d. assumption that the training and testing data are from the same distribution (Hendrycks et al., 2021; Hendrycks & Dietterich, 2019; Kar et al., 2022). Such failure cases are concerning, since distribu- tion shifts are common in real-world applications, e.g., im- age corruptions (Hendrycks & Dietterich, 2019), chang- ing weather conditions (Sakaridis et al., 2021), or security breaches (Goodfellow et al., 2014). Test Time Adaptation (TTA) (Saenko et al., 2010; Sun et al., 2020; Liu et al., 2021) has demonstrated promising results for solving the above problem. TTA leverages the unlabeled data that arrives at test time by adapting the forward pass of pre-trained DNNs according to some proxy task (Liang et al., 2020; Lee et al., 2013). Though recent methods have made significant progress at improving accuracy under dis- tribution shifts (Wang et al., 2020; Niu et al., 2022; Gao et al., 2022), many of them incur high computational over- head. For instance, some methods require self-supervised fine-tuning on the data (Chen et al., 2022), while others perform a diffusion process per input (Gao et al., 2022). The computational overhead of TTA methods decreases 1 arXiv:2304.04795v2  [cs.LG]  23 May 2024Evaluation of Test-Time Adaptation Under Computational Time Constraints their inference speed, which is a critical property in many real-world applications that require the TTA method to pro- duce predictions at the speed of the stream itself. This property, however, is overlooked in the current evaluation protocols for TTA methods. In particular, these protocols assume a setting, which neglects how events constantly un- fold regardless of the model’s speed, causing the model to miss incoming samples when it is busy processing previous ones. For TTA methods that adapt using test data, missing samples has a direct effect on the method’s accuracy, as it will have fewer samples for adaptation. That is, the slower the TTA method, the fewer samples it can leverage for adapt- ing to the distribution shift. Thus, the current protocol for evaluating TTA methods is not suitable for assessing their efficacy in real-world deployment. In this work, we propose a novel realistic evaluation proto- col that factors in inference speed to assess the real-world applicability of TTA methods. Our evaluation protocol is in- spired by Online Learning (Cai et al., 2021; Shalev-Shwartz et al., 2012) and mimics real-world scenarios by exposing all TTA methods to a constant-speed stream of data. In this setting, the performance of slow TTA methods is in- trinsically penalized, as the time spent adapting to a sample may lead to dropped samples that could have been useful for adaptation. Specifically, our protocol dictates that if a method gslow is k times slower than the stream, then it may only use every kth sample for adaptation. In contrast, a method gfast that is as fast as the stream is allowed to adapt to every sample. Figure 1 shows the effect of evaluating several methods under our proposed protocol, where slower methods (e.g., SAR (Niu14 et al., 2023)) are penalized and faster but simpler methods become better alternatives (e.g., SHOT (Liang et al., 2020) and AdaBN (Li et al., 2016)). We apply our proposed evaluation protocol to benchmark several TTA methods on multiple datasets, and provide a fair assessment of their performance subject to the realistic consequences of slower inference speeds. Our experimental results highlight the importance of developing TTA methods that adapt to distribution shifts with minimal impact on inference speed. Our contributions are two-fold: 1. We propose a realistic evaluation protocol for TTA methods that penalizes slower methods by providing them with fewer samples for adaptation. Our approach is effective at assessing TTA methods’ efficacy in sce- narios where data arrives as a constant-speed stream. 2. Following our proposed protocol, we provide a com- prehensive experimental analysis of 15 TTA methods evaluated on 3 large-scale datasets under 3 different evaluation scenarios. These scenarios consider adap- tation to a single domain and continual adaptation to several domains. Our analysis shows that, when in- ference speed is accounted for, simple (but faster) ap- proaches can benefit from adapting to more data, and thus outperform more sophisticated (but slower) meth- ods. Figure 1 demonstrates this for four TTA methods. We hope our evaluation scheme inspires future TTA methods to consider inference speed as a critical di- mension that affects their real-world performance. 2. Related Work Test Time Adaptation. The Test Time Adaptation (TTA) setup relaxes the “i.i.d” assumption between the training and testing distributions (Sun et al., 2020; Boudiaf et al., 2022). This relaxation is usually attained through a lifelong learning scheme on all received unlabeled data (Chen et al., 2022; Gong et al.). Earlier approaches such as TTT (Sun et al., 2020) and TTT++ (Liu et al., 2021), among others (Torralba & Efros, 2011; Tzeng et al., 2017), include a self-supervised loss (Gidaris et al., 2018) during training, which can then provide an error signal during adaptation. Despite their effectiveness, such approaches assume having control over how the model is trained. Fully Test Time Adaptation. Fully TTA methods are a subtype of TTA method that adapts at test time by modify- ing the model’s parameters (Liang et al., 2020; Lee et al., 2013; Mirza et al., 2022b; Mancini et al., 2018; Kojima et al., 2022) or its input (Gao et al., 2022) by using the incoming unlabeled data. Fully TTA methods are practi- cal, as they avoid assumptions on the training phase of a given model (Wang et al., 2020; Gao et al., 2022; Iwasawa & Matsuo, 2021). The first of these approaches adjusts the statistics of the Batch Normalization (BN) layers (Mirza et al., 2022a; Schneider et al., 2020; Li et al., 2016). For example, BN-adaptation (Schneider et al., 2020) leverages the statistics of the source data as a prior and infers the statis- tics for every received sample. On the other hand, AdaBN (Li et al., 2016) discards the statistics of the source domain and uses the statistics computed on the target domain. In line with light TTA methods, LAME (Boudiaf et al., 2022) proposes to only adapt the model’s output by finding the latent assignments that optimize a manifold-regularized like- lihood of the data. In this work, we found that such efficient methods preserve their accuracy under our proposed eval- uation. While fully TTA methods have been studied in the context of adversarial domain shifts (Alfarra et al., 2022; Croce et al., 2022; P´erez et al., 2021), in this work we focus on the context of natural shifts such as realistic image cor- ruptions (Hendrycks & Dietterich, 2019; Kar et al., 2022). Another line of work aims at adapting to distribution shifts by minimizing entropy. For instance, SHOT (Liang et al., 2020) adapts the feature extractor to minimize the entropy of individual predictions; while maximizing the entropy of the predicted classes. TENT (Wang et al., 2020) updates the learnable parameters of the BN layers to minimize the 2Evaluation of Test-Time Adaptation Under Computational Time Constraints Adapted SampleNon-AdaptedSampleTTA method Current evaluation . . . . . . Realistic evaluation . . . . . . Model Figure 2: Inference under the current and realistic evaluation protocols. The current evaluation setting (left) assumes that the incoming batches of stream S can wait until the adaptation process of a TTA method g finishes. This assumption is untenable in a real-time deployment scenario. Our proposed realistic evaluation (right) simulates a more realistic scenario where S reveals data at a constant speed. In this setup, slower TTA methods will adapt to a smaller portion of the stream. The remaining part of the stream will be predicted without adaptation by employing the most recent adapted model. We refer to the most recent adapted model as fθt+1 , with t denoting the time when the last sample was adapted to by g. When g is still adapting to a sample, the incoming sample is fed to fθt+1 to produce predictions. entropy of predictions. EATA (Niu et al., 2022) combines TENT with an active selection of reliable and non-redundant samples from the target domain and an anti-forgetting loss (Kirkpatrick et al., 2017). Further, SAR (Niu14 et al., 2023) equips TENT with an active sampling scheme that filters samples with noisy gradients. Other works use data-augmentation at test time (Ashukha et al., 2020). For example, MEMO (Zhang et al., 2021) adapts model parameters to minimize the entropy over a sample and multiple augmentations of it. CoTTA (Wang et al., 2022) uses augmentations to generate reliable pseudo- labels and then peform distillation. Finally, DDA (Gao et al., 2022) proposes to leverage a diffusion model (Ho et al., 2020) to restore corrupted inputs back to the source data distribution. These methods require multiple forward passes through the network or a diffusion model, leading to slower inference speeds. 3. Methodology In this section, we present our proposed Realistic TTA evalu- ation protocol. We first describe the current TTA evaluation protocol and its limitations Then, we introduce our Realistic TTA evaluation protocol, which addresses the shortcomings of the offline protocol. 3.1. Current Protocol TTA considers the practical setup, in which trained models are deployed in a target domain that exhibits distribution shifts to which they must adapt. Let fθ : X → Ybe a clas- sifier, parameterized by θ, that predicts the label y ∈ Yfor a given input x ∈ X. Before test time, fθ is assumed to have been trained on the dataset Dtrain ⊂ X × Y. At test time, i.e. when executing TTA,fθ is presented with a stream of data S, sampled from X, with potentially multiple distribution shifts w.r.t. Dtrain. Under this setup, a TTA method is a function g(θ, x) that sequentially adapts the model’s param- eters θ and/or the input x to enhance the performance under distributions shifts. Currently, TTA methods are evaluated in an offline setting. Formally, the Current TTA evaluation protocol simulates the interaction between the stream S and the TTA method g, at each time step t ∈ {0, 1, . . . ,∞}, as follows: Curr.1 S reveals a sample xt. Curr.2 g adapts xt to ˆxt, θt to ˆθt, generates prediction ˆyt, and updates parameters θt+1 = αθt + (1 − α)ˆθt.2 Note that all existing TTA methods can be modeled using this framework. For example, TENT (Wang et al., 2020) adapts network parameters to minimize entropy with α = 0, while leaving inputs unchanged, i.e. ˆxt = xt and θt+1 = ˆθt. DDA (Gao et al., 2022) adapts inputs via a diffusion process while preserving network parameters with α = 1, i.e. ˆxt = ˆxt and θt+1 = θt. CoTTA (Wang et al., 2022) applies knowledge distillation, and updates network parameters with an exponential moving average, i.e. setting 0 < α <1. Shortcomings of the Current TTA protocol.In the current protocol, the performance of a TTA method g is measured by comparing the ground truth labels yt with the predic- tions after adaptation ˆyt. An evaluation based only on this measure implicitly assumes that the stream is not constant 2Note that some methods abstain from adapting either xt or θt. 3Evaluation of Test-Time Adaptation Under Computational Time Constraints speed, but rather waits for g to adapt to xt (Curr.2) before revealing the next batch xt+1 (Curr.1). Figure 2 provides an illustration of this situation. This assumption results in the offline protocol favoring slower TTA methods, as the method’s performance is agnostic to its inference speed. However, in practical applications where the test data ar- rives at a constant speed, the offline protocol is not suitable for assessing a method’s performance. Next, we propose a remedy for this shortcoming. 3.2. Realistic Online Evaluation Protocol We propose a realistic evaluation of TTA methods that explicitly considers the relation between the speed of the method and the speed at which the stream reveals new data. This setup is more realistic, as it intrinsically penalizes the performance of slower TTA methods: long times spent in adaptation result in fewer samples to adapt to. A crucial aspect of our realistic TTA protocol is accounting for the implications of simulating a constant speed data stream S. For instance, consider a stream S that reveals data at a constant rate r samples per second. If a method gfast adapts to samples at speed r, then gfast will be able to adapt to every sample. On the other hand, if gslow adapts to samples at a speed r/2, then gslow will skip every other sample. We formalize the notion of the relation between the speed of the stream and the speed of a method g as the “relative adaptation speed of g”. This quantity, denoted by C(g) ∈ N, is simply the integer ratio of the speed of S to the speed of g. For instance, in the previous example, C(gfast) = 1, meaning gfast adjusts as fast as S reveals data, while C(gslow) = 2 , indicating S reveals its second batch while gslow is still adapting to the first one. Without loss of generality, we assume that fθ runs in real- time, i.e. that its speed is equal to r, and thus C(fθ) = 1 . This assumption allows us to suppose that the samples that are not processed by g can be processed by fθ. Under this setup, we define our realistic protocol by introducing the relative adaptation speed C(g) into the offline protocol. In particular, we simulate g’s availability by conditionally performing the adaptation step (Curr.2), depending on C(g). In this manner,g is only permitted to adapt when its previous adaptation step has finished. Formally, the realistic TTA evaluation protocol simulates the interaction between the constant speed stream S and the TTA method g, at each time step t ∈ {0, 1, . . . ,∞}, as follows: RTTA 1 S reveals a sample xt. RTTA 2 If (t mod C(g)) = 0, then g adapts xt to ˆxt, θt to ˆθt, generates a prediction ˆyt, and updates pa- rameters via θt+1 ← αθt + (1 − α)ˆθt. Otherwise, fθt generates a prediction ˆyt. Table 1: Average C(g(xt)). We report the average relative adaptation speed C(g) for 5 TTA methods. The higher C(g) is, the smaller the portion of data to which g adapts is. Method AdaBN TENT TTAC-NQ MEMO DDA C(g) 1 3 12 54 810 Here, “mod” represents the modulo operation. The above protocol assesses the performance of TTA methods by fac- toring in their speed. As such, faster methods are granted more adaptation steps and, conversely, slower methods are granted fewer (see Figure 2). Note that explicitly modeling the relative adaptation speeds allows us to evaluate TTA methods under different adaptation speeds by setting C(g) to arbitrary values. For instance, note that our realistic proto- col recovers the original offline protocol by settingC(g) = 1 for all methods. Next, we explain the calculation of C(g) for our realistic protocol. Online computation of C(g). In practice, estimating the relative adaptation speed C(g) can be a noisy process. The noise in this estimation essentially comes from two factors: hardware and input dependence. Hardware-induced noise applies to all methods, while input dependence applies to methods like ETA (Niu et al., 2022) which, upon receiving an input, may optionally abstain from adapting to it. This noise means that C(g) potentially varies across iterations. Our protocol accounts for this variability by conducting an online computation of C(g) on each revealed input. That is, instead of using a fixed value of C(g) at each itera- tion t, our protocol rather uses C (g(xt)). Formally, if we let R (g(x)) denote the speed at which g processes x, then the relative adaptation speed of g at x is defined as C (g(xt)) = ⌈r/R(g(x))⌉, where the ceiling function ac- counts for the stream’s discrete-time nature. Note that since we assumed C(fθ) = 1, then R (fθ(x)) = r. We report the empirical behavior of this online computation of C (g(xt)) for various TTA methods in Table 1, and leave the rest of the methods and the computation details to the Appendix. Next, we leverage our Realistic TTA protocol to conduct a comprehensive empirical study of several TTA methods. 4. Experiments We follow prior art (Wang et al., 2020; Niu14 et al., 2023; Gao et al., 2022) and focus on the task of image classifica- tion. In all our experiments, we assume that fθ is a ResNet- 50-BN3 (He et al., 2016) trained on ImageNet (Deng et al., 2009) (pretrained weights obtained from torchvision). We further assume that the stream S reveals batches of size 3SAR demonstrated the superiority of using batch independent normalization layers under batch size of 1. We leave this ablation to the Appendix along with experiments on other architectures. 4Evaluation of Test-Time Adaptation Under Computational Time Constraints Table 2: Episodic Error Rate on ImageNet-C. We report the error rate of different TTA methods on ImageNet-C benchmark under both the realistic and the current setup. A lower error rate indicates a better TTA method. The highlighted numbers indicate a better performance per method across setups. Episodic means the model will adapt to one corruption at a time. The model is reset back to the base model when moving to the next corruption. The current setup is merely the reproduction of every method. The first sub-table corresponds to methods that do not incur any or few extra computations, i.e. C(g) = 1. We show that methods generally perform worse in the realistic setup. The more computationally complex the TTA method is, the less data it will adapt to, and the worse is its performance. Noise Blur Weather DigitalMethod Realisticgauss. shot impul.defoc. glass motionzoom snow frost fog brigh. contr. elast. pixel. jpeg Avg. ∆ Source ✓ 97.8 97.1 98.1 82.1 90.2 85.2 77.5 83.1 76.7 75.6 41.1 94.6 83.0 79.4 68.4 82.0 - AdaBN ✓ 84.9 84.3 84.3 85.0 84.7 73.6 61.1 65.8 66.9 52.1 34.8 83.3 56.1 51.1 60.3 68.5 - LAME ✓ 98.3 97.6 98.6 82.4 90.9 86.1 78.1 84.5 77.5 77.3 41.4 94.8 84.8 80.0 68.9 82.7 - BN ✓ 84.6 83.9 83.8 80.1 80.2 71.7 60.4 65.4 65.2 51.6 34.6 76.3 54.4 49.7 59.2 66.7 - ✗ 73.4 70.2 73.0 76.6 75.5 59.8 53.8 54.2 63.4 44.7 35.5 79.3 46.9 43.2 49.7 59.9SHOT ✓ 73.6 69.0 71.1 74.6 74.8 60.0 52.9 54.1 61.3 44.1 34.1 77.8 46.8 43.1 49.2 59.1 (-0.8) ✗ 71.3 69.4 70.2 72.0 72.9 58.7 50.7 52.8 58.8 42.7 32.7 73.3 45.5 41.5 47.7 57.3TENT ✓ 75.7 78.3 75.2 76.3 77.3 64.6 55.6 57.3 61.4 45.9 33.5 77.1 50.1 44.2 51.4 61.6 (+4.3) ✗ 69.5 69.7 69.0 71.2 71.7 58.1 50.5 52.9 57.9 42.7 32.7 62.9 45.5 41.6 47.8 56.2SAR ✓ 79.4 78.5 78.1 79.9 79.3 67.5 56.1 60.5 63.1 47.4 34.0 75.3 51.7 46.6 53.8 63.4 (+7.2) ✗ 78.4 77.8 77.2 80.5 79.1 64.0 53.3 57.8 60.7 44.1 32.9 73.1 48.6 42.3 52.6 61.5CoTTA ✓ 82.9 81.6 81.9 87.4 85.6 75.6 61.1 63.1 64.9 49.9 34.8 91.2 54.0 48.8 56.6 68.0 (+6.5) ✗ 71.3 70.3 70.8 82.1 77.4 63.9 53.9 49.9 55.5 43.9 32.8 81.4 43.7 41.1 46.7 59.0TTAC-NQ ✓ 79.4 75.7 78.9 86.6 86.2 77.1 61.8 58.8 62.4 51.5 34.4 88.5 52.1 49.1 55.5 66.5 (+7.5) ✗ 65.5 62.4 63.5 66.6 67.2 52.0 47.3 48.2 54.1 39.9 32.1 55.0 42.3 39.2 44.8 52.0EATA ✓ 69.3 67.1 69.2 71.1 71.7 57.5 49.9 51.9 57.4 42.4 32.6 60.7 45.1 41.4 47.4 55.6 (+3.6) ✗ 92.5 91.3 91.0 84.0 87.0 79.3 72.4 74.6 71.3 67.9 39.0 89.0 76.2 67.0 62.4 76.3MEMO ✓ 97.7 97.0 98.0 82.1 90.1 85.1 77.4 83.0 76.6 75.4 41.0 94.5 82.9 79.2 68.2 81.9 (+5.6) ✗ 58.6 57.8 59.0 87.0 81.6 76.6 65.9 67.9 66.7 64.0 40.0 92.2 52.2 46.6 49.9 64.4DDA ✓ 97.8 97.0 98.1 82.1 90.2 85.2 77.5 83.1 76.7 75.6 41.1 94.6 83.0 79.4 68.3 82.0 (+17.6) 644, except for MEMO (Zhang et al., 2021), which pre- dicts on single images to incentivize prediction consistency over an input and its augmentations. Regarding datasets, we follow earlier works (Wang et al., 2020; Niu14 et al., 2023; Niu et al., 2022; Gao et al., 2022; Zhang et al., 2021), and thus evaluate on the ImageNet-C dataset (Hendrycks & Dietterich, 2019) with a corruption level of 5 for all 15 corruptions. We further extend our evaluation and consider CIFAR10-C, ImageNet-R (Hendrycks et al., 2021), and the more recent ImageNet-3DCC (Kar et al., 2022), which lever- ages depth estimates to construct more spatially-consistent corruptions. Our experiments compare the performance of the base- line model fθ (without test time adaptation) against 15 state-of-the-art TTA methods published in top-tier venues (e.g., CVPR, NeurIPS, and ICLR) between 2017 and 2023. In particular, we consider: BN (Schneider et al., 2020) and AdaBN (Li et al., 2016), which adjust the statistics of the batch normalization layers; SHOT (Liang et al., 2020) and SHOT-IM (Liang et al., 2020), which fine-tune the feature extractor to maximize mutual information; entropy mini- mization approaches such as TENT (Wang et al., 2020), 4This batch size is recommended by most baselines (Wang et al., 2020; Niu et al., 2022) ETA (Niu et al., 2022) (a more efficient version of TENT), and SAR (Niu14 et al., 2023), which trains the learnable parameters of the batch normalization layers; distillation approaches, such as CoTTA (Wang et al., 2022), Pseudo Labeling (PL) (Lee et al., 2013), and the very recent and efficient LAME (Boudiaf et al., 2022); EATA (Niu et al., 2022) and TTAC (Su et al., 2022) that assume access to the source training data; data-dependent approaches such as MEMO (Zhang et al., 2021) and the diffusion-based method DDA (Gao et al., 2022). For all methods, we use their official implementation with their recommended hyper- parameters. We report our experimental results on a subset of 12 baselines, while leaving ETA, SHOT-IM, and PL to the appendix due to space constraints and their similarity to SHOT and EATA. As mentioned in Section 3.2 , our protocol performs an online computation of the relative adaptation speed of g. In particular, for each batch revealed by the stream, we compute C (g(x)). Then, if C(g(xi)) = k, all the samples {xi+1, xi+2, . . . , xi+k} are processed by fθi without adap- tation. Otherwise, if C(g(xi)) = 1, then these samples are processed by g. For methods that accumulate parameter updates such as TENT (Wang et al., 2020), fθi is the most recent updated model g(fθi−1 ). We report all our main re- sults as the average across three seeds, and leave the detailed 5Evaluation of Test-Time Adaptation Under Computational Time Constraints SHOT TENT TTAC-NQ SAR EATA COTTA brigh.pixel.gauss.motionzoomglassimpul.jpegdefoc.elast.shotfrostsnowfog contr.clean 30 40 50 60 70 80 90 100Error Rate (%) (a) Current Continual TTA. brigh.pixel.gauss.motionzoomglassimpul.jpegdefoc.elast.shotfrostsnowfog contr.clean 30 40 50 60 70 80 90 100Error Rate (%)  (b) Realistic Continual TTA. Figure 3: Continual Error Rate on ImageNet-C. We report the continual error rate of several TTA methods on ImageNet-C benchmark under both realistic and current setups. A lower error rate indicates a better TTA method. Continual evaluation means the corruptions are presented in a sequence without resetting the model in between. We choose the same order as presented along the x-axis; starting with brightness and ending with clean validation set. In the current setup, we observe an increasing trend for SHOT, TENT, and TTAC-NQ. This is hypothesized to be due to overfitting on the early distribution shifts. This behavior is mitigated in the realistic setup due to adapting to fewer batches. EATA and SAR perform equally well in both realistic and current continual setups due to sample rejection. We report the standard deviation across 3 seeds. analysis to the Appendix. Throughout the experiments, we refer to our realistic evaluation protocol as “realistic/on- line”, and refer to the current protocol as “current/offline”. Next, we evaluate all methods on four different scenarios: (i) when domain shifts happen in an episodic manner, (ii) when domain shifts happen continually, i.e. one after the other, (iii) when the stream speed varies, (iii) when domain shifts happen continually with label correlation; practical evaluation (Yuan et al., 2023) ,and (v) when the baseline fθ is unavailable for evaluating the samples skipped by the TTA method g (left for the appendix). 4.1. Episodic Evaluation of TTA First, we consider an episodic evaluation of domain shifts, whereby S contains a single domain (e.g. one corruption) from ImageNet-C. We analyze this simple and most com- mon setup to assess the performance of TTA methods under real-time evaluation. We report the error rates on all corrup- tions in Table 2 and the average error rate across corruptions. We summarize the insights as follows: (i) The performance of TTA methods often degrades significantly under the realistic setup. Most methods induce a significant computational overhead, which prevents them from adapting to every sample from the test stream. For example, the error rate increases by 7.5% for TTAC- NQ and 4.3% for TENT, where C(gTTAC-NQ) = 12 and C(gTENT) = 3 (see Table 1). That is, TENT adapts to one- third of the batches revealed by the stream, while TTAC-NQ adapts to one every twelve batches. (ii) Very efficient methods, withC(g) = 1, such as LAME and BN, do not lose in performance. Evaluating such methods in offline or realistic setups is inconsequential, as their adaptation incurs negligible additional computation (since they adapt during the forward pass (Li et al., 2016; Schneider et al., 2020) or by adjusting the logits (Boudiaf et al., 2022) at a speed that pales in comparison to that of the stream). Interestingly, in our realistic evaluation, the simple BN (published in 2020) with an average error rate of 66.7% outperforms more recent and advanced methods such as SAR (published in 2023) by 1.7%. Furthermore, AdaBN (published in 2017) significantly outperforms the very recent diffusion-based DDA by a notable 13%. (iii) Data-dependent approaches, such as MEMO and DDA, are extremely inefficient. Despite the independence of MEMO and DDA on batch size, they incur a massive computational burden. For instance, C(gMEMO) = 54 and C(gDDA) = 810. Thus, both methods will be busy adapting for considerable portions of the stream, leaving most predic- tions to the non-adapted classifier. This phenomenon is the reason behind the reported performance of these methods being so close to that of fθ (i.e. around 82%). This result calls for future research to focus on increasing the efficiency of data-dependent adaptation methods. (iv) Sample rejection-oriented methods can perform well under the realistic protocol. EATA adapts efficiently due to its fast sample rejection algorithm, which relies solely on 6Evaluation of Test-Time Adaptation Under Computational Time Constraints the forward pass to admit samples for adaptation. EATA’s low error rate of 55.6%, combined with a small performance drop of less than 4%, positions it as the top performer under the realistic evaluation protocol on ImageNet-C. On the other hand, SAR does not benefit from sample rejection. SAR’s performance drop of 7.5% is due to its dependence on gradients for sample rejection, which reduces its speed. (v) SHOT benefits from the realistic protocol. Interest- ingly, we found that SHOT (and SHOT-IM in the Appendix), a fine-tuning-based approach, benefits from our realistic evaluation. In particular, we found that SHOT’s error rate decreases by 2% on fog corruption and by 0.8% on average. This observation could suggest that SHOT could potentially improve performance by disposing of fine-tuning on every batch. It is also worth mentioning that, under our realis- tic evaluation, SHOT (introduced in 2020) outperforms all methods except EATA. (vi) Performance changes are consistent across corrup- tions. Note that all methods that are somewhat efficient can improve the source model across all corruptions, in both the offline and realistic setups. Furthermore, the performance changes when comparing the offline and realistic setups are consistent across all corruptions. This finding suggests that the performance of these methods is independent of the do- main shift being considered. We further test this hypothesis by benchmarking these methods on two other datasets with other types of domain shifts in Section 4.4. 4.2. Continual Evaluation of TTA Next, we analyze the more challenging continual setup, fol- lowing (Wang et al., 2022; Niu et al., 2022). In particular, we construct the stream S by concatenating all corruptions from ImageNet-C. That is, we adapt TTA methods continu- ally on all corruptions followed by the clean validation set, without ever resetting the network weights. We introduce the notion of realistic adaptation to the continual setup to study the effects of a constant stream speed on the bench- mark. We report results in Figure 3 for both the offline and realistic protocols, where the horizontal-axis shows how cor- ruptions are ordered in the stream. We limit the experiments in this section to six TTA methods (SHOT, TENT, TTAC- NQ, COTTA, EATA, and SAR), and leave the remaining details for the Appendix. We observe: (i) Methods that do not perform sample rejection (SHOT, TENT, TTAC) scale poorly in the offline-continual setup. This phenomenon can be attributed to these methods over- fitting to early distributions. However, methods that do perform sample rejection (SAR and EATA) do not overfit as easily to corruptions, and can thus adapt to the rest of the stream. Even worse, such methods tend to even significantly degrade the performance on clean data. 1/16 1/8 1/4 1/2 1 η 52 55 58 61 64 67Error Rate (%) SHOT TENT TTAC-NQ SAR EATA Figure 4: Average Error Rate on ImageNet-C Under Slower Stream Speeds. We report the average error rate for several TTA methods on ImageNet-C under slower stream speeds. In our proposed realistic model evaluation, the stream speed r is normalized by the time needed for a for- ward pass using the base model. We evaluate different TTA methods under a stream with speed ηr with η ∈ (0, 1]. An η = 1/16 means the stream is 16 times slower than the forward pass of the base model. We report the standard deviation across 3 different random seeds. Different TTA methods degrade differently when varying η. (ii) In the realistic-continual setup, methods that do not perform sample rejection benefit from skipping adapta- tion on some batches, and become competitive with the methods that perform sample rejection. That is, while skipping parts of the stream deteriorated the performance of such methods in the episodic evaluation , this skipping actu- ally helped in preventing these methods from over-fitting in the continual setup. 4.3. Stream Speed Analysis In the previous experiments, we normalized the stream speed to be the same as that of fθ’s forward pass. That is, we assumed that the rate r at which S reveals new batches is equal to R (fθ(x)). However, some applications may enjoy a slower stream, giving TTA methods more time to adapt to samples. To explore this scenario, we vary the speed at which the stream reveals new data. In particular, let the new stream rate be η rwith η ∈ (0, 1]. Hence, as η → 0, the stream slows down and allows methods to adapt to all samples. Conversely, as η → 1, the stream speeds up, and at η = 1 we recover our realistic evaluation protocol. We experiment with the stream speed by setting η ∈ {1/16, 1/8, 1/4, 1/2, 1}, and evaluate five representative TTA methods (SHOT, TENT, TTAC-NQ, SAR, and EATA) in the episodic setup . Figure 4 summarizes our results by reporting the average error rate across all corruptions. We next list our observations: (i) The performance of TTA methods varies widely.For 7Evaluation of Test-Time Adaptation Under Computational Time Constraints Table 3: Episodic Error Rate on ImageNet-C with ViT. We report the error rate of three baselines (Source, Tent, SAR) on the 15 different corruptions on ImageNet-C when the backbone is ViT architecture pretrained on ImageNet. We observe that while generally better backbones yield smaller error rate, expensive methods perform worse under our realistic evaluation. The more expensive the method is (e.g. SAR compared to Tent), the more performance reduction it suffers. Noise Blur Weather DigitalMethodRealisticgauss. shot impul. defoc. glass motionzoom snow frost fog brigh. contr. elast. pixel. jpeg Avg. ∆ Source ✓ 90.5 93.3 91.8 71.0 76.6 66.1 72.9 84.1 73.5 52.8 45.3 55.9 69.5 55.5 52.2 70.1 - ✗ 69.9 95.9 68.9 55.8 62.0 52.3 57.9 57.2 53.6 41.8 28.9 40.7 59.1 39.7 42.0 55.0Tent ✓ 80.7 88.9 81.0 63.0 69.5 58.3 64.9 65.8 59.7 47.7 33.2 47.3 64.6 45.1 46.4 61.1 (-6.1) ✗ 55.5 56.9 55.1 47.5 50.4 44.3 48.7 42.4 47.3 33.6 25.4 35.6 44.8 33.5 36.4 43.8SAR ✓ 70.0 72.5 69.4 56.6 63.4 54.0 60.0 56.4 53.5 43.0 30.5 43.3 58.7 41.5 43.8 54.5 (-10.7) example, TTAC-NQ starts degrading faster (at η = 1/16) due to its slow adaptation speed. For other methods, the η at which they degrade varies. For instance, while TENT has a higher error rate than SAR in slow streams (η ≤ 1/8), TENT outperforms SAR in the regime of faster streams η ≤ 1/4. Interestingly, SHOT (Liang et al., 2020) ranks the worst at η ≤ 1/8, then ranks second when η ≥ 1/2, becoming a viable alternative. At last, the order of different methods significantly changes depending on the speed of the stream. For example, SAR changes from being second best at η ≤ 1/8 to third at η = 1/4 and then to fifth ( i.e. second worst) at η ≥ 1/2. (ii) EATA provides a good trade-off between speed and performance. In fact, EATA gives the best overall perfor- mance (lowest error rate) independent of the stream’s speed. This virtue is attributable to EATA’s combination of good performance and adaptation speed based on efficient sample rejection. Results on other datasets are in the Appendix. 4.4. Results on Other Benchmarks and Architectures We extend our evaluation protocol to cover ImageNet- 3DCC (Kar et al., 2022) and ImageNet-R (Hendrycks et al., 2021) datasets and ResNet-18 (results in the ap- pendix) and ViT (Kolesnikov et al., 2021) architectures. ImageNet-R contains rendition versions of ImageNet span- ning 200 classes. ImageNet-3DCC constructs more spatially-consistent corruptions than ImageNet-C by lever- aging depth estimates. For ViT, we conduct episodic evalu- ation on ImageNet-C in a similar setup to Section 4.1 and report the results in Table 3 for the non-adapted model, Tent, and SAR. For ImageNet-R and ImageNet-3DCC, we fix the architecture to ResNet-50 and experiment on the entire datasets and set the severity level to 5 in ImageNet-3DCC. Due to the space constraint, we limit our experiments to the episodic evaluation, and leave other results and analyses to the Appendix. We evaluate the effectiveness of 10 TTA methods in Table 4, where we report the average error rate across all corruptions. We observe that our results are consistent across all con- Table 4: Average Error Rate on ImageNet-R and ImageNet-3DCC. We report the average error rate of dif- ferent TTA methods on ImageNet-R and ImageNet-3DCC under both the realistic and current setups. A lower error rate indicates a better TTA method. The highlighted num- bers indicate a better performance per method across setups. We observe that methods generally perform worse in the more realistic realistic setup. The conclusions are consistent with what we observed on ImageNet-C (Table 2). Method ImageNet-R ImageNet-3DCC Current Realistic ∆ Current Realistic ∆ Source 63.8 63.8 - 73.9 73.9 - AdaBN 60.6 60.6 0 72.1 72.1 0 BN 60.0 60.0 0 70.5 70.5 0 LAME 60.5 60.5 0 72.1 72.1 0 SHOT 70.3 62.6 (+7.7) 69.2 67.0 (+2.2) TENT 58.1 59.1 (-1.0) 64.5 66.8 (-2.3) SAR 57.5 59.6 (-2.1) 63.5 71.4 (-7.9) CoTTA 57.3 61.5 (-4.5) 66.4 75.6 (-9.2) EATA 55.7 57.1 (-1.4) 60.9 63.1 (-2.2) TTAC-NQ 59.2 60.8 (-1.6) 65.7 73.6 (-7.9) sidered datasets and architectures. Similar to our results in Table 2, the more computationally involved SAR de- grades more than Tent when leveraging ViT architecture. Regarding other datasets, we find that simple methods that adapt during the forward pass are unaffected by the realis- tic setup. All the other methods, except SHOT, experience degradation in their results on both datasets. We observe again that, on these two datasets, while SHOT actually ben- efits from the realistic evaluation, EATA remains the best alternative on both ImageNet-R and ImageNet-3DCC. 4.5. Evaluation under Practical TTA Recently, (Yuan et al., 2023) extended the continual test- time adaptation evaluation to include label-imbalances; known as Practical Test-Time Adaptation (PTTA) setup. In this setting, the stream not only reveals a continual se- quence of distribution shifts, but also the revealed batches 8Evaluation of Test-Time Adaptation Under Computational Time Constraints Table 5: Episodic Error Rate on CIFAR10-C under Practical Evaluation (Yuan et al., 2023).We report the error rate of two baselines (Source, RoTTA (Yuan et al., 2023)) on the 15 different corruptions on CIFAR10-C when the backbone is ResNet-18. We observe that under our computational constrained evaluation, the only method tailored to this setting; RoTTA, performs worse than the non-adapted baseline. Noise Blur Weather DigitalMethodRealisticgauss. shot impul. defoc. glass motionzoom snow frost fog brigh. contr. elast. pixel. jpeg Avg. ∆ Source ✓ 72.3 65.7 72.9 46.9 54.3 34.8 42.0 25.1 41.3 26.0 9.3 46.7 26.6 58.5 30.3 43.5 - ✗ 36.9 34.9 45.8 16.6 44.2 19.9 16.53 21.6 22.4 18.8 9.8 20.6 28.4 27.1 34.5 26.5RoTTA ✓ 55.0 54.4 63.2 43.3 62.3 43.7 43.5 44.8 47.7 43.4 35.3 41.8 54.0 47.7 54.6 49.0 (-22.5) have significant label imbalances. To combat this combined challenge, the work of (Yuan et al., 2023) proposed to lever- age a balanced memory bank for adaptation. In this section, we extend our computational constrained evaluation to the PTTA setup and compare RoTTA (Yuan et al., 2023) with a non-adapted model on CIFAR10-C benchmark. Table 5 summarizes the results. We observe that while RoTTA indeed reduces the error rate under the PTTA setup on CIFAR10-C (17% below the non-adapted model), our realistic evaluation uncovers its computational limitation. We found that RoTTA’s error rate increases by over 22% surpassing the error rate of the non-adapted model. Note that RoTTA stores samples from the stream in a memory bank then adapts the model on sampled samples from the memory bank. Thus, the slower the adaptation of RoTTA, the less diverse the samples in the memory bank, hindering its adaptation. 4.6. Effect of Hyper-parameter Tuning The performance of different TTA methods heavily depends on their hyper-parameter settings (Zhao et al., 2023). Here, we assess the impact of our proposed evaluation on TTA methods when tuning their hyperparameters. For that regard, we conduct hyper parameter search for Tent (as a fundamen- tal baseline) and experiment with different learning rates (the only hyper-parameter for Tent). Table 6 summarizes the results under episodic evaluation for 4 different corruptions on ImageNet-C. We observe that while conducting hyper-parameter search indeed improves the performance of TENT, its error rate increases under our realistic evaluation across all hyperparameters. That is, while conducting hyper-parameter search might indeed result in a better performance for TTA methods, the insights obtained through our proposed evaluation scheme remains consistent: more efficient TTA methods will have a smaller performance drop under the realistic evaluation. 5. Conclusions In this work, we find that the performance of Test Time Adaptation (TTA) methods can vary depending on the con- Table 6: Effect of our evaluation under hyperparameter tuning. We report the error rate for Tent under different learning rates under both the current and our proposed real- istic evaluation. While carefully tuning the learning rate for Tent results in a better performance, our realistic evaluation causes a performance drop under all learning rates. lr Realisticgauss. motion fog pixel. Avg. ∆ ✗ 74.1 63.3 44.7 43.5 56.41×10−4 ✓ 79.7 69.0 47.8 46.8 60.8 (-4.4) ✗ 71.1 59.7 43.1 41.9 53.92×10−4 ✓ 77.6 66.1 46.0 45.0 58.7 (-4.7) ✗ 69.6 58.1 42.4 41.1 52.83×10−4 ✓ 74.9 64.0 45.0 44.0 57.0 (-4.2) ✗ 68.8 57.1 42.0 40.8 52.24×10−4 ✓ 73.7 62.3 44.5 43.2 55.9 (-3.7) text in which they are used. In the episodic evaluation, the efficiency of the method is the most important factor, with more efficient methods like AdaBN and BN showing consistent performance, while data-dependent approaches suffer. Sample rejection methods generally perform well, and fine-tuning approaches such as SHOT can even improve when adapting to fewer samples. In the continual evalua- tion, methods that do not perform sample rejection scale poorly in the offline-continual setup but benefit from skip- ping adaptation on some batches in the realistic-continual setup. Furthermore, our stream speed analysis shows that the performance of TTA methods can vary widely at differ- ent speeds. Our findings are consistent across corruptions and multiple datasets. They can help researchers and practi- tioners to better understand the strengths and weaknesses of different TTA methods, and to choose the most appropriate method for their specific use case. Acknowledgements This work was partially done during a research internship of the first author at Intel Labs. This work was supported by the King Abdullah University of Science and Technol- ogy (KAUST) Office of Sponsored Research (OSR) under Award No. OSR-CRG2021-4648. We would like to thank Yasir Ghunaim and Mattia Soldan for the helpful discussion. 9Evaluation of Test-Time Adaptation Under Computational Time Constraints Impact Statement Our work advances Machine Learning by proposing a re- alistic evaluation protocol for Test Time Adaptation meth- ods, prioritizing computational efficiency. This approach promotes the development of AI systems that are both ac- cessible in resource-limited settings and environmentally sustainable, by favoring simpler, faster methods. Such ad- vancements contribute to more inclusive and responsible AI deployment, aligning with ethical goals of broadening access and reducing environmental impacts References Alfarra, M., P´erez, J. C., Thabet, A., Bibi, A., Torr, P. H., and Ghanem, B. Combating adversaries with anti-adversaries. In Proceedings of the AAAI Conference on Artificial In- telligence, volume 36, pp. 5992–6000, 2022. Ashukha, A., Lyzhov, A., Molchanov, D., and Vetrov, D. Pitfalls of in-domain uncertainty estimation and ensem- bling in deep learning. arXiv preprint arXiv:2002.06470, 2020. Boudiaf, M., Mueller, R., Ben Ayed, I., and Bertinetto, L. Parameter-free online test-time adaptation. In Proceed- ings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 8344–8353, 2022. Cai, Z., Sener, O., and Koltun, V . Online continual learning with natural distribution shifts: An empirical study with visual data. In Proceedings of the IEEE/CVF Interna- tional Conference on Computer Vision, pp. 8281–8290, 2021. Chen, D., Wang, D., Darrell, T., and Ebrahimi, S. Con- trastive test-time adaptation. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 295–305, 2022. Croce, F., Gowal, S., Brunner, T., Shelhamer, E., Hein, M., and Cemgil, T. Evaluating the adversarial robustness of adaptive test-time defenses. In International Conference on Machine Learning, pp. 4421–4435. PMLR, 2022. Deng, J., Dong, W., Socher, R., Li, L.-J., Li, K., and Fei-Fei, L. Imagenet: A large-scale hierarchical image database. In 2009 IEEE conference on computer vision and pattern recognition, pp. 248–255. Ieee, 2009. Gao, J., Zhang, J., Liu, X., Darrell, T., Shelhamer, E., and Wang, D. Back to the source: Diffusion-driven test-time adaptation. arXiv preprint arXiv:2207.03442, 2022. Gidaris, S., Singh, P., and Komodakis, N. Unsupervised rep- resentation learning by predicting image rotations. arXiv preprint arXiv:1803.07728, 2018. Gong, T., Jeong, J., Kim, T., Kim, Y ., Shin, J., and Lee, S.-J. Note: Robust continual test-time adaptation against temporal correlation. In Advances in Neural Information Processing Systems. Goodfellow, I. J., Shlens, J., and Szegedy, C. Explain- ing and harnessing adversarial examples. arXiv preprint arXiv:1412.6572, 2014. He, K., Zhang, X., Ren, S., and Sun, J. Deep residual learn- ing for image recognition. In Proceedings of the IEEE conference on computer vision and pattern recognition, pp. 770–778, 2016. Hendrycks, D. and Dietterich, T. Benchmarking neural network robustness to common corruptions and pertur- bations. Proceedings of the International Conference on Learning Representations, 2019. Hendrycks, D., Basart, S., Mu, N., Kadavath, S., Wang, F., Dorundo, E., Desai, R., Zhu, T., Parajuli, S., Guo, M., Song, D., Steinhardt, J., and Gilmer, J. The many faces of robustness: A critical analysis of out-of-distribution generalization. ICCV, 2021. Ho, J., Jain, A., and Abbeel, P. Denoising diffusion proba- bilistic models. Advances in Neural Information Process- ing Systems, 33:6840–6851, 2020. Iwasawa, Y . and Matsuo, Y . Test-time classifier adjustment module for model-agnostic domain generalization. Ad- vances in Neural Information Processing Systems , 34: 2427–2440, 2021. Kar, O. F., Yeo, T., Atanov, A., and Zamir, A. 3d common corruptions and data augmentation. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 18963–18974, 2022. Kirkpatrick, J., Pascanu, R., Rabinowitz, N., Veness, J., Des- jardins, G., Rusu, A. A., Milan, K., Quan, J., Ramalho, T., Grabska-Barwinska, A., et al. Overcoming catastrophic forgetting in neural networks. Proceedings of the national academy of sciences, 114(13):3521–3526, 2017. Kojima, T., Matsuo, Y ., and Iwasawa, Y . Robustifying vision transformer without retraining from scratch by test- time class-conditional feature alignment. arXiv preprint arXiv:2206.13951, 2022. Kolesnikov, A., Dosovitskiy, A., Weissenborn, D., Heigold, G., Uszkoreit, J., Beyer, L., Minderer, M., Dehghani, M., Houlsby, N., Gelly, S., Unterthiner, T., and Zhai, X. An image is worth 16x16 words: Transformers for image recognition at scale. 2021. 10Evaluation of Test-Time Adaptation Under Computational Time Constraints Lee, D.-H. et al. Pseudo-label: The simple and efficient semi-supervised learning method for deep neural net- works. In Workshop on challenges in representation learning, ICML, volume 3, pp. 896, 2013. Li, Y ., Wang, N., Shi, J., Liu, J., and Hou, X. Revisit- ing batch normalization for practical domain adaptation. arXiv preprint arXiv:1603.04779, 2016. Liang, J., Hu, D., and Feng, J. Do we really need to access the source data? source hypothesis transfer for unsuper- vised domain adaptation. In International Conference on Machine Learning, pp. 6028–6039. PMLR, 2020. Liu, Y ., Kothari, P., Van Delft, B., Bellot-Gurlet, B., Mordan, T., and Alahi, A. Ttt++: When does self-supervised test-time training fail or thrive? Advances in Neural Information Processing Systems, 34:21808–21820, 2021. Mancini, M., Karaoguz, H., Ricci, E., Jensfelt, P., and Ca- puto, B. Kitting in the wild through online domain adap- tation. In 2018 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS), pp. 1103–1109. IEEE, 2018. Mirza, M. J., Micorek, J., Possegger, H., and Bischof, H. The norm must go on: dynamic unsupervised do- main adaptation by normalization. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 14765–14775, 2022a. Mirza, M. J., Soneira, P. J., Lin, W., Kozinski, M., Possegger, H., and Bischof, H. Actmad: Activation matching to align distributions for test-time-training, 2022b. URL https://arxiv.org/abs/2211.12870. Niu, S., Wu, J., Zhang, Y ., Chen, Y ., Zheng, S., Zhao, P., and Tan, M. Efficient test-time model adaptation with- out forgetting. In International conference on machine learning, pp. 16888–16905. PMLR, 2022. Niu14, S., Wu, J., Zhang, Y ., Wen, Z., Chen, Y ., Zhao, P., and Tan15, M. Towards stable test-time adaptation in dynamic wild world. International Conference on Learning Representations, 2023. P´erez, J. C., Alfarra, M., Jeanneret, G., Rueda, L., Thabet, A., Ghanem, B., and Arbel´aez, P. Enhancing adversarial robustness via test-time transformation ensembling. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pp. 81–91, 2021. Saenko, K., Kulis, B., Fritz, M., and Darrell, T. Adapting visual category models to new domains. In Computer Vision–ECCV 2010: 11th European Conference on Com- puter Vision, Heraklion, Crete, Greece, September 5-11, 2010, Proceedings, Part IV 11 , pp. 213–226. Springer, 2010. Sakaridis, C., Dai, D., and Van Gool, L. Acdc: The ad- verse conditions dataset with correspondences for seman- tic driving scene understanding. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pp. 10765–10775, 2021. Schneider, S., Rusak, E., Eck, L., Bringmann, O., Brendel, W., and Bethge, M. Improving robustness against com- mon corruptions by covariate shift adaptation. Advances in Neural Information Processing Systems, 2020. Shalev-Shwartz, S. et al. Online learning and online con- vex optimization. Foundations and Trends® in Machine Learning, 4(2):107–194, 2012. Su, Y ., Xu, X., and Jia, K. Revisiting realistic test-time training: Sequential inference and adaptation by anchored clustering. arXiv preprint arXiv:2206.02721, 2022. Sun, Y ., Wang, X., Liu, Z., Miller, J., Efros, A., and Hardt, M. Test-time training with self-supervision for generaliza- tion under distribution shifts. In International conference on machine learning, pp. 9229–9248. PMLR, 2020. Torralba, A. and Efros, A. A. Unbiased look at dataset bias. In CVPR 2011, pp. 1521–1528. IEEE, 2011. Tzeng, E., Hoffman, J., Saenko, K., and Darrell, T. Adver- sarial discriminative domain adaptation. In Proceedings of the IEEE conference on computer vision and pattern recognition, pp. 7167–7176, 2017. Wang, D., Shelhamer, E., Liu, S., Olshausen, B., and Darrell, T. Tent: Fully test-time adaptation by entropy minimiza- tion. arXiv preprint arXiv:2006.10726, 2020. Wang, Q., Fink, O., Van Gool, L., and Dai, D. Continual test- time domain adaptation. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 7201–7211, 2022. Yuan, L., Xie, B., and Li, S. Robust test-time adaptation in dynamic scenarios. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 15922–15932, 2023. Zhang, M., Levine, S., and Finn, C. Memo: Test time ro- bustness via adaptation and augmentation. arXiv preprint arXiv:2110.09506, 2021. Zhao, H., Liu, Y ., Alahi, A., and Lin, T. On pitfalls of test- time adaptation. International Conference on MAchine Learning, 2023. 11Evaluation of Test-Time Adaptation Under Computational Time Constraints A. Methodology A.1. Online Computation of C(g) Section 3.2 discussed the online evaluation protocol of TTA methods. Here, we give more details on the calcu- lation of C(g), the relative adaptation speed of g, during our online evaluation. First, we set R (g(x)) as the time recording function for g to perform a forward pass for a single batch. To ensure a reliable time calculation, we exe- cute torch.cuda.synchronize() before starting the timer and before ending it. This ensures all GPU operations are finished for the moment time is computed. To alleviate hardware dependence, we also calculate R(fθ(x)) for each evaluation step computing the relative adaptation complex- ity. It is worth mentioning that C(g) for SHOT, EATA, SAR, and COTTA are[3, 3, 8, 103] on average, respectively. B. Experiments B.1. Episodic Evaluation of TTA SHOT, PL, and ETA For completeness, we report the results on 3 baselines: Pseudo Label (Lee et al., 2013), SHOT-IM (Liang et al., 2020), and ETA (Niu et al., 2022) in Table 7. We follow the same setup as in the main paper. Our results are consistent with the findings of Section 4.1 and Table 2. In particular, SHOT-IM improves its perfor- mance under the online evaluation, similar to SHOT. Further, the performance of ETA and PL degrades under the online evaluation due to the additional computational burden. Nev- ertheless, ETA is similar to EATA in providing the best tradeoff between additional computational requirements and performance improvements. SAR with GN We equip our results to include ResNet50 with Group Normalization (GN) layers, following (Niu14 Figure 5: C(g) computation across iterations. We report our online calculations for the relative adaptation speed ofg, C(g), for SAR, SHOT, EATA, and TENT throughout a full evaluation episode. We observe that, overall, C(g) has a stable behavior throughout evaluation iterations. et al., 2023). We report the results in Table 7, where we observe that: (i) Under a relatively large batch size (64), ResNet50 with GN underperforms ResNet50 with Batch Normalization. In fact, the average error rate for SAR in- creases from 56.2% to 65.8%. (ii) The online evaluation penalizes SAR in both architecture choices with a perfor- mance degradation of 3.6% under the GN-based ResNet. Finally, it is worth mentioning that SAR with GN layers attains a similar performance under a batch size of 1. Ablating Batch Sizes In the experiments section, we fixed the batch size to 64 following the recommendations of ear- lier works (Wang et al., 2020; Niu et al., 2022). Here, we investigate the effect of our proposed online evaluation un- der different choices of batch sizes. To that end, we vary the batch size in {1, 16, 32, 128}, and report the results in Figure 6. We draw the following observations: Table 7: Episodic Error Rate on ImageNet-C. We report the error rate of different TTA methods on the ImageNet-C benchmark under both the online and offline setups. A lower error rate indicates a better TTA method. The highlighted numbers indicate a better performance per method across setups. Episodic means the model will adapt to one corruption at a time. The model is reset back to the base model when moving to the next corruption. The offline setup is merely the reproduction of every method. We show that methods generally perform worse in the more realistic online setup. The more computationally complex the TTA method is, the less data it will adapt to, and the worse its performance. SAR-GN represents SAR when deployed on ResNet50 with Group Normalization (GN) layers, following (Niu14 et al., 2023). Noise Blur Weather DigitalMethod Online gauss. shot impul. defoc. glass motionzoom snow frost fog brigh. contr. elast. pixel. jpeg Avg. ∆ ✗ 73.1 69.8 72.0 76.9 75.9 58.5 52.7 53.3 62.2 43.8 34.6 82.6 46.0 42.3 48.9 59.5SHOT-IM ✓ 71.1 68.6 70.7 73.2 73.6 59.1 51.9 52.8 60.5 43.7 33.6 77.3 45.7 42.1 48.6 58.2 (-0.3) ✗ 92.2 92.2 92.8 97.0 89.8 57.7 49.6 50.7 57.1 41.5 32.6 91.1 44.3 40.3 46.6 65.0PL ✓ 90.6 86.3 83.6 93.2 89.7 63.0 51.7 55.0 59.3 43.8 32.9 92.3 47.3 42.4 49.3 65.3 (+0.3) ✗ 64.9 62.7 63.6 66.4 66.3 52.4 47.3 48.2 54.1 40.2 32.2 54.8 42.3 39.2 44.7 52.0ETA ✓ 70.2 67.0 69.6 71.5 71.5 56.9 50.2 51.9 57.0 42.0 32.5 60.5 44.6 40.8 47.1 55.6 (+3.6) ✗ 71.8 69.0 70.3 81.5 81.0 69.6 69.5 57.1 56.6 94.3 29.2 56.0 84.8 51.4 44.7 65.8SAR-GN ✓ 82.0 80.2 82.1 80.2 88.6 78.5 75.1 59.6 53.9 66.9 30.7 63.3 81.3 71.3 47.5 69.4 (+3.6) 12Evaluation of Test-Time Adaptation Under Computational Time Constraints 1 16 32 128 Batch Size 50 60 70 80 90 100Avg. Error Rate (%) ADABN OFFLINE ONLINE 1 16 32 128 Batch Size 50 60 70 80 90 100  BN-ADAPTATION OFFLINE ONLINE 1 16 32 128 Batch Size 50 60 70 80 90 100 COTTA OFFLINE ONLINE 1 16 32 128 Batch Size 50 60 70 80 90 100Avg. Error Rate (%) EATA OFFLINE ONLINE 1 16 32 128 Batch Size 50 60 70 80 90 100 ETA OFFLINE ONLINE 1 16 32 128 Batch Size 50 60 70 80 90 100  LAME OFFLINE ONLINE 1 16 32 128 Batch Size 50 60 70 80 90 100Avg. Error Rate (%) PL OFFLINE ONLINE 1 16 32 128 Batch Size 50 60 70 80 90 100 SAR OFFLINE ONLINE 1 16 32 128 Batch Size 50 60 70 80 90 100 SHOT OFFLINE ONLINE 1 16 32 128 Batch Size 50 60 70 80 90 100Avg. Error Rate (%) SHOTIM OFFLINE ONLINE 1 16 32 128 Batch Size 50 60 70 80 90 100 TENT OFFLINE ONLINE 1 16 32 128 Batch Size 50 60 70 80 90 100 TTAC-NQ OFFLINE ONLINE Figure 6: Batch Size Analysis current vs. realistic setups for every method. We assess the performance variation of 12 different TTA methods under varying batch sizes. We experiment with batch sizes in{1, 16, 32, 128}. We do not include the baseline, MEMO, and DDA, since they are data-dependent approaches and are unaffected by batch size. All TTA methods, except LAME, are severely affected by smaller batch sizes. Nonetheless, the realistic evaluation degrades the performance of all methods, except SHOT and SHOT-IM. (i) Online evaluation improves the performance of SHOT and SHOT-IM. This result is consistent with the earlier observations in Table 2. Note that PL shares a similar trend as well. (ii) The performance of TTA methods degrades when switching from offline to online evaluation, regardless of the batch size. This result is highlighted in COTTA, ETA, EATA, SAR, TENT, and TTAC-NQ. (iii) Performance of TTA methods vastly varies when varying the batch size. This result is consistent with earlier findings in the literature (Gao et al., 2022; Niu14 et al., 2023), where most TTA methods fail with small batch sizes. At last, and to ease comparison across methods, we summa- rize all the plots for all methods in Figure 7. Consistency with 3 random seeds. For all of our exper- iments, we run each experiment with 3 random seeds. In most of our results, we found out that the standard deviation of performance across runs is very small. Our results in Figures 3 and 4 demonstrate this variation in the shaded area for 5 different TTA methods. B.2. Continual Evaluation of TTA We further explore another setup for the continual evalua- tion of TTA. In particular, we follow (Wang et al., 2022) in concatenating all corruptions in ImageNet-C with 11 differ- ent orders. We then report the average performance of each method across all runs and corruptions in Table 8. We run each experiment with 3 random seeds, and report our results with standard deviations. For the remaining implementation 13Evaluation of Test-Time Adaptation Under Computational Time Constraints 1 16 32 128 Batch Size 50 60 70 80 90 100Avg. Error Rate (%) OFFLINE 1 16 32 128 Batch Size 50 60 70 80 90 100 ONLINE ADABN BN-ADAPTATION COTTA EATA ETA LAME PL SAR SHOT SHOTIM TENT TTAC-NQ Figure 7: Summary of batch size analysis: current vs. realistic setups. Left: Current evaluation, i.e.,Section 3.1. Right: Realistic evaluation,i.e.,Section 3.2. While EATA achieves the lowest error rate under batch sizes≥ 32, SHOT becomes a very competitive baseline, outperforming EATA, at a batch size of 128. Table 8: Continual Error Rate on ImageNet-C. We report the average continual error rate for 11 different corruption orders, with 3 different seeds, under both the offline and online setups with a corruption severity level of 5. Continual refers to continually adapting after each corruption without resetting. This metric indicates the model’s capability to learn from previous corruptions. The offline setup refers to the performance of the model in a continual learning scheme, whereas the online setup refers to the performance of the model in a continual learning scheme, under our more realistic online setup. We show that the more complex a method is, the fewer samples it adapts to, achieving better performance in a continual learning scheme. Avg. Error (%) COTTA ETA TENT SAR EATA SHOT TTAC-NQ Offline 65.3 ± 5.9 56 .4 ± 2.3 84 .6 ± 16.0 59 .8 ± 3.0 56 .4 ± 2.3 88 .4 ± 11.4 81 .8 ± 11.4 Online 69.3 ± 2.8 57 .7 ± 2.0 65 .6 ± 5.0 60 .4 ± 1.8 57 .7 ± 1.9 78 .2 ± 7.7 65 .1 ± 3.8 details, we follow our setup in main paper. We observe that, similar to our conclusions in Section 4.2, online eval- uation helps methods that do not perform sample rejection (e.g.,TENT). Nonetheless, both ETA and EATA provide the best trade-off between performance and additional compu- tational burden. B.3. Stream Speed Analysis For completeness, we extend our stream speed analysis in Section 4.3 to cover the ImageNet-3DCC dataset. We preserve our experimental setup by varying the stream speed according to ηr, with η ∈ {1/16, 1/8, 1/4, 1/2, 1. Figure 8 summarizes our results for SHOT, TENT, TTAC-NQ, EATA, and SAR. We observe similar trends to the ones in Figure 4, where the performance of different TTA methods varies widely under different stream speeds. The large relative adaptation speed of TTAC-NQ degrades its performance under even slow streams (e.g.,η = 1/8), while SHOT reduces its error rate under faster streams. Furthermore, EATA is consistently outperforming all other considered approaches under different stream speeds. B.4. Evaluation on Other Benchmarks We report the error rates on all corruptions of ImageNet- 3DCC (Kar et al., 2022), along with the overall average error rate, in Table 9. The conclusions we draw for ImageNet- 3DCC (Kar et al., 2022) are very similar to the ones ob- served on ImageNet-C (Hendrycks & Dietterich, 2019) (in Section 4.1). We observe that efficient methods, with C(g) = 1, such as LAME and BN, maintain performance. Furthermore, the performance of some TTA methods (Wang et al., 2020; Niu14 et al., 2023; Niu et al., 2022; Wang et al., 2022) degrades in the online setup, while others that use pseudo labeling (Lee et al., 2013; Liang et al., 2020) actually improve. This degradation seems to be directly proportional to the amount of data a method misses according to its C(g). 14Evaluation of Test-Time Adaptation Under Computational Time Constraints Table 9: Episodic Error Rate on ImageNet-3DCommonCorruptions. We report the error rate of different TTA methods on ImageNet-3DCC (Kar et al., 2022) benchmark under both the realistic and offline setups. A lower error rate indicates a better TTA method. The highlighted numbers indicate a better performance per method across setups. Episodic means the model will adapt to one corruption at a time. The model is reset back to the base model when moving to the next corruption. The offline setup corresponds to reproducing the reported performance of every method. The first sub-table corresponds to methods that incur none or few additional computations, i.e.,C(g) = 1. We show that methods generally perform worse in the more realistic setup. The more computationally complex the TTA method is, the fewer data it will adapt to, and the worse its performance. Depth of field Noise LightingWeather Video Camera motionMethod RealisticNear focus Far focusColor quant. ISO noise Low lightFlash Fog 3DBit error H.265 ABR H.265 CRFXY-mot. blur Z-mot. blurAvg. ∆ Source ✓ 46.9 55.6 82.5 94.0 71.7 78.7 75.3 88.6 70.6 65.4 82.0 75.3 73.9 -AdaBN ✓ 45.2 55.0 71.8 76.8 64.1 80.8 75.0 91.8 80.9 76.7 79.1 67.5 72.1 -LAME ✓ 45.3 55.0 71.9 76.9 64.1 80.8 75.1 91.8 80.9 76.8 79.2 67.6 72.1 -BN ✓ 43.9 54.3 72.3 76.6 60.9 80.1 72.4 90.9 78.7 73.8 76.9 65.6 70.5 - PL ✗ 39.8 49.8 65.5 72.6 48.9 79.0 66.1 97.5 92.1 86.2 88.7 57.6 70.3(-1.6)✓ 41.0 51.3 66.5 71.5 52.8 77.4 68.1 95.6 86.0 78.7 77.0 59.2 68.7 SHOT ✗ 43.0 53.6 67.1 64.2 51.9 81.1 73.2 97.2 83.5 77.8 77.3 60.1 69.2(-2.2)✓ 41.7 51.4 64.4 63.8 51.6 77.5 71.6 95.1 79.9 74.6 73.7 58.5 67.0 SHOT-IM✗ 42.2 52.7 66.6 63.7 51.0 81.0 72.1 97.0 83.3 77.6 75.6 59.2 68.5(-1.9)✓ 41.2 51.2 64.4 63.3 51.3 77.5 70.9 94.9 79.4 74.1 72.3 58.3 66.6 TENT ✗ 39.9 49.6 62.4 62.2 50.7 75.6 68.5 91.6 75.7 70.2 70.4 57.0 64.5(+2.3)✓ 41.7 51.4 65.5 67.2 54.7 77.4 70.1 90.7 76.8 71.9 74.0 60.8 66.8 SAR ✗ 40.3 50.0 62.0 61.2 50.6 73.8 65.8 90.1 73.9 68.8 69.1 56.8 63.5(+6.9)✓ 44.9 54.7 71.1 75.4 62.6 80.3 73.8 91.7 80.5 76.1 78.6 66.9 71.4 ETA ✗ 38.7 47.9 59.1 56.7 46.8 71.0 62.1 90.6 72.8 67.3 64.7 52.9 60.9(+2.3)✓ 39.7 49.3 61.6 60.7 50.0 73.5 65.2 90.3 74.4 69.1 68.8 55.9 63.2 CoTTA ✗ 40.8 50.9 66.3 68.3 54.6 77.2 68.0 90.2 76.4 71.1 73.1 60.4 66.4(+9.2)✓ 55.4 63.1 74.1 77.0 64.7 83.4 78.1 93.7 84.0 80.3 81.7 71.9 75.6 TTAC-NQ✗ 40.7 50.5 61.0 61.1 51.5 72.8 66.6 93.8 81.1 74.7 75.7 59.1 65.7(+7.9)✓ 49.9 57.0 69.3 72.3 58.9 79.8 76.3 95.8 86.5 83.0 84.6 69.8 73.6 EATA ✗ 38.6 47.8 59.2 56.6 46.9 71.2 62.2 90.9 72.5 67.4 64.6 52.9 60.9(+2.2)✓ 39.8 49.3 61.6 60.5 49.9 73.5 64.8 90.6 73.7 69.1 68.6 55.7 63.1 C. Single Model Evaluation Scheme In Section 3.2, we assume fθt can generate predictions whenever g is occupied with adapting to a batch. This setup assumes the capacity to concurrently deploy two models. However, this assumption might be unfair to methods with C(g) = 1, since it allows expensive methods to skip batches without large penalties. We thus also study the case where only one model can be deployed. Studying this setup requires establishing a policy on how samples missed by the TTA method g are treated. That is, when g is busy adapting, all skipped samples still must be predicted without access to fθt . Depending on the applica- tion, this prediction could leverage prior knowledge about the problem e.g. temporal correlation across samples, or the bias of the distribution. In our setup, we consider the most strict scenario in which, whenever g is busy, a ran- dom classifier generates predictions for the incoming sam- ples. This naive design choice results from our evaluation on ImageNet-based datasets, which contain images whose classes display no bias nor temporal correlation. We conduct episodic evaluation, similar to Section 4.1, on ImageNet-C dataset. We average the error rates per corruption category (e.g. averaging error rates for gaussian, shot, and impulse noises) and present the results of this study in Table 10. We draw the following observation. Single model evaluation strongly favors methods with C(g) = 1. We observe that all models that are slower than the stream are heavily penalized to the point that using the original pre-trained model becomes a better alternative. However, methods that can be as fast as the stream, like AdaBN or BN, become the best alternative due to their speed. This result encourages more research toward devel- oping efficient TTA methods that have negligible additional computational overhead. D. Results on ResNet18 In our experiments in the main paper, we focused on the stan- dard ResNet18-architecture, following the common practice in the literature. Here, and for completeness, we extend our results to cover the smaller and more efficient ResNet18 architecture. Teble 11 summarizes the episodic evaluation of 6 TTA methods on ImageNet-C dataset. Similar to our conclusions in the episodic evaluation section in the main paper, more expensive adaptation methods degrade more under our realistic evaluation scheme. 15Evaluation of Test-Time Adaptation Under Computational Time Constraints Table 10: Per Corruption Category Average Error Rate Using Single Model Evaluation on ImageNet-C. We re- port the average error rate per corruption category of dif- ferent TTA methods under single model realistic evaluation mode on ImageNet-C. Single model mode assumes the de- ployment of a single modelg instead of two under a constant speed stream S. We assume the most extreme scenario, that is if a model g is occupied adapting to a batch, the incoming batch is fed to a random classifier. We observe that the best TTA methods to use in this scenario are AdaBN (Li et al., 2016) and BN (Schneider et al., 2020), which simply adapt the BN statistics. Method Realistic Noise Blur Weather Digital Avg. Source ✓ 97.7 83.8 69.1 81.4 82.0 AdaBN ✓ 84.5 76.1 54.9 62.7 68.5 BN ✓ 84.1 73.1 54.2 59.9 66.7 SHOT ✓ 92.6 91.3 87.0 88.5 89.7 TENT ✓ 91.9 89.4 83.0 85.0 87.0 SAR ✓ 95.6 94.0 90.1 91.3 92.6 EATA ✓ 89.4 87.6 82.0 83.2 85.3 TTAC-NQ ✓ 96.6 96.9 96.3 96.4 96.5 Table 11: Evaluating different TTA methods with ResNet- 18 architecture on ImageNet-C. We report the average error rate across all different types of corruptions (lower is bet- ter). TTA methods generally perform worse in the more realistic setup. The more computationally complex the TTA method is, the less data it will adapt to, and the worse is its performance. Method Basic BN SHOT Tent EATA SAR Current 85.4 70.1 64.4 64.9 59.7 63.8 Realistic 85.4 70.1 64.5 68.3 63.2 69.5 Diff - - 0.1 3.4 3.5 5.7 1/16 1/8 1/4 1/2 1 η 62 64 66 68 70 72 74Error Rate (%) SHOT TENT TTAC-NQ SAR EATA Figure 8: Average Error Rate on ImageNet-3DCC Under Slower Stream Speeds. We report the average error rate for several TTA methods on ImageNet-3DCC under slower stream speeds. In our proposed online model evaluation, the stream speed r is normalized by the time needed for a forward pass using the base model. We evaluate different TTA methods under a stream with speed ηr with η ∈ (0, 1]. An η = 1/16 means the stream is 16 times slower than the forward pass of the base model. We report the standard deviation across 3 random seeds. Different TTA methods degrade differently when varying η. 16",
      "meta_data": {
        "arxiv_id": "2304.04795v2",
        "authors": [
          "Motasem Alfarra",
          "Hani Itani",
          "Alejandro Pardo",
          "Shyma Alhuwaider",
          "Merey Ramazanova",
          "Juan C. Pérez",
          "Zhipeng Cai",
          "Matthias Müller",
          "Bernard Ghanem"
        ],
        "published_date": "2023-04-10T18:01:47Z",
        "pdf_url": "https://arxiv.org/pdf/2304.04795v2.pdf"
      },
      "llm_extracted_info": {
        "main_contributions": "The paper proposes a novel online evaluation protocol for Test-Time Adaptation (TTA) methods that explicitly accounts for computational time constraints. It penalizes slower methods by providing them with fewer samples for adaptation, thereby offering a more realistic assessment of their real-world applicability. The study benchmarked 15 TTA methods on multiple datasets and scenarios using this protocol, demonstrating that simpler, faster approaches can often outperform more sophisticated but slower methods (e.g., SHOT outperforming SAR) when inference speed is considered. This highlights the critical importance of developing TTA methods that are both accurate and efficient.",
        "methodology": "The core methodology is the proposed 'Realistic TTA evaluation protocol.' Unlike current offline protocols, this protocol simulates an online setting where data arrives in a constant-speed stream. The key concept is the 'relative adaptation speed C(g),' defined as the integer ratio of the stream's speed (r) to the TTA method's processing speed (R(g(x))). If C(g) = k, the method is allowed to adapt to only every k-th sample; the skipped samples are processed by the most recent adapted model (fθt) or the base model without adaptation. C(g) is computed online for each input as C(g(xt)) = ⌈r/R(g(x))⌉. This intrinsically penalizes slower methods, as longer adaptation times result in fewer samples being utilized for adaptation.",
        "experimental_setup": "The evaluation focused on image classification using a ResNet-50-BN backbone (pretrained on ImageNet), also extended to ViT and ResNet-18 architectures. Experiments were conducted primarily on ImageNet-C (corruption level 5 across 15 corruptions), and further extended to CIFAR10-C, ImageNet-R, and ImageNet-3DCC. A batch size of 64 was primarily used, with ablations for {1, 16, 32, 128}. Fifteen state-of-the-art TTA methods (e.g., AdaBN, SHOT, TENT, SAR, EATA, MEMO, DDA) from 2017-2023 were benchmarked using their official implementations and recommended hyperparameters. Evaluation scenarios included episodic adaptation, continual adaptation, varying stream speeds (ηr with η ∈ {1/16, 1/8, 1/4, 1/2, 1}), and practical TTA with label imbalances. All main results are reported as the average across three random seeds.",
        "limitations": "The paper identifies that data-dependent TTA approaches like MEMO and DDA are extremely inefficient, incurring massive computational burdens that significantly degrade their performance under the realistic protocol, making them perform close to the non-adapted baseline. It also notes that methods not employing sample rejection (e.g., SHOT, TENT, TTAC) can scale poorly in offline-continual settings due to overfitting to early distributions. While sample rejection methods like SAR are generally better, SAR's reliance on gradients for rejection reduces its speed, leading to a performance drop. The online computation of C(g) can be noisy due to hardware and input dependence. The default evaluation assumes the concurrent deployment of a base model (fθ) to process skipped samples, which might not be feasible in all real-world scenarios. In a single-model deployment (where only one model can be deployed, and skipped samples are handled by a random classifier), even efficient TTA methods face significant performance penalties if not as fast as the stream.",
        "future_research_directions": "The paper suggests that future research should prioritize developing practical TTA methods that are both accurate and computationally efficient, explicitly considering inference speed as a critical performance dimension. Specifically, there's a need to increase the efficiency of data-dependent adaptation methods, which currently suffer significantly under computational constraints. For deployment scenarios where only a single model can operate, fostering the development of TTA methods with negligible additional computational overhead is crucial. Furthermore, exploring more sophisticated policies for handling samples skipped by a busy TTA method (e.g., leveraging temporal correlation or distribution bias) when a separate baseline model is unavailable is an open area."
      }
    },
    {
      "title": "Leveraging Proxy of Training Data for Test-Time Adaptation"
    },
    {
      "title": "TTT++: When Does Self-Supervised Test-Time Training Fail or Thrive?"
    },
    {
      "title": "Persistent Test-time Adaptation in Recurring Testing Scenarios",
      "abstract": "Current test-time adaptation (TTA) approaches aim to adapt a machine learning\nmodel to environments that change continuously. Yet, it is unclear whether TTA\nmethods can maintain their adaptability over prolonged periods. To answer this\nquestion, we introduce a diagnostic setting - recurring TTA where environments\nnot only change but also recur over time, creating an extensive data stream.\nThis setting allows us to examine the error accumulation of TTA models, in the\nmost basic scenario, when they are regularly exposed to previous testing\nenvironments. Furthermore, we simulate a TTA process on a simple yet\nrepresentative $\\epsilon$-perturbed Gaussian Mixture Model Classifier, deriving\ntheoretical insights into the dataset- and algorithm-dependent factors\ncontributing to gradual performance degradation. Our investigation leads us to\npropose persistent TTA (PeTTA), which senses when the model is diverging\ntowards collapse and adjusts the adaptation strategy, striking a balance\nbetween the dual objectives of adaptation and model collapse prevention. The\nsupreme stability of PeTTA over existing approaches, in the face of lifelong\nTTA scenarios, has been demonstrated over comprehensive experiments on various\nbenchmarks. Our project page is available at https://hthieu166.github.io/petta.",
      "full_text": "Persistent Test-time Adaptation in Recurring Testing Scenarios Trung-Hieu Hoang1 Duc Minh Vo2 Minh N. Do1,3 1Department of Electrical and Computer Engineering, University of Illinois at Urbana-Champaign 2The University of Tokyo 3VinUni-Illinois Smart Health Center, VinUniversity {hthieu, minhdo}@illinois.edu vmduc@nlab.ci.i.u-tokyo.ac.jp Abstract Current test-time adaptation (TTA) approaches aim to adapt a machine learn- ing model to environments that change continuously. Yet, it is unclear whether TTA methods can maintain their adaptability over prolonged periods. To answer this question, we introduce a diagnostic setting - recurring TTA where envi- ronments not only change but also recur over time, creating an extensive data stream. This setting allows us to examine the error accumulation of TTA models, in the most basic scenario, when they are regularly exposed to previous testing environments. Furthermore, we simulate a TTA process on a simple yet repre- sentative ϵ-perturbed Gaussian Mixture Model Classifier, deriving theoretical insights into the dataset- and algorithm-dependent factors contributing to gradual performance degradation. Our investigation leads us to propose persistent TTA (PeTTA), which senses when the model is diverging towards collapse and adjusts the adaptation strategy, striking a balance between the dual objectives of adaptation and model collapse prevention. The supreme stability of PeTTA over existing approaches, in the face of lifelong TTA scenarios, has been demonstrated over comprehensive experiments on various benchmarks. Our project page is available at https://hthieu166.github.io/petta. 1 Introduction Machine learning (ML) models have demonstrated significant achievements in various areas [18, 38, 47, 23]. Still, they are inherently susceptible to distribution-shift [46, 13, 48, 21, 6] (also known as the divergence between the training and testing environments), leading to a significant degradation in model performance. The ability to deviate from the conventional testing setting appears as a crucial aspect in boosting ML models’ adaptability when confronted with a new testing environment that has been investigated [ 30, 53, 14]. Among common domain generalization methods [ 58, 24, 1], test-time adaptation (TTA) takes the most challenging yet rewarding path that leverages unlabeled data available at test time for self-supervised adaptation prior to the final inference [57, 39, 8, 41, 59]. Early TTA studies have concentrated on a simply ideal adaptation scenario where the test samples come from a fixed single domain [57, 39, 41]. As a result, such an assumption is far from the ever- changing and complex testing environments. To confront continually changing environments [59, 12], Yuan et al. [61] proposed a practical TTA scenario where distribution changing and correlative sampling occur [15] simultaneously. Though practical TTA is more realistic than what the previous assumptions have made, it still assumes that any environment only appears once in the data stream, a condition which does not hold true. Taking a surveillance camera as an example, it might accom- modate varying lighting conditions recurringly day after day (Fig. 1-left). Based on this reality, we hypothesize that the recurring of those conditions may reveal the error accumulation phenomenon in TTA, resulting in performance degradation over a long period. To verify our hypothesis, we simulate a 38th Conference on Neural Information Processing Systems (NeurIPS 2024). arXiv:2311.18193v4  [cs.CV]  2 Nov 2024Testing Error Time Day 1 Illumination Condition Day 2 Day 3 0 50 100 150 200 250 300 0 0.2 0.4 0.6 0.8 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 201 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 Test-time adaptation step Testing Error No TTA RoTTA PeTTA (ours) Figure 1: Recurring Test-time Adaption (TTA). (left) Testing environments may change recurringly and preserving adaptability when visiting the same testing condition is not guaranteed. (right) The testing error of RoTTA [61] progressively raises (performance degradation) and exceeds the error of the source model (no TTA) while our PeTTA demonstrates its stability when adapting to the test set of CIFAR-10-C [19] 20 times. The bold lines denote the running mean and the shaded lines in the background represent the testing error on each domain (excluding the source model, for clarity). recurring testing environment and observe the increasing error rate by recurringly adapting to the test set of CIFAR-10-C [19] multiple times. We showcase the testing error of RoTTA [61] after 20 cycles of adaptation in Fig. 1-right. As expected, RoTTA can successfully adapt and deliver encouraging outcomes within the first few passes. However, this advantage is short-lived as our study uncovers a significant issue: TTA approaches in this setting may experience severe and persistent degradation in performance. Consequently, the testing error of RoTTA gradually escalates over time and quickly surpasses the model without adaptation. This result confirms the risk of TTA deployment in our illustrative scenario, as an algorithm might work well in the first place and gradually degenerate. Therefore, ensuring sustainable quality is crucial for real-world applications, especially given the recurring nature of testing environments. This study examines whether the adaptability of a TTA algorithm persists over an extended testing stream. Specifically, in the most basic scenario, where the model returns to a previously encountered testing environment after undergoing various adjustments. We thus propose a more general testing scenario than the practical TTA [61], namely recurring TTA, where the environments not only change gradually but also recur in a correlated manner over time. We first analyze a simulation using the ϵ−perturbed Gaussian Mixture Model Classifier (ϵ−GMMC) on a synthesized dataset and derive a theoretical analysis to confirm our findings, offering insights to tackle similar issues in deep neural networks. The analysis provides hints for reasoning the success of many recent robust continual TTA approaches [61, 12, 59, 15] and leading us to propose a simple yet effective baseline to avoid performance degradation, namely Persistent TTA (PeTTA). PeTTA continuously monitors the chance of collapsing and adjusts the adaptation strategy on the fly, striking a balance between the two objectives: adaptation and collapse prevention. Our contributions can be summarized as follows: • First, this work proposes a testing scenario - recurring TTA, a simple yet sufficient setup for diagnosing the overlooked gradual performance degradation phenomenon of TTA. • Second, we formally define the phenomenon of TTA collapsing and undertake a theoretical analysis on an ϵ-GMMC, shedding light on dataset-dependent and algorithm-dependent factors that contribute to the error accumulation during TTA processes. • Third, we introduce persistent TTA (PeTTA)- a simple yet effective adaptation scheme that surpasses all baseline models and demonstrates a persisting performance. For more context on related work, readers are directed to visit our discussions in Appdx. A. 2 Background Test-time Adaptation (TTA). A TTA algorithm operates on an ML classifier ft : X → Ywith parameter θt ∈ Θ (parameter space) gradually changing over time (t ∈ T) that maps an input image x ∈ Xto a category (label) y ∈ Y. Let the capital letters (Xt, Yt) ∈ X × Ydenote a pair of random variables with the joint distribution Pt(x, y) ∈ Pd, t∈ T. Here, Pd belongs to collection of D sets of testing scenarios (domains) {Pd}D d=1. The covariate shift [46] is assumed: Pt(x) and Pt′(x) 2could be different but Pt(y|x) = Pt′(y|x) holds ∀t ̸= t′. At t = 0, θ0 is initialized by a supervised model trained on P0 ∈ P0 (source dataset). The model then explores an online stream of testing data. For each t >0, it receives Xt (typically in form of a batch of Nt testing samples) for adapting itself ft−1 → ft before making the final prediction ft (Xt). TTA with Mean Teacher Update. To achieve a stable optimization process, the main (teacher) model ft are updated indirectly through a student model with parameters θ′ t [57, 61, 12, 15, 55]. At first, the teacher model in the previous step introduces a pseudo label [28] ˆYt for each Xt: ˆYt = ft−1(Xt). (1) With a classification loss LCLS (e.g., cross-entropy [16]), and a model parameters regularizer R, the student model is first updated with a generic optimization operatorOptim, followed by an exponential moving average (EMA) update of the teacher model parameter θt−1: θ′ t = Optim θ′∈Θ EPt h LCLS \u0010 ˆYt, Xt; θ′ \u0011i + λR(θ′), (2) θt = (1 − α)θt−1 + αθ′ t, (3) with α ∈ (0, 1) - the update rate of EMA, andλ ∈ R+ - the weighting coefficient of the regularization term, are the two hyper-parameters. Practical TTA. In practical TTA [61], two characteristics of the aforementioned distribution of data stream are noticeable. Firstly, Pt’s can be partitioned by td’s in which {Pt}td t=td−1 ⊂ Pd. Here, each partition of consecutive steps follows the same underlying distribution which will change continually through D domains [59] (P1 → P2 ··· → PD). Secondly, the category distribution in each testing batch is temporally correlated [15]. This means within a batch, a small subset of categories is dominant over others, making the marginal distribution Pt(y) = 0, ∀y ̸∈ Yt ⊂ Yeven though the category distribution over all batches are balanced. Optimizing under this low intra-batch diversity (|Yt| ≪ |Y|) situation can slowly degenerate the model [7]. 3 Recurring TTA and Theoretical Analysis This section conducts a theoretical analysis on a concrete failure case of a simple TTA model. The results presented at the end of Sec. 3.2 will elucidate the factors contributing to the collapse (Sec. 3.1), explaining existing good practices (Sec. 3.3) and give insights into potential solutions (Sec. 4). 3.1 Recurring TTA and Model Collapse Recurring TTA.To study the gradual performance degradation (or model collapse), we propose anew testing scenario based on practical TTA [61]. Conducting a single pass through D distributions, as done in earlier studies [61, 59], may not effectively identify the degradation. To promote consistency, our recurring TTA performs revisiting the previous distributions K times to compare the incremental error versus the previous visits. For example, a sequence with K = 2 could be P1 → P2 → ··· → PD → P1 → P2 → ··· → PD. Appdx. D extends our justifications on constructing recurring TTA. Definition 1 (Model Collapse). A model is said to be collapsed from step τ ∈ T, τ <∞ if there exists a non-empty subset of categories ˜Y ⊂ Ysuch that Pr{Yt ∈ ˜Y} > 0 but the marginal Pr{ˆYt ∈ ˜Y} converges to zero in probability: lim t→τ Pr{ˆYt ∈ ˜Y} = 0. Here, upon collapsing, a model tends to ignore almost categories in ˜Y. As it is irrecoverable once collapsed, the only remedy would be resetting all parameters back to θ0. 3.2 Simulation of Failure and Theoretical Analysis Collapsing behavior varies across datasets and the adaptation processes. Formally studying this phenomenon on a particular real dataset and a TTA algorithm is challenging. Therefore, we propose a theoretical analysis on ϵ-perturbed binary Gaussian Mixture Model Classifier (ϵ-GMMC) that shares the typical characteristics by construction and demonstrates the same collapsing pattern in action (Sec. 5.1) as observed on real continual TTA processes (Sec. 5.3). 3Pseudo-label Predictor ˆYt = argmax y∈Y Pr(Xt|y;θt−1) Xt Mean-teacher Update θ′ t = Optim θ′∈Θ EPt h LCLS \u0010ˆYt, Xt;θ′\u0011i θt = (1−α)θt−1 +αθ′ t ϵt ··· θt−1 θt ··· Figure 2: ϵ-perturbed binary Gaussian Mix- ture Model Classifier, imitating a continual TTA algorithm for theoretical analysis. Two main components include a pseudo-label predictor (Eq. 1), and a mean teacher up- date (Eqs. 2, 3). The predictor is perturbed for retaining a false negative rate of ϵt to simulate an undesirable TTA testing stream. Simulated Testing Stream. Observing a testing stream with (Xt, Yt) ∈ X × Y= R × {0, 1} and the underlying joint distribution Pt(x, y) = py,t · N(x; µy, σ2 y). The main task is predicting Xt was sampled from cluster 0 or 1 (negative or positive). Conveniently, let py,t ∆ = Pt(y) = Pr(Yt = y) and ˆpy,t ∆ = Pr( ˆYt = y) be the marginal distribution of the true label Yt and pseudo label ˆYt. GMMC and TTA. GMMC first implies an equal prior distribution by construction which is desirable for the actual TTA algorithms (e.g., category-balanced sampling strategies in [ 61, 15]). Thus, it simplifies ft into a maximum likelihood estimation ft(x) = argmaxy∈Y Pr(x|y; θt) with Pr(x|y; θt) = N(x; ˆµy,t, ˆσ2 y,t). The goal is estimating a set of parameters θt = {ˆµy,t, ˆσ2 y,t}y∈Y. A perfect classifier θ0 = {µy, σ2 y}y∈Y is initialized at t = 0. For the consecutive steps, the simplicity of GMMC allows solving the Optim (for finding θ′ t, Eq. 2) perfectly by computing the empirical mean and variance of new samples, approximating EPt. The mean teacher update (Eq. 3) for GMMC is: ˆµy,t = ( (1 − α)ˆµy,t−1 + αEPt h Xt|ˆYt i if ˆYt = y ˆµy,t−1 otherwise . (4) The update of ˆσ2 y,t is similar. ˆYt = ft−1(Xt) can be interpreted as a pseudo label (Eq. 1). ϵ-GMMC. Severe distribution shifts or low intra-batch category diversity of recurring TTA/practical TTA both result in an increase in the error rate of the predictor . Instead of directly modeling the dynamic changes of py,t (which can be complicated depending on the dataset), we study an ϵ−pertubed GMMC (ϵ−GMMC), where py,t is assumed to be static (defined below) and the pseudo- label predictor of this model is perturbed to simulate undesirable effects of the testing stream on the predictor. Two kinds of errors appear in a binary classifier [4]. Let ϵt = Pr{Yt = 1|ˆYt = 0} (5) be the false negative rate (FNR) of the model at step t. Without loss of generality, we study the increasing type II collapse of ϵ-GMMC. By intentionally flipping the true positive pseudo labels in simulation, an FNR of ϵt is maintained (Fig. 2). Assumption 1 (Static Data Stream). The marginal distribution of the true label follows the same Bernoulli distribution Ber(p0): p0,t = p0, (p1,t = p1 = 1 − p0), ∀t ∈ T. Lemma 1 (Increasing FNR). Under Assumption 1, a binary ϵ-GMMC would collapsed (Def. 1) with lim t→τ ˆp1,t = 0 (or lim t→τ ˆp0,t = 1, equivalently) if and only if lim t→τ ϵt = p1. Lemma 1 states the negative correlation between ˆp1,t and ϵt. Unsurprisingly, towards the collapsing point where all predictions are zeros, the FNR also increases at every step and eventually reaches the highest possible FNR of p1. Lemma 2 (ϵ-GMMC After Collapsing ). For a binary ϵ-GMMC model, with Assumption 1, if lim t→τ ˆp1,t = 0 (collapsing), the cluster 0 in GMMC converges in distribution to a single-cluster GMMC with parameters: N(ˆµ0,t, ˆσ2 0,t) d. → N(p0µ0 + p1µ1, p0σ2 0 + p1σ2 1 + p0p1(µ0 − µ1)2). Lemma 2 states the resulting ϵ−GMMC after collapsing. Cluster 0 now covers the whole data distribution (and assigning label 0 for all samples). Furthermore, collapsing happens when ˆµ0,t moves toward µ1. We next investigate the factors and conditions for this undesirable convergence. 4Theorem 1 (Convergence of ϵ−GMMC). For a binary ϵ-GMMC model, with Assumption 1, let the distance from ˆµ0,t toward µ1 is d0→1 t = |EPt [ˆµ0,t] − µ1|, then: d0→1 t − d0→1 t−1 ≤ α · p0 · \u0012 |µ0 − µ1| −d0→1 t−1 1 − ϵt \u0013 . From Thm. 1, we observe that the distance d0→1 t ’s converges (also indicating the convergence to the distribution in Lemma 2) if d0→1 t < d0→1 t−1 . The model collapse happens when this condition holds for a sufficiently long period. Corollary 1 (A Condition forϵ−GMMC Collapse). With fixedp0, α, µ0, µ1, ϵ−GMMC is collapsed if there exists a sequence of {ϵt}τ τ−∆τ (τ ≥ ∆τ > 0) such that: p1 ≥ ϵt > 1 − d0→1 t−1 |µ0 − µ1|, t ∈ [τ − ∆τ , τ]. Corollary 1 introduces a condition ϵ-GMMC collapse. Here, ϵt’s are non-decreasing, lim t→τ ϵt = p1. Remarks. Thm. 1 concludes two sets of factors contributing to collapse: (i) data-dependent factors: the prior data distribution (p0), the nature difference between two categories (|µ0 − µ1|); and (ii) algorithm-dependent factors: the update rate (α), the FNR at each step (ϵt). ϵ-GMMC analysis sheds light on explaining model collapse on real datasets (Sec. 5.3), reasons the existing approaches (Sec. 3.3) and motivates the development of our baseline (Sec. 4). 3.3 Connection to Existing Solutions Prior TTA algorithms have already incorporated implicit mechanisms to mitigate model collapse. The theoretical results in the previous section explain the rationale behind these effective strategies. Regularization Term for θt. Knowing that f0 is always well-behaved, an attempt is restricting the divergence of θt from θ0, e.g. using R(θt) ∆ = ∥θ0 − θt∥2 2 regularization [40]. The key idea is introducing a penalty term to avoid an extreme divergence as happening in Thm. 1. Memory Bank for Harmonizing Pt(x). Upon receiving Xt, samples in this batch are selectively updated to a memory bank M (which already contains a subset of some instances ofXt′, t′ < tin the previous steps). By keeping a balanced number of samples from each category, distribution PM t (y) of samples in M is expected to have less zero entries than Pt(y), making the optimization step over PM t more desirable. From Thm. 1, M moderates the extreme value of the category distribution (p0 term) which typically appears on batches with low intra-batch category diversity. 4 Persistent Test-time Adaptation (PeTTA) Now we introduce our Persistent TTA (PeTTA) approach. Further inspecting Thm. 1, while ϵt (Eq. 5) is not computable without knowing the true labels, the measure of divergence from the initial distribution (analogously to d0→1 t−1 term) can provide hints to fine-tune the adaptation process. Key Idea. A proper adjustment toward the TTA algorithm can break the chain of monotonically increasing ϵt’s in Corollary 1 to prevent the model collapse. In the mean teacher update, the larger value of λ (Eq. 2) prioritizes the task of preventing collapse on one hand but also limits its adaptability to the new testing environment. Meanwhile, α (Eq. 3) controls the weight on preserving versus changing the model from the previous step. Drawing inspiration from the exploration-exploitation tradeoff [49, 25] encountered in reinforcement learning [54], we introduce a mechanism for adjusting λ and α on the fly, balancing between the two primary objectives: adaptation and preventing model collapse. Our strategy is prioritizing collapse prevention (increasing λ) and preserving the model from previous steps (decreasing α) when there is a significant deviation from θ0. In [40, 61, 59], λ and α were fixed through hyper-parameter tuning. This is suboptimal due to varying TTA environments and the lack of validation set [62]. Furthermore, Thm. 1 suggests the convergence rate quickly escalates when ϵt increases, making constant λ, αinsufficient to prevent collapse. Sensing the Divergence of θt. We first equip PeTTA with a mechanism for measuring its divergence from θ0. Since ft(x) = argmax y∈Y Pr(y|x; θt), we can decompose Pr(y|x; θt) = [h (ϕθt(x))]y, with ϕθt(·) is a θt-parameterized deep feature extractor followed by a fixed classification head (a linear and softmax layer) h(·). The operator [·]y extracts the yth component of a vector. 5Since h(·) remains unchanged, instead of comparing the divergence in the parameter space (Θ) or between the output probability Pr(y|x; θt) and Pr(y|x; θ0), we suggest an inspection over the feature embedding space that preserves a maximum amount of information in our case (data processing inequality [9]). Inspired by [31] and under Gaussian assumption, the Mahalanobis distance of the first moment of the feature embedding vectors is compared. Let z = ϕθt(x), we keep track of a collection of the running mean of feature vector z: {ˆµy t }y∈Y in which ˆµy t is EMA updated with vector z if ft(x) = y. The divergence of θt at step t, evaluated on class y is defined as: γy t = 1 − exp \u0010 −(ˆµy t − µy 0)T (Σy 0)−1 (ˆµy t − µy 0) \u0011 , (6) where µy 0 and Σy 0 are the pre-computed empirical mean and covariant matrix of feature vectors in the source dataset (P0). The covariant matrix here is diagonal for simplicity. In practice, without directly accessing the training set, we assume a small set of unlabeled samples can be drawn from the source distribution for empirically computing these values (visit Appdx. E.4 for further details). Here, we implicitly expect the independence of each entry in z and TTA approaches learn to align feature vectors of new domains back to the source domain (P0). Therefore, the accumulated statistics of these feature vectors at each step should be concentrated near the vectors of the initial model. The value of γy t ∈ [0, 1] is close to 0 when θt = θ0 and increases exponentially as ˆµy t diverging from µy 0. Adaptive Regularization and Model Update. With α0, λ0 are initial values, utilizing γy t derived in Eq. 6, a pair of (λt, αt) is adaptively chosen at each step: ¯γt = 1 | ˆYt| X y∈ ˆYt γy t , ˆYt = n ˆY (i) t |i = 1, ··· , Nt o ; λt = ¯γt · λ0, α t = (1 − ¯γt) · α0, (7) ˆYt is a set of unique pseudo labels in a testing batch ( ˆY (i) t is the ith realization of ˆYt). Anchor Loss. Penalizing the divergence with regular vector norms in high-dimensional space (Θ) is insufficient (curse of dimensionality [5, 51]), especially with a large model and limited samples. Anchor loss LAL can nail down the similarity between ft and f0 in the probability space [32, 12]: LAL(Xt; θ) = − X y∈Y Pr(y|Xt; θ0) log Pr(y|Xt; θ), (8) which is equivalent to minimizing the KL divergence DKL (Pr(y|Xt; θ0)∥Pr(y|Xt; θ)). Persistent TTA.Having all the ingredients, we design our approach, PeTTA, following the convention setup of the mean teacher update, with the category-balanced memory bank and the robust batch normalization layer from [61]. Appdx. E.1 introduces the pseudo code of PeTTA. ForLCLS, either the self-training scheme [12] or the regular cross-entropy [16] is adopted. With R(θ), cosine similarity or L2 distance are both valid metrics for measuring the distance between θ and θ0 in the parameter space. Fisher regularizer coefficient [ 40, 27] can also be used, optionally. To sum up, the teacher model update of PeTTA is an elaborated version of EMA with λt, αt (Eq. 7) and LAL (Eq. 8): θ′ t = Optim θ′∈Θ EPt h LCLS \u0010 ˆYt, Xt; θ′ \u0011 + LAL (Xt; θ′) i + λtR(θ′), θt = (1 − αt)θt−1 + αtθ′ t. 5 Experimental Results 5.1 ϵ−MMC Simulation Result Simulation Setup. A total of 6000 samples from two Gaussian distributions: N(µ0 = 0, σ2 0 = 1) and N(µ1 = 2, σ2 1 = 1) with p0 = p1 = 1 2 are synthesized and gradually released in a batch of B = 10 samples. For evaluation, an independent set of 2000 samples following the same distribution is used for computing the prediction frequency, and the false negative rate (FNR). ϵ−GMMC update follows Eq. 4 with α = 5e−2. To simulate model collapse, the predictor is intercepted and 10% of the true-postive pseudo labels at each testing step are randomly flipped (Corollary 1). Simulation Result. In action, both the likelihood of predicting class 0 (Fig. 3a-left) and theϵt (Eq. 5) (Fig. 3c-right, solid line) gradually increases over time as expected (Lemma 1). After collapsing, 60 120 240 360 480 6000 0.2 0.4 0.6 0.8 1 Testing Step (t) 0 120 240 360 480 6000 0.2 0.4 0.6 0.8 1 Testing Step (t) −4 −2 0 2 4x−4 −2 0 2 40 0.2 0.4 0.6 0.8 1 x Probability density N(µ0, σ0) N(µ1, σ1) N(ˆµ0, ˆσ0) N(ˆµ1, ˆσ1) 0 100 200 300 400 500 600 0.8 1.2 1.6 2.0 Testing step (t) |ˆµ0,t −µ1| Numerical Simulation Theoretical Result 0 100 200 300 400 500 600 0.1 0.2 0.3 0.4 0.5 Testing step (t) ϵt Prediction Frequency GMMCϵ-GMMC ϵ-GMMC GMMC (a) (b) (c) Figure 3: Simulation result on ϵ-perturbed Gaussian Mixture Model Classifier ( ϵ-GMMC) and GMMC (perturbed-free). (a) Histogram of model predictions through time. A similar prediction frequency pattern is observed on CIFAR-10-C (Fig. 5a-left). (b) The probability density function of the two clusters after convergence versus the true data distribution. The initial two clusters of ϵ-GMMC collapsed into a single cluster with parameters stated in Lemma 2. In the perturbed-free, GMMC converges to the true data distribution. (c) Distance toward µ1 (|EPt [ˆµ0,t] − µ1|) and false- negative rate (ϵt) in simulation coincides with the result in Thm. 1 (with ϵt following Corollary 1). ϵ-GMMC merges the two initial clusters, resulting in a single one (Fig. 3b-left) with parameters that match Lemma 2. The distance from ˆµ0,t (initialized at µ0) towards µ1 converges (Fig. 3c-left, solid line), coincided with the analysis in Thm. 1 when ϵt is chosen following Corollary 1 (Fig. 3c, dashed line). GMMC (perturbed-free) stably produces accurate predictions (Fig. 3a-right) and approximates the true data distribution (Fig. 3b-right). The simulation empirically validates our analysis (Sec. 3.2), confirming the vulnerability of TTA models when the pseudo labels are inaccurately estimated. 5.2 Setup - Benchmark Datasets Datasets. We benchmark the performance on four TTA classification tasks. Specifically, CIFAR10 → CIFAR10-C, CIFAR100→ CIFAR100-C, and ImageNet → ImageNet-C [19] are three corrupted images classification tasks (corruption level 5, the most severe). Additionally, we incorporate DomainNet [44] with 126 categories from four domains for the task real → clipart, painting, sketch. Compared Methods. Besides PeTTA, the following algorithms are investigated: CoTTA [ 59], EATA [40], RMT [12], MECTA [22], RoTTA [61], ROID [37] and TRIBE [52]. Noteworthy, only RoTTA is specifically designed for the practical TTA setting while others fit the continual TTA setting in general. A parameter-free approach: LAME [ 7] and a reset-based approach (i.e., reverting the model to the source model after adapting to every 1, 000 images): RDumb [45] are also included. Recurring TTA. Following the practical TTA setup, multiple testing scenarios from each testing set will gradually change from one to another while the Dirichlet distribution (Dir(0.1) for CIFAR10- C, DomainNet, and ImageNet-C, and Dir(0.01) for CIFAR100-C) generates category temporally correlated batches of data. For all experiments, we set the number of revisits K = 20 (times) as this number is sufficient to fully observe the gradual degradation on existing TTA baselines. Implementation Details. We use PyTorch [43] for implementation. RobustBench [10] and torchvision [35] provide pre-trained source models. Hyper-parameter choices are kept as close as possible to the original selections of authors. Visit Sec. G for more implementation details. Unless otherwise noted, for all PeTTA experiments, the EMA update rate for robust batch normalization [61] and feature embedding statistics is set to 5e−2; α0 = 1e−3 and cosine similarity regularizer is used. On CIFAR10/100-C and ImageNet-C we use the self-training loss in [ 12] for LCLS and λ0 = 10 while the regular cross-entropy loss [ 13] and λ0 = 1 (severe domain shift requires prioritizing 7Table 1: Average classification error of the task CIFAR-10→ CIFAR-10-C in recurring TTA. The lowest error is in bold,(∗)average value across 5 runs (different random seeds) is reported for PeTTA. Recurring TTA visit− − − − − − − − − − − − − − − − − − − − − − − − − →Method1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 Avg Source 43.5 43.5 LAME [7] 31.1 31.1 CoTTA [59]82.2 85.6 87.2 87.8 88.2 88.5 88.7 88.7 88.9 88.9 88.9 89.2 89.2 89.2 89.1 89.2 89.2 89.1 89.3 89.388.3EATA [40]81.6 87.0 88.7 88.7 88.9 88.7 88.6 89.0 89.3 89.6 89.5 89.6 89.7 89.7 89.3 89.6 89.6 89.8 89.9 89.488.8RMT [12]77.5 76.9 76.5 75.8 75.5 75.5 75.4 75.4 75.5 75.3 75.5 75.6 75.5 75.5 75.7 75.6 75.7 75.6 75.7 75.875.8MECTA [22]72.2 82.0 85.2 86.3 87.0 87.3 87.3 87.5 88.1 88.8 88.9 88.9 88.6 89.1 88.7 88.8 88.5 88.6 88.3 88.886.9RoTTA [61]24.6 25.5 29.6 33.6 38.2 42.8 46.2 50.6 52.2 54.1 56.5 57.5 59.4 60.2 61.7 63.0 64.8 66.1 68.2 70.351.3RDumb [45]31.1 32.1 32.3 31.6 31.9 31.8 31.8 31.9 31.9 32.1 31.7 32.0 32.5 32.0 31.9 31.6 31.9 31.4 32.3 32.431.9ROID [37]72.7 72.6 73.1 72.4 72.7 72.8 72.7 72.7 72.9 72.8 72.9 72.9 72.8 72.5 73.0 72.8 72.5 72.5 72.7 72.772.7TRIBE [52]15.3 16.6 16.6 16.3 16.7 17.0 17.3 17.4 17.4 18.0 17.9 18.0 17.9 18.6 18.2 18.8 18.0 18.2 18.4 18.017.5PeTTA(ours)(∗) 24.323.022.622.422.422.522.322.522.822.822.622.722.722.922.622.722.622.822.923.022.8 adaptability) are applied in DomainNet experiments. In Appdx. F.5, we provide a sensitivity analysis on the choice of hyper-parameter λ0 in PeTTA. 5.3 Result - Benchmark Datasets Recurring TTA Performance. Fig. 1-right presents the testing error on CIFAR-10-C in recurring TTA setting. RoTTA [61] exhibits promising performance in the first several visits but soon raises and eventually exceeds the source model (no TTA). The classification error of compared methods on CIFAR-10→CIFAR-10-C, and ImageNet → ImageNet-C [19] tasks are shown in Tab. 1, and Tab. 2. Appdx. F.1 provides the results on the other two datasets. The observed performance degradation of CoTTA [59], EATA [40], RoTTA [61], and TRIBE [52] confirms the risk of error accumulation for an extensive period. While RMT [12], MECTA [22], and ROID [37] remain stable, they failed to adapt to the temporally correlated test stream at the beginning, with a higher error rate than the source model. LAME [7] (parameter-free TTA) and RDumb [45] (reset-based TTA) do not suffer from collapsing. However, their performance is lagging behind, and knowledge accumulation is limited in these approaches that could potentially favor a higher performance as achieved by PeTTA. Furthermore, LAME [7] is highly constrained by the source model, and selecting a precise reset frequency in RDumb [45] is challenging in practice (see Appdx. F.3 for a further discussion). 0 10 20 30 40 16 18 20 22 24 Recurring TTA Visit Classification Error PeTTA (ours) TRIBE [52] Figure 4: Classification error of TRIBE [ 52] and PeTTA (ours) of the task CIFAR-10→CIFAR10-C task in recurring TTA with 40 visits. In average, PeTTA outperforms almost every baseline approaches and persists across 20 vis- its over the three datasets. The only exception is at the case of TRIBE [ 52] on CIFAR-10- C. While this state-of-the-art model provides stronger adaptability, outweighing the PeTTA, and baseline RoTTA [61] in several recurrences, the risk of the model collapsing still presents in TRIBE [52]. This can be clearly observed when we increase the observation period to 40 recur- ring visits in Fig. 4. As the degree of freedom for adaptation in PeTTA is more constrained, it takes a bit longer for adaptation but remains sta- ble afterward. Fig. 5b-bottom exhibits the con- fusion matrix at the last visit with satisfactory accuracy. The same results are also observed when shuffling the order of domain shifts within each recurrence (Appdx. D.3), or extending the number of recurrences to 40 visits (Appdx. F.4). Continuously Changing Corruption (CCC) [45] Performance. Under CCC [45], Tab. 3 reveals the supreme performance of PeTTA over RoTTA [61] and RDumb [45]. Here, we report the average classification error between two consecutive adaptation step intervals. An adaptation step in this table corresponds to a mini-batch of data with 64 images. The model is adapted to 80, 000 steps in total with more than 5.1M images, significantly longer than 20 recurring TTA visits. Undoubtedly, PeTTA still achieves good performance where the corruptions are algorithmically generated, non-cyclic with two or more corruption types can happen simultaneously. This experiment also empirically justifies the construction of our recurring TTA as a diagnostic tool (Appdx. D.2) where similar observations are concluded on the two settings. Obviously, our recurring TTA is notably simpler than CCC [45]. 8Table 2: Average classification error of the task ImageNet → ImageNet-C in recurring TTA scenario. Recurring TTA visit− − − − − − − − − − − − − − − − − − − − − − − − − →Method1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 Avg Source 82.0 82.0 LAME [7] 80.9 80.9 CoTTA [59]98.6 99.1 99.4 99.4 99.5 99.5 99.5 99.5 99.6 99.7 99.6 99.6 99.6 99.6 99.6 99.6 99.6 99.6 99.7 99.799.5EATA [40]60.4 59.3 65.4 72.6 79.1 84.2 88.7 92.7 95.2 96.9 97.7 98.1 98.4 98.6 98.7 98.8 98.8 98.9 98.9 99.089.0RMT [12]72.3 71.0 69.9 69.1 68.8 68.5 68.4 68.3 70.0 70.2 70.1 70.2 72.8 76.8 75.6 75.1 75.1 75.2 74.8 74.771.8MECTA [22]77.2 82.8 86.1 87.9 88.9 89.4 89.8 89.9 90.0 90.4 90.6 90.7 90.7 90.8 90.8 90.9 90.8 90.8 90.7 90.889.0RoTTA [61]68.3 62.1 61.8 64.5 68.4 75.4 82.7 95.1 95.8 96.6 97.1 97.9 98.3 98.7 99.0 99.1 99.3 99.4 99.5 99.687.9RDumb [45]72.2 73.0 73.2 72.8 72.2 72.8 73.3 72.7 71.9 73.0 73.2 73.1 72.0 72.7 73.3 73.1 72.1 72.6 73.3 73.172.8ROID [37]62.7 62.3 62.3 62.3 62.5 62.3 62.4 62.4 62.3 62.6 62.5 62.3 62.5 62.4 62.5 62.4 62.4 62.5 62.4 62.562.4TRIBE [52]63.664.0 64.9 67.8 69.6 71.7 73.5 75.5 77.4 79.8 85.0 96.5 99.4 99.8 99.9 99.8 99.8 99.9 99.9 99.984.4PeTTA(ours)(∗) 65.361.759.859.159.459.659.859.359.460.060.361.060.760.460.660.760.860.760.460.260.5 Table 3: Average classification error on CCC [45] setting. Each column presents the average error within an adaptation interval (e.g., the second column provides the average error between the 6701 and 13400 adaptation steps). Each adaptation step here is performed on a mini-batch of 64 images. CCC [45] Adaptation Step− − − − − − − − − − − − − − − − − − − − − − − − − → Method6700 13400 20100 26800 33500 40200 46900 53600 60200 66800 73400 80000Avg Source 0.83 0.83 0.83 0.83 0.83 0.84 0.84 0.83 0.84 0.83 0.83 0.83 0.83 RoTTA [61]0.70 0.85 0.92 0.96 0.98 1.00 1.00 1.00 1.00 1.00 1.00 1.00 0.95 RDumb [45]0.78 0.74 0.75 0.77 0.75 0.72 0.75 0.77 0.75 0.74 0.75 0.75 0.75 PeTTA(ours) 0.67 0.63 0.62 0.65 0.65 0.64 0.64 0.68 0.63 0.63 0.65 0.65 0.64 0.46 0.44 0.4 0.43 0.46 0.47 0.44 0.43 0.48 0.4 0.43 0.43 0.41 airplane bird cat dog frog ship auto deer horse truck 0.13 0.34 0.44 0.32 0.14 0.44 0.51 0.46 0.46 0.34 airplane bird catdog frog ship auto deer horse truck Inter-category cosine similarity (source model)Misclassification rate of collapsed RoTTA 0: airplane 1: auto2: bird3: cat4: deer5: dog6: frog7: horse8: ship9: truck 0 1 2 3 4 5 6 7 8 9 0.69 0 0 0 0.08 0 0.04 0 0.13 0.05 0.34 0.17 0 0 0.1 0 0.03 0 0.23 0.13 0.24 0 0.1 0.03 0.44 0 0.11 0 0.07 0.01 0.21 0 0 0.11 0.32 0 0.21 0 0.14 0.01 0.14 0 0 0.01 0.71 0 0.06 0.01 0.06 0.01 0.21 0 0 0.07 0.44 0 0.16 0 0.1 0.01 0.05 0 0 0.08 0.51 0 0.25 0 0.1 0.01 0.17 0 0 0.03 0.46 0 0.04 0.21 0.06 0.04 0.46 0 0 0.01 0.06 0 0.03 0 0.41 0.03 0.34 0 0 0 0.12 0 0.03 0 0.18 0.32 0: airplane 1: auto2: bird3: cat4: deer5: dog6: frog7: horse8: ship9: truck 0 1 2 3 4 5 6 7 8 9 0.73 0.01 0.06 0.04 0.02 0 0.03 0.01 0.08 0.02 0.01 0.88 0.01 0.01 0 0 0.02 0 0.02 0.05 0.04 0 0.75 0.07 0.05 0.02 0.05 0.01 0.01 0 0.01 0 0.06 0.72 0.05 0.04 0.06 0.02 0.01 0.01 0.02 0 0.06 0.07 0.76 0.01 0.05 0.02 0.01 0 0 0 0.07 0.19 0.05 0.59 0.05 0.02 0.01 0.01 0 0 0.03 0.07 0.02 0.01 0.84 0 0.01 0.01 0.01 0 0.06 0.06 0.08 0.02 0.02 0.74 0 0.01 0.04 0.02 0.02 0.02 0.01 0 0.03 0 0.84 0.02 0.01 0.05 0.02 0.03 0.01 0 0.02 0.01 0.04 0.82 1 5 10 15 200 0.2 0.4 0.6 0.8 1 Visits 1 5 10 15 200 0.2 0.4 0.6 0.8 1 Visits RoTTA [61] PeTTA (ours) PeTTA (ours) - 20th visit RoTTA [61] - 20th visit Predicted label (a) (b)(c) True labelTrue label Prediction Frequency Figure 5: Recurring TTA (20 visits) on CIFAR-10 →CIFAR10-C task. (a) Histogram of model predictions (10 labels are color-coded). PeTTA achieves a persisting performance while RoTTA [61] degrades. (b) Confusion matrix at the last visit, RoTTA classifies all samples into a few categories (e.g., 0: airplane, 4: deer). (c) Force-directed graphs showing (left) the most prone to misclassification pairs (arrows indicating the portion and pointing from the true to the misclassified category); (right) similar categories tend to be easily collapsed. Edges denote the average cosine similarity of feature vectors (source model), only the highest similar pairs are shown. Best viewed in color. Collapsing Pattern. The rise in classification error (Fig. 1-right) can be reasoned by the prediction frequency of RoTTA [ 61] in an recurring TTA setting (Fig. 5a-left). Similar to ϵ-GMMC, the likelihood of receiving predictions on certain categories gradually increases and dominates the others. Further inspecting the confusion matrix of a collapsed model (Fig. 5b-top) reveals two major groups of categories are formed and a single category within each group represents all members, thereby becoming dominant. To see this, Fig. 5c-left simplifies the confusion matrix by only visualizing the 9Table 4: Average (across 20 visits) error of multiple variations of PeTTA: without (w/o) R(θ), LAL; LAL only; fixed regularization coefficient λ; adaptive coef- ficient λt, update rate αt; using anchor loss LAL. Method CF-10-CCF-100-CDN IN-C Baseline w/oR(θ),LAL 42.6 63.0 77.9 93.4 R(θ)fixedλ= 0.1λ0 43.3 65.0 80.0 92.5R(θ)fixedλ=λ0 42.0 64.6 66.6 92.9 LALonly 25.4 56.5 47.5 68.1 PeTTA -λt 27.1 55.0 59.7 92.7PeTTA -λt +αt 23.9 41.4 44.5 75.7PeTTA -λt +LAL 26.2 36.3 43.2 62.0 PeTTA -λt +αt +LAL 22.8 35.1 42.9 60.5 Table 5: Average (across 20 visits) error of PeTTA. PeTTA favors various choices of reg- ularizers R(θ): L2 and cosine similarity in conjunction with Fisher [27, 40] coefficient. Method CF-10-CCF-100-CDN IN-CR(θ) Fisher L2 ✗ 23.0 35.6 43.1 70.8✓ 22.7 36.0 43.9 70.0 Cosine ✗ 22.8 35.1 42.9 60.5✓ 22.6 35.9 43.3 63.8 CF: CIFAR, DN: DomainNet, IN: ImageNet top prone-to-misclassified pair of categories. Here, label deer is used for almost every living animal while airplane represents transport vehicles. The similarity between categories in the feature space of the source model (Fig. 5c-right) is correlated with the likelihood of being merged upon collapsing. As distance in feature space is analogous to |µ0 − µ1| (Thm. 1), closer clusters are at a higher risk of collapsing. This explains and showcases that the collapsing behavior is predictable up to some extent. 5.4 Ablation Study Effect of Each Component. Tab. 4 gives an ablation study on PeTTA, highlighting the use of a regularization term (R(θ)) with a fixed choice of λ, αnot only fails to mitigate model collapse but may also introduce a negative effect (rows 2-3). Trivially applying the anchor loss (LAL) alone is also incapable of eliminating the lifelong performance degradation in continual TTA (row 4). Within PeTTA, adopting the adaptiveλt scheme alone (row 5) or in conjunction with either αt or anchor loss LAL (rows 6-7) partially stabilizes the performance. Under the drastic domain shifts with a larger size of categories or model parameters (e.g., on CIFAR-100-C, DomainNet, ImageNet-C), restricting αt adjustment limits the ability of PeTTA to stop undesirable updates while a common regularization term without LAL is insufficient to guide the adaptation. Thus, leveraging all elements secures the persistence of PeTTA (row 8). Various Choices of Regularizers. The design of PeTTA is not coupled with any specific regu- larization term. Demonstrated in Tab. 5, PeTTA works well for the two common choices: L2 and cosine similarity. The conjunction use of Fisher coefficent [27, 40] for weighting the model parameter importance is also studied. While the benefit (in terms of improving accuracy) varies across datasets, PeTTA accommodates all choices, as the model collapse is not observed in any of the options. 6 Discussions and Conclusion On a Potential Risk of TTA in Practice. We provide empirical and theoretical evidence on the risk of deploying continual TTA algorithms. Existing studies fail to detect this issue with a single pass per test set. The recurring TTA could be conveniently adopted as astraightforward evaluation, where its challenging test stream magnifies the error accumulation that a model might encounter in practice. Limitations. PeTTA takes one step toward mitigating the gradual performance degradation of TTA. Nevertheless, a complete elimination of error accumulation cannot be guaranteed rigorously through regularization. Future research could delve deeper into expanding our efforts to develop an algorithm that achieves error accumulation-free by construction. Furthermore, as tackling the challenge of the temporally correlated testing stream is not the focus of PeTTA, using a small memory bank as in [61, 15] is necessary. It also assumes the features statistics from the source distribution are available (Appdx. E.3, E.4). These constraints potentially limit its scalability in real-world scenarios. Conclusion. Towards trustworthy and reliable TTA applications, we rigorously study theperformance degradation problem of TTA. The proposed recurring TTAsetting highlights the limitations of modern TTA methods, which struggle to prevent the error accumulation when continuously adapting to demanding test streams. Theoretically inspecting a failure case of ϵ−GMMC paves the road for designing PeTTA- a simple yet efficient solution that continuously assesses the model divergence for harmonizing the TTA process, balancing adaptation, and collapse prevention. 10Acknowledgements This work was supported by the Jump ARCHES Endowment through the Health Care Engineering Systems Center, JSPS/MEXT KAKENHI JP24K20830, ROIS NII Open Collaborative Research 2024-24S1201, in part by the National Institute of Health (NIH) under Grant R01 AI139401, and in part by the Vingroup Innovation Foundation under Grant VINIF.2021.DA00128. References [1] Kartik Ahuja, Ethan Caballero, Dinghuai Zhang, Jean-Christophe Gagnon-Audet, Yoshua Bengio, Ioannis Mitliagkas, and Irina Rish. Invariance principle meets information bottleneck for out-of-distribution gener- alization. In A. Beygelzimer, Y . Dauphin, P. Liang, and J. Wortman Vaughan, editors,Advances in Neural Information Processing Systems, 2021. URL https://openreview.net/forum?id=jlchsFOLfeF. [2] Rahaf Aljundi, Eugene Belilovsky, Tinne Tuytelaars, Laurent Charlin, Massimo Caccia, Min Lin, and Lucas Page-Caccia. Online continual learning with maximal interfered retrieval. In H. Wallach, H. Larochelle, A. Beygelzimer, F. d'Alché-Buc, E. Fox, and R. Garnett, editors,Advances in Neural Information Processing Systems, volume 32, 2019. URL https://proceedings.neurips.cc/paper_files/paper/2019/ file/15825aee15eb335cc13f9b559f166ee8-Paper.pdf. [3] Rahaf Aljundi, Min Lin, Baptiste Goujaud, and Yoshua Bengio. Gradient based sample se- lection for online continual learning. In Advances in Neural Information Processing Systems , volume 32, 2019. URL https://proceedings.neurips.cc/paper_files/paper/2019/file/ e562cd9c0768d5464b64cf61da7fc6bb-Paper.pdf. [4] Amitav Banerjee, U. B. Chitnis, S. L. Jadhav, J. S. Bhawalkar, and S. Chaudhury. Hypothesis testing, type I and type II errors. Industrial Psychiatry Journal, 18(2):127–131, 2009. ISSN 0972-6748. doi: 10.4103/0972-6748.62274. URL https://www.ncbi.nlm.nih.gov/pmc/articles/PMC2996198/. [5] Richard Bellman. Dynamic Programming. Princeton University Press, Princeton, NJ, USA, 1957. [6] Arno Blaas, Andrew Miller, Luca Zappella, Joern-Henrik Jacobsen, and Christina Heinze-Deml. Con- siderations for distribution shift robustness in health. In ICLR 2023 Workshop on Trustworthy Machine Learning for Healthcare, 2023. URL https://openreview.net/forum?id=y7XveyWYzIB. [7] Malik Boudiaf, Romain Mueller, Ismail Ben Ayed, and Luca Bertinetto. Parameter-free online test-time adaptation. In 2022 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pages 8334–8343, 2022. doi: 10.1109/CVPR52688.2022.00816. [8] Dian Chen, Dequan Wang, Trevor Darrell, and Sayna Ebrahimi. Contrastive test-time adaptation. In Proceedings of the IEEE International Conference on Computer Vision, 2022. [9] Thomas M. Cover and Joy A. Thomas.Elements of Information Theory (Wiley Series in Telecommunications and Signal Processing). Wiley-Interscience, USA, 2006. ISBN 0471241954. [10] Francesco Croce, Maksym Andriushchenko, Vikash Sehwag, Edoardo Debenedetti, Nicolas Flammarion, Mung Chiang, Prateek Mittal, and Matthias Hein. Robustbench: a standardized adversarial robustness benchmark. In Thirty-fifth Conference on Neural Information Processing Systems Datasets and Benchmarks Track, 2021. URL https://openreview.net/forum?id=SSKZPJCt7B. [11] Matthias De Lange, Rahaf Aljundi, Marc Masana, Sarah Parisot, Xu Jia, Ales Leonardis, Gregory Slabaugh, and Tinne Tuytelaars. A continual learning survey: Defying forgetting in classification tasks. IEEE Transactions on Pattern Analysis and Machine Intelligence, 44(7):3366–3385, 2022. doi: 10.1109/ TPAMI.2021.3057446. [12] Mario Döbler, Robert A. Marsden, and Bin Yang. Robust mean teacher for continual and gradual test-time adaptation. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pages 7704–7714, June 2022. [13] Yaroslav Ganin and Victor Lempitsky. Unsupervised domain adaptation by backpropagation. In Proceedings of the 32nd International Conference on Machine Learning , volume 37 of Proceedings of Machine Learning Research , pages 1180–1189, Lille, France, 07–09 Jul 2015. PMLR. URL https://proceedings.mlr.press/v37/ganin15.html. [14] Yaroslav Ganin, Evgeniya Ustinova, Hana Ajakan, Pascal Germain, Hugo Larochelle, François Laviolette, Mario Marchand, and Victor Lempitsky. Domain-Adversarial Training of Neural Networks, pages 189– 209. Springer International Publishing, 2017. doi: 10.1007/978-3-319-58347-1_10. URL https: //doi.org/10.1007/978-3-319-58347-1_10 . [15] Taesik Gong, Jongheon Jeong, Taewon Kim, Yewon Kim, Jinwoo Shin, and Sung-Ju Lee. NOTE: Robust continual test-time adaptation against temporal correlation. In Advances in Neural Information Processing Systems, 2022. 11[16] Yves Grandvalet and Yoshua Bengio. Semi-supervised learning by entropy minimization. In L. Saul, Y . Weiss, and L. Bottou, editors, Advances in Neural Information Processing Systems , volume 17, 2004. URL https://proceedings.neurips.cc/paper_files/paper/2004/file/ 96f2b50b5d3613adf9c27049b2a888c7-Paper.pdf. [17] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image recognition. arXiv preprint arXiv:1512.03385, 2015. [18] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Delving deep into rectifiers: Surpassing human-level performance on ImageNet classification. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR), pages 1026–1034, 2015. [19] Dan Hendrycks and Thomas Dietterich. Benchmarking neural network robustness to common corruptions and perturbations. Proceedings of the International Conference on Learning Representations, 2019. [20] Dan Hendrycks, Norman Mu, Ekin D. Cubuk, Barret Zoph, Justin Gilmer, and Balaji Lakshminarayanan. AugMix: A simple data processing method to improve robustness and uncertainty. Proceedings of the International Conference on Learning Representations (ICLR), 2020. [21] Dan Hendrycks, Steven Basart, Norman Mu, Saurav Kadavath, Frank Wang, Evan Dorundo, Rahul Desai, Tyler Zhu, Samyak Parajuli, Mike Guo, Dawn Song, Jacob Steinhardt, and Justin Gilmer. The many faces of robustness: A critical analysis of out-of-distribution generalization. In 2021 IEEE/CVF International Conference on Computer Vision (ICCV), pages 8320–8329, 2021. doi: 10.1109/ICCV48922.2021.00823. [22] Junyuan Hong, Lingjuan Lyu, Jiayu Zhou, and Michael Spranger. MECTA: Memory-economic continual test-time model adaptation. In The Eleventh International Conference on Learning Representations, 2023. URL https://openreview.net/forum?id=N92hjSf5NNh. [23] Fabian Isensee, Paul F. Jaeger, Simon A. A. Kohl, Jens Petersen, and Klaus H. Maier-Hein. nnU-Net: a self-configuring method for deep learning-based biomedical image segmentation. Nature Methods, 18(2):203–211, February 2021. ISSN 1548-7105. doi: 10.1038/s41592-020-01008-z. URL https: //www.nature.com/articles/s41592-020-01008-z . [24] Yusuke Iwasawa and Yutaka Matsuo. Test-time classifier adjustment module for model-agnostic domain generalization. In M. Ranzato, A. Beygelzimer, Y . Dauphin, P.S. Liang, and J. Wort- man Vaughan, editors, Advances in Neural Information Processing Systems , volume 34, pages 2427–2440, 2021. URL https://proceedings.neurips.cc/paper_files/paper/2021/file/ 1415fe9fea0fa1e45dddcff5682239a0-Paper.pdf. [25] Michael N. Katehakis and Arthur F. Veinott. The multi-armed bandit problem: Decomposition and compu- tation. Mathematics Operations Research, 12:262–268, 1987. URL https://api.semanticscholar. org/CorpusID:656323. [26] Diederik P. Kingma and Jimmy Ba. Adam: A method for stochastic optimization. In Yoshua Bengio and Yann LeCun, editors, 3rd International Conference on Learning Representations, ICLR 2015, San Diego, CA, USA, May 7-9, 2015, Conference Track Proceedings, 2015. URL http://arxiv.org/abs/1412. 6980. [27] James Kirkpatrick, Razvan Pascanu, Neil Rabinowitz, Joel Veness, Guillaume Desjardins, Andrei A. Rusu, Kieran Milan, John Quan, Tiago Ramalho, Agnieszka Grabska-Barwinska, Demis Hassabis, Claudia Clopath, Dharshan Kumaran, and Raia Hadsell. Overcoming catastrophic forgetting in neural networks.Pro- ceedings of the National Academy of Sciences, 114(13):3521–3526, 2017. doi: 10.1073/pnas.1611835114. URL https://www.pnas.org/doi/abs/10.1073/pnas.1611835114. [28] Dong-Hyun Lee. Pseudo-label : The simple and efficient semi-supervised learning method for deep neural networks. ICML 2013 Workshop : Challenges in Representation Learning (WREPL), 07 2013. [29] T. Lee, S. Chottananurak, T. Gong, and S. Lee. Aetta: Label-free accuracy estimation for test-time adaptation. In 2024 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pages 28643–28652, Los Alamitos, CA, USA, jun 2024. IEEE Computer Society. doi: 10.1109/CVPR52733. 2024.02706. URL https://doi.ieeecomputersociety.org/10.1109/CVPR52733.2024.02706. [30] Haoliang Li, Sinno Jialin Pan, Shiqi Wang, and Alex C. Kot. Domain generalization with adversarial feature learning. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR), June 2018. [31] Yanghao Li, Naiyan Wang, Jianping Shi, Jiaying Liu, and Xiaodi Hou. Revisiting batch normalization for practical domain adaptation. In International Conference on Learning Representations Workshop, 2017. URL https://openreview.net/forum?id=BJuysoFeg. [32] Zhizhong Li and Derek Hoiem. Learning without forgetting. IEEE Transactions on Pattern Analysis and Machine Intelligence, 40(12):2935–2947, 2018. doi: 10.1109/TPAMI.2017.2773081. [33] Jian Liang, Dapeng Hu, and Jiashi Feng. Do we really need to access the source data? Source hypothesis transfer for unsupervised domain adaptation. In International Conference on Machine Learning (ICML), pages 6028–6039, 2020. 12[34] Sen Lin, Peizhong Ju, Yingbin Liang, and Ness Shroff. Theory on forgetting and generalization of continual learning. In Proceedings of the 40th International Conference on Machine Learning, ICML’23, 2023. [35] TorchVision maintainers and contributors. Torchvision: Pytorch’s computer vision library. https: //github.com/pytorch/vision, 2016. [36] Robert A Marsden, Mario Döbler, and Bin Yang. Gradual test-time adaptation by self-training and style transfer. arXiv preprint arXiv:2208.07736, 2022. [37] Robert A Marsden, Mario Döbler, and Bin Yang. Universal test-time adaptation through weight ensem- bling, diversity weighting, and prior correction. In Proceedings of the IEEE/CVF Winter Conference on Applications of Computer Vision, pages 2555–2565, 2024. [38] Ben Mildenhall, Pratul P. Srinivasan, Matthew Tancik, Jonathan T. Barron, Ravi Ramamoorthi, and Ren Ng. NeRF: Representing scenes as neural radiance fields for view synthesis. In Proceedings of the European Conference on Computer Vision (ECCV), 2020. [39] A. Tuan Nguyen, Thanh Nguyen-Tang, Ser-Nam Lim, and Philip Torr. TIPI: Test time adaptation with transformation invariance. In Conference on Computer Vision and Pattern Recognition 2023, 2023. URL https://openreview.net/forum?id=NVh1cy37Ge. [40] Shuaicheng Niu, Jiaxiang Wu, Yifan Zhang, Yaofo Chen, Shijian Zheng, Peilin Zhao, and Mingkui Tan. Efficient test-time model adaptation without forgetting. In The Internetional Conference on Machine Learning, 2022. [41] Shuaicheng Niu, Jiaxiang Wu, Yifan Zhang, Zhiquan Wen, Yaofo Chen, Peilin Zhao, and Mingkui Tan. Towards stable test-time adaptation in dynamic wild world. In The Eleventh International Conference on Learning Representations, 2023. URL https://openreview.net/forum?id=g2YraF75Tj. [42] K. R. Parthasarathy. Introduction to Probability and Measure , volume 33 of Texts and Readings in Mathematics. Hindustan Book Agency, Gurgaon, 2005. ISBN 978-81-85931-55-5 978-93-86279-27-9. doi: 10.1007/978-93-86279-27-9. URL http://link.springer.com/10.1007/978-93-86279-27-9 . [43] Adam Paszke, Sam Gross, Francisco Massa, Adam Lerer, James Bradbury, Gregory Chanan, Trevor Killeen, Zeming Lin, Natalia Gimelshein, Luca Antiga, Alban Desmaison, Andreas Köpf, Edward Yang, Zach DeVito, Martin Raison, Alykhan Tejani, Sasank Chilamkurthy, Benoit Steiner, Lu Fang, Junjie Bai, and Soumith Chintala. Pytorch: An imperative style, high-performance deep learning library, 2019. [44] Xingchao Peng, Qinxun Bai, Xide Xia, Zijun Huang, Kate Saenko, and Bo Wang. Moment matching for multi-source domain adaptation. In Proceedings of the IEEE International Conference on Computer Vision, pages 1406–1415, 2019. [45] Ori Press, Steffen Schneider, Matthias Kuemmerer, and Matthias Bethge. RDumb: A simple approach that questions our progress in continual test-time adaptation. In Thirty-seventh Conference on Neural Information Processing Systems, 2023. URL https://openreview.net/forum?id=VfP6VTVsHc. [46] Joaquin Quionero-Candela, Masashi Sugiyama, Anton Schwaighofer, and Neil D. Lawrence. Dataset Shift in Machine Learning. The MIT Press, 2009. ISBN 0262170051. [47] Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, Gretchen Krueger, and Ilya Sutskever. Learning transferable visual models from natural language supervision. In Marina Meila and Tong Zhang, editors, Proceedings of the 38th International Conference on Machine Learning, volume 139 of Proceedings of Machine Learning Research, pages 8748–8763. PMLR, 18–24 Jul 2021. URL https://proceedings. mlr.press/v139/radford21a.html. [48] Benjamin Recht, Rebecca Roelofs, Ludwig Schmidt, and Vaishaal Shankar. Do ImageNet classifiers generalize to ImageNet? In Kamalika Chaudhuri and Ruslan Salakhutdinov, editors, Proceedings of the 36th International Conference on Machine Learning , volume 97 of Proceedings of Machine Learning Research, pages 5389–5400. PMLR, 09–15 Jun 2019. URL https://proceedings.mlr.press/v97/ recht19a.html. [49] Mooweon Rhee and Tohyun Kim. Exploration and Exploitation, pages 543–546. Palgrave Macmillan UK, London, 2018. ISBN 978-1-137-00772-8. doi: 10.1057/978-1-137-00772-8_388. URL https: //doi.org/10.1057/978-1-137-00772-8_388 . [50] Matthew Riemer, Ignacio Cases, Robert Ajemian, Miao Liu, Irina Rish, Yuhai Tu, , and Gerald Tesauro. Learning to learn without forgetting by maximizing transfer and minimizing interference. In Interna- tional Conference on Learning Representations, 2019. URL https://openreview.net/forum?id= B1gTShAct7. [51] Tanin Sirimongkolkasem and Reza Drikvandi. On Regularisation Methods for Analysis of High Di- mensional Data. Annals of Data Science , 6(4):737–763, December 2019. ISSN 2198-5812. doi: 10.1007/s40745-019-00209-4. URL https://doi.org/10.1007/s40745-019-00209-4 . 13[52] Yongyi Su, Xun Xu, and Kui Jia. Towards real-world test-time adaptation: Tri-net self-training with balanced normalization. Proceedings of the AAAI Conference on Artificial Intelligence, 38(13):15126– 15135, 2024. [53] Yu Sun, Xiaolong Wang, Zhuang Liu, John Miller, Alexei Efros, and Moritz Hardt. Test-time training with self-supervision for generalization under distribution shifts. In Hal Daumé III and Aarti Singh, editors, Proceedings of the 37th International Conference on Machine Learning, volume 119 of Proceedings of Machine Learning Research, pages 9229–9248. PMLR, 13–18 Jul 2020. URL https://proceedings. mlr.press/v119/sun20b.html. [54] Richard S. Sutton and Andrew G. Barto. Reinforcement Learning: An Introduction. MIT Press, Cambridge, MA, 2018. [55] Antti Tarvainen and Harri Valpola. Mean teachers are better role models: Weight-averaged consistency targets improve semi-supervised deep learning results. In Proceedings of the 31st International Conference on Neural Information Processing Systems, NIPS’17, page 1195–1204, 2017. ISBN 9781510860964. [56] Daniel Vela, Andrew Sharp, Richard Zhang, Trang Nguyen, An Hoang, and Oleg S. Pianykh. Temporal quality degradation in AI models. Scientific Reports, 12(1):11654, July 2022. ISSN 2045-2322. doi: 10.1038/s41598-022-15245-z. URL https://www.nature.com/articles/s41598-022-15245-z . [57] Dequan Wang, Evan Shelhamer, Shaoteng Liu, Bruno Olshausen, and Trevor Darrell. Tent: Fully test-time adaptation by entropy minimization. In International Conference on Learning Representations, 2021. URL https://openreview.net/forum?id=uXl3bZLkr3c. [58] Jindong Wang, Cuiling Lan, Chang Liu, Yidong Ouyang, and Tao Qin. Generalizing to unseen domains: A survey on domain generalization. In Zhi-Hua Zhou, editor, Proceedings of the Thirtieth International Joint Conference on Artificial Intelligence, IJCAI-21, pages 4627–4635. International Joint Conferences on Artificial Intelligence Organization, 8 2021. doi: 10.24963/ijcai.2021/628. URL https://doi.org/10. 24963/ijcai.2021/628. Survey Track. [59] Qin Wang, Olga Fink, Luc Van Gool, and Dengxin Dai. Continual test-time domain adaptation. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pages 7201–7211, June 2022. [60] Zachary Young and Robert Steele. Empirical evaluation of performance degradation of machine learning-based predictive models – a case study in healthcare information systems. International Journal of Information Management Data Insights , 2(1):100070, 2022. ISSN 2667-0968. doi: https: //doi.org/10.1016/j.jjimei.2022.100070. URL https://www.sciencedirect.com/science/article/ pii/S2667096822000143. [61] Longhui Yuan, Binhui Xie, and Shuang Li. Robust test-time adaptation in dynamic scenarios. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 15922– 15932, 2023. [62] Hao Zhao, Yuejiang Liu, Alexandre Alahi, and Tao Lin. On pitfalls of test-time adaptation. In ICLR 2023 Workshop on Pitfalls of limited data and computation for Trustworthy ML , 2023. URL https: //openreview.net/forum?id=0Go_RsG_dYn. 14Persistent Test-time Adaptation in Recurring Testing Scenarios Technical Appendices Table of Contents A Related Work 16 B Proof of Lemmas and Theorems 16 B.1 Proof of Lemma 1 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 17 B.2 Proof of Lemma 2. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 17 B.3 Proof of Theorem 1 and Corollary 1. . . . . . . . . . . . . . . . . . . . . . . . 18 C Further Justifications on Gaussian Mixture Model Classifier 19 D Further Justifications on the Recurring Testing Scenario 20 D.1 Recurring TTA Follows the Design of a Practical TTA Stream . . . . . . . . . . 20 D.2 Recurring TTA as a Diagnostic Tool . . . . . . . . . . . . . . . . . . . . . . . . 20 D.3 Recurring TTA with Random Orders . . . . . . . . . . . . . . . . . . . . . . . 20 E Further Justifications on Persistent TTA (PeTTA) 21 E.1 Pseudo Code . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 21 E.2 Anchor Loss . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 22 E.3 The Use of the Memory Bank . . . . . . . . . . . . . . . . . . . . . . . . . . . 23 E.4 Empirical Mean and Covariant Matrix of Feature Vectors on the Source Dataset . 23 E.5 Novelty of PeTTA . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 24 F Additional Experimental Results of PeTTA 24 F.1 Performance of PeTTA Versus Compared Methods . . . . . . . . . . . . . . . . 24 F.2 An Inspection of PeTTA . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 25 F.3 Does Model Reset Help? . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 25 F.4 PeTTA with 40 Recurring Visits . . . . . . . . . . . . . . . . . . . . . . . . . . 27 F.5 The Sensitivity of Hyper-parameter Choices in PeTTA . . . . . . . . . . . . . . 27 F.6 More Details on the Ablation Study . . . . . . . . . . . . . . . . . . . . . . . . 27 F.7 More Confusion Matrices in Recurring TTA Setting . . . . . . . . . . . . . . . 29 G Experimental Details 29 G.1 Computing Resources . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 29 G.2 Experiments on CCC Testing Stream . . . . . . . . . . . . . . . . . . . . . . . 29 G.3 Test-time Adaptation Methods . . . . . . . . . . . . . . . . . . . . . . . . . . . 29 G.4 The Use of Existing Assets . . . . . . . . . . . . . . . . . . . . . . . . . . . . 30 15A Related Work Towards Robust and Practical TTA. While forming the basis, early single-target TTA ap- proaches [53, 57, 39, 41, 33] is far from practice. Observing the dynamic of many testing envi- ronments, a continual TTA setting is proposed where an ML model continuously adapts to a sequence of multiple shifts [36, 59]. Meanwhile, recent studies [15, 7] point out that the category distribution realistic streams is highly temporally correlated. Towards real-world TTA setting, Yuanet al. [61] launch the practical TTA which considers the simultaneous occurrence of the two aforementioned challenges. For a robust and gradual adaptation, an update via the mean teacher [55] mechanism is exploited in many continual TTA algorithms [59, 61, 12, 22]. To moderate the temporally correlated test stream, common approaches utilize a small memory bank for saving a category-balanced subset of testing samples [15, 61], inspired by the replay methods [50, 2] to avoid forgetting in the task of continual learning [34, 3, 11]. Our study emphasizes another perspective: beyond a supreme performance, a desirable TTA should also sustain it for an extended duration. Temporal Performance Degradation.By studying the quality of various ML models across multiple industry applications [56, 60] the issue of AI “aging\" with the temporal model degradation progress, even with data coming from a stable process has been confirmed. In TTA, the continuous changes of model parameters through gradient descent aggravate the situation, as also recently noticed in [45]. Apart from observation, we attempt to investigate and provide theoretical insights towards the mechanism of this phenomenon. Accumulated Errors in TTA. In TTA, the issue of accumulated error has been briefly acknowledged. Previous works strive to avoid drastic changes to model parameters as a good practice. Up to some degree, it helps to avoid performance degradation. Nevertheless, it is still unclear whether their effectiveness truly eliminates the risk. To preserve in-distribution performance, regularization [27, 40] or replaying of training samples at test-time [ 12] have been used. Other studies explore reset (recovering the initial model parameters) strategies [59, 45], periodically or upon the running entropy loss approaches a threshold [ 41]. Unfortunately, knowledge accumulated in the preceding steps will vanish, and a bad heuristic choice of threshold or period leads to highly frequent model resets. Noteworthy, tuning those hyper-parameters is exceedingly difficult due to the unavailability of the validation set [62]. LAME [ 7] suggests a post-processing step for adaptation (without updating the parameters). This approach, however, still limits the knowledge accumulation. Our PeTTA is reset-free by achieving an adaptable continual test-time training. B Proof of Lemmas and Theorems In this section, we prove the theoretical results regarding the ϵ−perturbed Gaussian Mixture Model Classifier (ϵ−GMMC) introduced in Sec. 3.2. We first briefly summarize the definition of model collapse and the static data stream assumption: Definition 1 (Model Collapse). A model is said to be collapsed from step τ ∈ T, τ <∞ if there exists a non-empty subset of categories ˜Y ⊂ Ysuch that Pr{Yt ∈ ˜Y} > 0 but the marginal Pr{ˆYt ∈ ˜Y} converges to zero in probability: lim t→τ Pr{ˆYt ∈ ˜Y} = 0. Assumption 1 (Static Data Stream). The marginal distribution of the true label follows the same Bernoulli distribution Ber(p0): p0,t = p0, (p1,t = p1 = 1 − p0), ∀t ∈ T. Preliminary. Following the same set of notations introduced in the main text, recall that we denoted py,t ∆ = Pr{Yt = y}, ˆpy,t ∆ = Pr{ˆYt = y} (marginal distribution of the true label Yt and pseudo label ˆYt receiving label y, respectively) and ϵt = Pr{Yt = 1|ˆYt = 0} (the false negative rate (FNR) of 16ϵ−GMMC). At testing step t, we obtain the following relations: EPt h Xt|ˆYt = 0 i = (1 − ϵt)µ0 + ϵtµ1, (9) EPt h Xt|ˆYt = 1 i = µ1, (10) VarPt \u0010 Xt|ˆYt = 0 \u0011 = (1 − ϵt)σ2 0 + ϵtσ2 1 + ϵt(1 − ϵt)(µ0 − µ1)2, (11) VarPt \u0010 Xt|ˆYt = 1 \u0011 = σ2 1. (12) In addition, under Assumption 1, the marginal distribution Pt(x) (also referred as data distribution in our setup) is: Pt(x) = N(x; p0µ0 + p1µ1, p0σ2 0 + p1σ2 1 + p0p1(µ0 − µ1)2) ∀t ∈ T. (13) B.1 Proof of Lemma 1 Lemma 1 (Increasing FNR). Under Assumption 1, a binary ϵ-GMMC would collapsed (Def. 1) with lim t→τ ˆp1,t = 0 (or lim t→τ ˆp0,t = 1, equivalently) if and only if lim t→τ ϵt = p1. Proof. Under Assumption 1, we have EPt [Xt] = p0µ0 + (1 − p0)µ1. Also note that: EPt [Xt] = EPt h EPt h Xt|ˆYt ii = EPt h Xt|ˆYt = 0 i ˆp0,t + EPt h Xt|ˆYt = 1 i ˆp1,t (14) = [(1 − ϵt)µ0 + ϵtµ1] ˆp0,t + µ1(1 − ˆp0,t) = [(1 − ϵt)ˆp0,t] µ0 + [1 − ˆp0,t(1 − ϵt)] µ1 = p0µ0 + (1 − p0)µ1, where the second equality follows Eqs. 9-10. Therefore: ˆp0,t = p0 1 − ϵt . (15) Eq. 15 shows positive correlation between ˆp0,t and ϵt. Given lim t→τ ϵt = p1, taking the limit introduces: lim t→τ ˆp0,t = lim t→τ p0 1 − ϵt = p0 1 − p1 = 1. Similarly, having lim t→τ ˆp0,t = 1, the false negative rate ϵt when t → τ is: lim t→τ ϵt = 1 − p0 = p1. Since ˆp0,t + ˆp1,t = 1, lim t→τ ˆp1,t = 0, equivalently. Towards the collapsing point, the model tends to predict a single label (class 0 in the current setup). In addition, the FNR of the model ϵt also raises correspondingly. B.2 Proof of Lemma 2. Lemma 2 (ϵ-GMMC After Collapsing ). For a binary ϵ-GMMC model, with Assumption 1, if lim t→τ ˆp1,t = 0 (collapsing), the cluster 0 in GMMC converges in distribution to a single-cluster GMMC with parameters: N(ˆµ0,t, ˆσ2 0,t) d. → N(p0µ0 + p1µ1, p0σ2 0 + p1σ2 1 + p0p1(µ0 − µ1)2). Proof. From Eqs. 9-10, under the increasing type II collapse of ϵ−GMMC setting, the perturbation does not affect the approximation of µ1. Meanwhile, when ϵt increases, one can expect that ˆµ0,t 17moves further away from µ0 toward µ1. Frist, the mean teacher model of GMMC (Eq. 4, main text) gives: EPt h ˆµ0,t|ˆYt = 1 i = EPt−1 [ˆµ0,t−1] , EPt h ˆµ0,t|ˆYt = 0 i = (1 − α)EPt−1 h ˆµ0,t−1|ˆYt = 0 i + αEPt h Xt|ˆYt = 0 i = (1 − α)EPt−1 [ˆµ0,t−1] + α \u0010 EPt h Xi|ˆYt = 0 i\u0011 , EPt h ˆµ1,t|ˆYt = 1 i = (1 − α)EPt−1 h ˆµ1,t−1|ˆYt = 1 i + αEPt h Xt|ˆYt = 1 i = (1 − α)EPt−1 [ˆµ1,t−1] + α \u0010 EPt h Xi|ˆYt = 1 i\u0011 , EPt h ˆµ1,t|ˆYt = 0 i = EPt−1 [ˆµ1,t−1] . By defining uy,t = EPt [ˆµy,t], we obtain the following recurrence relation between u0,t and u0,t−1: u0,t = EPt h ˆµ0,t|ˆYt = 0 i ˆp0,t + EPt h ˆµ0,t|ˆYt = 1 i ˆp1,t = \u0010 (1 − α)u0,t−1 + αEPt h Xt|ˆYt = 0 i\u0011 ˆp0,t + u0,t−1 ˆp1,t = [(1 − α)ˆp0,t + ˆp1,t] u0,t−1 + αˆp0,tEPt h Xt|ˆYt = 0 i = (1 − αˆp0,t)u0,t−1 + αˆp0,tEPt h Xt|ˆYt = 0 i = (1 − αˆp0,t)u0,t−1 + αˆp0,t [(1 − ϵt)µ0 + ϵtµ1] . (16) Given lim t→τ ˆp0,t = 1, it follows that lim t→τ ϵ0,t = p1 by Lemma 1. From this point: u0,t = (1 − α)u0,t−1 + α (p0µ0 + p1µ1) ∀t > τ. Taking the limit t → ∞: lim t→∞ u0,t = lim t→∞ (1 − α)u0,t−1 + α (p0µ0 + p1µ1) = lim t→∞ (1 − α)t ˆµ0,0 + α tX i=1 (1 − α)i−1 (p0µ0 + p1µ1) = lim t→∞ (1 − α)t ˆµ0,0 + (1 − (1 − α)t)(p0µ0 + p1µ1) = p0µ0 + p1µ1. The second equation is obtained by solving the recurrence relation. When lim t→τ ˆp0,t = 1, {ˆµy,t}y∈{0,1} becomes a deterministic values. Hence, giving uy,t = EPt [ˆµy,t] = ˆµ0,t(∀t > τ) and lim t→∞ ˆµ0,t = lim t→∞ u0,t = p0µ0 + p1µ1. (17) Repeating the steps above with Eqs. 11-12 in place of Eqs. 9-10, we obtain a similar result for σ2 0,t: lim t→∞ ˆσ2 0,t = p0σ2 0 + p1σ2 1 + p0p1(µ0 − µ1)2. (18) By Lévy’s continuity theorem (p. 302, [ 42]), from Eqs. 17-18, when t → ∞, the estimated distribution of the first cluster N(x; ˆµ0,tˆσ2 0,t) converges to the whole data distribution Pt(x) (Eq. 13) when collapsing. B.3 Proof of Theorem 1 and Corollary 1. Theorem 1 (Convergence of ϵ−GMMC). For a binary ϵ-GMMC model, with Assumption 1, let the distance from ˆµ0,t toward µ1 is d0→1 t = |EPt [ˆµ0,t] − µ1|, then: d0→1 t − d0→1 t−1 ≤ α · p0 · \u0012 |µ0 − µ1| −d0→1 t−1 1 − ϵt \u0013 . 18Proof. Substituting Eq. 15 into ˆp0,t of Eq. 16 gives: u0,t = \u0012 1 − αp0 1 − ϵt \u0013 u0,t−1 + αp0 1 − ϵt [(1 − ϵt)µ0 + ϵtµ1] . Hence, we have the distance from u0,t toward µ1: |u0,t − µ1| = \f\f\f\f \u0012 1 − αp0 1 − ϵt \u0013 u0,t−1 + αp0µ0 + αp0ϵtµ1 1 − ϵt − µ1 \f\f\f\f = \f\f\f\f \u0012 1 − αp0 1 − ϵt \u0013 (u0,t−1 − µ1) + αp0µ0 + αp0ϵtµ1 1 − ϵt − αp0µ1 1 − ϵt \f\f\f\f = \f\f\f\f \u0012 1 − αp0 1 − ϵt \u0013 (u0,t−1 − µ1) + αp0µ0 − αp0µ1(1 − ϵt) 1 − ϵt \f\f\f\f = \f\f\f\f \u0012 1 − αp0 1 − ϵt \u0013 (u0,t−1 − µ1) + αp0(µ0 − µ1) \f\f\f\f ≤ \u0012 1 − αp0 1 − ϵt \u0013 |u0,t−1 − µ1| + αp0|µ0 − µ1|. The last inequality holds due to the triangle inequality. Equivalently, |u0,t − µ1| − |u0,t−1 − µ1| ≤α · p0 · \u0012 |µ0 − µ1| −|u0,t−1 − µ1| 1 − ϵt \u0013 . Let d0→1 t = |EPt [ˆµ0,t] − µ1|, we conclude that: d0→1 t − d0→1 t−1 ≤ α · p0 · \u0012 |µ0 − µ1| −d0→1 t−1 1 − ϵt \u0013 . Corollary 1 (A Condition forϵ−GMMC Collapse). With fixedp0, α, µ0, µ1, ϵ−GMMC is collapsed if there exists a sequence of {ϵt}τ τ−∆τ (τ ≥ ∆τ > 0) such that: p1 ≥ ϵt > 1 − d0→1 t−1 |µ0 − µ1|, t ∈ [τ − ∆τ , τ]. Proof. Initialized at µ0, ϵ-GMMC is collapsing when ˆµ0,t converges to the mid-point p0µ0 + p1µ1 (Lemma 2), i.e., moving closer to µ1. From Thm. 1, the distance towards µ1 d0→1 t < d0→1 t−1 if |µ0 − µ1| −|u0,t−1 − µ1| 1 − ϵt < 0 ⇔ |µ0 − µ1| < |u0,t−1 − µ1| 1 − ϵt ⇔ ϵt > 1 − |u0,t−1 − µ1| |µ0 − µ1| . When there exists this sequence{ϵt}τ τ−∆τ (τ ≥ ∆τ > 0) it follows that d0→1 t < d0→1 t−1 and ϵt > ϵt−1 is guaranteed ∀t ∈ [τ − ∆τ , τ]. Hence, lim t→τ ϵt = p1 (model collapsed, by Lemma 1). C Further Justifications on Gaussian Mixture Model Classifier One may notice that in ϵ-GMMC (Sec. 4.2), the classifier is defined ft(x) = argmaxy∈Y Pr(x|y; θt) (maximum likelihood estimation) while in general, ft(x) = argmaxy∈Y Pr(y|x; θt) (maximum a posterior estimation), parameterized by a neural network. In this case, since the equal prior (i.e., Pr(y; θt) = Pr(y′; θt), ∀y, y′ ∈ C) is enforced in ϵ-GMMC, the two definitions are equivalent. Proof. Having: argmaxy∈Y Pr(y|x; θt) = argmaxy∈Y Pr(x|y; θt) Pr(y; θt)P y′∈Y Pr(x|y′; θt) Pr(y′; θt) = argmaxy∈Y Pr(x|y; θt). We conclude that the two definitions are equivalent. In fact, it is well-known that maximum likelihood estimation is a special case of maximum a posterior estimation when the prior is uniform. 19D Further Justifications on the Recurring Testing Scenario D.1 Recurring TTA Follows the Design of a Practical TTA Stream Note that in recurring TTA, besides the recurrence of environments (or corruptions) as in [59, 40], the distribution of class labels is also temporally correlated (non-i.i.d.) as suggested by [15, 61] to reflect the practical testing stream better. In short, recurring TTA is formed by recurring the environments of practical TTA scenario introduced in [61] multiple times (readers are encouraged to visit the original paper for additional motivations on this scenario). D.2 Recurring TTA as a Diagnostic Tool Noticeably, CoTTA [59] also performed 10-round repetition across multiple domain shifts to simulate a lifelong TTA testing stream just like our recurring TTA. However, the key difference is CoTTA assumes the distribution of class labels is i.i.d., which does not hold in many real-life testing scenarios as argued in [ 15, 61]. Our recurring TTA lifts this assumption and allows temporally correlated (non-i.i.d.) label distribution (more challenging, more practical). This extension allows recurring TTA to spot the risk of model collapse on CoTTA [59] and other methods. The over-simplicity of the repeating scheme in CoTTA for spotting performance degradation is also suggested in [45]. Clearly, it seems not to be a problem at first glance in Tab. 5 of [59] (CoTTA’s 10-round repetition), but in fact, the risk in CoTTA remains, as explored in our scenario and also on CCC [45]. The construction of our recurring TTA is notably simple - a technical effort to extend the testing stream. However, this simplicity is on purpose, serving as a diagnostic tool for lifelong continual TTA. Counterintuitively, our experiments on four different tasks with the latest methods verify that even if the model is exposed to the same environment(the most basic case), their adaptability and performance are still consistently reduced (demonstrated visually in Fig. 1, quantitatively in Sec. 5.3). We believe that the extensive testing stream by recurrence in our setup is a simple yet sufficient scenario to demonstrate the vulnerability of existing continual TTA methods when facing the issue of model collapse (compared to CCC [45], a notably more complicated scenario than our recurring TTA). Indeed, recurring shifts are sufficient to show this failure mode and any lifelong TTA method should necessarily be able to handle recurring conditions. D.3 Recurring TTA with Random Orders Recall that in Sec. 3.1,recurring TTAis constructed by repeatingthe same sequence of D distributions K times. For example, a sequence with K = 2 could be P1 → P2 → ··· → PD → P1 → P2 → ··· → PD. For simplicity and consistency that promote reproducibility, the same order of image corruptions (following [61]) is used for all recurrences. This section presents supplementary experimental findings indicating that the order of image corruptions within each recurrence, indeed, does not affect the demonstration of TTA model collapse and the performance of our PeTTA. Experiment Setup. We refer to the setting same-order as using one order of image corruptions in [61] for all recurrences (specifically, on CIFAR-10/100-C and ImageNet-C:motion → snow → fog → shot → defocus → contrast → zoom → brightness → frost → elastic → glass → gaussian → pixelated → jpeg → impulse). Conversely, in random-order, the order of image corruptions is randomly shuffled at the beginning of each recurrence. Hence, the corruption orders across K recurrences are now entirely different. We redo the experiment of the second setting three times (with different random seeds = 0, 1, 2). Nevertheless, different TTA methods are ensured to be evaluated on the same testing stream, since it is fixed after generation. Without updating its parameters, the performance of the source model is trivially independent of the order of corruptions. Experimental Result. The experimental results are visualized in Fig. 6. The first column plots the experiments under the same-order, while the remaining three columns plot the experiments in the random-order setting, with varying random seeds. Note that the message conveyed by each sub-figure entirely matches that of Fig. 1-right. Discussions. Clearly, a similar collapsing pattern is observed in all three TTA tasks, with three combinations of 20 image corruption orders. This pattern also matches the easiest setting using the same order of image corruptions we promoted in recurring TTA. 201 5 10 15 20 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 1 5 10 15 20 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 1 5 10 15 20 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 1 5 10 15 20 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 Same-order Random-order (seed=0) Random-order (seed=1) Random-order (seed=2) Testing Error Recurring TTA visit Recurring TTA visit Recurring TTA visit Recurring TTA visit (a) CIFAR-10 → CIFAR-10-C task. 1 5 10 15 20 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1.0 1 5 10 15 20 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1.0 1 5 10 15 20 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1.0 1 5 10 15 20 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1.0 Same-order Random-order (seed=0) Random-order (seed=1) Random-order (seed=2) Testing Error Recurring TTA visit Recurring TTA visit Recurring TTA visit Recurring TTA visit (b) CIFAR-100 → CIFAR-100-C task. 1 5 10 15 20 0.5 0.6 0.7 0.8 0.9 1.0 1 5 10 15 20 0.5 0.6 0.7 0.8 0.9 1.0 1 5 10 15 20 0.5 0.6 0.7 0.8 0.9 1.0 1 5 10 15 20 0.5 0.6 0.7 0.8 0.9 1.0 Same-order Random-order (seed=0) Random-order (seed=1) Random-order (seed=2) Testing Error Recurring TTA visit Recurring TTA visit Recurring TTA visit Recurring TTA visit (c) ImageNet → ImageNet-C task. Figure 6: Recurring TTA with different order of corruptions. This figure plots the testing error of two TTA approaches: RoTTA - - [61], and, PeTTA- - (ours), and source model-×- as a reference performance under our recurring TTA (with 20 visits) across three TTA tasks. On the same-order experiments (column 1), the same order of image corruptions is applied for all 20 visits. Meanwhile, in random-order, this order is reshuffled at the beginning of each visit (columns 2-4). Random-order experiments are redone three times with different random seeds. Here, we empirically validate that using the same order of domain shifts (image corruptions) in our recurring TTA is sufficient to showcase the model collapse and evaluate the persistence of our PeTTA. Best viewed in color. E Further Justifications on Persistent TTA (PeTTA) E.1 Pseudo Code We summarize the key steps of our proposed PeTTA in Alg. 1, with the key part (lines 4-13) highlighted in blue. Our approach fits well in the general workflow of a TTA algorithm, enhancing the regular mean-teacher update step. Appdx. E.5 elaborates more on our contributions in PeTTA, distinguishing them from other components proposed in previous work. The notations and definitions of all components follow the main text (described in detail in Sec. 4). On line 8 of Alg. 1, as a 21Algorithm 1 Persistent TTA (PeTTA) Input: Classification model ft and its deep feature extractor ϕθt, both parameterized by θt ∈ Θ. Testing stream {Xt}T t=0, initial model parameter (θ0), initial update rate (α0), regularization term coefficient (λ0), empirical mean ({µy 0}y∈Y) and covariant matrix ({Σy 0}y∈Y) of feature vectors in the training set, ˆµy t EMA update rate (ν). 1 ˆµy 0 ← µy 0, ∀y ∈ Y; // Initialization 2 for t ∈ [1, ··· , T] do 3 ˆYt ← ft−1(Xt) ; // Obtaining pseudo-labels for all samples in Xt 4 // Persistent TTA (PeTTA) 5 ˆYt ← n ˆY (i) t |i = 1, ··· , Nt o ; // Set of (unique) pseudo-labels in Xt 6 ¯γt ← 0 ; 7 for y ∈ ˆYt do 8 γy t ← 1 − exp \u0010 −(ˆµy t − µy 0)T (Σy 0)−1 (ˆµy t − µy 0) \u0011 ; // Divergence sensing term on category y 9 ¯γt ← ¯γt + γy t | ˆYt| ; // Average divergence sensing term for step t 10 ˆµy t ← (1 − ν)ˆµy t−1 + νϕθt−1 (Xt|ˆYt = y) ; // EMA update of ˆµy t for samples with ˆYt = y 11 end 12 λt ← ¯γt · λ0 ; // Computing adaptive regularization term coefficient 13 αt ← (1 − ¯γt) · α0 ; // Computing adaptive update rate 14 // Regular Mean-teacher Update 15 θ′ t ← Optim θ′∈Θ EPt h LCLS \u0010 ˆYt, Xt; θ′ \u0011 + LAL (Xt; θ′) i + λtR(θ′) ; // Student model update 16 θt ← (1 − αt)θt−1 + αtθ′ t. ; // Teacher model update 17 // Final prediction 18 yeild ft(Xt) ; // Returning the final inference with updated model ft 19 end shorthand notation, ϕθt−1 (Xt|ˆYt = y) denotes the empirical mean of all feature vectors of X(i) t (extracted by ϕθt−1 \u0010 X(i) t \u0011 ) if ˆY (i) t = y, i= 1, ··· , Nt in the current testing batch. E.2 Anchor Loss KL Divergence Minimization-based Interpretation of Anchor Loss. In Sec. 4, we claimed that minimizing the anchor loss LAL is equivalent to minimizing the relative entropy (or KL divergence) between the output probability of two models parameterized by θ0 and θ. Proof. Having: DKL (Pr(y|Xt; θ0)||Pr(y|Xt; θ)) = X y∈Y Pr(y|Xt; θ0) log Pr(y|Xt; θ0) Pr(y|Xt; θ) = − X y∈Y Pr(y|Xt; θ0) log Pr(y|Xt; θ) | {z } LAL(Xt;θ) −H(Pr(y|Xt; θ0))| {z } constant . Hence, argmin θ∈Θ LAL(Xt; θ) = argmin θ∈Θ DKL (Pr(y|Xt; θ0)||Pr(y|Xt; θ)) . 22Intuitively, a desirable TTA solution should be able to adapt to novel testing distributions on the one hand, but it should not significantly diverge from the initial model. LAL fits this purpose, constraining the KL divergence between two models at each step. Connections between Anchor Loss and Regularizer Term. While supporting the same objective (collapse prevention by avoiding the model significantly diverging from the source model), the major difference between Anchor loss ( LAL) and the Regularizer term ( R(θ)) is that the anchor loss operates on the probability space of model prediction while the regularizer term works on the model parameter spaces. Tab. 4 (lines 1 and 5) summarizes the ablation study when each of them is eliminated. We see the role of the regularization term is crucial for avoiding model collapse, while the anchor loss guides the adaptation under the drastic domain shift. Nevertheless, fully utilizing all components is suggested for maintaining TTA persistence. E.3 The Use of the Memory Bank The size of Memory Bank. The size of the memory bank in PeTTA is relatively small, equal to the size of one mini-batch for update (64 images, specifically). The Use of the Memory Bank in PeTTA is Fair with Respect To the Compared Methods.Our directly comparable method - RoTTA [61] also takes this advantage (referred to as category-balanced sampling, Sec. 3.2 of [ 61]). Hence, the comparison between PeTTA and RoTTA is fair in terms of additional memory usage. Noteworthy, the use of a memory bank is a common practice in TTA literature (e.g., [15, 8, 61]), especially in situations where the class labels are temporally correlated or non-i.i.d. distributed (as we briefly summarized in Appdx. A - Related Work section). CoTTA [59], EATA [40] and MECTA [ 22] (compared method) assume labels are i.i.d. distributed. Hence, a memory bank is unnecessary, but their performance under temporally correlated label distribution has dropped significantly as a trade-off. The RMT [12] (compared method) does not require a memory bank but it needs to cache a portion of the source training set for replaying (Sec. 3.3 in [12]) which even requires more resources than the memory bank. Eliminating the Need for a Memory Bank. As addressing the challenge of temporally correlated label distribution on the testing stream is not the focus of PeTTA, we have conveniently adopted the use of the memory bank proposed in [61]. Since this small additional memory requirement is not universally applied in every real-world scenario, we believe that this is a reasonable assumption, and commonly adopted in TTA practices. Nevertheless, exploring alternative ways for reducing the memory size (e.g., storing the embedded features instead of the original image) would be an interesting future direction. E.4 Empirical Mean and Covariant Matrix of Feature Vectors on the Source Dataset Two Ways of Computing µy 0 and Σy 0 in Practice. One may notice that in PeTTA, computing γy t requires the pre-computed empirical mean (µy 0) and covariance (Σy 0) of the source dataset . This requirement may not be met in real-world situations where the source data is unavailable. In practice, the empirical mean and covariance matrix computed on the source distribution can be provided in the following two ways: 1. Most ideally, these values are computed directly by inference on the entire training set once the model is fully trained. They will be provided alongside the source-distribution pre-trained model as a pair for running TTA. 2. With only the source pre-trained model available, assume we can sample a set of unlabeled data from the source distribution. The (pseudo) labels for them are obtained by inferring from the source model. Since the source model is well-performed in this case, using pseudo is approximately as good as the true label. Accessing the Source Distribution Assumption in TTA. In fact, the second way is typically assumed to be possible in previous TTA methods such as EATA [40], and MECTA [22] (a compared method) to estimate a Fisher matrix (for anti-forgetting regularization purposes). Our work - PeTTA follows the same second setup as the previous approaches mentioned above. A variation of RMT [12] (a compared method) approach even requires having the fully labeled source data available at test-time for source replaying (Sec. 3.3 of [12]). This variation is used for comparison in our experiments. 23We believe that having the empirical mean and covariant matrix pre-computed on a portion of the source distribution in PeTTA is a reasonable assumption . Even in the ideal way, revealing the statistics might not severely violate the risk of data privacy leakage or require notable additional computing resources. Number of Samples Needed for Computation. To elaborate more on the feasibility of setting (2) mentioned above, we perform a small additional experiment on the performance of PeTTA while varying the number of samples used for computing the empirical mean and covariant matrix on the source distribution. In this setting, we use the test set of CIFAR-10, CIFAR-100, DomainNet validation set of ImageNet (original images, without corruption, or the real domain test set of DomainNet), representing samples from the source distribution. The total number of images is 10, 000 in CIFAR-10/A00, 50, 000 in ImageNet, and 69, 622 in DomainNet. We randomly sample 25%, 50%, 75%, and 100% of the images in this set to run PeTTA for 20 rounds of recurring. The result is provided in Tab. 6 below. Table 6: Average classification error of PeTTA (across 20 visits) with varying sizes of source samples used for computing feature empirical mean (µy 0) and covariant matrix (Σy 0). TTA Task 25% 50% 75% 100% CIFAR-10→CIFAR-10-C 22.96 22.99 23.03 22.75 CIFAR-100→CIFAR-100-C 35.01 35.11 35.09 35.15 DomainNet:real→clip→paint→sketch 43.18 43.12 43.15 42.89 ImageNet→ImageNet-C 61.37 59.68 61.05 60.46 The default choice of PeTTA is using 100% samples of the validation set of the source dataset. However, we showcase that it is possible to reduce the number of unlabeled samples from the source distribution to compute the empirical mean and covariant matrix for PeTTA, without significantly impacting its performance. E.5 Novelty of PeTTA PeTTA is composed of multiple components. Among them, the anchor loss is an existing idea (examples of previous work utilizing this idea are [ 32, 12]). Similarly, the mean-teacher update; and regularization are well-established techniques and very useful for the continual or gradual TTA scenario. Hence, we do not aim to improve or alternate these components. Nevertheless, the novelty of our contribution is the sensing of the divergence and adaptive model update, in which the importance of minimizing the loss (adaptation) and regularization (collapse prevention) is changed adaptively. In short, we propose a harmonic way of combining those elements adaptively to achieve a persistent TTA process. The design of PeTTA draws inspiration from a theoretical analysis (Sec. 3.2), empirically surpassing both the conventional reset-based approach [45] (Appdx. F.3) and other continual TTA approaches [61, 12, 59, 22, 7] on our proposed recurring TTA (Sec. 3.1, Appdx. F.1), as well as the previously established CCC [45] benchmark. F Additional Experimental Results of PeTTA F.1 Performance of PeTTA Versus Compared Methods Performance on CIFAR-100-C and Domainnet Datasets. Due to the length constraint, the classification errors on the tasks CIFAR-100→CIFAR-100-C, and real → clipart, painting, sketch of DomainNet are provided in Tab. 7 and Tab. 8. To prevent model collapse, the adaptability of PeTTA is more constrained. As a result, it requires more time for adaptation initially (e.g., in the first visit) but remains stable thereafter. Generally, consistent trends and observations are identified across all four TTA tasks. Standard Deviation of PeTTA Performance Across Multiple Runs. For PeTTA experiments marked with (*) in Tab. 1, Tab. 2, Tab. 7, and Tab. 8, the average performance across five independent runs with different random seeds is reported. Due to the space constraint, the corresponding standard deviation values are now reported in Tab. 9. Generally, the average standard deviation across runs 24Table 7: Average classification error of the task CIFAR-100 → CIFAR-100-C in recurring TTA scenario. The lowest error is highlighted in bold, (∗)average value across 5 runs (different random seeds) is reported for PeTTA. Recurring TTA visit− − − − − − − − − − − − − − − − − − − − − − − − − →Method1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 Avg Source 46.5 46.5 LAME [7] 40.5 40.5 CoTTA [59]53.4 58.4 63.4 67.6 71.4 74.9 78.2 81.1 84.0 86.7 88.8 90.7 92.3 93.5 94.7 95.6 96.3 97.0 97.3 97.683.1EATA [40]88.5 95.0 96.8 97.3 97.4 97.2 97.2 97.3 97.4 97.5 97.5 97.5 97.6 97.7 97.7 97.7 97.8 97.8 97.7 97.796.9RMT [12]50.5 48.6 47.9 47.4 47.3 47.1 46.9 46.9 46.6 46.8 46.7 46.5 46.5 46.6 46.5 46.5 46.5 46.5 46.5 46.547.1MECTA [22]44.8 44.3 44.6 43.1 44.8 44.2 44.4 43.8 43.8 43.9 44.6 43.8 44.4 44.6 43.9 44.2 43.8 44.4 44.9 44.244.2RoTTA [61]35.5 35.2 38.5 41.9 45.3 49.2 52.0 55.2 58.1 61.5 64.6 67.5 70.7 73.2 75.4 77.1 79.2 81.5 82.8 84.561.4RDumb [45]36.7 36.7 36.6 36.6 36.7 36.8 36.7 36.5 36.6 36.5 36.7 36.6 36.5 36.7 36.5 36.6 36.6 36.7 36.6 36.536.6ROID [37]76.4 76.4 76.2 76.2 76.3 76.1 75.9 76.1 76.3 76.3 76.6 76.3 76.8 76.7 76.6 76.3 76.2 76.0 75.9 76.076.3TRIBE [52]33.8 33.335.334.935.335.137.1 37.2 37.2 39.1 39.2 41.1 41.0 43.1 45.1 45.1 45.0 44.9 44.9 44.939.6PeTTA(ours)(∗) 35.834.434.735.035.135.135.235.335.335.335.235.335.235.235.135.235.235.235.235.235.1 Table 8: Average classification error of the task real → clipart → painting → sketch on DomainNet dataset in recurring TTA scenario. Episodic TTA visit− − − − − − − − − − − − − − − − − − − − − − − − − →Method1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 Avg Source 45.3 45.3 LAME [7] 45.6 45.6 CoTTA [59]96.2 97.1 97.4 97.8 98.1 98.2 98.4 98.4 98.4 98.5 98.6 98.6 98.6 98.6 98.6 98.7 98.7 98.7 98.7 98.798.3RMT [12]76.2 77.1 77.3 77.3 77.2 77.1 76.8 76.9 76.5 76.4 76.4 76.3 76.4 76.2 76.2 76.1 76.4 76.1 76.0 75.876.5MECTA [22]94.6 98.4 98.6 98.8 99.1 99.0 99.0 99.0 99.0 99.0 99.0 99.0 99.0 99.0 99.0 99.0 99.0 99.0 99.0 99.098.7RoTTA [61]44.3 43.8 44.7 46.7 48.7 50.8 52.7 55.0 57.1 59.7 62.7 65.1 68.0 70.3 72.7 75.2 77.2 79.6 82.6 85.362.1RDumb [45]44.3 44.4 44.3 44.5 44.2 44.2 44.3 44.5 44.4 44.2 44.3 44.3 44.3 44.3 44.5 44.3 44.2 44.3 44.4 44.344.3PeTTA(ours)(∗) 43.842.642.342.342.642.842.843.042.942.943.143.042.943.043.043.143.042.842.942.942.9 stays within ±0.1% for small datasets (CIFAR-10-C, CIFAR-100-C) and±0.5% for larger datasets (ImageNet-C, DomainNet). Table 9: Mean and standard deviation classification error of PeTTA on the four datasets: CIFAR-10-C (CF-10-C), CIFAR-100-C (CF-100-C), DomainNet (DN), and ImageNet-C (IN-C) with recurring TTA scenario. Each experiment is run 5 times with different random seeds. Recurring TTA visit− − − − − − − − − − − − − − − − − − − − − − − − − →Dataset1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 Avg CF-10-C24.3 23.0 22.6 22.4 22.4 22.5 22.3 22.5 22.8 22.8 22.6 22.7 22.7 22.9 22.6 22.7 22.6 22.8 22.9 23.022.8±0.4±0.3±0.4±0.3±0.3±0.3±0.4±0.2±0.3±0.4±0.4±0.2±0.1±0.3±0.5±0.2±0.2±0.3±0.4±0.5 ±0.1 CF-100-C35.8 34.4 34.7 35.0 35.1 35.1 35.2 35.3 35.3 35.3 35.2 35.3 35.2 35.2 35.1 35.2 35.2 35.2 35.2 35.235.1±0.4±0.4±0.2±0.2±0.1±0.1±0.2±0.2±0.1±0.2±0.1±0.2±0.2±0.1±0.1±0.1±0.1±0.1±0.2±0.2 ±0.1 DN43.8 42.6 42.3 42.3 42.6 42.8 42.8 43.0 42.9 42.9 43.1 43.0 42.9 43.0 43.0 43.1 43.0 42.8 42.9 42.942.9±0.1±0.1±0.2±0.2±0.3±0.3±0.3±0.4±0.4±0.4±0.4±0.4±0.4±0.3±0.3±0.2±0.4±0.3±0.3±0.3 ±0.3 IN-C65.3 61.7 59.8 59.1 59.4 59.6 59.8 59.3 59.4 60.0 60.3 61.0 60.7 60.4 60.6 60.7 60.8 60.7 60.4 60.260.5±0.6±0.5±0.5±0.5±1.4±1.1±1.0±0.5±0.8±0.9±0.4±0.8±0.9±0.8±0.9±0.8±1.0±0.6±0.6±0.7 ±0.5 F.2 An Inspection of PeTTA In Fig. 7, we showcase an inspection of our PeTTA on the task CIFAR-10→ CIFAR-10-C [19] in a typical recurring TTA with 20 visits. Specifically, the visualizations of PeTTA parameters ( ¯γt, λt, and αt), adaptation losses (LCLS, LAL) and regularization term (R(θ)) are provided. Here, we observe the values of adaptive parameters λt and αt continuously changing through time, as the testing scenarios evolve during recurring TTA. This proposed mechanismstabilizes the value of the loss functions, and regularization term, balancing between the two primary objectives: adaptation and preventing model collapse. Thus, the error rate persists as a result. A similar pattern is observed on other datasets (CIFAR-100-C [19] and DomainNet [44]). F.3 Does Model Reset Help? Experiment Setup. We use the term “model reset” to represent the action of “reverting the current TTA model to the source model” . This straightforward approach is named RDumb [ 45]. We thoroughly conducted experiments to compare the performance of RDumb with PeTTA. The implementation of RDumb in this setting is as follows. We employ RoTTA [61] as the base test-time adaptor due to the characteristics of the practical TTA [ 61] stream. The model (including model 25parameters, the optimizer state, and the memory bank) is reset after adapting itself to T images.1 For each dataset, three values of this hyper-parameter T are selected: • T = 1, 000: This is the value selected by the RDumb’s authors [ 45]. Unless specifically stated, we use this value when reporting the performance of RDumb [45] in all other tables. • T = 10, 000 (CIFAR-10/100-C), T = 5, 000 (ImageNet-C) and T = 24, 237 (Domain- Net).2 This value is equal to the number of samples in the test set of a single corruption type, i.e., the model is reset exactly after visiting each Pi’s (see Sec. 3.1 for notations). For DomainNet [44], since the number of images within each domain is unequal, the average number of images is used instead. • T = 150, 000 (CIFAR-10/100-C), T = 75, 000 (ImageNet-C) and T = 72, 712 (Domain- Net). This number is equal to the number of samples in one recurrence of our recurring TTA, i.e., the model is reset exactly after visitingP1 → ··· → PD. Here, D = 15 - types of corruptions [19] for CIFAR-10/100-C and ImageNet-C and D = 3 for DomainNet (clipart, painting, sketch). For example, the model is reset 20 times within a recurring TTA setting with 20 recurrences under this choice of T. The second and the last reset scheme could be interpreted as assuming the model has access to an oracle model with a capability of signaling the transitions between domains, or recurrences. Typically, this is an unrealistic capability in real-world scenarios, and a desirable continual TTA algorithm should be able to operate independently without knowing when the domain shift happening. Experimental Results. An empirical comparison between RDumb [45] and our PeTTA are reported in Tab. 10, Tab. 11, Tab. 12 and Tab. 13 for all four tasks. Table 10: Average classification error comparison between RDumb [45] (a reset-based approach) with different reset frequencies and our PeTTA on CIFAR-10→ CIFAR-10-C task. Recurring TTA visit− − − − − − − − − − − − − − − − − − − − − − − − − →Reset Every1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 Avg T= 100031.1 32.1 32.3 31.6 31.9 31.8 31.8 31.9 31.9 32.1 31.7 32.0 32.5 32.0 31.9 31.6 31.9 31.4 32.3 32.431.9T= 1000025.8 25.9 26.5 26.1 26.4 25.4 25.8 25.8 26.1 26.2 26.1 26.1 26.1 26.1 26.1 25.9 25.5 25.5 25.7 26.226.0T= 15000024.8 25.3 24.3 24.1 25.3 25.4 25.4 24.5 25.0 24.9 25.0 24.8 25.0 24.5 24.9 24.1 24.0 24.7 24.9 24.424.8 PeTTA(ours)(∗) 24.323.022.622.422.422.522.322.522.822.822.622.722.722.922.622.722.622.822.923.022.8 Table 11: Average classification error comparison between RDumb [45] (a reset-based approach) with different reset frequencies and our PeTTA on CIFAR-100-C dataset. Recurring TTA visit− − − − − − − − − − − − − − − − − − − − − − − − − →Reset Every1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 Avg T= 100036.7 36.7 36.6 36.6 36.7 36.8 36.7 36.5 36.6 36.5 36.7 36.6 36.5 36.7 36.5 36.6 36.6 36.7 36.6 36.536.6T= 1000043.5 43.6 43.7 43.7 43.4 43.5 43.6 43.4 43.5 43.6 43.8 43.5 43.5 43.6 43.4 43.6 43.5 43.8 43.7 43.643.6T= 15000035.435.4 35.4 35.3 35.4 35.4 35.5 35.6 35.4 35.4 35.535.3 35.235.435.135.835.135.6 35.3 35.835.4 PeTTA(ours)(∗) 35.834.434.735.035.135.135.235.335.335.335.235.335.235.235.135.235.235.235.235.235.1 Table 12: Average classification error comparison between RDumb [45] (a reset-based approach) with different reset frequencies and our PeTTA on DomainNet dataset. Recurring TTA visit− − − − − − − − − − − − − − − − − − − − − − − − − →Reset Every1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 Avg T= 100044.3 44.4 44.3 44.5 44.2 44.2 44.3 44.5 44.4 44.2 44.3 44.3 44.3 44.3 44.5 44.3 44.2 44.3 44.4 44.344.3T= 2423744.1 44.3 43.9 44.2 44.1 44.3 44.2 44.4 44.1 44.1 44.0 44.3 44.1 44.0 44.0 44.2 44.1 44.1 44.1 44.444.1T= 7271244.3 44.3 44.0 44.3 44.1 44.3 44.2 44.4 44.2 44.1 44.0 44.1 44.2 44.1 44.1 44.1 44.1 44.0 44.0 44.344.2 PeTTA(ours)(∗) 43.842.642.342.342.642.842.843.042.942.943.143.042.943.043.043.143.042.842.942.942.9 Discussions. Across datasets and reset frequencies, our PeTTA approach is always better than RDumb [45]. The supreme performance holds even when RDumb has access to the oracle information that can reset the model exactly at the transition between each domain shift or recurrence. Importantly, this oracle information is typically unavailable in practice. 1A slight abuse of notation. T here is the number of images between two consecutive resets, following the notation on Sec. 3 of [45], not the sample indices in our notations. 2A subset of 5, 000 samples from ImageNet-C are selected following RobustBench [10] for a consistent evaluation with other benchmarks. 26Table 13: Average classification error comparison between RDumb [45] (a reset-based approach) with different reset frequencies and our PeTTA on ImageNet-C dataset. Recurring TTA visit− − − − − − − − − − − − − − − − − − − − − − − − − →Reset Every1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 Avg T= 100072.2 73.0 73.2 72.8 72.2 72.8 73.3 72.7 71.9 73.0 73.2 73.1 72.0 72.7 73.3 73.1 72.1 72.6 73.3 73.172.8T= 500070.2 70.8 71.6 72.1 72.4 72.6 72.9 73.1 73.2 73.6 73.7 73.9 74.0 74.0 74.3 74.1 74.1 73.8 73.5 71.973.0T= 7500067.0 67.1 67.2 67.5 67.5 67.6 67.8 67.6 67.6 67.6 67.5 67.7 67.6 67.9 68.1 67.9 67.4 67.5 67.7 67.567.6 PeTTA(ours)(∗) 65.361.759.859.159.459.659.859.359.460.060.361.060.760.460.660.760.860.760.460.260.5 Noteworthy, it is clear that the performance of RDumb varies when changing the choice of the reset frequency. For a given choice of T, the better performance on one dataset does not guarantee the same performance on other datasets. For example, T = 1, 000 - the best empirical value found by RDumb authors [45] on CCC, does not give the best performance on our recurring TTA scenario; the second choice of T negatively impact the performance on many tasks; the third choice gives the best results, but knowing this exact recurrence frequency of the testing stream is unrealistic. The result highlights the challenge in practice when tuning this parameter (too slow/frequent), especially in the TTA setting where a validation set is unavailable. Our PeTTA, in contrast, is reset-free. F.4 PeTTA with 40 Recurring Visits To demonstrate the persistence of PeTTA over an even longer testing stream, in Tab. 14 and Fig. 8, we provide the evaluation results of PeTTA on recurring with 40 recurrences. F.5 The Sensitivity of Hyper-parameter Choices in PeTTA Table 15: Sensitivity of PeTTA with different choices ofλ0. Dataset λ0 = 1e0 λ0 = 5e0 λ0 = 1e1 λ0 = 5e1 λ0 = 1e2 CIFAR-10-C 22.9 22.7 22.8 23.2 24.1 CIFAR-100-C 35.7 35.3 35.1 35.6 36.1 ImageNet-C 61.2 61.0 60.5 61.3 62.4 There are two hyper-parameters in PeTTA: α0 and λ0. The initial learning rate of α0 = 1e−3 is used for all experiments. We do not tune this hyper-parameter, and the choice of α0 is universal across all datasets, following the previous works/compared methods (e.g., RoTTA [61], CoTTA [59]). Since λ0 is more specific to PeTTA, we included a sensitive analysis with different choices of λ0 on PeTTA, evaluated with images from CIFAR-10/100-C and ImageNet-C in Tab. 15. Overall, the choice of λ0 is not extremely sensitive, and while the best value is1e1 on most datasets, other choices such as 5e0 or 5e1 also produce roughly similar performance. Selecting λ0 is intuitive, the larger value of λ0 stronger prevents the model from collapsing but also limits its adaptability as a trade-off. In action, λ0 is an initial value and will be adaptively scaled with the sensing model divergence mechanism in PeTTA, meaning it does not require careful tuning. More generally, this hyper- parameter can be tuned similarly to the hyper-parameters of other TTA approaches, via an additional validation set, or some accuracy prediction algorithm [29] when labeled data is unavailable. F.6 More Details on the Ablation Study We provide the detailed classification error for each visit in the recurring TTA setting of each row entry in Tab. 4 (PeTTA Ablation Study): Tab. 16, Tab. 17, Tab. 18, Tab. 19; and Tab. 5 (PeTTA with various choices of regularizers): Tab. 20, Tab. 21, Tab. 22, Tab. 23. Fig. 9 presents an additional examination of the ablation study conducted on the task CIFAR-100 → CIFAR-100-C [19] for our PeTTA approach. We plot the classification error (top) and the value of ¯γt (bottom) for various PeTTA variations. As the model diverges from the initial state, the value of ¯γt increases. Unable to adjust αt or constraint the probability space via LAL limits the ability of PeTTA to prevent model collapse. In all variations with the model collapse in ablation studies, the rapid saturation of ¯γt is all observed. Therefore, incorporating all components in PeTTA is necessary. 27Table 16: Average classification error of multiple variations of PeTTA. Experiments on CIFAR10→ CIFAR10-C [19] task. Episodic TTA visit− − − − − − − − − − − − − − − − − − − − − − − − − →Method 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 Avg Baseline w/oR(θ) 23.5 24.0 27.4 29.9 33.4 35.6 38.0 40.7 43.1 45.0 46.0 48.6 50.0 49.7 50.8 51.5 52.3 53.3 54.3 55.542.6 R(θ)fixedλ= 0.1λ0 23.5 24.0 27.2 29.8 33.4 35.3 37.9 40.5 43.3 45.3 46.8 49.3 50.9 51.0 52.1 53.2 54.0 54.8 56.0 57.643.3R(θ)fixedλ=λ0 23.5 23.6 26.2 28.4 31.6 33.5 36.4 38.7 41.1 43.1 44.8 47.6 49.3 49.5 50.9 52.1 53.1 54.2 55.6 57.042.0 PeTTA-λt 24.9 25.3 26.0 26.4 27.2 26.5 27.2 27.1 27.4 27.7 27.8 28.0 27.5 28.0 27.7 27.4 27.0 27.6 27.8 27.827.1PeTTA-λt+αt 25.5 24.5 23.7 23.1 23.222.423.3 23.2 23.7 24.1 23.9 24.5 24.3 24.0 23.8 23.9 23.8 24.1 24.6 24.723.9PeTTA-λt+LAL 23.323.9 24.6 25.3 26.2 25.9 26.4 26.6 26.9 26.6 26.7 26.7 26.7 26.8 26.8 27.2 26.9 26.9 26.8 27.026.2 PeTTAαt+LAL 24.323.0 22.6 22.4 22.422.522.3 22.5 22.8 22.8 22.6 22.7 22.7 22.9 22.6 22.7 22.6 22.8 22.9 23.022.8 Table 17: Average classification error of multiple variations of PeTTA. Experiments on CIFAR-100 → CIFAR100-C [19] task. Episodic TTA visit− − − − − − − − − − − − − − − − − − − − − − − − − →Method 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 Avg Baseline w/oR(θ) 40.2 46.3 51.2 54.4 57.3 59.4 61.3 62.6 63.9 65.1 66.3 67.1 68.1 68.9 69.6 70.3 71.1 71.6 72.4 72.963.0 R(θ)fixedλ= 0.1λ0 40.5 46.1 51.5 55.1 58.2 60.5 62.6 64.2 65.7 67.3 68.6 69.5 70.6 71.6 72.5 73.4 74.2 74.9 75.8 76.565.0R(θ)fixedλ=λ0 41.8 47.6 52.6 56.1 58.9 60.7 62.5 63.9 65.0 66.2 67.1 68.3 69.5 70.3 71.4 72.4 73.4 74.1 75.0 75.664.6 PeTTA-λt 39.4 43.4 46.6 49.1 51.0 52.6 53.8 54.7 55.7 56.5 57.1 57.7 58.3 58.8 59.3 59.9 60.6 61.0 61.6 62.155.0PeTTA-λt+αt 39.4 40.1 40.8 40.7 41.2 41.5 41.4 41.6 41.5 41.5 41.7 41.6 41.8 41.7 41.8 42.0 41.9 41.9 42.0 41.841.4PeTTA-λt+LAL 36.2 35.6 35.7 36.1 36.2 36.4 36.4 36.5 36.2 36.2 36.6 36.5 36.5 36.6 36.5 36.6 36.5 36.5 36.3 36.536.3 PeTTAλt+αt+LAL 35.8 34.4 34.7 35.0 35.1 35.1 35.2 35.3 35.3 35.3 35.2 35.3 35.2 35.2 35.1 35.2 35.2 35.2 35.2 35.235.1 Table 18: Average classification error of multiple variations of PeTTA. Experiments onreal → clipart, painting, sketch task from DomainNet [44] task. Recurring TTA visit− − − − − − − − − − − − − − − − − − − − − − − − − →Method 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 Avg Baseline w/oR(θ) 52.3 69.0 68.6 68.6 69.4 70.5 71.8 73.4 75.6 77.6 78.8 81.0 82.8 84.3 85.9 87.4 88.5 89.9 90.8 92.177.9 R(θ)fixedλ= 0.1λ0 52.5 70.0 69.8 70.0 71.1 72.5 74.6 76.1 77.8 80.4 81.9 83.5 85.2 87.2 89.1 90.2 91.5 93.2 94.1 94.980.0R(θ)fixedλ=λ0 54.6 69.8 63.7 56.0 61.7 76.4 70.4 62.5 58.2 76.0 73.6 66.8 58.6 62.3 80.8 75.5 67.0 59.9 59.3 78.366.6 PeTTA-λt 49.2 64.5 62.4 60.9 59.6 58.6 57.7 57.8 57.6 57.7 58.0 58.5 59.0 59.5 59.8 61.1 62.0 62.6 63.6 64.959.7PeTTA-λt+αt 43.942.5 42.3 42.3 42.6 42.843.1 43.7 43.9 44.3 44.6 45.1 45.4 45.7 45.7 46.1 46.1 46.2 46.5 46.444.5PeTTA-λt+LAL 43.6 42.542.6 42.6 42.9 43.0 43.3 43.4 43.1 43.243.143.3 43.3 43.2 43.2 43.9 43.7 43.0 43.2 43.543.2 PeTTAλt+αt+LAL 43.8 42.642.3 42.3 42.6 42.8 42.8 43.0 42.9 42.9 43.1 43.0 42.9 43.0 43.0 43.1 43.0 42.8 42.9 42.942.9 Table 19: Average classification error of multiple variations of PeTTA. Experiments on ImageNet→ ImageNet-C [19] task. Recurring TTA visit− − − − − − − − − − − − − − − − − − − − − − − − − →Method 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 Avg Baseline w/oR(θ) 66.9 61.9 72.7 93.6 97.4 97.8 98.0 98.2 98.3 98.3 98.4 98.4 98.5 98.5 98.6 98.6 98.6 98.6 98.7 98.793.4 R(θ)fixedλ= 0.1λ0 65.5 70.9 79.1 85.2 90.3 92.6 95.8 95.8 95.4 97.3 96.9 97.7 97.9 98.2 98.0 98.7 98.6 98.4 98.4 98.792.5R(θ)fixedλ=λ0 66.5 62.1 73.0 93.5 97.0 97.2 97.5 97.5 97.6 97.5 97.7 97.7 97.7 97.8 97.9 97.9 98.0 98.0 98.0 97.992.9 PeTTA-λt 65.9 62.1 76.3 96.7 97.0 96.9 96.9 96.9 97.0 97.1 97.0 97.2 97.0 97.1 97.1 97.0 97.0 97.0 97.0 97.092.7PeTTA-λt+αt 64.870.5 74.6 75.8 75.5 75.8 76.1 76.2 76.2 76.5 76.7 77.0 76.9 77.4 77.1 77.3 77.2 77.4 77.6 77.475.7PeTTA-λt+LAL 64.8 61.160.0 59.8 60.4 60.4 61.2 61.2 61.8 61.9 62.1 62.2 62.1 62.9 62.1 62.8 62.7 62.1 62.8 66.662.0 PeTTA(ours)(∗) 65.3 61.7 59.8 59.1 59.4 59.6 59.8 59.3 59.4 60.0 60.3 61.0 60.7 60.4 60.6 60.7 60.8 60.7 60.4 60.260.5 Table 20: Average classification error of PeTTA with various choices of regularizers. Experiments on CIFAR-10 → CIFAR-10-C [19] task. Episodic TTA visit− − − − − − − − − − − − − − − − − − − − − − − − − →Method 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 Avg L2 25.6 24.8 23.8 23.1 23.2 22.7 23.0 22.7 22.7 22.7 22.8 22.7 22.8 22.7 22.522.3 22.2 22.4 22.7 22.823.0L2+Fisher25.2 23.7 22.5 21.8 22.3 21.5 22.3 22.1 22.5 22.8 22.6 22.622.622.8 22.6 22.9 22.6 22.9 23.0 23.322.7 Cosine 24.3 23.022.6 22.4 22.4 22.5 22.3 22.5 22.8 22.8 22.6 22.7 22.7 22.9 22.6 22.7 22.6 22.8 22.9 23.022.8Cosine+Fisher25.1 23.822.2 21.6 22.0 21.4 22.0 21.8 22.1 22.3 22.5 22.4 22.6 22.6 22.422.7 22.6 22.8 22.8 23.322.6 Table 21: Average classification error of PeTTA with various choices of regularizers. Experiments on CIFAR-100 → CIFAR-100-C [19] task. Recurring TTA visit− − − − − − − − − − − − − − − − − − − − − − − − − →Method 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 Avg L2 36.9 35.5 35.5 35.5 35.7 35.6 35.6 35.5 35.5 35.4 35.6 35.5 35.7 35.7 35.7 35.7 35.8 35.5 35.4 35.535.6L2+Fisher36.8 35.4 35.4 35.8 35.9 36.0 35.9 35.9 35.9 35.8 36.1 36.1 36.1 36.1 36.1 36.1 36.2 36.0 36.0 35.936.0 Cosine 35.8 34.4 34.7 35.0 35.1 35.1 35.2 35.3 35.3 35.3 35.2 35.3 35.2 35.2 35.1 35.2 35.2 35.2 35.2 35.235.1Cosine+Fisher36.7 35.2 35.5 35.6 35.9 35.9 36.1 36.0 36.0 35.9 36.0 36.0 36.0 36.1 36.0 36.0 35.9 35.9 35.9 36.035.9 28Table 22: Average classification error of PeTTA with various choices of regularizers. Experiments on real → clipart, painting, sketch task from DomainNet [44] dataset. Recurring TTA visit− − − − − − − − − − − − − − − − − − − − − − − − − →Method 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 Avg L2 43.8 42.7 42.5 42.4 42.8 42.9 43.0 43.1 43.1 43.2 43.4 43.3 43.2 43.3 43.2 43.2 43.4 43.0 43.1 43.143.1L2+Fisher43.9 42.8 42.7 43.0 43.2 43.4 43.6 43.8 43.9 44.1 44.0 44.2 44.2 44.2 44.4 44.4 44.5 44.5 44.5 44.543.9 Cosine 43.8 42.642.3 42.3 42.6 42.8 42.8 43.0 42.9 42.9 43.1 43.0 42.9 43.0 43.0 43.1 43.0 42.8 42.9 42.942.9Cosine+Fisher43.7 42.542.5 42.6 42.9 43.2 43.2 43.5 43.4 43.5 43.4 43.5 43.4 43.6 43.5 43.5 43.4 43.5 43.3 43.443.3 Table 23: Average classification error of PeTTA with various choices of regularizers. Experiments on ImageNet → ImageNet-C [19] task. Recurring TTA visit− − − − − − − − − − − − − − − − − − − − − − − − − →Method 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 Avg L2 70.8 72.2 71.5 69.8 72.3 69.3 70.3 70.5 70.0 70.8 70.2 72.1 71.4 70.8 70.9 70.9 69.7 71.0 71.1 70.470.8L2+Fisher70.5 70.0 69.5 69.4 69.6 69.9 69.2 69.3 72.2 70.4 71.0 70.5 71.7 71.5 71.3 68.4 68.6 68.8 68.7 68.770.0 Cosine 65.361.7 59.8 59.1 59.4 59.6 59.8 59.3 59.4 60.0 60.3 61.0 60.7 60.4 60.6 60.7 60.8 60.7 60.4 60.260.5Cosine+Fisher65.1 61.760.9 61.2 61.9 62.6 62.8 63.2 64.2 63.4 64.3 64.4 63.9 64.3 65.8 65.5 64.9 65.0 65.2 65.263.8 F.7 More Confusion Matrices in Recurring TTA Setting For the task CIFAR-10→ CIFAR-10-C [19] in recurring TTA setting (with 20 visits), we additionally showcase the confusion matrix of RoTTA [61] (Fig. 10) and our proposed PeTTA (Fig. 11) at each visit. Our PeTTA persistently achieves competitive performance across 20 visits while RoTTA [61] gradually degrades. G Experimental Details G.1 Computing Resources A computer cluster equipped with an Intel(R) Core(TM) 3.80GHz i7-10700K CPU, 64 GB RAM, and one NVIDIA GeForce RTX 3090 GPU (24 GB VRAM) is used for our experiments. G.2 Experiments on CCC Testing Stream In this section, we further evaluate the performance of our PeTTA on the testing data stream of Continuous Changing Corruption (CCC) [ 45] setting. Here we use the baseline accuracy 20%, transition speed 1000, and random seed 44.3 The compared methods are source model (ResNet 50), PeTTA, RoTTA [61], and RDumb [45]. Noteworthy, different from recurring TTA, the class labels here are i.i.d. distributed. The adaptation configuration of PeTTA follows the same settings as used on ImageNet-C, while the same setting introduced in Sec. F.3, with T = 1000 is used for RDumb [45]. G.3 Test-time Adaptation Methods Pre-trained Model on Source Distribution. Following previous studies [57, 61, 12, 59], only the batch norm layers are updated. As stated in Sec. 5.2, RobustBench [10] and torchvision [35] provide pre-trained models trained on source distributions. Specifically, for ImageNet-C and Do- mainNet experiments, a ResNet50 model [17] pre-trained on ImageNet V2 (specifically, checkpoint ResNet50_Weights.IMAGENET1K_V2 of torchvision) is used. From RobustBench, the model with checkpoint Standard and Hendrycks2020AugMix_ResNeXt [20] are adopted for CIFAR10-C and CIFAR-100-C experiments, respectively. Lastly, experiments on DomainNet dataset utilize the checkpoint (best_real_2020) provided in AdaContrast [8] study.4 Optimizer. Without specifically stated, Adam [26] optimizer with learning rate equal 1e−3, and β = (0.9, 0.999) is selected as a universal choice for all experiments. More Details on PeTTA. Since designing the batch normalization layers, and the memory bank is not the key focus of PeTTA, we conveniently adopt the implementation of the Robust Batch Norm layer and the Category-balanced Sampling strategy using a memory bank introduced in RoTTA [61]. 3https://github.com/oripress/CCC 4https://github.com/DianCh/AdaContrast 29G.4 The Use of Existing Assets Many components of PeTTA is utilized from the official repository of RoTTA [61] 5 and RMT [12]. 6 These two assets are released under MIT license. All the datasets, including CIFAR-10-C, CIFAR- 100-C and ImageNet-C [ 19] are publicly available online, released under Apache-2.0 license. 7 DomainNet dataset [44] (cleaned version) is also released for research purposes.8 5https://github.com/BIT-DA/RoTTA 6https://github.com/mariodoebler/test-time-adaptation 7https://github.com/hendrycks/robustness 8https://ai.bu.edu/M3SDA/ 300 10000 20000 30000 40000 Test-time adaptation step (t) 0.40 0.60 0.80 1.00 ¯γt 0 10000 20000 30000 40000 Test-time adaptation step (t) 4.00 6.00 8.00 10.00λt 0 10000 20000 30000 40000 Test-time adaptation step (t) 0.00 0.25 0.50 0.75 1.00αt 1e 3 0 10000 20000 30000 40000 Test-time adaptation step (t) 0.00 1.00 2.00 3.00 4.00LCLS 0 10000 20000 30000 40000 Test-time adaptation step (t) 0.00 2.50 5.00 7.50 10.00LAL 0 10000 20000 30000 40000 Test-time adaptation step (t) 0.00 0.50 1.00 1.50 2.00 R(θ) 0 10000 20000 30000 40000 Test-time adaptation step (t) 0.00 0.25 0.50 0.75 1.00Testing error Figure 7: An inspection of PeTTA on the task CIFAR-10 → CIFAR-10-C [19] in a recurring with 20 visits (visits are separated by the vertical dashed lines). Here, we visualize (rows 1-3) the dynamic of PeTTA adaptive parameters (¯γt, λt, αt), (rows 4-5) the value of the loss functions (LCLS, LAL) and (row 6) the value of the regularization term (R(θ)) and (row 7) the classification error rate at each step. The solid line in the foreground of each plot denotes the running mean. The plots show an adaptive change of λt, αt through time in PeTTA, which stabilizes TTA performance, making PeTTA achieve a persisting adaptation process in all observed values across 20 visits. 31Figure 8: Testing error of PeTTA with 40 recurring TTA visits. Total Visits CF-10-C CF-100-C IN-C 20 visits 22.8 35.1 60.5 40 visits 22.9 35.1 61.0 Table 14: Average testing error of PeTTA in recurring TTA with 20 and 40 visits. PeTTA demonstrates its persistence over an extended testing time horizon beyond the 20 th visit milestone (Fig. 8’s horizontal dashed line). 0 10000 20000 30000 40000 Test-time adaptation step (t) 0.20 0.30 0.40 0.50 0.60 0.70 0.80Testing Error PeTTA - λt Baseline w/o R(θ) PeTTA - λt + αt R(θ) fixed λ= 0.1λ0 PeTTA - λt + LAL R(θ) fixed λ= λ0 PeTTA - λt  + αt  + LAL 0 10000 20000 30000 40000 Test-time adaptation step (t) 0.20 0.40 0.60 0.80 1.00 ¯γt PeTTA - λt PeTTA - λt + αt PeTTA - λt + LAL PeTTA - λt  + αt  + LAL Figure 9: An inspection on the ablation study of multiple variations of PeTTA on the task CIFAR-100 → CIFAR-100-C [19] in an episodic TTA with 20 visits (visits are separated by the vertical dashed lines). (top): testing error of multiple variations of PeTTA. The performance of PeTTA without (w/o) R(θ), or fixed regularization coefficient ( λ = λ0/0.1λ0) degrades through time (the top 3 lines). The degradation of PeTTA -λt is still happening but at a slower rate (justification below). The performance of the other three variations persists through time with PeTTA -λt + αt + LAL achieves the best performance. (bottom): changes of ¯γt in multiple variations of PeTTA. When limiting the degree of freedom in adjusting αt or lacking of supervision from LAL (e.g., PeTTA -λt + αt, PeTTA -λt + LAL, and especially PeTTA -λt), the value of γt, unfortunately, escalates and eventually saturated. After this point, PeTTA has the same effect as using a fixed regularization coefficient. Therefore, fully utilizing all components is necessary to preserve the persistence of PeTTA. Best viewed in color. 320: airplane 1: auto2: bird3: cat4: deer5: dog6: frog7: horse8: ship9: truck 0 1 2 3 4 5 6 7 8 9 0.79 0.01 0.04 0.03 0.02 0.01 0.01 0.02 0.05 0.02 0.02 0.82 0.01 0.01 0 0.01 0.01 0.01 0.01 0.09 0.06 0 0.68 0.07 0.04 0.03 0.06 0.03 0.01 0.01 0.02 0.01 0.04 0.66 0.04 0.08 0.07 0.05 0.01 0.02 0.03 0 0.04 0.06 0.68 0.02 0.06 0.09 0.01 0.01 0.03 0 0.05 0.15 0.03 0.61 0.03 0.07 0.01 0.01 0.02 0.01 0.03 0.07 0.02 0.02 0.8 0.02 0 0.01 0.01 0 0.02 0.03 0.03 0.02 0.01 0.87 0 0.01 0.09 0.02 0.02 0.02 0.01 0 0.02 0.01 0.77 0.04 0.03 0.03 0.01 0.01 0 0 0.01 0.01 0.03 0.85 0: airplane 1: auto2: bird3: cat4: deer5: dog6: frog7: horse8: ship9: truck 0 1 2 3 4 5 6 7 8 9 0.76 0.01 0.03 0.03 0.01 0 0.03 0.02 0.07 0.03 0.02 0.76 0 0.01 0 0 0.03 0.01 0.02 0.16 0.07 0 0.63 0.08 0.06 0.02 0.08 0.04 0.01 0.01 0.02 0 0.04 0.7 0.04 0.04 0.09 0.05 0.01 0.02 0.03 0 0.03 0.05 0.73 0.01 0.06 0.08 0.01 0.01 0.01 0 0.03 0.23 0.04 0.53 0.06 0.08 0.01 0.01 0.02 0 0.02 0.1 0.02 0.01 0.81 0.01 0 0.01 0.01 0 0.01 0.05 0.03 0.01 0.01 0.87 0 0.01 0.08 0.01 0.01 0.02 0.01 0 0.02 0.01 0.8 0.04 0.03 0.02 0.01 0.02 0 0 0.02 0.01 0.02 0.87 0: airplane 1: auto2: bird3: cat4: deer5: dog6: frog7: horse8: ship9: truck 0 1 2 3 4 5 6 7 8 9 0.7 0.01 0.03 0.04 0.02 0 0.03 0.03 0.09 0.06 0.01 0.72 0 0.01 0 0 0.04 0 0.01 0.2 0.07 0 0.56 0.1 0.08 0.02 0.09 0.04 0.01 0.02 0.01 0 0.03 0.7 0.05 0.02 0.13 0.04 0 0.02 0.04 0 0.03 0.07 0.69 0 0.08 0.07 0.01 0.01 0.01 0 0.04 0.26 0.05 0.42 0.13 0.07 0 0.01 0.01 0 0.02 0.11 0.03 0 0.8 0.01 0 0.01 0.01 0 0.02 0.06 0.05 0.01 0.04 0.8 0 0.01 0.07 0.01 0.01 0.03 0.01 0 0.03 0.01 0.78 0.05 0.02 0.01 0.01 0.02 0.01 0 0.04 0.01 0.02 0.86 0: airplane 1: auto2: bird3: cat4: deer5: dog6: frog7: horse8: ship9: truck 0 1 2 3 4 5 6 7 8 9 0.62 0.01 0.03 0.06 0.03 0 0.05 0.04 0.09 0.08 0.01 0.66 0 0.02 0.01 0 0.04 0 0.02 0.25 0.07 0 0.48 0.13 0.1 0.02 0.13 0.03 0.01 0.02 0.01 0 0.02 0.68 0.05 0.02 0.17 0.03 0 0.02 0.03 0 0.02 0.07 0.67 0 0.12 0.07 0.01 0.01 0.01 0 0.02 0.29 0.07 0.39 0.14 0.06 0 0.01 0.01 0 0.01 0.11 0.04 0 0.8 0.01 0 0.01 0.01 0 0.02 0.08 0.06 0.01 0.06 0.75 0 0.01 0.05 0.01 0.01 0.04 0.02 0 0.05 0.01 0.74 0.07 0.01 0.01 0 0.03 0.01 0 0.05 0.01 0.02 0.86 True label 1st visit 2nd visit 3rd visit 4th visit 0: airplane 1: auto2: bird3: cat4: deer5: dog6: frog7: horse8: ship9: truck 0 1 2 3 4 5 6 7 8 9 0.56 0 0.03 0.07 0.04 0 0.07 0.04 0.1 0.1 0.01 0.61 0 0.01 0.01 0 0.07 0 0.02 0.26 0.08 0 0.42 0.13 0.13 0.02 0.15 0.03 0.01 0.02 0.02 0 0.01 0.62 0.06 0.02 0.21 0.03 0 0.02 0.03 0 0.02 0.06 0.66 0 0.16 0.06 0.01 0.01 0.01 0 0.02 0.3 0.08 0.34 0.17 0.06 0 0.02 0.01 0 0.01 0.12 0.07 0 0.76 0.01 0 0.02 0.01 0 0.02 0.1 0.08 0.01 0.08 0.69 0 0.02 0.05 0.01 0.01 0.05 0.02 0 0.09 0.01 0.68 0.09 0.01 0.01 0 0.03 0.02 0 0.09 0.01 0.02 0.83 0: airplane 1: auto2: bird3: cat4: deer5: dog6: frog7: horse8: ship9: truck 0 1 2 3 4 5 6 7 8 9 0.51 0 0.02 0.07 0.04 0 0.09 0.03 0.1 0.14 0.01 0.56 0 0.01 0.02 0 0.09 0 0.02 0.29 0.08 0 0.35 0.15 0.16 0.02 0.18 0.03 0.01 0.03 0.02 0 0.01 0.57 0.07 0.02 0.27 0.02 0 0.03 0.04 0 0.01 0.08 0.62 0 0.18 0.05 0.01 0.01 0.01 0 0.01 0.29 0.09 0.3 0.21 0.05 0 0.02 0.01 0 0.01 0.12 0.09 0 0.75 0 0 0.01 0.02 0 0.01 0.11 0.12 0.01 0.1 0.6 0 0.03 0.06 0.01 0 0.04 0.02 0 0.09 0 0.66 0.11 0.01 0.01 0 0.02 0.03 0 0.11 0 0.02 0.8 0: airplane 1: auto2: bird3: cat4: deer5: dog6: frog7: horse8: ship9: truck 0 1 2 3 4 5 6 7 8 9 0.48 0 0.02 0.08 0.04 0 0.11 0.03 0.11 0.13 0.01 0.54 0 0.01 0.02 0 0.11 0 0.02 0.28 0.09 0 0.3 0.16 0.16 0.02 0.21 0.02 0.01 0.03 0.02 0 0.01 0.51 0.08 0.01 0.33 0.01 0.01 0.02 0.03 0 0.01 0.05 0.65 0 0.21 0.03 0.01 0.01 0.02 0 0.01 0.27 0.11 0.25 0.28 0.03 0 0.02 0.01 0 0.01 0.12 0.1 0 0.75 0 0 0.01 0.02 0 0.01 0.11 0.13 0.01 0.13 0.56 0 0.03 0.06 0 0 0.06 0.03 0 0.13 0 0.6 0.11 0.02 0.01 0 0.03 0.04 0 0.15 0 0.02 0.73 0: airplane 1: auto2: bird3: cat4: deer5: dog6: frog7: horse8: ship9: truck 0 1 2 3 4 5 6 7 8 9 0.46 0 0.01 0.07 0.06 0 0.13 0.01 0.09 0.15 0.01 0.48 0 0.01 0.04 0 0.16 0 0.01 0.28 0.09 0 0.27 0.15 0.19 0.01 0.23 0.01 0.01 0.03 0.02 0 0.01 0.44 0.12 0.01 0.37 0.01 0.01 0.02 0.04 0 0.01 0.05 0.63 0 0.23 0.02 0.01 0.01 0.02 0 0.01 0.25 0.13 0.22 0.33 0.02 0 0.01 0.01 0 0 0.11 0.15 0 0.71 0 0 0.01 0.02 0 0.01 0.09 0.22 0 0.15 0.47 0 0.02 0.08 0 0 0.06 0.05 0 0.15 0 0.55 0.1 0.02 0.01 0 0.04 0.05 0 0.16 0 0.02 0.7 True label 5th visit 6th visit 7th visit 8th visit 0: airplane 1: auto2: bird3: cat4: deer5: dog6: frog7: horse8: ship9: truck 0 1 2 3 4 5 6 7 8 9 0.47 0 0.01 0.06 0.06 0 0.13 0.01 0.1 0.16 0.02 0.47 0 0.01 0.04 0 0.13 0 0.03 0.29 0.1 0 0.24 0.12 0.22 0.01 0.24 0.01 0.01 0.03 0.03 0 0 0.4 0.12 0.01 0.39 0 0.01 0.02 0.05 0 0.01 0.06 0.61 0 0.23 0.02 0.01 0.01 0.03 0 0.01 0.22 0.15 0.2 0.35 0.02 0 0.02 0.01 0 0 0.11 0.15 0 0.7 0 0.01 0.01 0.03 0 0.01 0.08 0.25 0 0.15 0.44 0.01 0.03 0.09 0 0 0.04 0.07 0 0.14 0 0.55 0.1 0.02 0.01 0 0.03 0.05 0 0.16 0 0.02 0.7 0: airplane 1: auto2: bird3: cat4: deer5: dog6: frog7: horse8: ship9: truck 0 1 2 3 4 5 6 7 8 9 0.46 0 0.01 0.05 0.07 0 0.14 0.01 0.1 0.16 0.04 0.43 0 0.02 0.07 0 0.14 0 0.03 0.27 0.11 0 0.22 0.11 0.23 0.01 0.26 0.01 0.02 0.03 0.04 0 0 0.33 0.16 0.01 0.43 0 0.01 0.02 0.05 0 0 0.03 0.66 0 0.23 0.01 0.01 0.01 0.04 0 0.01 0.22 0.15 0.18 0.37 0.01 0.01 0.02 0.01 0 0 0.1 0.19 0 0.68 0 0.01 0.01 0.03 0 0.01 0.08 0.28 0.01 0.16 0.41 0.01 0.03 0.11 0 0 0.04 0.05 0 0.14 0 0.56 0.09 0.04 0.01 0 0.02 0.08 0 0.18 0 0.02 0.65 0: airplane 1: auto2: bird3: cat4: deer5: dog6: frog7: horse8: ship9: truck 0 1 2 3 4 5 6 7 8 9 0.47 0 0.01 0.04 0.07 0 0.14 0 0.1 0.16 0.04 0.42 0 0.01 0.07 0 0.15 0 0.05 0.26 0.11 0 0.21 0.1 0.26 0.01 0.26 0 0.02 0.03 0.05 0 0 0.31 0.18 0.01 0.42 0 0.01 0.02 0.06 0 0 0.04 0.65 0 0.21 0.01 0.01 0.02 0.04 0 0.01 0.17 0.21 0.15 0.39 0.01 0.01 0.02 0.01 0 0 0.1 0.24 0 0.64 0 0.01 0.01 0.04 0 0.01 0.09 0.28 0 0.16 0.39 0 0.03 0.14 0 0 0.03 0.07 0 0.14 0 0.52 0.09 0.05 0.01 0 0.03 0.1 0 0.18 0 0.03 0.61 0: airplane 1: auto2: bird3: cat4: deer5: dog6: frog7: horse8: ship9: truck 0 1 2 3 4 5 6 7 8 9 0.49 0 0.01 0.03 0.06 0 0.14 0 0.11 0.17 0.07 0.4 0 0.01 0.07 0 0.12 0 0.07 0.27 0.13 0 0.19 0.08 0.27 0.01 0.25 0 0.02 0.03 0.07 0 0 0.27 0.19 0 0.43 0 0.02 0.03 0.07 0 0 0.02 0.64 0 0.23 0.01 0.01 0.01 0.06 0 0.01 0.19 0.18 0.13 0.39 0.01 0.01 0.02 0.02 0 0 0.09 0.22 0 0.65 0 0.01 0.01 0.05 0 0 0.07 0.32 0 0.15 0.36 0.01 0.04 0.17 0 0 0.03 0.07 0 0.12 0 0.53 0.08 0.06 0.01 0 0.01 0.13 0 0.17 0 0.03 0.59 True label 9th visit 10th visit 11th visit 12th visit 0: airplane 1: auto2: bird3: cat4: deer5: dog6: frog7: horse8: ship9: truck 0 1 2 3 4 5 6 7 8 9 0.5 0 0 0.02 0.08 0 0.13 0 0.1 0.15 0.09 0.37 0 0.01 0.11 0 0.11 0 0.08 0.24 0.15 0 0.18 0.07 0.31 0.01 0.24 0 0.03 0.02 0.09 0 0 0.24 0.17 0 0.44 0 0.02 0.03 0.08 0 0 0.02 0.66 0 0.19 0.01 0.02 0.02 0.08 0 0.01 0.15 0.23 0.11 0.38 0.01 0.01 0.02 0.02 0 0 0.08 0.31 0 0.55 0 0.02 0.01 0.05 0 0 0.05 0.37 0 0.14 0.34 0.01 0.04 0.2 0 0 0.03 0.06 0 0.12 0 0.52 0.08 0.08 0.01 0 0.01 0.11 0 0.15 0 0.04 0.59 0: airplane 1: auto2: bird3: cat4: deer5: dog6: frog7: horse8: ship9: truck 0 1 2 3 4 5 6 7 8 9 0.54 0 0 0.02 0.06 0 0.11 0 0.12 0.15 0.13 0.35 0 0.01 0.1 0 0.09 0 0.12 0.21 0.16 0 0.18 0.07 0.29 0.01 0.24 0 0.03 0.02 0.11 0 0 0.22 0.19 0 0.42 0 0.03 0.03 0.08 0 0 0.03 0.65 0 0.2 0.01 0.02 0.01 0.09 0 0.01 0.12 0.29 0.08 0.37 0 0.02 0.02 0.02 0 0 0.09 0.29 0 0.56 0 0.02 0.01 0.06 0 0 0.05 0.39 0 0.13 0.32 0.01 0.04 0.23 0 0 0.02 0.07 0 0.1 0 0.51 0.07 0.12 0.01 0 0.01 0.11 0 0.13 0 0.05 0.57 0: airplane 1: auto2: bird3: cat4: deer5: dog6: frog7: horse8: ship9: truck 0 1 2 3 4 5 6 7 8 9 0.56 0 0 0.02 0.08 0 0.1 0 0.12 0.12 0.18 0.32 0 0 0.11 0 0.08 0 0.13 0.19 0.18 0 0.15 0.05 0.34 0 0.2 0 0.04 0.02 0.12 0 0 0.19 0.27 0 0.36 0 0.04 0.02 0.09 0 0 0.02 0.69 0 0.15 0.01 0.02 0.02 0.11 0 0 0.1 0.33 0.07 0.33 0 0.03 0.01 0.03 0 0 0.09 0.35 0 0.5 0 0.02 0.01 0.08 0 0 0.04 0.43 0 0.1 0.29 0.01 0.04 0.26 0 0 0.02 0.08 0 0.08 0 0.51 0.06 0.15 0.01 0 0.01 0.12 0 0.1 0 0.07 0.55 0: airplane 1: auto2: bird3: cat4: deer5: dog6: frog7: horse8: ship9: truck 0 1 2 3 4 5 6 7 8 9 0.58 0 0 0.01 0.07 0 0.09 0 0.13 0.11 0.16 0.32 0 0 0.11 0 0.07 0 0.16 0.18 0.18 0 0.15 0.05 0.36 0 0.19 0 0.04 0.02 0.14 0 0 0.18 0.26 0 0.35 0 0.05 0.02 0.1 0 0 0.01 0.69 0 0.15 0.01 0.03 0.01 0.11 0 0 0.1 0.36 0.05 0.32 0 0.04 0.01 0.03 0 0 0.08 0.38 0 0.46 0 0.03 0.01 0.09 0 0 0.04 0.43 0 0.09 0.29 0.02 0.04 0.29 0 0 0.02 0.09 0 0.08 0 0.47 0.06 0.18 0.01 0 0.01 0.11 0 0.08 0 0.1 0.5 True label 13th visit 14th visit 15th visit 16th visit 0: airplane 1: auto2: bird3: cat4: deer5: dog6: frog7: horse8: ship9: truck 0 1 2 3 4 5 6 7 8 9 0.6 0 0 0.01 0.08 0 0.08 0 0.13 0.1 0.2 0.28 0 0 0.1 0 0.06 0 0.19 0.17 0.2 0 0.14 0.05 0.36 0 0.18 0 0.05 0.02 0.17 0 0 0.16 0.28 0 0.29 0 0.08 0.02 0.1 0 0 0.01 0.71 0 0.11 0.01 0.04 0.02 0.13 0 0 0.1 0.4 0.04 0.27 0 0.05 0.01 0.04 0 0 0.09 0.4 0 0.41 0 0.04 0.01 0.1 0 0 0.04 0.45 0 0.07 0.27 0.03 0.04 0.34 0 0 0.01 0.08 0 0.05 0 0.47 0.05 0.22 0.01 0 0.01 0.13 0 0.06 0 0.12 0.44 0: airplane 1: auto2: bird3: cat4: deer5: dog6: frog7: horse8: ship9: truck 0 1 2 3 4 5 6 7 8 9 0.62 0 0 0.01 0.09 0 0.08 0 0.13 0.08 0.24 0.26 0 0 0.1 0 0.05 0 0.19 0.15 0.2 0 0.13 0.04 0.41 0 0.16 0 0.05 0.02 0.16 0 0 0.14 0.3 0 0.29 0 0.09 0.02 0.11 0 0 0.01 0.7 0 0.1 0.01 0.05 0.02 0.14 0 0 0.09 0.42 0.02 0.27 0 0.05 0.01 0.03 0 0 0.09 0.44 0 0.39 0 0.04 0.01 0.12 0 0 0.04 0.43 0 0.06 0.28 0.03 0.04 0.35 0 0 0.01 0.07 0 0.06 0 0.46 0.04 0.26 0.01 0 0.01 0.13 0 0.06 0 0.13 0.41 0: airplane 1: auto2: bird3: cat4: deer5: dog6: frog7: horse8: ship9: truck 0 1 2 3 4 5 6 7 8 9 0.67 0 0 0 0.1 0 0.05 0 0.11 0.06 0.3 0.21 0 0 0.1 0 0.03 0 0.21 0.13 0.26 0 0.11 0.04 0.4 0 0.11 0 0.06 0.02 0.2 0 0 0.13 0.32 0 0.21 0 0.12 0.02 0.13 0 0 0.01 0.72 0 0.07 0.01 0.05 0.01 0.2 0 0 0.09 0.42 0.01 0.19 0 0.08 0.01 0.04 0 0 0.08 0.49 0 0.3 0 0.07 0.01 0.16 0 0 0.03 0.45 0 0.04 0.24 0.05 0.03 0.42 0 0 0.01 0.06 0 0.04 0 0.43 0.04 0.33 0.01 0 0 0.11 0 0.03 0 0.15 0.36 0: airplane 1: auto2: bird3: cat4: deer5: dog6: frog7: horse8: ship9: truck 0 1 2 3 4 5 6 7 8 9 0.69 0 0 0 0.08 0 0.04 0 0.13 0.05 0.34 0.17 0 0 0.1 0 0.03 0 0.23 0.13 0.24 0 0.1 0.03 0.44 0 0.11 0 0.07 0.01 0.21 0 0 0.11 0.32 0 0.21 0 0.14 0.01 0.14 0 0 0.01 0.71 0 0.06 0.01 0.06 0.01 0.21 0 0 0.07 0.44 0 0.16 0 0.1 0.01 0.05 0 0 0.08 0.51 0 0.25 0 0.1 0.01 0.17 0 0 0.03 0.46 0 0.04 0.21 0.06 0.04 0.46 0 0 0.01 0.06 0 0.03 0 0.41 0.03 0.34 0 0 0 0.12 0 0.03 0 0.18 0.32 Predicted label Predicted label Predicted label Predicted label True label 17th visit 18th visit 19th visit 20th visit Figure 10: The dynamic of the confusion matrix of RoTTA [61] in episodic TTA with 20 visits. 330: airplane 1: auto2: bird3: cat4: deer5: dog6: frog7: horse8: ship9: truck 0 1 2 3 4 5 6 7 8 9 0.77 0.01 0.04 0.03 0.03 0.01 0.02 0.02 0.05 0.02 0.02 0.84 0.01 0.02 0 0.01 0.02 0.01 0.02 0.06 0.04 0 0.69 0.07 0.05 0.05 0.05 0.02 0.01 0.01 0.04 0.01 0.05 0.62 0.05 0.1 0.06 0.04 0.01 0.02 0.03 0 0.06 0.07 0.68 0.05 0.04 0.05 0.01 0.01 0.01 0 0.04 0.14 0.03 0.7 0.03 0.04 0.01 0.01 0.01 0.01 0.04 0.06 0.03 0.03 0.78 0.01 0.01 0.01 0.03 0 0.03 0.04 0.04 0.04 0.01 0.79 0.01 0.01 0.08 0.02 0.02 0.02 0.01 0.01 0.02 0.01 0.8 0.03 0.03 0.05 0.02 0.02 0.01 0.01 0.01 0.01 0.03 0.82 0: airplane 1: auto2: bird3: cat4: deer5: dog6: frog7: horse8: ship9: truck 0 1 2 3 4 5 6 7 8 9 0.77 0.01 0.04 0.03 0.02 0.01 0.03 0.01 0.06 0.02 0.01 0.87 0.01 0.01 0 0.01 0.01 0 0.02 0.05 0.04 0 0.7 0.09 0.05 0.03 0.06 0.02 0.01 0.01 0.03 0.01 0.06 0.64 0.05 0.08 0.06 0.04 0.01 0.02 0.02 0 0.05 0.06 0.74 0.03 0.05 0.04 0.01 0.01 0.01 0 0.05 0.15 0.04 0.66 0.04 0.04 0.01 0.01 0.02 0.01 0.04 0.06 0.02 0.02 0.78 0.01 0.01 0.03 0.02 0 0.03 0.05 0.05 0.03 0.01 0.81 0 0.01 0.05 0.02 0.01 0.02 0.01 0 0.02 0.01 0.83 0.03 0.02 0.05 0.01 0.02 0.01 0.01 0.01 0.01 0.03 0.83 0: airplane 1: auto2: bird3: cat4: deer5: dog6: frog7: horse8: ship9: truck 0 1 2 3 4 5 6 7 8 9 0.74 0.01 0.05 0.03 0.02 0 0.03 0.01 0.06 0.02 0.02 0.87 0.01 0.02 0 0 0.01 0 0.02 0.05 0.05 0 0.7 0.07 0.05 0.03 0.06 0.02 0.01 0.01 0.02 0.01 0.05 0.68 0.05 0.07 0.07 0.03 0.01 0.02 0.02 0 0.05 0.06 0.77 0.02 0.04 0.03 0 0 0.01 0 0.07 0.15 0.04 0.65 0.04 0.03 0.01 0.01 0.01 0 0.03 0.07 0.03 0.02 0.83 0.01 0 0.01 0.01 0 0.03 0.04 0.04 0.02 0.01 0.82 0 0.01 0.06 0.02 0.01 0.02 0.01 0 0.02 0 0.85 0.02 0.02 0.05 0.01 0.02 0.01 0 0.01 0.01 0.03 0.85 0: airplane 1: auto2: bird3: cat4: deer5: dog6: frog7: horse8: ship9: truck 0 1 2 3 4 5 6 7 8 9 0.76 0.01 0.05 0.04 0.02 0 0.02 0.01 0.07 0.03 0.01 0.87 0.01 0.01 0 0 0.01 0 0.02 0.05 0.04 0 0.73 0.06 0.05 0.03 0.06 0.01 0.01 0.01 0.01 0.01 0.05 0.71 0.05 0.06 0.06 0.03 0.01 0.01 0.02 0 0.04 0.05 0.78 0.02 0.04 0.03 0.01 0 0.01 0 0.06 0.17 0.04 0.64 0.04 0.03 0.01 0.01 0.01 0 0.03 0.06 0.03 0.01 0.85 0.01 0 0.01 0.01 0 0.04 0.04 0.05 0.02 0.01 0.81 0.01 0.01 0.05 0.02 0.01 0.02 0.01 0 0.02 0 0.84 0.02 0.02 0.05 0.01 0.02 0.01 0 0.02 0.01 0.04 0.83 True label 1st visit 2nd visit 3rd visit 4th visit 0: airplane 1: auto2: bird3: cat4: deer5: dog6: frog7: horse8: ship9: truck 0 1 2 3 4 5 6 7 8 9 0.76 0.02 0.04 0.04 0.02 0 0.02 0.01 0.08 0.02 0.02 0.86 0.01 0.02 0 0 0.01 0 0.02 0.05 0.04 0 0.73 0.07 0.05 0.03 0.05 0.01 0.01 0.01 0.01 0.01 0.06 0.69 0.05 0.06 0.07 0.02 0.01 0.01 0.02 0 0.05 0.07 0.76 0.02 0.04 0.03 0.01 0 0.01 0 0.07 0.17 0.04 0.64 0.03 0.03 0.01 0.01 0.01 0 0.03 0.07 0.02 0.01 0.84 0.01 0.01 0.01 0.01 0 0.04 0.04 0.06 0.02 0.01 0.81 0.01 0.01 0.04 0.02 0.02 0.02 0.01 0 0.02 0 0.86 0.02 0.02 0.05 0.02 0.02 0.01 0 0.02 0.01 0.03 0.82 0: airplane 1: auto2: bird3: cat4: deer5: dog6: frog7: horse8: ship9: truck 0 1 2 3 4 5 6 7 8 9 0.74 0.01 0.05 0.04 0.02 0 0.03 0.01 0.07 0.02 0.01 0.88 0.01 0.01 0 0 0.01 0 0.02 0.05 0.05 0 0.74 0.07 0.05 0.02 0.05 0.01 0.01 0 0.01 0 0.05 0.7 0.06 0.06 0.07 0.02 0.01 0.01 0.01 0 0.04 0.06 0.79 0.02 0.04 0.02 0.01 0 0.01 0 0.06 0.17 0.04 0.65 0.04 0.03 0.01 0.01 0.01 0 0.03 0.07 0.02 0.01 0.85 0 0 0.01 0.01 0 0.04 0.04 0.06 0.02 0.01 0.8 0.01 0.01 0.04 0.02 0.01 0.02 0.01 0 0.02 0 0.87 0.02 0.02 0.05 0.02 0.02 0 0 0.02 0.01 0.04 0.83 0: airplane 1: auto2: bird3: cat4: deer5: dog6: frog7: horse8: ship9: truck 0 1 2 3 4 5 6 7 8 9 0.76 0.01 0.04 0.04 0.02 0 0.03 0.01 0.07 0.02 0.01 0.88 0.01 0.01 0 0 0.01 0 0.02 0.05 0.04 0 0.74 0.06 0.05 0.02 0.05 0.01 0.01 0.01 0.01 0.01 0.06 0.68 0.05 0.06 0.08 0.02 0.01 0.01 0.01 0 0.05 0.06 0.79 0.01 0.04 0.02 0 0 0.01 0 0.07 0.18 0.05 0.61 0.04 0.03 0.01 0.01 0 0 0.03 0.06 0.02 0.01 0.86 0 0 0 0.01 0 0.04 0.04 0.06 0.02 0.01 0.8 0 0.01 0.06 0.02 0.02 0.02 0.01 0 0.02 0 0.84 0.02 0.02 0.05 0.01 0.03 0.01 0 0.02 0.01 0.04 0.81 0: airplane 1: auto2: bird3: cat4: deer5: dog6: frog7: horse8: ship9: truck 0 1 2 3 4 5 6 7 8 9 0.75 0.01 0.04 0.04 0.02 0 0.03 0.01 0.08 0.02 0.01 0.87 0.01 0.01 0 0 0.02 0 0.02 0.06 0.04 0 0.73 0.08 0.05 0.02 0.05 0.01 0.01 0.01 0.01 0 0.05 0.73 0.05 0.05 0.07 0.02 0.01 0.01 0.01 0 0.05 0.06 0.79 0.01 0.05 0.02 0.01 0 0.01 0 0.06 0.18 0.04 0.63 0.04 0.02 0.01 0.01 0 0 0.03 0.08 0.02 0.01 0.83 0 0 0 0.01 0 0.04 0.05 0.07 0.02 0.01 0.79 0 0.01 0.04 0.02 0.01 0.02 0.01 0 0.02 0 0.86 0.02 0.02 0.04 0.01 0.03 0.01 0 0.03 0.01 0.04 0.81 True label 5th visit 6th visit 7th visit 8th visit 0: airplane 1: auto2: bird3: cat4: deer5: dog6: frog7: horse8: ship9: truck 0 1 2 3 4 5 6 7 8 9 0.74 0.01 0.05 0.04 0.02 0 0.03 0.01 0.08 0.02 0.01 0.88 0.01 0.01 0 0 0.02 0 0.02 0.05 0.04 0 0.74 0.07 0.05 0.02 0.06 0.01 0.01 0.01 0.01 0 0.06 0.71 0.05 0.05 0.07 0.02 0.01 0.01 0.01 0 0.04 0.07 0.79 0.01 0.04 0.02 0.01 0 0.01 0 0.07 0.19 0.05 0.62 0.04 0.02 0.01 0 0 0 0.03 0.07 0.02 0.01 0.84 0 0 0.01 0.01 0 0.05 0.05 0.08 0.02 0.02 0.77 0 0.01 0.04 0.02 0.02 0.02 0.01 0 0.02 0 0.85 0.02 0.02 0.05 0.02 0.02 0.01 0 0.02 0.01 0.04 0.83 0: airplane 1: auto2: bird3: cat4: deer5: dog6: frog7: horse8: ship9: truck 0 1 2 3 4 5 6 7 8 9 0.74 0.01 0.05 0.04 0.02 0 0.03 0.01 0.08 0.02 0.01 0.88 0.01 0.01 0 0 0.02 0 0.02 0.06 0.04 0 0.73 0.07 0.05 0.02 0.05 0.01 0.01 0.01 0.01 0.01 0.05 0.7 0.06 0.05 0.08 0.02 0.02 0.02 0.02 0 0.05 0.07 0.79 0.01 0.04 0.02 0.01 0 0.01 0 0.07 0.19 0.05 0.6 0.04 0.03 0.01 0.01 0 0 0.04 0.07 0.02 0.01 0.84 0 0 0.01 0.01 0 0.04 0.05 0.08 0.02 0.01 0.78 0 0 0.04 0.02 0.02 0.02 0.01 0 0.02 0 0.85 0.02 0.02 0.05 0.02 0.02 0.01 0 0.02 0.01 0.04 0.81 0: airplane 1: auto2: bird3: cat4: deer5: dog6: frog7: horse8: ship9: truck 0 1 2 3 4 5 6 7 8 9 0.73 0.02 0.06 0.05 0.02 0 0.04 0.01 0.07 0.02 0.01 0.87 0.01 0.01 0 0 0.02 0 0.02 0.06 0.04 0 0.74 0.08 0.05 0.02 0.05 0.01 0.01 0.01 0.01 0 0.06 0.73 0.05 0.05 0.07 0.01 0.01 0.01 0.02 0 0.06 0.07 0.76 0.01 0.04 0.02 0.01 0 0 0 0.06 0.19 0.05 0.61 0.05 0.02 0.01 0 0 0 0.03 0.07 0.02 0.01 0.86 0 0 0 0.01 0 0.04 0.05 0.08 0.02 0.01 0.77 0 0.01 0.04 0.02 0.02 0.02 0.01 0 0.03 0 0.84 0.02 0.01 0.04 0.02 0.03 0.01 0 0.02 0 0.03 0.83 0: airplane 1: auto2: bird3: cat4: deer5: dog6: frog7: horse8: ship9: truck 0 1 2 3 4 5 6 7 8 9 0.72 0.01 0.05 0.04 0.02 0 0.04 0.01 0.08 0.02 0.01 0.87 0.01 0.01 0 0 0.02 0 0.02 0.04 0.05 0 0.72 0.08 0.05 0.02 0.05 0.01 0.01 0 0.01 0 0.06 0.73 0.05 0.04 0.06 0.02 0.01 0.01 0.02 0 0.05 0.06 0.79 0.01 0.04 0.02 0.01 0 0.01 0 0.06 0.19 0.05 0.61 0.04 0.03 0.01 0 0 0 0.03 0.09 0.02 0.01 0.83 0 0 0 0.01 0 0.05 0.05 0.07 0.02 0.01 0.78 0 0.01 0.03 0.02 0.02 0.02 0.01 0 0.03 0 0.85 0.02 0.01 0.05 0.01 0.03 0.01 0 0.02 0 0.04 0.83 True label 9th visit 10th visit 11th visit 12th visit 0: airplane 1: auto2: bird3: cat4: deer5: dog6: frog7: horse8: ship9: truck 0 1 2 3 4 5 6 7 8 9 0.73 0.01 0.05 0.04 0.02 0 0.03 0.01 0.09 0.02 0.01 0.86 0.01 0.01 0 0 0.02 0 0.02 0.06 0.04 0 0.73 0.08 0.05 0.02 0.05 0.01 0.01 0 0.02 0 0.06 0.73 0.05 0.04 0.06 0.02 0.01 0.01 0.01 0 0.05 0.06 0.8 0.01 0.04 0.02 0.01 0 0.01 0 0.07 0.19 0.05 0.6 0.04 0.02 0.01 0.01 0 0 0.03 0.07 0.02 0.01 0.86 0 0 0 0.01 0 0.05 0.05 0.07 0.02 0.01 0.77 0 0 0.03 0.02 0.02 0.02 0.01 0 0.04 0 0.83 0.02 0.01 0.05 0.02 0.02 0.01 0 0.02 0 0.04 0.82 0: airplane 1: auto2: bird3: cat4: deer5: dog6: frog7: horse8: ship9: truck 0 1 2 3 4 5 6 7 8 9 0.75 0.01 0.05 0.04 0.02 0 0.03 0.01 0.08 0.02 0.01 0.87 0.01 0.02 0 0 0.02 0 0.02 0.05 0.05 0 0.72 0.08 0.05 0.02 0.05 0.01 0.01 0 0.01 0.01 0.05 0.73 0.05 0.05 0.07 0.01 0.01 0.01 0.01 0 0.05 0.06 0.79 0.01 0.05 0.02 0.01 0 0.01 0 0.07 0.21 0.05 0.57 0.05 0.02 0.01 0 0 0 0.03 0.07 0.02 0.01 0.86 0 0 0 0.01 0 0.05 0.05 0.08 0.02 0.02 0.76 0 0.01 0.04 0.02 0.02 0.02 0.01 0 0.02 0 0.85 0.02 0.02 0.05 0.02 0.03 0.01 0 0.02 0 0.04 0.81 0: airplane 1: auto2: bird3: cat4: deer5: dog6: frog7: horse8: ship9: truck 0 1 2 3 4 5 6 7 8 9 0.72 0.01 0.05 0.05 0.02 0 0.03 0.01 0.08 0.02 0.01 0.88 0.01 0.01 0 0 0.02 0 0.02 0.05 0.04 0 0.73 0.08 0.05 0.02 0.05 0.01 0.01 0 0.01 0 0.06 0.72 0.05 0.04 0.07 0.01 0.01 0.01 0.02 0 0.04 0.06 0.79 0.01 0.04 0.02 0.01 0 0.01 0 0.07 0.2 0.05 0.6 0.04 0.02 0.01 0.01 0 0 0.04 0.07 0.02 0.01 0.85 0 0 0 0.01 0 0.05 0.05 0.08 0.02 0.01 0.78 0 0.01 0.04 0.02 0.02 0.02 0.01 0 0.02 0 0.85 0.02 0.02 0.05 0.02 0.02 0.01 0 0.02 0 0.04 0.82 0: airplane 1: auto2: bird3: cat4: deer5: dog6: frog7: horse8: ship9: truck 0 1 2 3 4 5 6 7 8 9 0.75 0.01 0.05 0.04 0.02 0 0.02 0.01 0.09 0.02 0.01 0.86 0.01 0.02 0 0 0.02 0 0.02 0.05 0.04 0 0.74 0.07 0.05 0.02 0.05 0.01 0.01 0.01 0.02 0 0.06 0.73 0.05 0.04 0.07 0.01 0.01 0.01 0.02 0 0.05 0.06 0.78 0.01 0.05 0.02 0.01 0 0.01 0 0.07 0.19 0.05 0.6 0.05 0.02 0.01 0.01 0 0 0.03 0.07 0.02 0.01 0.85 0 0 0.01 0.01 0 0.04 0.06 0.08 0.02 0.01 0.77 0 0.01 0.04 0.02 0.03 0.03 0.01 0 0.04 0 0.8 0.02 0.01 0.05 0.02 0.02 0.01 0 0.02 0 0.04 0.83 True label 13th visit 14th visit 15th visit 16th visit 0: airplane 1: auto2: bird3: cat4: deer5: dog6: frog7: horse8: ship9: truck 0 1 2 3 4 5 6 7 8 9 0.73 0.01 0.06 0.04 0.02 0 0.04 0.01 0.07 0.02 0.01 0.88 0.01 0.01 0 0 0.02 0 0.02 0.05 0.04 0 0.75 0.07 0.05 0.02 0.05 0.01 0.01 0 0.01 0 0.06 0.74 0.05 0.05 0.06 0.01 0.01 0.01 0.01 0 0.05 0.06 0.8 0.01 0.04 0.02 0 0 0.01 0 0.07 0.2 0.05 0.59 0.06 0.02 0.01 0.01 0 0 0.04 0.08 0.02 0.01 0.84 0 0 0 0.01 0 0.05 0.05 0.08 0.02 0.02 0.76 0 0.01 0.05 0.01 0.01 0.02 0.01 0 0.02 0 0.85 0.02 0.02 0.05 0.02 0.03 0.01 0 0.03 0 0.03 0.81 0: airplane 1: auto2: bird3: cat4: deer5: dog6: frog7: horse8: ship9: truck 0 1 2 3 4 5 6 7 8 9 0.72 0.01 0.05 0.04 0.02 0 0.03 0.01 0.08 0.02 0.01 0.88 0.01 0.01 0 0 0.02 0 0.02 0.04 0.04 0 0.73 0.07 0.06 0.02 0.06 0.01 0.01 0 0.01 0 0.06 0.73 0.05 0.04 0.07 0.01 0.01 0.01 0.01 0 0.06 0.06 0.79 0.01 0.05 0.02 0.01 0 0.01 0 0.07 0.21 0.05 0.59 0.04 0.02 0.01 0 0 0 0.04 0.07 0.02 0.01 0.86 0 0 0 0.01 0 0.05 0.05 0.08 0.02 0.01 0.76 0.01 0.01 0.05 0.02 0.02 0.03 0.01 0 0.02 0 0.85 0.01 0.02 0.05 0.02 0.03 0.01 0 0.03 0.01 0.04 0.8 0: airplane 1: auto2: bird3: cat4: deer5: dog6: frog7: horse8: ship9: truck 0 1 2 3 4 5 6 7 8 9 0.73 0.01 0.06 0.04 0.02 0 0.03 0.01 0.08 0.02 0.01 0.88 0.01 0.01 0 0 0.02 0 0.02 0.05 0.04 0 0.73 0.06 0.05 0.02 0.06 0.01 0.01 0.01 0.01 0 0.06 0.73 0.05 0.05 0.07 0.01 0.01 0.01 0.01 0 0.05 0.06 0.78 0.01 0.05 0.02 0.01 0 0.01 0 0.07 0.21 0.05 0.58 0.05 0.02 0.01 0.01 0 0 0.03 0.07 0.02 0.01 0.85 0 0.01 0.01 0.01 0 0.06 0.05 0.08 0.02 0.02 0.75 0 0.01 0.03 0.02 0.02 0.02 0.01 0 0.03 0 0.85 0.02 0.02 0.05 0.02 0.02 0.01 0 0.02 0 0.04 0.83 0: airplane 1: auto2: bird3: cat4: deer5: dog6: frog7: horse8: ship9: truck 0 1 2 3 4 5 6 7 8 9 0.73 0.01 0.06 0.04 0.02 0 0.03 0.01 0.08 0.02 0.01 0.88 0.01 0.01 0 0 0.02 0 0.02 0.05 0.04 0 0.75 0.07 0.05 0.02 0.05 0.01 0.01 0 0.01 0 0.06 0.72 0.05 0.04 0.06 0.02 0.01 0.01 0.02 0 0.06 0.07 0.76 0.01 0.05 0.02 0.01 0 0 0 0.07 0.19 0.05 0.59 0.05 0.02 0.01 0.01 0 0 0.03 0.07 0.02 0.01 0.84 0 0.01 0.01 0.01 0 0.06 0.06 0.08 0.02 0.02 0.74 0 0.01 0.04 0.02 0.02 0.02 0.01 0 0.03 0 0.84 0.02 0.01 0.05 0.02 0.03 0.01 0 0.02 0.01 0.04 0.82 Predicted label Predicted label Predicted label Predicted label True label 17th visit 18th visit 19th visit 20th visit Figure 11: The dynamic of the confusion matrix of PeTTA (ours) in episodic TTA with 20 visits. 34NeurIPS Paper Checklist 1. Claims Question: Do the main claims made in the abstract and introduction accurately reflect the paper’s contributions and scope? Answer: [Yes] Justification: We have highlighted the three main claims and contributions of our work in both the abstract (highlighted in bold font) and the introduction section (listed as bullet points). Guidelines: • The answer NA means that the abstract and introduction do not include the claims made in the paper. • The abstract and/or introduction should clearly state the claims made, including the contributions made in the paper and important assumptions and limitations. A No or NA answer to this question will not be perceived well by the reviewers. • The claims made should match theoretical and experimental results, and reflect how much the results can be expected to generalize to other settings. • It is fine to include aspirational goals as motivation as long as it is clear that these goals are not attained by the paper. 2. Limitations Question: Does the paper discuss the limitations of the work performed by the authors? Answer: [Yes] Justification: We have discussed the limitations and potential future work of our study in Sec. 6. Specifically, three main limitations are included: (1) Collapse prevention can not be guaranteed through regularization, PeTTA requires (2) the use of a relatively small memory bank is available and (3) the empirical mean and covariant matrix of feature vectors on the source dataset is computable. We also include discussions in Appdx. E.3 and Appdx. E.4 to further elaborate (2), and (3) respectively. Guidelines: • The answer NA means that the paper has no limitation while the answer No means that the paper has limitations, but those are not discussed in the paper. • The authors are encouraged to create a separate \"Limitations\" section in their paper. • The paper should point out any strong assumptions and how robust the results are to violations of these assumptions (e.g., independence assumptions, noiseless settings, model well-specification, asymptotic approximations only holding locally). The authors should reflect on how these assumptions might be violated in practice and what the implications would be. • The authors should reflect on the scope of the claims made, e.g., if the approach was only tested on a few datasets or with a few runs. In general, empirical results often depend on implicit assumptions, which should be articulated. • The authors should reflect on the factors that influence the performance of the approach. For example, a facial recognition algorithm may perform poorly when image resolution is low or images are taken in low lighting. Or a speech-to-text system might not be used reliably to provide closed captions for online lectures because it fails to handle technical jargon. • The authors should discuss the computational efficiency of the proposed algorithms and how they scale with dataset size. • If applicable, the authors should discuss possible limitations of their approach to address problems of privacy and fairness. • While the authors might fear that complete honesty about limitations might be used by reviewers as grounds for rejection, a worse outcome might be that reviewers discover limitations that aren’t acknowledged in the paper. The authors should use their best judgment and recognize that individual actions in favor of transparency play an impor- tant role in developing norms that preserve the integrity of the community. Reviewers will be specifically instructed to not penalize honesty concerning limitations. 353. Theory Assumptions and Proofs Question: For each theoretical result, does the paper provide the full set of assumptions and a complete (and correct) proof? Answer: [Yes] Justification: We have provided the full proof of all lemmas and theorem in Appdx. B. Guidelines: • The answer NA means that the paper does not include theoretical results. • All the theorems, formulas, and proofs in the paper should be numbered and cross- referenced. • All assumptions should be clearly stated or referenced in the statement of any theorems. • The proofs can either appear in the main paper or the supplemental material, but if they appear in the supplemental material, the authors are encouraged to provide a short proof sketch to provide intuition. • Inversely, any informal proof provided in the core of the paper should be complemented by formal proofs provided in appendix or supplemental material. • Theorems and Lemmas that the proof relies upon should be properly referenced. 4. Experimental Result Reproducibility Question: Does the paper fully disclose all the information needed to reproduce the main ex- perimental results of the paper to the extent that it affects the main claims and/or conclusions of the paper (regardless of whether the code and data are provided or not)? Answer: [Yes] Justification: This study propose a new TTA approach - PeTTA. A full description of this approach is given in Sec. 4 with its pseudo-code provided in Appdx. E.1. The implementation of PeTTA in Python is also attached as supplemental material. Additionally, Sec. 5.2 and Appdx. G are dedicated to providing further implementation details for reproducing the main experimental results. Lastly, the construction of recurring TTA is notably simple, and can be easily extended to other TTA streams. Its configuration on each tasks is described in the Recurring TTA paragraph of Sec. 5.2. Guidelines: • The answer NA means that the paper does not include experiments. • If the paper includes experiments, a No answer to this question will not be perceived well by the reviewers: Making the paper reproducible is important, regardless of whether the code and data are provided or not. • If the contribution is a dataset and/or model, the authors should describe the steps taken to make their results reproducible or verifiable. • Depending on the contribution, reproducibility can be accomplished in various ways. For example, if the contribution is a novel architecture, describing the architecture fully might suffice, or if the contribution is a specific model and empirical evaluation, it may be necessary to either make it possible for others to replicate the model with the same dataset, or provide access to the model. In general. releasing code and data is often one good way to accomplish this, but reproducibility can also be provided via detailed instructions for how to replicate the results, access to a hosted model (e.g., in the case of a large language model), releasing of a model checkpoint, or other means that are appropriate to the research performed. • While NeurIPS does not require releasing code, the conference does require all submis- sions to provide some reasonable avenue for reproducibility, which may depend on the nature of the contribution. For example (a) If the contribution is primarily a new algorithm, the paper should make it clear how to reproduce that algorithm. (b) If the contribution is primarily a new model architecture, the paper should describe the architecture clearly and fully. (c) If the contribution is a new model (e.g., a large language model), then there should either be a way to access this model for reproducing the results or a way to reproduce the model (e.g., with an open-source dataset or instructions for how to construct the dataset). 36(d) We recognize that reproducibility may be tricky in some cases, in which case authors are welcome to describe the particular way they provide for reproducibility. In the case of closed-source models, it may be that access to the model is limited in some way (e.g., to registered users), but it should be possible for other researchers to have some path to reproducing or verifying the results. 5. Open access to data and code Question: Does the paper provide open access to the data and code, with sufficient instruc- tions to faithfully reproduce the main experimental results, as described in supplemental material? Answer: [Yes] Justification: This study does not involve any private datasets. All datasets used in our exper- iments are publicly available online from previous works (more information in Appdx. G.4). The source code of PeTTA is also attached as supplemental material. Guidelines: • The answer NA means that paper does not include experiments requiring code. • Please see the NeurIPS code and data submission guidelines ( https://nips.cc/ public/guides/CodeSubmissionPolicy) for more details. • While we encourage the release of code and data, we understand that this might not be possible, so “No” is an acceptable answer. Papers cannot be rejected simply for not including code, unless this is central to the contribution (e.g., for a new open-source benchmark). • The instructions should contain the exact command and environment needed to run to reproduce the results. See the NeurIPS code and data submission guidelines (https: //nips.cc/public/guides/CodeSubmissionPolicy) for more details. • The authors should provide instructions on data access and preparation, including how to access the raw data, preprocessed data, intermediate data, and generated data, etc. • The authors should provide scripts to reproduce all experimental results for the new proposed method and baselines. If only a subset of experiments are reproducible, they should state which ones are omitted from the script and why. • At submission time, to preserve anonymity, the authors should release anonymized versions (if applicable). • Providing as much information as possible in supplemental material (appended to the paper) is recommended, but including URLs to data and code is permitted. 6. Experimental Setting/Details Question: Does the paper specify all the training and test details (e.g., data splits, hyper- parameters, how they were chosen, type of optimizer, etc.) necessary to understand the results? Answer: [Yes] Justification: The experimental settings of the key results in the paper have been provided in Sec. 5.1 (Simulation Setup) and Sec. 5.2 (Setup - Benchmark Datasets). In the supplementary material, any additional experimental results beyond the main paper, such as those in Appdx. D.3, and Appdx. F.3, are consistently preceded by a subsection titledExperiment Setup summarizing the experimental details before presenting the results. Guidelines: • The answer NA means that the paper does not include experiments. • The experimental setting should be presented in the core of the paper to a level of detail that is necessary to appreciate the results and make sense of them. • The full details can be provided either with the code, in appendix, or as supplemental material. 7. Experiment Statistical Significance Question: Does the paper report error bars suitably and correctly defined or other appropriate information about the statistical significance of the experiments? 37Answer: [Yes] Justification: Due to the limited computing resources, we only extensively evaluate the performance of our proposed method (PeTTA) across 5 independent runs, with different random seeds. Specifically, the mean values in 5 runs are reported in Tab. 1, Tab. 2, Tab. 7, and Tab. 8. The corresponding standard deviation values are provided in Appdx. F.1. Guidelines: • The answer NA means that the paper does not include experiments. • The authors should answer \"Yes\" if the results are accompanied by error bars, confi- dence intervals, or statistical significance tests, at least for the experiments that support the main claims of the paper. • The factors of variability that the error bars are capturing should be clearly stated (for example, train/test split, initialization, random drawing of some parameter, or overall run with given experimental conditions). • The method for calculating the error bars should be explained (closed form formula, call to a library function, bootstrap, etc.) • The assumptions made should be given (e.g., Normally distributed errors). • It should be clear whether the error bar is the standard deviation or the standard error of the mean. • It is OK to report 1-sigma error bars, but one should state it. The authors should preferably report a 2-sigma error bar than state that they have a 96% CI, if the hypothesis of Normality of errors is not verified. • For asymmetric distributions, the authors should be careful not to show in tables or figures symmetric error bars that would yield results that are out of range (e.g. negative error rates). • If error bars are reported in tables or plots, The authors should explain in the text how they were calculated and reference the corresponding figures or tables in the text. 8. Experiments Compute Resources Question: For each experiment, does the paper provide sufficient information on the com- puter resources (type of compute workers, memory, time of execution) needed to reproduce the experiments? Answer: [Yes] Justification: We have provided the information on the computing resources used in our experiments in Appdx. G.1. Guidelines: • The answer NA means that the paper does not include experiments. • The paper should indicate the type of compute workers CPU or GPU, internal cluster, or cloud provider, including relevant memory and storage. • The paper should provide the amount of compute required for each of the individual experimental runs as well as estimate the total compute. • The paper should disclose whether the full research project required more compute than the experiments reported in the paper (e.g., preliminary or failed experiments that didn’t make it into the paper). 9. Code Of Ethics Question: Does the research conducted in the paper conform, in every respect, with the NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines? Answer: [Yes] Justification: The authors have reviewed and to the best of our judgment, this study has conformed to the NeurIPS Code of Ethics. Guidelines: • The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics. • If the authors answer No, they should explain the special circumstances that require a deviation from the Code of Ethics. 38• The authors should make sure to preserve anonymity (e.g., if there is a special consid- eration due to laws or regulations in their jurisdiction). 10. Broader Impacts Question: Does the paper discuss both potential positive societal impacts and negative societal impacts of the work performed? Answer: [No] Justification: This study advances the research in test-time adaptation area in general, and not tied to particular applications. Hence, there are no significant potential societal consequences of our work which we feel must be specifically highlighted here. Guidelines: • The answer NA means that there is no societal impact of the work performed. • If the authors answer NA or No, they should explain why their work has no societal impact or why the paper does not address societal impact. • Examples of negative societal impacts include potential malicious or unintended uses (e.g., disinformation, generating fake profiles, surveillance), fairness considerations (e.g., deployment of technologies that could make decisions that unfairly impact specific groups), privacy considerations, and security considerations. • The conference expects that many papers will be foundational research and not tied to particular applications, let alone deployments. However, if there is a direct path to any negative applications, the authors should point it out. For example, it is legitimate to point out that an improvement in the quality of generative models could be used to generate deepfakes for disinformation. On the other hand, it is not needed to point out that a generic algorithm for optimizing neural networks could enable people to train models that generate Deepfakes faster. • The authors should consider possible harms that could arise when the technology is being used as intended and functioning correctly, harms that could arise when the technology is being used as intended but gives incorrect results, and harms following from (intentional or unintentional) misuse of the technology. • If there are negative societal impacts, the authors could also discuss possible mitigation strategies (e.g., gated release of models, providing defenses in addition to attacks, mechanisms for monitoring misuse, mechanisms to monitor how a system learns from feedback over time, improving the efficiency and accessibility of ML). 11. Safeguards Question: Does the paper describe safeguards that have been put in place for responsible release of data or models that have a high risk for misuse (e.g., pretrained language models, image generators, or scraped datasets)? Answer: [NA] Justification: To the best of our judgment, this study poses no risks for misuse. Guidelines: • The answer NA means that the paper poses no such risks. • Released models that have a high risk for misuse or dual-use should be released with necessary safeguards to allow for controlled use of the model, for example by requiring that users adhere to usage guidelines or restrictions to access the model or implementing safety filters. • Datasets that have been scraped from the Internet could pose safety risks. The authors should describe how they avoided releasing unsafe images. • We recognize that providing effective safeguards is challenging, and many papers do not require this, but we encourage authors to take this into account and make a best faith effort. 12. Licenses for existing assets Question: Are the creators or original owners of assets (e.g., code, data, models), used in the paper, properly credited and are the license and terms of use explicitly mentioned and properly respected? 39Answer: [Yes] Justification: The original papers that produced the code package or dataset have been properly cited throughout the paper. Further information on the licenses of used assets are provided in Appdx. G.4. Guidelines: • The answer NA means that the paper does not use existing assets. • The authors should cite the original paper that produced the code package or dataset. • The authors should state which version of the asset is used and, if possible, include a URL. • The name of the license (e.g., CC-BY 4.0) should be included for each asset. • For scraped data from a particular source (e.g., website), the copyright and terms of service of that source should be provided. • If assets are released, the license, copyright information, and terms of use in the package should be provided. For popular datasets, paperswithcode.com/datasets has curated licenses for some datasets. Their licensing guide can help determine the license of a dataset. • For existing datasets that are re-packaged, both the original license and the license of the derived asset (if it has changed) should be provided. • If this information is not available online, the authors are encouraged to reach out to the asset’s creators. 13. New Assets Question: Are new assets introduced in the paper well documented and is the documentation provided alongside the assets? Answer: [NA] Justification: This study does not release new assets. Guidelines: • The answer NA means that the paper does not release new assets. • Researchers should communicate the details of the dataset/code/model as part of their submissions via structured templates. This includes details about training, license, limitations, etc. • The paper should discuss whether and how consent was obtained from people whose asset is used. • At submission time, remember to anonymize your assets (if applicable). You can either create an anonymized URL or include an anonymized zip file. 14. Crowdsourcing and Research with Human Subjects Question: For crowdsourcing experiments and research with human subjects, does the paper include the full text of instructions given to participants and screenshots, if applicable, as well as details about compensation (if any)? Answer: [NA] Justification: This study does not involve crowdsourcing nor research with human subjects. Guidelines: • The answer NA means that the paper does not involve crowdsourcing nor research with human subjects. • Including this information in the supplemental material is fine, but if the main contribu- tion of the paper involves human subjects, then as much detail as possible should be included in the main paper. • According to the NeurIPS Code of Ethics, workers involved in data collection, curation, or other labor should be paid at least the minimum wage in the country of the data collector. 15. Institutional Review Board (IRB) Approvals or Equivalent for Research with Human Subjects 40Question: Does the paper describe potential risks incurred by study participants, whether such risks were disclosed to the subjects, and whether Institutional Review Board (IRB) approvals (or an equivalent approval/review based on the requirements of your country or institution) were obtained? Answer: [NA] Justification: This study does not involve crowdsourcing nor research with human subjects. Guidelines: • The answer NA means that the paper does not involve crowdsourcing nor research with human subjects. • Depending on the country in which research is conducted, IRB approval (or equivalent) may be required for any human subjects research. If you obtained IRB approval, you should clearly state this in the paper. • We recognize that the procedures for this may vary significantly between institutions and locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the guidelines for their institution. • For initial submissions, do not include any information that would break anonymity (if applicable), such as the institution conducting the review. 41",
      "meta_data": {
        "arxiv_id": "2311.18193v4",
        "authors": [
          "Trung-Hieu Hoang",
          "Duc Minh Vo",
          "Minh N. Do"
        ],
        "published_date": "2023-11-30T02:24:44Z",
        "pdf_url": "https://arxiv.org/pdf/2311.18193v4.pdf"
      },
      "llm_extracted_info": {
        "main_contributions": "This research introduces the recurring Test-time Adaptation (TTA) scenario to diagnose gradual performance degradation and error accumulation in TTA models over prolonged periods, especially when encountering previously seen environments. It formally defines model collapse and provides a theoretical analysis on an \"e-perturbed Gaussian Mixture Model Classifier\" (e-GMMC) to explain dataset- and algorithm-dependent factors leading to this degradation. Based on these insights, the paper proposes Persistent TTA (PeTTA), an adaptation scheme that monitors model divergence and dynamically adjusts the adaptation strategy to balance between adaptation and preventing model collapse, demonstrating superior stability compared to existing approaches in lifelong TTA scenarios.",
        "methodology": "The paper first introduces the recurring TTA setting, which extends practical TTA by allowing environments to recur over time. It then conducts a theoretical analysis using an e-perturbed binary Gaussian Mixture Model Classifier (e-GMMC) to simulate and understand model collapse, where the pseudo-label predictor is perturbed to maintain a false negative rate. This analysis explains factors contributing to collapse: prior data distribution, intrinsic category differences, update rate, and false negative rate (FNR). PeTTA, the proposed method, equips TTA with a mechanism to sense divergence from the initial model (theta0) by measuring the Mahalanobis distance of feature embedding vectors' first moments (gamma_y_t). It then adaptively adjusts the regularization term coefficient (lambda_t) and the EMA update rate (alpha_t) based on this sensed divergence, prioritizing collapse prevention when deviation is significant. PeTTA integrates these adaptive parameters with a mean teacher update framework, category-balanced memory bank, robust batch normalization, and an anchor loss (LAL) which minimizes KL divergence between current and source model predictions.",
        "experimental_setup": "The performance of PeTTA is benchmarked against CoTTA, EATA, RMT, MECTA, RoTTA, ROID, TRIBE, LAME, and RDumb on four classification tasks: CIFAR-10 --> CIFAR-10-C, CIFAR-100 --> CIFAR-100-C, ImageNet --> ImageNet-C (corruption level 5), and DomainNet (real --> clipart, painting, sketch). All experiments are conducted in the proposed recurring TTA scenario, where multiple testing scenarios from each test set gradually change and recur K=20 times. For CIFAR-10/100-C and ImageNet-C, a Dirichlet distribution (Dir(0.1) or Dir(0.01)) generates temporally correlated batches. An additional experiment on the Continuously Changing Corruption (CCC) benchmark is performed, covering 80,000 adaptation steps with 5.1M images. Source models are pre-trained on RobustBench and torchvision; PyTorch is used for implementation. Adam optimizer is used with a learning rate of 1e-3. PeTTA's adaptive EMA update rate for robust batch normalization and feature embedding statistics is 5e-2, initial alpha_0 is 1e-3, and lambda_0 is 10 for CIFAR/ImageNet-C and 1 for DomainNet.",
        "limitations": "The paper notes that a complete elimination of error accumulation cannot be rigorously guaranteed solely through regularization. Additionally, PeTTA relies on a small memory bank to handle temporally correlated testing streams, which may limit scalability. It also assumes the availability of feature statistics (empirical mean and covariant matrix) from the source distribution, which might not always be accessible in real-world settings without accessing unlabeled source samples or pre-computed values.",
        "future_research_directions": "Future research could focus on developing algorithms that achieve error accumulation-free adaptation by construction, beyond regularization. Exploring alternative methods for reducing memory bank size (e.g., storing embedded features instead of original images) is also a potential direction. Furthermore, research could investigate how to operate PeTTA without the assumption of having access to feature statistics from the source distribution."
      }
    },
    {
      "title": "Active Test-Time Adaptation: Theoretical Analyses and An Algorithm",
      "abstract": "Test-time adaptation (TTA) addresses distribution shifts for streaming test\ndata in unsupervised settings. Currently, most TTA methods can only deal with\nminor shifts and rely heavily on heuristic and empirical studies.\n  To advance TTA under domain shifts, we propose the novel problem setting of\nactive test-time adaptation (ATTA) that integrates active learning within the\nfully TTA setting.\n  We provide a learning theory analysis, demonstrating that incorporating\nlimited labeled test instances enhances overall performances across test\ndomains with a theoretical guarantee. We also present a sample entropy\nbalancing for implementing ATTA while avoiding catastrophic forgetting (CF). We\nintroduce a simple yet effective ATTA algorithm, known as SimATTA, using\nreal-time sample selection techniques. Extensive experimental results confirm\nconsistency with our theoretical analyses and show that the proposed ATTA\nmethod yields substantial performance improvements over TTA methods while\nmaintaining efficiency and shares similar effectiveness to the more demanding\nactive domain adaptation (ADA) methods. Our code is available at\nhttps://github.com/divelab/ATTA",
      "full_text": "Published as a conference paper at ICLR 2024 ACTIVE TEST-TIME ADAPTATION : T HEORETICAL ANALYSES AND AN ALGORITHM Shurui Gui∗ Texas A&M University College Station, TX 77843 shurui.gui@tamu.edu Xiner Li* Texas A&M University College Station, TX 77843 lxe@tamu.edu Shuiwang Ji Texas A&M University College Station, TX 77843 sji@tamu.edu ABSTRACT Test-time adaptation (TTA) addresses distribution shifts for streaming test data in unsupervised settings. Currently, most TTA methods can only deal with minor shifts and rely heavily on heuristic and empirical studies. To advance TTA under domain shifts, we propose the novel problem setting of active test-time adaptation (ATTA) that integrates active learning within the fully TTA setting. We provide a learning theory analysis, demonstrating that incorporating limited labeled test instances enhances overall performances across test domains with a theoretical guarantee. We also present a sample entropy balancing for implementing ATTA while avoiding catastrophic forgetting (CF). We introduce a simple yet effective ATTA algorithm, known as SimATTA, using real-time sample selection techniques. Extensive experimental results confirm consistency with our theoretical analyses and show that the proposed ATTA method yields substantial performance improvements over TTA methods while maintaining efficiency and shares similar effectiveness to the more demanding active domain adaptation (ADA) methods. Our code is available at https://github.com/divelab/ATTA. 1 I NTRODUCTION Deep learning has achieved remarkable success across various fields, attaining high accuracy in numerous applications (Krizhevsky et al., 2017; Simonyan and Zisserman, 2014). Nonetheless, When training and test data follow distinct distributions, models often experience significant performance degradation during test. This phenomenon, known as the distribution shift or out-of-distribution (OOD) problem, is extensively studied within the context of both domain generalization (DG) (Gulra- jani and Lopez-Paz, 2020; Koh et al., 2021; Gui et al., 2022) and domain adaptation (DA) (Ganin et al., 2016; Sun and Saenko, 2016). While these studies involve intensive training of models with considerable generalization abilities towards target domains, they overlook an important application property; namely, continuous adaptivity to real-time streaming data under privacy, resource, and efficiency constraints. This gap leads to the emergence of test-time adaptation (TTA) tasks, targeting on-the-fly adaptation to continuous new domains during the test phase or application deployment. The study of TTA encompasses two main categories; namely test-time training (TTT) methods (Sun et al., 2020; Liu et al., 2021c) and fully test-time adaptation (FTTA) (Niu et al., 2023; Wang et al., 2021). The TTT pipeline incorporates retraining on the source data, whereas FTTA methods adapt arbitrary pre-trained models to the given test mini-batch by conducting entropy minimization, without access to the source data. Nevertheless, most TTA methods can only handle corrupted distribution shifts (Hendrycks and Dietterich, 2019b) (e.g., Gaussian noise,) and rely heavily on human intuition or empirical studies. To bridge this gap, our paper focuses on tackling significant domain distribution shifts in real time with theoretical insights. We investigate FTTA, which is more general and adaptable than TTT, particularly under data ac- cessibility, privacy, and efficiency constraints. Traditional FTTA aims at adapting a pre-trained model to streaming test-time data from diverse domains under unsupervised settings. However, recent works (Lin et al., 2022; Pearl, 2009) prove that it is theoretically infeasible to achieve OOD generalization without extra information such as environment partitions. Since utilizing environment partitions requires heavy pretraining, contradicting the nature of TTA, we are motivated to incorporate extra information in a different way,i.e., integrating a limited number of labeled test-time samples to alleviate distribution shifts, following the active learning (AL) paradigm (Settles, 2009). To this end, we propose the novel problem setting of active test-time adaptation (ATTA) by incorporating ∗Equal contributions 1 arXiv:2404.05094v1  [cs.LG]  7 Apr 2024Published as a conference paper at ICLR 2024 AL within FTTA. ATTA faces two major challenges; namely, catastrophic forgetting (CF) (Kemker et al., 2018; Li and Hoiem, 2017) and real-time active sample selection. CF problem arises when a model continually trained on a sequence of domains experiences a significant performance drop on previously learned domains, due to the inaccessibility of the source data and previous test data. Real-time active sample selection requires AL algorithms to select informative samples from a small buffer of streaming test data for annotation, without a complete view of the test distribution. In this paper, we first formally define the ATTA setting. We then provide its foundational analysis under the learning theory’s paradigm to guarantee the mitigation of distribution shifts and avoid CF. Aligned with our empirical validations, while the widely used entropy minimization (Wang et al., 2021; Grandvalet and Bengio, 2004) can cause CF, it can conversely become the key to preventing CF problems with our sample selection and balancing techniques. Building on the analyses, we then introduce a simple yet effective ATTA algorithm, SimATTA, incorporating balanced sample selections and incremental clustering. Finally, we conducted a comprehensive experimental study to evaluate the proposed ATTA settings with three different settings in the order of low to high requirement restrictiveness, i.e., TTA, Enhanced TTA, and Active Domain Adaptation (ADA). Intensive experiments indicate that ATTA jointly equips with the efficiency of TTA and the effectiveness of ADA, rendering an uncompromising real-time distribution adaptation direction. Comparison to related studies. Compared to TTA methods, ATTA requires extra active labels, but the failure of TTA methods (Sec. 5.1) and the theoretical proof of Lin et al. (2022); Pearl (2009) justify its necessity and rationality. Compared to active online learning, ATTA focuses on lightweight real-time fine-tuning without round-wise re-trainings as Saran et al. (2023) and emphasizes the importance of CF avoidance instead of resetting models and losing learned distributions. In fact, active online learning is partially similar to our enhanced TTA setting (Sec. 5.2. Compared to ADA methods (Prabhu et al., 2021; Ning et al., 2021), ATTA does not presuppose access to source data, model parameters, or pre-collected target samples. Furthermore, without this information, ATTA can still perform on par with ADA methods (Sec. 5.3). The recent source-free active domain adaptation (SFADA) method SALAD (Kothandaraman et al., 2023) still requires access to model parameter gradients, pre-collected target data, and training of additional networks. Our ATTA, in contrast, with non-regrettable active sample selection on streaming data, is a much lighter and more realistic approach distinct from ADA and SFADA. More related-work discussions are provided in Appx. C. 2 T HE ACTIVE TEST-TIME ADAPTATION FORMULATION TTA methods aim to solve distribution shifts by dynamically optimizing a pre-trained model based on streaming test data. We introduce the novel problem setting of Active Test-Time Adaptation (ATTA), which incorporates active learning during the test phase. In ATTA, the model continuously selects the most informative instances from the test batch to be labeled by an explicit or implicit oracle (e.g., human annotations, self-supervised signals) and subsequently learned by the model, aiming to improve future adaptations. Considering the labeling costs in real-world applications, a “budget” is established for labeled test instances. The model must effectively manage this budget distribution and ensure that the total number of label requests throughout the test phase does not surpass the budget. We now present a formal definition of the ATTA problem. Consider a pre-trained modelf(x; ϕ) with parameters ϕ trained on the source dataset DS = (x, y)|DS|, with each data sample x ∈ Xand a label y ∈ Y. We aim to adapt model parameters θ, initialized as ϕ, to an unlabeled test-time data stream. The streaming test data exhibit distribution shifts from the source data and varies continuously with time, forming multiple domains to which we must continuously adapt. The test phase commences at time step t = 1 and the streaming test data is formulated in batches. The samples are then actively selected, labeled (by the oracle) and collected as Dte(t) = ActAlg(Ute(t)), where ActAlg(·) denotes an active selection/labeling algorithm. The labeled samples Dte(t) are subsequently incorporated into the ATTA training setDtr(t). Finally, we conclude time step t by performing ATTA training, updating model parameters θ(t) using Dtr(t), with θ(t) initialized as the previous final state θ(t − 1). Definition 1 (The ATTA problem). Given a model f(x; θ), with parameters θ, initialized with parameters θ(0) = ϕ obtained by pre-training on source domain data, and streaming test data batches Ute(t) continually changing over time, the ATTA task aims to optimize the model at any time stept (with test phase commencing at t = 1) as θ(t)∗ := argmin θ(t) (E(x,y,t)∈Dtr(t)[ℓCE (f(x; θ(t)), y)] + E(x,t)∈Ute(t)[ℓU (f(x; θ(t)))]), (1) 2Published as a conference paper at ICLR 2024 where Dtr(t) = ( ∅, t = 0 Dtr(t − 1) ∪ Dte(t), t ≥ 1, s.t. |Dtr(t)| ≤ B, (2) Dte(t) = ActAlg(Ute(t)) is actively selected and labeled, ℓCE is the cross entropy loss, ℓU is an unsupervised learning loss, and B is the budget. 3 T HEORETICAL STUDIES In this section, we conduct an in-depth theoretical analysis of TTA based on learning theories. We mainly explore two questions: How can significant distribution shifts be effectively addressed under the TTA setting? How can we simultaneously combat the issue of CF? Sec. 3.1 provides a solution with theoretical guarantees to the first question, namely, active TTA (ATTA), along with the conditions under which distribution shifts can be well addressed. Sec. 3.2 answers the second question with an underexplored technique, i.e., selective entropy minimization, building upon the learning bounds established in Sec. 3.1. We further validate these theoretical findings through experimental analysis. Collectively, we present a theoretically supported ATTA solution that effectively tackles both distribution shift and CF. 3.1 A LLEVIATING DISTRIBUTION SHIFTS THROUGH ACTIVE TEST-TIME ADAPTATION Traditional TTA is performed in unsupervised or self-supervised context. In contrast, ATTA introduces supervision into the adaptation setting. In this subsection, we delve into learning bounds and establish generalization bounds to gauge the efficacy of ATTA in solving distribution shifts. We scrutinize the influence of active learning and evidence that the inclusion of labeled test instances markedly enhances overall performances across incremental test domains. Following Kifer et al. (2004), we examine statistical guarantees for binary classification. A hypothesis is a function h : X → {0, 1}, which can serve as the prediction function within this context. In the ATTA setting, the mapping ofh varies with time as h(x, t). We use H∆H-distance following Ben- David et al. (2010), which essentially provides a measure to quantify the distribution shift between two distributions D1 and D2, and can also be applied between datasets. The probability that an estimated hypothesis h disagrees with the true labeling function g : X → {0, 1} according to distribution D is defined as ϵ(h(t), g) = E(x)∼D[|h(x, t) − g(x)|], which we also refer to as the error or risk ϵ(h(t)). While the source data is inaccessible under ATTA settings, we consider the existence of source dataset DS for accurate theoretical analysis. Thus, we initialize Dtr as Dtr(0) = DS. For every time step t, the test and training data can be expressed asUte(t) and Dtr(t) = DS ∪Dte(1) ∪Dte(2) ∪···∪ Dte(t). Building upon two lemmas (provided in Appx. D), we establish bounds on domain errors under the ATTA setting when minimizing the empirical weighted error using the hypothesish at time t. Theorem 1. Let H be a hypothesis class of VC-dimension d. At time step t, for ATTA data domains DS, Ute(1), ··· , Ute(t), ··· , Si are unlabeled samples of sizem sampled from each of thet+1 domains respectively. The total number of samples in Dtr(t) is N and the ratio of sample numbers in each component is λ = (λ0, ··· , λt). If ˆh(t) ∈ Hminimizes the empirical weighted error ˆϵw(h(t)) with the weight vector w = (w0, ··· , wt) on Dtr(t), and h∗ j (t) = arg minh∈H ϵj(h(t)) is the optimal hypothesis on the jth domain, then for any δ ∈ (0, 1), with probability of at least 1 − δ, we have ϵj(ˆh(t)) ≤ ϵj(h∗ j (t)) + 2 tX i=0,i̸=j wi  1 2 ˆdH∆H(Si, Sj) + 2 s 2d log(2m) + log 2 δ m + γi   + 2C, where C = r\u0010Pt i=0 w2 i λi \u0011\u0010 d log(2N)−log(δ) 2N \u0011 and γi = minh∈H{ϵi(h(t)) + ϵj(h(t))}. For future test domains j = t + k (k >0), assuming k′ = argmink′∈{0,1,...t} dH∆H(D(k′), Ute(t + k)) and min dH∆H (D(k′), Ute(t + k)) ≤ δD, where 0 ≤ δD ≪ +∞, then ∀δ, with probability of at least 1 − δ, we have ϵt+k(ˆh(t)) ≤ ϵt+k(h∗ t+k(t)) + tX i=0 wi  ˆdH∆H(Si, Sk′ ) + 4 s 2d log(2m) + log 2 δ m + δD + 2γi   + 2C. The adaptation performance on a test domain is majorly bounded by the composition of (labeled) training data, estimated distribution shift, and ideal joint hypothesis performance, which correspond to C, ˆdH∆H(Si, Sj), and γi, respectively. The ideal joint hypothesis error γi gauges the inherent adaptability between domains. Further theoretical analysis are in Appx. D. 3Published as a conference paper at ICLR 2024 Figure 1: (a) Empirical validation of Thm. 1. We train a series of models on N = 2000 samples from the PACS (Li et al., 2017) dataset given differentλ0 and w0 and display the test domain loss of each model. Red points are the test loss minimums given a fixed λ0. The orange line is the reference where w0 = λ0. We observe that w0 with loss minimums are located closed to the orange line but slightly smaller than λ0, which validates our findings in Eq. (4). (b) Empirical analysis with an uncertainty balancing. Given source pre-trained models, we fine-tune the models on 500 samples with different λ0 and w0, and display the combined error surface of test and source error. Although a small λ0 is good for test domain error, it can lead to non-trivial source error exacerbation. Therefore, we can observe that the global loss minimum (green X) locates in a relatively high-λ0 region. If we consider the multiple test data distributions as a single test domain,i.e., St i=1 Ute(i), Thm. 1 can be reduced into bounds for the source domain error ϵS and test domain error ϵT . Given the optimal test/source hypothesis h∗ T (t) = arg minh∈H ϵT (h(t)) and h∗ S(t) = arg minh∈H ϵS(h(t)), we have |ϵT (ˆh(t)) − ϵT (h∗ T (t))| ≤w0A + s w2 0 λ0 + (1 − w0)2 1 − λ0 B, (3a) |ϵS(ˆh(t)) − ϵS(h∗ S(t))| ≤(1 − w0)A + s w2 0 λ0 + (1 − w0)2 1 − λ0 B, (3b) where the distribution divergence termA = ˆdH∆H(S0, ST )+4 q 2d log(2m)+log 2 δ m +2γ, the empirical gap term B = 2 q d log(2N)−log(δ) 2N , ST is sampled from St i=1 Ute(i), and γ = minh∈H{ϵ0(h(t)) + ϵT (h(t))}. Our learning bounds demonstrates the trade-off between the small amount of budgeted test-time data and the large amount of less relevant source data. Next, we provide an approximation of the condition necessary to achieve optimal adaptation performance, which is calculable from finite samples and can be readily applied in practical ATTA scenarios. Following Eq. (3.a), with approximatelyB = c1 p d/N, the optimal value w∗ 0 to tighten the test error bound is a function of λ0 and A: w∗ 0 = λ0 − s A2N c2 1d − A2Nλ0(1 − λ0), for λ 0 ≥ 1 − d A2N , (4) where c1 is a constant. Note that λ0 ≥ 1 − d A2N should be the satisfied condition in practical ATTA settings, where the budget is not sufficiently big while the source data amount is relatively large. The following theorem offers a direct theoretical guarantee that ATTA reduces the error bound on test domains in comparison to TTA without the integration of active learning. Theorem 2. Let H be a hypothesis class of VC-dimension d. For ATTA data domains DS, Ute(1), Ute(2), ··· , Ute(t), considering the test-time data as a single test domain St i=1 Ute(i), if ˆh(t) ∈ H minimizes the empirical weighted error ˆϵw(h(t)) with the weight vector w on Dtr(t), let the test error be upper-bounded with |ϵT (ˆh(t)) − ϵT (h∗ T (t))| ≤EBT (w, λ, N, t). Let w′ and λ′ be the weight and sample ratio vectors when no active learning is included, i.e., w′ and λ′ s.t. w′ 0 = λ′ 0 = 1 and w′ i = λ′ i = 0 for i ≥ 1, then for any λ ̸= λ′, there exists w s.t. EBT (w, λ, N, t) < EBT (w′, λ′, N, t). (5) Therefore, the incorporation of labeled test instances in ATTA theoretically enhances the overall performance across test domains, substantiating the significance of the ATTA setting in addressing distribution shifts. All proofs are provided in Appx. E. Finally, we support the theoretical findings with experimental analysis and show the numerical results of applying the principles on real-world datasets, as shown in Fig. 1. For rigorous analysis, note that our theoretical results rest on the underlying condition that N should at least be of the same scale as d, according to the principles of VC-dimension theory. The empirical alignment of our experiments with the theoretical framework can be attributed to the assumption that fine-tuning a model is roughly equivalent to learning a model with a relatively small d. Experiment details and other validations can be found in Appx. H. 4Published as a conference paper at ICLR 2024 3.2 M ITIGATING CATASTROPHIC FORGETTING WITH BALANCED ENTROPY MINIMIZATION Catastrophic forgetting (CF), within the realm of Test-Time Adaptation (TTA), principally manifests as significant declines in overall performance, most notably in the source domain. Despite the lack of well-developed learning theories for analyzing training with series data, empirical studies have convincingly illustrated the crucial role of data sequential arrangement in model learning, thereby accounting for the phenomenon of CF. Traditionally, the mitigation of CF in adaptation tasks involves intricate utilization of source domain data. However, under FTTA settings, access to the source dataset is unavailable, leaving the problem of CF largely unexplored in the data-centric view. Table 1: Correlation analysis of high/low en- tropy samples and domains. We use a source pre-trained model to select samples with low- est/highest entropy, and 1.retrain the model on 2000 samples; 2.fine-tune the model on 300 sam- ples. We report losses on source/test domains for each setting, showing that low-entropy samples form distributions close to the source domain. Sample type Retrain Fine-tune ϵS ϵT ϵS ϵT Low entropy 0.5641 0.8022 0.0619 1.8838 High entropy 2.5117 0.3414 0.8539 0.7725 To overcome this challenge of source dataset ab- sence, we explore the acquisition of “source-like” data. In TTA scenarios, it is generally assumed that the amount of source data is considerably large. We also maintain this assumption in ATTA, practically assuming the volume of source data greatly surpasses the test-time budget. As a re- sult, we can safely assume that the pre-trained model is well-trained on abundant source do- main data DS. Given this adequately trained source model, we can treat it as a “true” source data labeling function f(x; ϕ). The model es- sentially describes a distribution, Dϕ,S(X, Y) = {(x, ˆy) ∈ (X, Y) | ˆy = f(x; ϕ), x∈ DS}. The entropy of the model prediction is defined as H(ˆy) = −P c p(ˆyc) logp(ˆyc), ˆy = f(x; ϕ), where c denotes the class. Lower entropy indicates that the model assigns high probability to one of the classes, suggesting a high level of certainty or confidence in its prediction, which can be interpreted as the sample being well-aligned or fitting closely with the model’s learned distribution. In other words, the model recognizes the sample as being similar to those it was trained on. Thus entropy can be used as an indicator of how closely a sample x aligns with the model distribution Dϕ,S. Since the model distribution is approximately the source distribution, selecting (and labeling) low-entropy samples using f(x; ϕ) essentially provides an estimate of sampling from the source dataset. Therefore, in place of the inaccessible DS, we can feasibly include the source-like dataset into the ATTA training data at each time stept: Dϕ,S(t) = {(x, f(x; ϕ))|x ∈ Ute(t), H(f(x; ϕ)) < el}, (6) where el is the entropy threshold. The assumption that Dϕ,S(t) is an approximation of DS can be empirically validated, as shown by the numerical results on PACS in Tab. 1. In contrast, high-entropy test samples typically deviate more from the source data, from which we select Dte(t) for active labeling. Following the notations in Thm. 1, we are practically minimizing the empirical weighted error of hypothesis h(t) as ˆϵ′ w(h(t)) = tX j=0 wjˆϵj(h(t)) = w0 λ0N X x∈Dϕ,S(t) |h(x, t) − f(x; ϕ)| + tX j=1 wj λjN X x,y∈Dte(j) |h(x, t) − y|. (7) By substituting DS with Dϕ,S(t) in Thm. 1, the bounds of Thm. 1 continue to hold for the test domains. In the corollary below, we bound the source error for practical ATTA at each time stept. Corollary 3. At time step t, for ATTA data domains Dϕ,S(t), Ute(1), Ute(2), ··· , Ute(t), Si are unla- beled samples of size m sampled from each of the t + 1 domains respectively, and SS is unlabeled samples of size m sampled from DS. If ˆh(t) ∈ Hminimizes ˆϵ′ w(h(t)) while other conditions remain identical to Thm. 1, then ϵS(ˆh(t)) ≤ ϵS(h∗ S(t)) + tX i=0 wi  ˆdH∆H(Si, SS) + 4 s 2d log(2m) + log 2 δ m + 2γi   + 2C, with probability at least 1 − δ, where C follows Thm. 1 and γi = minh∈H{ϵi(h(t)) + ϵS(h(t))}. Further analysis and proofs are in Appx. D and E. The following corollary provides direct theoretical support that our strategy conditionally reduces the error bound on the source domain. Corollary 4. At time step t, for ATTA data domains Dϕ,S(t), Ute(1), Ute(2), ··· , Ute(t), suppose that ˆh(t) ∈ Hminimizes ˆϵw′(h(t)) under identical conditions to Thm. 2. Let’s denote the source error upper bound with |ϵS(ˆh(t)) − ϵS(h∗ S(t))| ≤EBS(w, λ, N, t). Let w′ and λ′ be the weight 5Published as a conference paper at ICLR 2024 <latexit sha1_base64=\"NxhXSyFABPQk4q8627/odirDspg=\">AAAB9XicbVDLSgMxFM34rPVVdekmWARXZab4WhbcuKzYF7S1ZNI7bWgmMyR3lDL0P9y4UMSt/+LOvzHTdqGtBwKHc87l3hw/lsKg6347K6tr6xubua389s7u3n7h4LBhokRzqPNIRrrlMwNSKKijQAmtWAMLfQlNf3ST+c1H0EZEqobjGLohGygRCM7QSg/3mIWFGtAaGOwVim7JnYIuE29OimSOaq/w1elHPAlBIZfMmLbnxthNmUbBJUzyncRAzPiIDaBtqWIhmG46vXpCT63Sp0Gk7VNIp+rviZSFxoxD3yZDhkOz6GXif147weC6mwoVJwiKzxYFiaQY0awC2hcaOMqxJYxrYW+lfMg042iLytsSvMUvL5NGueRdli7uysXK+byOHDkmJ+SMeOSKVMgtqZI64USTZ/JK3pwn58V5dz5m0RVnPnNE/sD5/AFnsJJq</latexit> Streaming Test <latexit sha1_base64=\"a41BOKrutEYSWO9+8CjkPZKHvb8=\">AAAB73icbVBNS8NAEJ3Ur1q/qh69BIvgqSTiR48FLx4r2A9oQ9lsN+3SzSbuToQQ+ie8eFDEq3/Hm//GTZuDtj4YeLw3w8w8PxZco+N8W6W19Y3NrfJ2ZWd3b/+genjU0VGiKGvTSESq5xPNBJesjRwF68WKkdAXrOtPb3O/+8SU5pF8wDRmXkjGkgecEjRSbzAhmKWzyrBac+rOHPYqcQtSgwKtYfVrMIpoEjKJVBCt+64To5cRhZwKNqsMEs1iQqdkzPqGShIy7WXze2f2mVFGdhApUxLtufp7IiOh1mnom86Q4EQve7n4n9dPMGh4GZdxgkzSxaIgETZGdv68PeKKURSpIYQqbm616YQoQtFElIfgLr+8SjoXdfe6fnV/WWs2ijjKcAKncA4u3EAT7qAFbaAg4Ble4c16tF6sd+tj0Vqyiplj+APr8wfpIY/e</latexit> ˆy <latexit sha1_base64=\"SJEOE2ZYxLL1SU/QahOlMH6fop4=\">AAAB8HicbVBNSwMxEM3Wr1q/qh69BItQL2VX/Oix4MVjBbettEvJptk2NMkuyaxQlv4KLx4U8erP8ea/MW33oK0PBh7vzTAzL0wEN+C6305hbX1jc6u4XdrZ3ds/KB8etUycasp8GotYd0JimOCK+cBBsE6iGZGhYO1wfDvz209MGx6rB5gkLJBkqHjEKQErPfr9DNi0Cuf9csWtuXPgVeLlpIJyNPvlr94gpqlkCqggxnQ9N4EgIxo4FWxa6qWGJYSOyZB1LVVEMhNk84On+MwqAxzF2pYCPFd/T2REGjORoe2UBEZm2ZuJ/3ndFKJ6kHGVpMAUXSyKUoEhxrPv8YBrRkFMLCFUc3srpiOiCQWbUcmG4C2/vEpaFzXvunZ1f1lp1PM4iugEnaIq8tANaqA71EQ+okiiZ/SK3hztvDjvzseiteDkM8foD5zPH2KnkB4=</latexit> U te ( t ) <latexit sha1_base64=\"7rdY0fXtveVAqOkqa7z+i6K3Rp0=\">AAAB+XicbVDLSsNAFJ34rPUVdelmsAh1UxLxUXBTcOOygn1AE8pkMmmHTiZh5qZQQv/EjQtF3Pon7vwbp20W2nrgwuGce7n3niAVXIPjfFtr6xubW9ulnfLu3v7BoX103NZJpihr0UQkqhsQzQSXrAUcBOumipE4EKwTjO5nfmfMlOaJfIJJyvyYDCSPOCVgpL5tR1WPhgncYQ+GDMhF3644NWcOvErcglRQgWbf/vLChGYxk0AF0brnOin4OVHAqWDTspdplhI6IgPWM1SSmGk/n18+xedGCXGUKFMS8Fz9PZGTWOtJHJjOmMBQL3sz8T+vl0FU93Mu0wyYpItFUSYwJHgWAw65YhTExBBCFTe3YjokilAwYZVNCO7yy6ukfVlzb2rXj1eVRr2Io4RO0RmqIhfdogZ6QE3UQhSN0TN6RW9Wbr1Y79bHonXNKmZO0B9Ynz9h0pLV</latexit> f ( · ; ✓ ) <latexit sha1_base64=\"ud3dFXm+F2nsLD2/MdusutzkLvU=\">AAAB9HicbVDLSgNBEJyNrxhfUY9eBoPgKeyKr2PAixchgnlAsoTZ2d5kyMzOOjMbDEu+w4sHRbz6Md78GyfJHjSxoKGo6qa7K0g408Z1v53Cyura+kZxs7S1vbO7V94/aGqZKgoNKrlU7YBo4CyGhmGGQztRQETAoRUMb6Z+awRKMxk/mHECviD9mEWMEmMlvysC+ZTdyRD4pNQrV9yqOwNeJl5OKihHvVf+6oaSpgJiQznRuuO5ifEzogyjHCalbqohIXRI+tCxNCYCtJ/Njp7gE6uEOJLKVmzwTP09kRGh9VgEtlMQM9CL3lT8z+ukJrr2MxYnqYGYzhdFKcdG4mkCOGQKqOFjSwhVzN6K6YAoQo3NaRqCt/jyMmmeVb3L6sX9eaV2nsdRREfoGJ0iD12hGrpFddRAFD2iZ/SK3pyR8+K8Ox/z1oKTzxyiP3A+fwCmlpH9</latexit> Model SimATTA <latexit sha1_base64=\"bhVea6W/pzUPuDRNfs2xbDF7qAk=\">AAAB73icbVC7SgNBFL3rM8ZX1NJmMAhWYTf4KgM2FhYRzAOSJcxOZpMhs7PrzF0hhPyEjYUitv6OnX/jbLKFJh4YOJxzD3PvCRIpDLrut7Oyura+sVnYKm7v7O7tlw4OmyZONeMNFstYtwNquBSKN1Cg5O1EcxoFkreC0U3mt564NiJWDzhOuB/RgRKhYBSt1L6jQRYd9Eplt+LOQJaJl5My5Kj3Sl/dfszSiCtkkhrT8dwE/QnVKJjk02I3NTyhbEQHvGOpohE3/mS275ScWqVPwljbp5DM1N+JCY2MGUeBnYwoDs2il4n/eZ0Uw2t/IlSSIlds/lGYSoIxyY4nfaE5Qzm2hDIt7K6EDammDG1FRVuCt3jyMmlWK95l5eK+Wq6d53UU4BhO4Aw8uIIa3EIdGsBAwjO8wpvz6Lw4787HfHTFyTNH8AfO5w/1SI/i</latexit> Labeling <latexit sha1_base64=\"7rdY0fXtveVAqOkqa7z+i6K3Rp0=\">AAAB+XicbVDLSsNAFJ34rPUVdelmsAh1UxLxUXBTcOOygn1AE8pkMmmHTiZh5qZQQv/EjQtF3Pon7vwbp20W2nrgwuGce7n3niAVXIPjfFtr6xubW9ulnfLu3v7BoX103NZJpihr0UQkqhsQzQSXrAUcBOumipE4EKwTjO5nfmfMlOaJfIJJyvyYDCSPOCVgpL5tR1WPhgncYQ+GDMhF3644NWcOvErcglRQgWbf/vLChGYxk0AF0brnOin4OVHAqWDTspdplhI6IgPWM1SSmGk/n18+xedGCXGUKFMS8Fz9PZGTWOtJHJjOmMBQL3sz8T+vl0FU93Mu0wyYpItFUSYwJHgWAw65YhTExBBCFTe3YjokilAwYZVNCO7yy6ukfVlzb2rXj1eVRr2Io4RO0RmqIhfdogZ6QE3UQhSN0TN6RW9Wbr1Y79bHonXNKmZO0B9Ynz9h0pLV</latexit> f ( · ; ✓ ) <latexit sha1_base64=\"DPrA95GNP27SFW5vSoLC/hYa644=\">AAAB9XicbVDLSsNAFJ3UV62vqks3g0Wom5KIj4KbghuXFewDmlgmk0k7dJIJMzdKCf0PNy4Uceu/uPNvnLZZaOuBC4dz7uXee/xEcA22/W0VVlbX1jeKm6Wt7Z3dvfL+QVvLVFHWolJI1fWJZoLHrAUcBOsmipHIF6zjj26mfueRKc1lfA/jhHkRGcQ85JSAkR7CqksDCdfYTYb8tF+u2DV7BrxMnJxUUI5mv/zlBpKmEYuBCqJ1z7ET8DKigFPBJiU31SwhdEQGrGdoTCKmvWx29QSfGCXAoVSmYsAz9fdERiKtx5FvOiMCQ73oTcX/vF4KYd3LeJykwGI6XxSmAoPE0whwwBWjIMaGEKq4uRXTIVGEggmqZEJwFl9eJu2zmnNZu7g7rzTqeRxFdISOURU56Ao10C1qohaiSKFn9IrerCfrxXq3PuatBSufOUR/YH3+AFKlkbs=</latexit> f ( · ; \u0000 ) <latexit sha1_base64=\"DPrA95GNP27SFW5vSoLC/hYa644=\">AAAB9XicbVDLSsNAFJ3UV62vqks3g0Wom5KIj4KbghuXFewDmlgmk0k7dJIJMzdKCf0PNy4Uceu/uPNvnLZZaOuBC4dz7uXee/xEcA22/W0VVlbX1jeKm6Wt7Z3dvfL+QVvLVFHWolJI1fWJZoLHrAUcBOsmipHIF6zjj26mfueRKc1lfA/jhHkRGcQ85JSAkR7CqksDCdfYTYb8tF+u2DV7BrxMnJxUUI5mv/zlBpKmEYuBCqJ1z7ET8DKigFPBJiU31SwhdEQGrGdoTCKmvWx29QSfGCXAoVSmYsAz9fdERiKtx5FvOiMCQ73oTcX/vF4KYd3LeJykwGI6XxSmAoPE0whwwBWjIMaGEKq4uRXTIVGEggmqZEJwFl9eJu2zmnNZu7g7rzTqeRxFdISOURU56Ao10C1qohaiSKFn9IrerCfrxXq3PuatBSufOUR/YH3+AFKlkbs=</latexit> f ( · ; \u0000 ) <latexit sha1_base64=\"ipQ+JKlINPDcPjrbUYUkqyyzp40=\">AAAB+nicbVC7TsMwFHXKq5RXCiOLRYXEQpVUvMZKLIxF0IfURpXj3LRWHSeyHVBV+iksDCDEypew8Te4aQZoOZKlo3Puy8dPOFPacb6twsrq2vpGcbO0tb2zu2eX91sqTiWFJo15LDs+UcCZgKZmmkMnkUAin0PbH13P/PYDSMVica/HCXgRGQgWMkq0kfp2+S6bdNqQoCUxQ4K+XXGqTga8TNycVFCORt/+6gUxTSMQmnKiVNd1Eu1NiNSMcpiWeqmChNARGUDXUEEiUN4kO32Kj40S4DCW5gmNM/V3x4RESo0j31RGRA/VojcT//O6qQ6vvAkTSapB0PmiMOVYx3iWAw6YBKr52BBCJTO3YjokklBt0iqZENzFLy+TVq3qXlTPb2uV+lkeRxEdoiN0glx0ieroBjVQE1H0iJ7RK3qznqwX6936mJcWrLznAP2B9fkDSAyT+w==</latexit> Source-Pretrained <latexit sha1_base64=\"ud3dFXm+F2nsLD2/MdusutzkLvU=\">AAAB9HicbVDLSgNBEJyNrxhfUY9eBoPgKeyKr2PAixchgnlAsoTZ2d5kyMzOOjMbDEu+w4sHRbz6Md78GyfJHjSxoKGo6qa7K0g408Z1v53Cyura+kZxs7S1vbO7V94/aGqZKgoNKrlU7YBo4CyGhmGGQztRQETAoRUMb6Z+awRKMxk/mHECviD9mEWMEmMlvysC+ZTdyRD4pNQrV9yqOwNeJl5OKihHvVf+6oaSpgJiQznRuuO5ifEzogyjHCalbqohIXRI+tCxNCYCtJ/Njp7gE6uEOJLKVmzwTP09kRGh9VgEtlMQM9CL3lT8z+ukJrr2MxYnqYGYzhdFKcdG4mkCOGQKqOFjSwhVzN6K6YAoQo3NaRqCt/jyMmmeVb3L6sX9eaV2nsdRREfoGJ0iD12hGrpFddRAFD2iZ/SK3pyR8+K8Ox/z1oKTzxyiP3A+fwCmlpH9</latexit> Model <latexit sha1_base64=\"5LNAmmVR/AN9Lc2T+FRV/is2yz8=\">AAAB8nicbVDLSgNBEJyNrxhfUY9eBoPgKewGX8eACB48RDAP2CxhdjKbDJmdWWZ6lbDkM7x4UMSrX+PNv3GS7EETCxqKqm66u8JEcAOu++0UVlbX1jeKm6Wt7Z3dvfL+QcuoVFPWpEoo3QmJYYJL1gQOgnUSzUgcCtYOR9dTv/3ItOFKPsA4YUFMBpJHnBKwkn+nnvCNBK2Sca9ccavuDHiZeDmpoByNXvmr21c0jZkEKogxvucmEGREA6eCTUrd1LCE0BEZMN9SSWJmgmx28gSfWKWPI6VtScAz9fdERmJjxnFoO2MCQ7PoTcX/PD+F6CrIuExSYJLOF0WpwKDw9H/c55pREGNLCNXc3orpkGhCwaZUsiF4iy8vk1at6l1Uz+9rlfpZHkcRHaFjdIo8dInq6BY1UBNRpNAzekVvDjgvzrvzMW8tOPnMIfoD5/MHKbiRJQ==</latexit> Low Entropy <latexit sha1_base64=\"vLgKkEyV9E/djVdgAkvKuOUQOTU=\">AAAB7nicbVDLSgMxFL1TX7W+qi7dBIvgqswUX8uCG5cV7QPaoWTSTBuaZEKSEcrQj3DjQhG3fo87/8a0nYW2HrhwOOde7r0nUpwZ6/vfXmFtfWNzq7hd2tnd2z8oHx61TJJqQpsk4YnuRNhQziRtWmY57ShNsYg4bUfj25nffqLasEQ+2omiocBDyWJGsHVS+wELxanplyt+1Z8DrZIgJxXI0eiXv3qDhKSCSks4NqYb+MqGGdaWEU6npV5qqMJkjIe066jEgpowm587RWdOGaA40a6kRXP190SGhTETEblOge3ILHsz8T+vm9r4JsyYVKmlkiwWxSlHNkGz39GAaUosnziCiWbuVkRGWGNiXUIlF0Kw/PIqadWqwVX18r5WqV/kcRThBE7hHAK4hjrcQQOaQGAMz/AKb57yXrx372PRWvDymWP4A+/zB19wj48=</latexit> Samples <latexit sha1_base64=\"wuZucU3JbeEJSquG2WgqGdYMCR8=\">AAAB83icbVDLSgMxFL3js9ZX1aWbYBFclZnia1kQocsK9gHtUDJppg3NJCHJCGXob7hxoYhbf8adf2PazkJbD1w4nHMv994TKc6M9f1vb219Y3Nru7BT3N3bPzgsHR23jEw1oU0iudSdCBvKmaBNyyynHaUpTiJO29H4bua3n6g2TIpHO1E0TPBQsJgRbJ3Uq7PhCN0Lq6Wa9Etlv+LPgVZJkJMy5Gj0S1+9gSRpQoUlHBvTDXxlwwxrywin02IvNVRhMsZD2nVU4ISaMJvfPEXnThmgWGpXwqK5+nsiw4kxkyRynQm2I7PszcT/vG5q49swY0KllgqyWBSnHFmJZgGgAdOUWD5xBBPN3K2IjLDGxLqYii6EYPnlVdKqVoLrytVDtVy7zOMowCmcwQUEcAM1qEMDmkBAwTO8wpuXei/eu/exaF3z8pkT+APv8wfIYpF9</latexit> High Entropy <latexit sha1_base64=\"vLgKkEyV9E/djVdgAkvKuOUQOTU=\">AAAB7nicbVDLSgMxFL1TX7W+qi7dBIvgqswUX8uCG5cV7QPaoWTSTBuaZEKSEcrQj3DjQhG3fo87/8a0nYW2HrhwOOde7r0nUpwZ6/vfXmFtfWNzq7hd2tnd2z8oHx61TJJqQpsk4YnuRNhQziRtWmY57ShNsYg4bUfj25nffqLasEQ+2omiocBDyWJGsHVS+wELxanplyt+1Z8DrZIgJxXI0eiXv3qDhKSCSks4NqYb+MqGGdaWEU6npV5qqMJkjIe066jEgpowm587RWdOGaA40a6kRXP190SGhTETEblOge3ILHsz8T+vm9r4JsyYVKmlkiwWxSlHNkGz39GAaUosnziCiWbuVkRGWGNiXUIlF0Kw/PIqadWqwVX18r5WqV/kcRThBE7hHAK4hjrcQQOaQGAMz/AKb57yXrx372PRWvDymWP4A+/zB19wj48=</latexit> Samples <latexit sha1_base64=\"1BO6D/gzkeZNQ7HNIaph5NqELCI=\">AAAB8nicbVDLSgMxFM3UV62vqks3wSK4KjPF17LgRncV7AOmQ8mkd9rQTDIkGaEM/Qw3LhRx69e482/MtLPQ1gOBwzn3kHtPmHCmjet+O6W19Y3NrfJ2ZWd3b/+genjU0TJVFNpUcql6IdHAmYC2YYZDL1FA4pBDN5zc5n73CZRmUjyaaQJBTEaCRYwSYyX/XlAFMQhD+KBac+vuHHiVeAWpoQKtQfWrP5Q0zdOUE619z01MkBFlGOUwq/RTDQmhEzIC31JBYtBBNl95hs+sMsSRVPYJg+fq70RGYq2ncWgnY2LGetnLxf88PzXRTZAxkaQGBF18FKUcG4nz+/GQKaCGTy0hVDG7K6Zjogg1tqWKLcFbPnmVdBp176p++dCoNS+KOsroBJ2ic+Sha9REd6iF2ogiiZ7RK3pzjPPivDsfi9GSU2SO0R84nz9y2ZFU</latexit> Incremental <latexit sha1_base64=\"Jmobmj50NeE6y3ftB4xt5xZD5Eg=\">AAAB8XicbVDLSgNBEOyNrxhfUY9eBoPgKewGX8dALh4jmAcmS5id9CZDZmeXmVkhLP6FFw+KePVvvPk3TpI9aGJBQ1HVTXdXkAiujet+O4W19Y3NreJ2aWd3b/+gfHjU1nGqGLZYLGLVDahGwSW2DDcCu4lCGgUCO8GkMfM7j6g0j+W9mSboR3QkecgZNVZ6aIhUG1Rcjgblilt15yCrxMtJBXI0B+Wv/jBmaYTSMEG17nluYvyMKsOZwKdSP9WYUDahI+xZKmmE2s/mFz+RM6sMSRgrW9KQufp7IqOR1tMosJ0RNWO97M3E/7xeasIbP+MySQ1KtlgUpoKYmMzeJ0OukBkxtYQyxe2thI2posymoEs2BG/55VXSrlW9q+rlXa1Sv8jjKMIJnMI5eHANdbiFJrSAgYRneIU3RzsvzrvzsWgtOPnMMfyB8/kDzgaQ+A==</latexit> Clustering <latexit sha1_base64=\"c4xrXg0yZYBSSDLHCxlf45OWNzg=\">AAAB7nicbVDLSgNBEOz1GeMr6tHLYBA8hd2Aj2PAi8eI5gHJEmYnnWTIzOwyMyuEJR/hxYMiXv0eb/6Nk2QPmljQUFR1090VJYIb6/vf3tr6xubWdmGnuLu3f3BYOjpumjjVDBssFrFuR9Sg4AoblluB7UQjlZHAVjS+nfmtJ9SGx+rRThIMJR0qPuCMWie1HqhMBJpeqexX/DnIKglyUoYc9V7pq9uPWSpRWSaoMZ3AT2yYUW05EzgtdlODCWVjOsSOo4pKNGE2P3dKzp3SJ4NYu1KWzNXfExmVxkxk5DoltSOz7M3E/7xOagc3YcZVklpUbLFokApiYzL7nfS5RmbFxBHKNHe3EjaimjLrEiq6EILll1dJs1oJriqX99VyrZrHUYBTOIMLCOAaanAHdWgAgzE8wyu8eYn34r17H4vWNS+fOYE/8D5/AF7Wj40=</latexit> Samples <latexit sha1_base64=\"eimCpRgfVxBfxhwCehIJdcsMsvY=\">AAAB8XicbVA9SwNBEJ2LXzF+RS1tFoNgFe5SRMtAGssI5gOTI+xt5pIle3vH7p4QjvwLGwtFbP03dv4bN8kVmvhg4PHeDDPzgkRwbVz32ylsbe/s7hX3SweHR8cn5dOzjo5TxbDNYhGrXkA1Ci6xbbgR2EsU0igQ2A2mzYXffUKleSwfzCxBP6JjyUPOqLHSY1Ok2qDicjwsV9yquwTZJF5OKpCjNSx/DUYxSyOUhgmqdd9zE+NnVBnOBM5Lg1RjQtmUjrFvqaQRaj9bXjwnV1YZkTBWtqQhS/X3REYjrWdRYDsjaiZ63VuI/3n91IS3fsZlkhqUbLUoTAUxMVm8T0ZcITNiZgllittbCZtQRZlNQZdsCN76y5ukU6t69Wr9vlZpuHkcRbiAS7gGD26gAXfQgjYwkPAMr/DmaOfFeXc+Vq0FJ585hz9wPn8AzSSQ9Q==</latexit> Clustering <latexit sha1_base64=\"JgGHFC5oztwX6+XjDtZWQo9C1hA=\">AAAB7nicbVA9TwJBEJ3DL8Qv1NJmIzGxIncUaImxscREwAQuZG8ZYMPe7mV3z4Rc+BE2Fhpj6++x89+4wBUKvmSSl/dmMjMvSgQ31ve/vcLG5tb2TnG3tLd/cHhUPj5pG5Vqhi2mhNKPETUouMSW5VbgY6KRxpHATjS5nfudJ9SGK/lgpwmGMR1JPuSMWid1biQbK2365Ypf9Rcg6yTISQVyNPvlr95AsTRGaZmgxnQDP7FhRrXlTOCs1EsNJpRN6Ai7jkoaowmzxbkzcuGUARkq7UpaslB/T2Q0NmYaR64zpnZsVr25+J/XTe3wOsy4TFKLki0XDVNBrCLz38mAa2RWTB2hTHN3K2FjqimzLqGSCyFYfXmdtGvVoF6t39cqDT+PowhncA6XEMAVNOAOmtACBhN4hld48xLvxXv3PpatBS+fOYU/8D5/AFOaj4U=</latexit> Anchors <latexit sha1_base64=\"eimCpRgfVxBfxhwCehIJdcsMsvY=\">AAAB8XicbVA9SwNBEJ2LXzF+RS1tFoNgFe5SRMtAGssI5gOTI+xt5pIle3vH7p4QjvwLGwtFbP03dv4bN8kVmvhg4PHeDDPzgkRwbVz32ylsbe/s7hX3SweHR8cn5dOzjo5TxbDNYhGrXkA1Ci6xbbgR2EsU0igQ2A2mzYXffUKleSwfzCxBP6JjyUPOqLHSY1Ok2qDicjwsV9yquwTZJF5OKpCjNSx/DUYxSyOUhgmqdd9zE+NnVBnOBM5Lg1RjQtmUjrFvqaQRaj9bXjwnV1YZkTBWtqQhS/X3REYjrWdRYDsjaiZ63VuI/3n91IS3fsZlkhqUbLUoTAUxMVm8T0ZcITNiZgllittbCZtQRZlNQZdsCN76y5ukU6t69Wr9vlZpuHkcRbiAS7gGD26gAXfQgjYwkPAMr/DmaOfFeXc+Vq0FJ585hz9wPn8AzSSQ9Q==</latexit> Clustering <latexit sha1_base64=\"JgGHFC5oztwX6+XjDtZWQo9C1hA=\">AAAB7nicbVA9TwJBEJ3DL8Qv1NJmIzGxIncUaImxscREwAQuZG8ZYMPe7mV3z4Rc+BE2Fhpj6++x89+4wBUKvmSSl/dmMjMvSgQ31ve/vcLG5tb2TnG3tLd/cHhUPj5pG5Vqhi2mhNKPETUouMSW5VbgY6KRxpHATjS5nfudJ9SGK/lgpwmGMR1JPuSMWid1biQbK2365Ypf9Rcg6yTISQVyNPvlr95AsTRGaZmgxnQDP7FhRrXlTOCs1EsNJpRN6Ai7jkoaowmzxbkzcuGUARkq7UpaslB/T2Q0NmYaR64zpnZsVr25+J/XTe3wOsy4TFKLki0XDVNBrCLz38mAa2RWTB2hTHN3K2FjqimzLqGSCyFYfXmdtGvVoF6t39cqDT+PowhncA6XEMAVNOAOmtACBhN4hld48xLvxXv3PpatBS+fOYU/8D5/AFOaj4U=</latexit> Anchors <latexit sha1_base64=\"KzBZ8R84UC9mpPFQBWeRHFxcqjw=\">AAAB7XicbVBNS8NAEJ3Ur1q/qh69LBbBU0mKVI8FLx4rmLbQhrLZbNq1m92wuxFK6H/w4kERr/4fb/4bt20O2vpg4PHeDDPzwpQzbVz32yltbG5t75R3K3v7B4dH1eOTjpaZItQnkkvVC7GmnAnqG2Y47aWK4iTktBtObud+94kqzaR4MNOUBgkeCRYzgo2VOn4aYUOH1ZpbdxdA68QrSA0KtIfVr0EkSZZQYQjHWvc9NzVBjpVhhNNZZZBpmmIywSPat1TghOogX1w7QxdWiVAslS1h0EL9PZHjROtpEtrOBJuxXvXm4n9ePzPxTZAzkWaGCrJcFGccGYnmr6OIKUoMn1qCiWL2VkTGWGFibEAVG4K3+vI66TTqXrPevG/UWldFHGU4g3O4BA+uoQV30AYfCDzCM7zCmyOdF+fd+Vi2lpxi5hT+wPn8AYuwjxQ=</latexit> Update <latexit sha1_base64=\"y2NH6tDs2GygUDqZYglGwvR4SpA=\">AAAB+nicbVBNSwMxEJ2tX7V+bfXoJVgEQSi7PVSPFS8eK9oPaEvJptk2NMkuSVYpa3+KFw+KePWXePPfmLZ70NYHA4/3ZpiZF8ScaeN5305ubX1jcyu/XdjZ3ds/cIuHTR0litAGiXik2gHWlDNJG4YZTtuxolgEnLaC8fXMbz1QpVkk780kpj2Bh5KFjGBjpb5bvMMi5lSjc3QlyShSuu+WvLI3B1olfkZKkKHed7+6g4gkgkpDONa643ux6aVYGUY4nRa6iaYxJmM8pB1LJRZU99L56VN0apUBCiNlSxo0V39PpFhoPRGB7RTYjPSyNxP/8zqJCS97KZNxYqgki0VhwpGJ0CwHNGCKEsMnlmCimL0VkRFWmBibVsGG4C+/vEqalbJfLVdvK6Wal8WRh2M4gTPw4QJqcAN1aACBR3iGV3hznpwX5935WLTmnGzmCP7A+fwBUnKTWg==</latexit> Samples + Anchors <latexit sha1_base64=\"u0BDOcH87PXd3DsT+o414+7cHnI=\">AAAB7XicbZC7SgNBFIbPxluMt6ilIINBsAq7FjGdARvLBMwFkhBmZ2eTMbMzy8ysEJaU9jYWitj6Cql8CDufwZdwcik0+sPAx/+fw5xz/JgzbVz308msrK6tb2Q3c1vbO7t7+f2DhpaJIrROJJeq5WNNORO0bpjhtBUriiOf06Y/vJrmzTuqNJPixoxi2o1wX7CQEWys1eiQQBrdyxfcojsT+gveAgqX75Pa1/3xpNrLf3QCSZKICkM41rrtubHpplgZRjgd5zqJpjEmQ9ynbYsCR1R309m0Y3RqnQCFUtknDJq5PztSHGk9inxbGWEz0MvZ1PwvaycmLHdTJuLEUEHmH4UJR0ai6eooYIoSw0cWMFHMzorIACtMjD1Qzh7BW175LzTOi16pWKq5hUoZ5srCEZzAGXhwARW4hirUgcAtPMATPDvSeXRenNd5acZZ9BzCLzlv33Yvk3g=</latexit> ··· <latexit sha1_base64=\"+7L/8ObZcl+JIZaSFhVO3t+lUUE=\">AAAB7XicbVDLSgNBEOyNrxhf8XHzMhiEeAm7ItFjQA8eI5gHJCHMTmaT0dnZZaZXCEv+wYsHRbz6P978GyebHDSxoKGo6qa7y4+lMOi6305uZXVtfSO/Wdja3tndK+4fNE2UaMYbLJKRbvvUcCkUb6BAydux5jT0JW/5j9dTv/XEtRGRusdxzHshHSoRCEbRSs2bvizjWb9YcituBrJMvDkp1Y6CDPV+8as7iFgScoVMUmM6nhtjL6UaBZN8UugmhseUPdIh71iqaMhNL82unZBTqwxIEGlbCkmm/p5IaWjMOPRtZ0hxZBa9qfif10kwuOqlQsUJcsVmi4JEEozI9HUyEJozlGNLKNPC3krYiGrK0AZUsCF4iy8vk+Z5xatWqnc2jQuYIQ/HcAJl8OASanALdWgAgwd4hld4cyLnxXl3PmatOWc+cwh/4Hz+AFjYkTs=</latexit> D l ( t ) <latexit sha1_base64=\"9C0bB8PYImk9DX0HLfGvGd44PFA=\">AAAB7XicbVDLSgNBEOyNrxhf8XHzMhiEeAm7ItFjQA8eI5gHJCHMTmaT0dnZZaZXCEv+wYsHRbz6P978GyebHDSxoKGo6qa7y4+lMOi6305uZXVtfSO/Wdja3tndK+4fNE2UaMYbLJKRbvvUcCkUb6BAydux5jT0JW/5j9dTv/XEtRGRusdxzHshHSoRCEbRSs2b/qiMZ/1iya24Gcgy8eakVDsKMtT7xa/uIGJJyBUySY3peG6MvZRqFEzySaGbGB5T9kiHvGOpoiE3vTS7dkJOrTIgQaRtKSSZ+nsipaEx49C3nSHFkVn0puJ/XifB4KqXChUnyBWbLQoSSTAi09fJQGjOUI4toUwLeythI6opQxtQwYbgLb68TJrnFa9aqd7ZNC5ghjwcwwmUwYNLqMEt1KEBDB7gGV7hzYmcF+fd+Zi15pz5zCH8gfP5A1K8kTc=</latexit> D h ( t ) <latexit sha1_base64=\"eNrtnhPGeU8n4BRDMStm5cjQ4ts=\">AAAB73icbVBNS8NAEJ34WetX1aOXxSJ4KkmR6rHQi8cK9gPaUDbbTbt0s4m7E6GE/gkvHhTx6t/x5r9x2+agrQ8GHu/NMDMvSKQw6Lrfzsbm1vbObmGvuH9weHRcOjltmzjVjLdYLGPdDajhUijeQoGSdxPNaRRI3gkmjbnfeeLaiFg94DThfkRHSoSCUbRS1zNIGlTKQansVtwFyDrxclKGHM1B6as/jFkacYVMUmN6npugn1GNgkk+K/ZTwxPKJnTEe5YqGnHjZ4t7Z+TSKkMSxtqWQrJQf09kNDJmGgW2M6I4NqveXPzP66UY3vqZUEmKXLHlojCVBGMyf54MheYM5dQSyrSwtxI2ppoytBEVbQje6svrpF2teLVK7b5arl/ncRTgHC7gCjy4gTrcQRNawEDCM7zCm/PovDjvzseydcPJZ87gD5zPH1Naj3k=</latexit> 1st Call <latexit sha1_base64=\"mxsL+XuWb2hqFND+pzTctrB1rcY=\">AAAB73icbVBNS8NAEJ34WetX1aOXxSJ4KkmR6rHQi8cK9gPaUDababt0s4m7G6GE/gkvHhTx6t/x5r9x2+agrQ8GHu/NMDMvSATXxnW/nY3Nre2d3cJecf/g8Oi4dHLa1nGqGLZYLGLVDahGwSW2DDcCu4lCGgUCO8GkMfc7T6g0j+WDmSboR3Qk+ZAzaqzUrcqQNKgQg1LZrbgLkHXi5aQMOZqD0lc/jFkaoTRMUK17npsYP6PKcCZwVuynGhPKJnSEPUsljVD72eLeGbm0SkiGsbIlDVmovycyGmk9jQLbGVEz1qveXPzP66VmeOtnXCapQcmWi4apICYm8+dJyBUyI6aWUKa4vZWwMVWUGRtR0Ybgrb68TtrViler1O6r5fp1HkcBzuECrsCDG6jDHTShBQwEPMMrvDmPzovz7nwsWzecfOYM/sD5/AE0o49l</latexit> 2nd Call <latexit sha1_base64=\"oSA1OFmXXL9y3PJtqoVxTIG9mto=\">AAAB8HicbVA9TwJBEJ3DL8Qv1NJmIzGxIncUaElCY2UwkQ8DF7K3zMGGvb3L7p6REH6FjYXG2Ppz7Pw3LnCFgi+Z5OW9mczMCxLBtXHdbye3sbm1vZPfLeztHxweFY9PWjpOFcMmi0WsOgHVKLjEpuFGYCdRSKNAYDsY1+d++xGV5rG8N5ME/YgOJQ85o8ZKD7f4ZEidCtEvltyyuwBZJ15GSpCh0S9+9QYxSyOUhgmqdddzE+NPqTKcCZwVeqnGhLIxHWLXUkkj1P50cfCMXFhlQMJY2ZKGLNTfE1MaaT2JAtsZUTPSq95c/M/rpia89qdcJqlByZaLwlQQE5P592TAFTIjJpZQpri9lbARVZQZm1HBhuCtvrxOWpWyVy1X7yqlWiWLIw9ncA6X4MEV1OAGGtAEBhE8wyu8Ocp5cd6dj2VrzslmTuEPnM8fSFeQCA==</latexit> Next Call Figure 2: Overview of the SimATTA framework. and sample ratio vectors when Dϕ,S(t) is not included, i.e., w′ and λ′ s.t. w′ 0 = λ′ 0 = 0 . If ˆdH∆H(DS, Dϕ,S(t)) < ˆdH∆H(DS, St i=1 Ute(i)), then for any λ ̸= λ′, there exists w s.t. EBS(w, λ, N, t) < EBS(w′, λ′, N, t). (8) Corollary 4 validates that the selected low-entropy samples can mitigate the CF problem under the assumption that these samples are source-like, which is also empirically validated in Fig. 1. Note that our strategy employs entropy minimization in a selective manner, aiming to solve CF rather than the main adaptation issue. While many FTTA works use entropy minimization to adapt across domains without guarantees, our use is more theoretically-sound. 4 A N ATTA ALGORITHM Building on our theoretical findings, we introduce a simple yet effective ATTA method, known as SimATTA, that innovatively integrates incremental clustering and selective entropy minimization techniques, as illustrated in Fig. 2. We start with an overview of our methodology, including the learning framework and the comprehensive sample selection strategies. We then proceed to discuss the details of the incremental clustering technique designed for real-time sample selections. 4.1 A LGORITHM OVERVIEW Let (x, y) be a labeled sample and f(·; θ) be our neural network, where ˆy = f(x; θ) and θ represents the parameters. We have a model pre-trained on source domains with the pre-trained parameters ϕ. We initialize model parameters as θ(0) = ϕ and aim to adapt the model f(·; θ) in real-time. During the test phase, the model continuously predicts labels for streaming-in test data and concurrently gets fine-tuned. We perform sample selection to enable active learning. As discussed in Sec. 3.2, we empirically consider informative high-entropy samples for addressing distribution shifts and source-like low-entropy samples to mitigate CF. As shown in Alg. 1, at each time step t, we first partition unlabeled test samples Ute(t) into high entropy and low entropy datasets, Uh(t) and Ul(t), using an entropy threshold. The source-pretrained model f(·; ϕ) is frozen to predict pseudo labels for low entropy data. We obtain labeled low-entropy data Dl(t) by labeling Ul(t) with f(·; ϕ) and combining it with Dl(t − 1). In contrast, the selection of high-entropy samples for active labeling is less straightforward. Since the complete test dataset is inaccessible for analyzing the target domain distribution, real-time sample selection is required. We design an incremental clustering sample selection technique to reduce sample redundancy and increase distribution coverage, detailed in Sec. 4.2. The incremental clustering algorithm outputs the labeled test samples Dh(t), also referred to as anchors, given Dh(t −1) and Uh(t). After sample selection, the model undergoes test-time training using the labeled test anchors Dh(t) and pseudo-labeled source-like anchors Dl(t). Following the analyses in Sec. 3.1, the training weights and sample numbers should satisfy w(t) ≈ λ(t) for Dh(t) and Dl(t) for optimal results. The analyses and results in Sec. 3.2 further indicate that balancing the source and target ratio is the key to mitigating CF. However, when source-like samples significantly outnumber test samples, the optimal w(t) for test domains can deviate from λ(t) according to Eq. (4). 4.2 I NCREMENTAL CLUSTERING We propose incremental clustering, a novel continual clustering technique designed to select informa- tive samples in unsupervised settings under the ATTA framework. The primary goal of this strategy is to store representative samples for distributions seen so far. Intuitively, we apply clusters to cover all seen distributions while adding new clusters to cover newly seen distributions. During this process with new clusters added, old clusters may be merged due to the limit of the cluster budget. Since 6Published as a conference paper at ICLR 2024 Algorithm 1 SIMATTA: A SIMPLE ATTA ALGORITHM Require: A fixed source pre-trained model f(·; ϕ) and a real-time adapting model f(·; θ(t)) with θ(0) = ϕ. Streaming test data Ute(t) at time step t. Entropy of predictions H(ˆy) = −P c p(ˆyc) logp(ˆyc). Low entropy and high entropy thresholds el and eh. The number of cluster centroid budget NC (t) at time step t. Centroid increase number k. Learning step size η. 1: for t = 1, . . . , Tdo 2: Model inference on Ute(t) using f(·; θ(t − 1)). 3: Dl(t) ← Dl(t − 1) ∪ {(x, f(x; ϕ))|x ∈ Ute(t), H(f(x; ϕ)) < el} 4: Uh(t) ← {x|x ∈ Ute(t), H(f(x; θ)) > eh} 5: Dh(t) ← Dh(t − 1) ∪ {(x, y)|∀x ∈ IC(Dh(t − 1), Uh(t), NC(t)), y= Oracle(x)} 6: λ(t) ← |Dl(t)|/(|Dl(t)| + |Dh(t)|), |Dh(t)|/(|Dl(t)| + |Dh(t)|) 7: w(t) ← GetW(λ(t)) ▷ Generally, GetW(λ(t)) = λ(t) is a fair choice. 8: θ(t) ← θ(t − 1) 9: for (xl, yl) in Dl and (xh, yh) in Dh do 10: θ(t) ← θ(t) − ηw0∇ℓCE (f(xl; θ(t)), yl) − η(1 − w0)∇ℓCE (f(xh; θ(t)), yh) 11: end for 12: NC (t + 1) ← UpdateCentroidNum(NC (t)) ▷ Naive choice: NC (t + 1) ← NC (t) + k. 13: end for clusters cannot be stored efficiently, we store the representative samples of clusters, named anchors, instead. In this work, we adopt weighted K-means (Krishna and Murty, 1999) as our base clustering method due to its popularity and suitability for new setting explorations. When we apply clustering with new samples, a previously selected anchor should not weigh the same as new samples since the anchor is a representation of a cluster,i.e., a representation of many samples. Instead, the anchor should be considered as a barycenter with a weight of the sum of its cluster’s sample weights. For a newly added cluster, its new anchor has the weight of the whole cluster. For clusters containing multiple old anchors, i.e., old clusters, the increased weights are distributed equally among these anchors. These increased weights are contributed by new samples that are close to these old anchors. Intuitively, this process of clustering is analogous to the process of planet formation. Where there are no planets, new planets (anchors) will be formed by the aggregation of the surrounding material (samples). Where there are planets, the matter is absorbed by the surrounding planets. This example is only for better understanding without specific technical meanings. Specifically, we provide the detailed Alg. 2 for incremental clustering. In each iteration, we apply weighted K-Means for previously selected anchors Danc and the new streaming-in unlabeled data Unew. We first extract all sample features using the model from the previous step f(·; θ(t − 1)), and then cluster these weighted features. The initial weights of the new unlabeled samples are 1, while anchors inherit weights from previous iterations. After clustering, clusters including old anchors are old clusters, while clusters only containing new samples are newly formed ones. For each new cluster, we select the centroid-closest sample as the new anchor to store. As shown in line 10 of Alg. 2, for both old and new clusters, we distribute the sample weights in this cluster as its anchors’ weights. With incremental clustering, although we can control the number of clusters in each iteration, we cannot control the number of new clusters/new anchors. This indirect control makes the increase of new anchors adaptive to the change of distributions, but it also leads to indirect budget control. Therefore, in experimental studies, we set the budget limit, but the actual anchor budget will not reach this limit. The overall extra storage requirement is O(B) since the number of saved unlabeled samples is proportional to the number of saved labeled samples (anchors). 5 E XPERIMENTAL STUDIES In this study, we aim to validate the effectiveness of our proposed method, as well as explore the various facets of the ATTA setting. Specifically, we design experiments around the following research questions: RQ1: Can TTA methods address domain distribution shifts? RQ2: Is ATTA as efficient as TTA? RQ3: How do the components of SimATTA perform? RQ4: Can ATTA perform on par with stronger Active Domain Adaptation (ADA) methods? We compare ATTA with three settings, TTA (Tab. 2), enhanced TTA (Tab. 3 and 5), and ADA (Tab. 4). Datasets. To assess the OOD performance of the TTA methods, we benchmark them using datasets from DomainBed (Gulrajani and Lopez-Paz, 2020) and Hendrycks and Dietterich (2019a). We employ PACS (Li et al., 2017), VLCS (Fang et al., 2013), Office-Home (Venkateswara et al., 2017), and Tiny-ImageNet-C datasets for our evaluations. For each dataset, we designate one domain as 7Published as a conference paper at ICLR 2024 Table 2: TTA comparisons on PACS and VLCS.This table includes the two data stream mentioned in the dataset setup and reports performances in accuracy. Results that outperform all TTA baselines are highlighted in bold font. N/A denotes the adaptations are not applied on the source domain. PACS Domain-wise data stream Post-adaptation Random data stream Post-adaptation P →A→ →C→ →S P A C S →1→ →2→ →3→ →4 P A C S BN w/o adapt 99.70 59.38 28.03 42.91 99.70 59.38 28.03 42.91 43.44 43.44 43.44 43.44 99.70 59.38 28.03 42.91BN w/ adapt 98.74 68.07 64.85 54.57 98.74 68.07 64.85 54.57 62.50 62.50 62.50 62.50 98.74 68.07 64.85 54.57 Tent (steps=1) N/A 67.29 64.59 44.67 97.60 66.85 64.08 42.58 56.35 54.09 51.83 48.58 97.19 63.53 60.75 41.56Tent (steps=10) N/A 67.38 57.85 20.23 62.63 34.52 40.57 13.59 47.36 31.01 22.84 20.33 50.78 23.68 20.95 19.62EATA N/A 67.04 64.72 50.27 98.62 66.50 62.46 48.18 57.31 56.06 58.17 59.78 98.62 69.63 65.70 54.26CoTTA N/A 65.48 62.12 53.17 98.62 65.48 63.10 53.78 56.06 54.33 57.16 57.42 98.62 65.97 62.97 54.62SAR (steps=1) N/A 66.75 63.82 49.58 98.32 66.94 62.93 45.74 56.78 56.35 56.68 56.70 98.44 68.16 64.38 52.53SAR (steps=10) N/A 69.38 68.26 49.02 96.47 62.16 56.19 54.62 53.51 51.15 51.78 45.60 94.13 56.64 56.02 36.37 SimATTA (B ≤300) N/A 76.86 70.90 75.39 98.80 84.47 82.25 81.52 69.47 76.49 82.45 82.22 98.98 84.91 83.92 86.00SimATTA (B ≤500) N/A 77.93 76.02 76.30 98.62 88.33 83.49 83.74 68.46 78.22 80.91 85.49 99.16 86.67 84.77 87.71 VLCS Domain-wise data stream Post-adaptation Random data stream Post-adaptation C →L→ →S→ →V C L S V →1→ →2→ →3→ →4 C L S V BN w/o adapt 100.00 33.55 41.10 49.05 100.00 33.55 41.10 49.05 41.23 41.23 41.23 41.23 100.00 33.55 41.10 49.05BN w/ adapt 85.16 37.31 33.27 52.16 85.16 37.31 33.27 52.16 40.91 40.91 40.91 40.91 85.16 37.31 33.27 52.16 Tent (steps=1) N/A 38.55 34.40 53.88 84.73 43.86 33.61 53.11 44.85 44.29 47.38 44.98 85.30 43.49 37.81 53.35Tent (steps=10) N/A 45.41 31.44 32.32 42.54 37.65 27.79 33.12 46.13 42.31 43.51 39.48 52.01 40.32 33.64 40.37EATA N/A 37.24 33.15 52.58 84.10 37.69 32.39 52.49 43.77 42.48 43.34 41.55 83.32 36.67 31.47 52.55CoTTA N/A 37.39 32.54 52.25 82.12 37.65 33.12 52.90 43.69 42.14 43.21 42.32 81.98 37.99 33.52 53.23SAR (steps=1) N/A 36.18 34.43 52.46 83.96 39.72 36.53 52.37 43.64 43.04 44.20 41.93 85.09 40.70 36.44 53.02SAR (steps=10) N/A 35.32 34.10 51.66 82.12 41.49 33.94 53.08 43.56 42.05 42.53 41.16 85.09 37.58 33.12 52.01 SimATTA (B ≤300) N/A 62.61 65.08 74.38 99.93 69.50 66.67 77.34 62.33 69.33 73.20 71.93 99.93 69.43 72.46 80.39SimATTA (B ≤500) N/A 63.52 68.01 76.13 99.51 70.56 73.10 78.35 62.29 70.45 73.50 72.02 99.43 70.29 72.55 80.18 the source domain and arrange the samples from the other domains to form the test data stream. For DomainBed datasets, we adopt two stream order strategies. The first order uses a domain-wise data stream, i.e., we finish streaming samples from one domain before starting streaming another domain. The second order is random, where we shuffle samples from all target domains and partition them into four splits 1, 2, 3, and 4, as shown in Tab. 2. More dataset details are provided in Appx. G.1. Baselines. For baseline models, we start with the common source-only models, which either utilize pre-calculated batch statistics (BN w/o adapt) or test batch statistics (BN w/ adapt). For comparison with other TTA methods, we consider four state-of-the-art TTA methods: Tent (Wang et al., 2021), EATA (Niu et al., 2022), CoTTA (Wang et al., 2022a), and SAR (Niu et al., 2023). The three of them except Tent provide extra design to avoid CF. To compare with ADA methods, we select algorithms that are partially comparable with our method, i.e., they should be efficient (e.g., uncertainty-based) without the requirements of additional networks. Therefore, we adopt random, entropy (Wang and Shang, 2014), k-means (Krishna and Murty, 1999), and CLUE (Prabhu et al., 2021) for comparisons. Settings. For TTA, we compare with general TTA baselines in streaming adaptation using the two aforementioned data streaming orders, domain-wise and random. We choose P in PACS and C in VLCS as source domains. For domain-wise data stream, we use order A → C → S for PACS and L → S → V for VLCS. We report the real-time adaptation accuracy results for each split of the data stream, as well as the accuracy on each domain after all adaptations through the data stream (under “post-adaptation” columns). Enhanced TTA is built on TTA with access to extra random sample labels. TTA baselines are further fine-tuned with these random samples. To further improve enhanced TTA, we use long-term label storage and larger unlabeled sample pools. To its extreme where the model can access the whole test set samples, the setting becomes similar to ADA, thus we also use ADA methods for comparisons. ADA baselines have access to all samples in the pre-collected target datasets but not source domain data, whereas our method can only access the streaming test data. 5.1 T HE FAILURE OF TEST-TIME ADAPTATION The failure of TTA methods on domain distribution shifts is one of the main motivations of the ATTA setting. As shown in Tab. 2, TTA methods cannot consistently outperform eventhe simplest baseline \"BN w/ adapt\" which uses test time batch statistics to make predictions, evidencing that current TTA methods cannot solve domain distribution shifts (RQ1). Additionally, Tent (step=10) exhibits significant CF issues, where \"step=10\" indicates 10 test-time training updates, i.e., 10 gradient backpropagation iterations. This failure of TTA methods necessitates the position of ATTA. In contrast, SimATTA, with a budget B less than 300, outperforms all TTA methods on both source and target domains by substantial margins. Moreover, compared to the source-only baselines, our method improves the target domain performances significantly with negligible source performance loss, showing that ATTA is a more practically effective setting for real-world distribution shifts. 5.2 E FFICIENCY & ENHANCED TTA SETTING COMPARISONS To validate the efficiency of ATTA and broaden the dataset choice, we conduct this study on Tiny- ImageNet-C which, though does not focus on domain shifts, is much larger than PACS and VLCS. we 8Published as a conference paper at ICLR 2024 Table 3: Comparisons with Enhanced TTA on Tiny-ImageNet-C (severity level 5). Tiny-ImageNet-C Time (sec)Noise Blur Weather Digital Gauss. Shot Impul. Defoc. Glass Motion Zoom Snow Frost Fog Contr. Elastic Pixel JPEG Avg. Tent (step=1) 68.83 9.32 11.97 8.86 10.43 7.00 12.20 14.34 13.58 15.46 13.55 3.99 13.31 17.79 18.61 12.17Tent (step=10) 426.90 0.86 0.63 0.52 0.52 0.55 0.54 0.50 0.50 0.50 0.50 0.50 0.50 0.50 0.50 0.54EATA 93.14 3.98 3.33 2.18 4.80 2.37 11.02 11.41 14.06 15.26 9.65 1.36 9.88 14.24 12.12 8.26CoTTA 538.78 5.63 7.12 6.31 8.05 5.74 9.68 10.55 11.75 12.00 11.15 4.17 5.35 7.82 8.90 8.16SAR (step=1) 113.76 8.90 3.11 1.67 1.55 1.47 1.35 1.19 1.03 1.04 0.93 0.83 1.00 0.74 0.77 1.83SAR (step=10) 774.11 2.67 3.26 2.38 1.64 1.85 2.49 3.16 3.81 2.72 3.12 0.81 3.47 4.04 1.76 2.66 SimATTA (step=10) 736.289.68 19.40 12.14 30.28 17.03 42.36 43.10 31.96 40.08 29.243.21 34.56 45.24 45.74 28.86 enhance the TTA setting by fine-tuning baselines on randomly selected labeled samples. Specifically, the classifier of ResNet18-BN is pre-adapted to the brightness corruption (source domain) before test-time adapting. SimATTA’s label budget is around 4,000, while all other TTA methods have budget 4,500 for randomly selected labeled samples. The data stream order is shown in Tab. 3. Time is measured across all corrupted images in the Noise and Blur noise types, and the values represent the average time cost for adapting 10,000 images. The results clearly evidence the efficiency of ATTA (RQ2), while substantially outperforming all enhanced TTA baselines. Simply accessing labeled samples cannot benefit TTA methods to match ATTA. With 10 training updates (step=10) for each batch, FTTA methods would suffer from severe CF problem. In contrast, ATTA covers a statistically significant distribution, achieving stronger performances with 10 training updates or even more steps till approximate convergences. In fact, longer training on Tent (step=10) leads to worse results (compared to step=1), which further motivates the design of the ATTA setting. The reason for higher absolute time cost in Tab. 3 is due to differences in training steps. In this experiment, SimATTA has a training step of 10, and similar time cost as SAR per step. Note that if the enhanced TTA setting is further improved to maintain distributions with a balanced CF mitigation strategy and an incremental clustering design, the design approaches ATTA. Specifically, we compare SimATTA with its variants as the ablation study (RQ3) in Appx. I.2. 5.3 C OMPARISONS TO A STRONGER SETTING : ACTIVE DOMAIN ADAPTATION Table 4: Comparisons to ADA baselines. Source domains are denoted as \"(S)\". Results are average accuracies (with standard deviations). PACS P (S) A C S Random (B= 300) 96.21 (0.80) 81.19 (0.48) 80.75 (1.27) 84.34 (0.18)Entropy (B= 300) 96.31 (0.64)88.00 (1.46)82.48 (1.71) 80.55 (1.01)Kmeans (B= 300) 93.71 (1.50) 79.31 (4.01) 79.64 (1.44) 83.92 (0.65)CLUE (B= 300) 96.69 (0.17)83.97 (0.57)84.77 (0.88) 86.91 (0.26) SimATTA (B ≤300) 98.89 (0.09)84.69 (0.22)83.09 (0.83)83.76 (2.24) VLCS C (S) L S V Random (B= 300) 96.21 (1.65) 66.67 (1.70) 70.72 (0.30) 72.14 (1.71)Entropy (B= 300) 97.74 (1.56) 69.29 (2.26)69.25 (4.77) 75.26 (3.07)Kmeans (B= 300) 98.61 (0.27)67.57 (1.64)70.77 (0.01)74.49 (0.97)CLUE (B= 300) 85.70 (10.09) 65.29 (1.49) 69.42 (2.64) 69.09 (6.05) SimATTA (B ≤300) 99.93 (0.00) 69.47 (0.03)69.57 (2.90)78.87 (1.53) In addtion to the above comparisons with (en- hanced) TTA, which necessitate the requirement of extra information in the ATTA setting, we com- pare ATTA with a stronger setting Active Domain Adaptation (ADA) to demonstrate another supe- riority of ATTA, i.e., weaker requirements for comparable performances (RQ4). ADA baselines are able to choose the global best active samples, while ATTA has to choose samples from a small sample buffer (e.g., a size of 100) and discard the rest. Tab. 4 presents the post-adaptation model per- formance results. All ADA results are averaged from 3 random runs, while ATTA results are the post-adaptation performances averaged from the two data stream orders. As can be observed, despite the lack of a pre-collected target dataset, SimATTA produces better or competitive results against ADA methods. Moreover, without source data access, SimATTA’s design for CF allows it to maintain superior source domain performances over ADA methods. Further experimental studies including the Office-Home dataset are provided in Appx. I. In conclusion, the significant improvement compared to weaker settings (TTA, enhanced TTA) and the comparable performance with the stronger setting, ADA, rendering ATTA a setting that is as efficient as TTA and as effective as ADA. This implies its potential is worthy of future explorations. 6 C ONCLUSION AND DISCUSSION There’s no denying that OOD generalization can be extremely challenging without certain information, often relying on various assumptions easily compromised by different circumstances. Thus, it’s prudent to seek methods to achieve significant improvements with minimal cost, e.g., DG methods leveraging environment partitions and ATTA methods using budgeted annotations. As justified in our theoretical and experimental studies, ATTA stands as a robust approach to achieve real-time OOD generalization. Although SimATTA sets a strong baseline for ATTA, there’s considerable scope for further investigation within the ATTA setting. One potential direction involves developing alternatives to prevent CF in ATTA scenarios. While selective entropy minimization on low-entropy samples has prove to be empirically effective, it relies on the quality of the pre-trained model and training on incorrectly predicted low-entropy samples may reinforce the errors. It might not be cost-effective to expend annotation budgets on low-entropy samples, but correcting them could be a viable alternative solution. We anticipate that our work will spur numerous further explorations in this field. 9Published as a conference paper at ICLR 2024 ACKNOWLEDGMENTS This work was supported in part by National Science Foundation grant IIS-2006861 and National Institutes of Health grant U01AG070112. REFERENCES Hana Ajakan, Pascal Germain, Hugo Larochelle, François Laviolette, and Mario Marchand. Domain- adversarial neural networks. arXiv preprint arXiv:1412.4446, 2014. Lucas Baier, Tim Schlör, Jakob Schöffer, and Niklas Kühl. Detecting concept drift with neural network model uncertainty. In Hawaii International Conference on System Sciences, 2021. URL https://api.semanticscholar.org/CorpusID:235731947. Shai Ben-David, John Blitzer, Koby Crammer, Alex Kulesza, Fernando Pereira, and Jennifer Wortman Vaughan. A theory of learning from different domains. Machine learning, 79:151–175, 2010. Davide Cacciarelli and Murat Kulahci. A survey on online active learning, 2023. Cheng Chen, Quande Liu, Yueming Jin, Qi Dou, and Pheng-Ann Heng. Source-free domain adaptive fundus image segmentation with denoised pseudo-labeling. In Medical Image Computing and Computer Assisted Intervention–MICCAI 2021: 24th International Conference, Strasbourg, France, September 27–October 1, 2021, Proceedings, Part V 24, pages 225–235. Springer, 2021. Li Chen, Tutian Tang, Zhitian Cai, Yang Li, Penghao Wu, Hongyang Li, Jianping Shi, Junchi Yan, and Yu Qiao. Level 2 autonomous driving on a single device: Diving into the devils of openpilot. arXiv preprint arXiv:2206.08176, 2022a. Weijie Chen, Luojun Lin, Shicai Yang, Di Xie, Shiliang Pu, and Yueting Zhuang. Self-supervised noisy label learning for source-free unsupervised domain adaptation. In 2022 IEEE/RSJ In- ternational Conference on Intelligent Robots and Systems (IROS) , pages 10185–10192. IEEE, 2022b. Yining Chen, Colin Wei, Ananya Kumar, and Tengyu Ma. Self-training avoids using spurious features under domain shift. Advances in Neural Information Processing Systems, 33:21061–21071, 2020. David A Cohn, Zoubin Ghahramani, and Michael I Jordan. Active learning with statistical models. Journal of artificial intelligence research, 4:129–145, 1996. Matthias De Lange, Rahaf Aljundi, Marc Masana, Sarah Parisot, Xu Jia, Aleš Leonardis, Gregory Slabaugh, and Tinne Tuytelaars. A continual learning survey: Defying forgetting in classification tasks. IEEE transactions on pattern analysis and machine intelligence, 44(7):3366–3385, 2021. Yuhe Ding, Lijun Sheng, Jian Liang, Aihua Zheng, and Ran He. Proxymix: Proxy-based mixup training with label refinery for source-free domain adaptation. arXiv preprint arXiv:2205.14566, 2022. Cian Eastwood, Ian Mason, Christopher KI Williams, and Bernhard Schölkopf. Source-free adaptation to measurement shift via bottom-up feature restoration. arXiv preprint arXiv:2107.05446, 2021. Jiahao Fan, Hangyu Zhu, Xinyu Jiang, Long Meng, Chen Chen, Cong Fu, Huan Yu, Chenyun Dai, and Wei Chen. Unsupervised domain adaptation by statistics alignment for deep sleep staging networks. IEEE Transactions on Neural Systems and Rehabilitation Engineering, 30:205–216, 2022. Chen Fang, Ye Xu, and Daniel N Rockmore. Unbiased metric learning: On the utilization of multiple datasets and web images for softening bias. In Proceedings of the IEEE International Conference on Computer Vision, pages 1657–1664, 2013. Yuqi Fang, Pew-Thian Yap, Weili Lin, Hongtu Zhu, and Mingxia Liu. Source-free unsupervised domain adaptation: A survey. arXiv preprint arXiv:2301.00265, 2022. Francois Fleuret et al. Uncertainty reduction for model adaptation in semantic segmentation. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 9613–9623, 2021. 10Published as a conference paper at ICLR 2024 Yaroslav Ganin and Victor Lempitsky. Unsupervised domain adaptation by backpropagation. In International conference on machine learning, pages 1180–1189. PMLR, 2015. Yaroslav Ganin, Evgeniya Ustinova, Hana Ajakan, Pascal Germain, Hugo Larochelle, François Laviolette, Mario Marchand, and Victor Lempitsky. Domain-adversarial training of neural networks. The journal of machine learning research, 17(1):2096–2030, 2016. Jakob Gawlikowski, Cedrique Rovile Njieutcheu Tassi, Mohsin Ali, Jongseok Lee, Matthias Humt, Jianxiang Feng, Anna Kruspe, Rudolph Triebel, Peter Jung, Ribana Roscher, et al. A survey of uncertainty in deep neural networks. arXiv preprint arXiv:2107.03342, 2021. Yves Grandvalet and Yoshua Bengio. Semi-supervised learning by entropy minimization. Advances in neural information processing systems, 17, 2004. Shurui Gui, Chaoyue Wang, Qihua Chen, and Dacheng Tao. Featureflow: Robust video interpolation via structure-to-texture generation. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 14004–14013, 2020. Shurui Gui, Xiner Li, Limei Wang, and Shuiwang Ji. GOOD: A graph out-of-distribution benchmark. In Thirty-sixth Conference on Neural Information Processing Systems Datasets and Benchmarks Track, 2022. URL https://openreview.net/forum?id=8hHg-zs_p-h. Ishaan Gulrajani and David Lopez-Paz. In search of lost domain generalization. arXiv preprint arXiv:2007.01434, 2020. Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image recognition. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 770–778, 2016. Dan Hendrycks and Thomas Dietterich. Benchmarking neural network robustness to common corruptions and perturbations. March 2019a. doi: 10.48550/ARXIV .1903.12261. Dan Hendrycks and Thomas Dietterich. Benchmarking neural network robustness to common corruptions and perturbations. arXiv preprint arXiv:1903.12261, 2019b. Steven CH Hoi, Rong Jin, Jianke Zhu, and Michael R Lyu. Semisupervised svm batch mode active learning with applications to image retrieval. ACM Transactions on Information Systems (TOIS), 27(3):1–29, 2009. Yihan Hu, Jiazhi Yang, Li Chen, Keyu Li, Chonghao Sima, Xizhou Zhu, Siqi Chai, Senyao Du, Tianwei Lin, Wenhai Wang, et al. Planning-oriented autonomous driving. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 17853–17862, 2023. Jiaxing Huang, Dayan Guan, Aoran Xiao, and Shijian Lu. Model adaptation: Historical contrastive learning for unsupervised domain adaptation without source data. Advances in Neural Information Processing Systems, 34:3635–3649, 2021. Masato Ishii and Masashi Sugiyama. Source-free domain adaptation via distributional alignment by matching batch normalization statistics. arXiv preprint arXiv:2101.10842, 2021. Yusuke Iwasawa and Yutaka Matsuo. Test-time classifier adjustment module for model-agnostic domain generalization. Advances in Neural Information Processing Systems, 34:2427–2440, 2021. Suyog Dutt Jain and Kristen Grauman. Active image segmentation propagation. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pages 2864–2873, 2016. Guoliang Kang, Lu Jiang, Yi Yang, and Alexander G Hauptmann. Contrastive adaptation network for unsupervised domain adaptation. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 4893–4902, 2019. Ashish Kapoor, Kristen Grauman, Raquel Urtasun, and Trevor Darrell. Active learning with gaussian processes for object categorization. In 2007 IEEE 11th international conference on computer vision, pages 1–8. IEEE, 2007. Neerav Karani, Ertunc Erdil, Krishna Chaitanya, and Ender Konukoglu. Test-time adaptable neural networks for robust medical image segmentation. Medical Image Analysis, 68:101907, 2021. 11Published as a conference paper at ICLR 2024 Ronald Kemker, Marc McClure, Angelina Abitino, Tyler Hayes, and Christopher Kanan. Measuring catastrophic forgetting in neural networks. In Proceedings of the AAAI conference on artificial intelligence, volume 32, 2018. Daniel Kifer, Shai Ben-David, and Johannes Gehrke. Detecting change in data streams. In VLDB, volume 4, pages 180–191. Toronto, Canada, 2004. James Kirkpatrick, Razvan Pascanu, Neil Rabinowitz, Joel Veness, Guillaume Desjardins, Andrei A Rusu, Kieran Milan, John Quan, Tiago Ramalho, Agnieszka Grabska-Barwinska, et al. Overcoming catastrophic forgetting in neural networks. Proceedings of the national academy of sciences, 114 (13):3521–3526, 2017. Pang Wei Koh, Shiori Sagawa, Henrik Marklund, Sang Michael Xie, Marvin Zhang, Akshay Bal- subramani, Weihua Hu, Michihiro Yasunaga, Richard Lanas Phillips, Irena Gao, et al. Wilds: A benchmark of in-the-wild distribution shifts. In International Conference on Machine Learning, pages 5637–5664. PMLR, 2021. Divya Kothandaraman, Sumit Shekhar, Abhilasha Sancheti, Manoj Ghuhan, Tripti Shukla, and Dinesh Manocha. Salad: Source-free active label-agnostic domain adaptation for classification, segmentation and detection. In Proceedings of the IEEE/CVF Winter Conference on Applications of Computer Vision, pages 382–391, 2023. K Krishna and M Narasimha Murty. Genetic k-means algorithm. IEEE Transactions on Systems, Man, and Cybernetics, Part B (Cybernetics), 29(3):433–439, 1999. Alex Krizhevsky, Ilya Sutskever, and Geoffrey E Hinton. Imagenet classification with deep convolu- tional neural networks. Communications of the ACM, 60(6):84–90, 2017. David Krueger, Ethan Caballero, Joern-Henrik Jacobsen, Amy Zhang, Jonathan Binas, Dinghuai Zhang, Remi Le Priol, and Aaron Courville. Out-of-distribution generalization via risk extrap- olation (REx). In International Conference on Machine Learning , pages 5815–5826. PMLR, 2021. Vinod K Kurmi, Venkatesh K Subramanian, and Vinay P Namboodiri. Domain impression: A source data free domain adaptation method. In Proceedings of the IEEE/CVF winter conference on applications of computer vision, pages 615–625, 2021. David D Lewis and Jason Catlett. Heterogeneous uncertainty sampling for supervised learning. In Machine learning proceedings 1994, pages 148–156. Elsevier, 1994. Aodong Li, Alex Boyd, Padhraic Smyth, and Stephan Mandt. Detecting and adapting to irregular distribution shifts in bayesian online learning. Advances in neural information processing systems, 34:6816–6828, 2021a. Da Li, Yongxin Yang, Yi-Zhe Song, and Timothy M Hospedales. Deeper, broader and artier domain generalization. In Proceedings of the IEEE international conference on computer vision, pages 5542–5550, 2017. Rui Li, Qianfen Jiao, Wenming Cao, Hau-San Wong, and Si Wu. Model adaptation: Unsupervised domain adaptation without source data. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 9641–9650, 2020. Xianfeng Li, Weijie Chen, Di Xie, Shicai Yang, Peng Yuan, Shiliang Pu, and Yueting Zhuang. A free lunch for unsupervised domain adaptive object detection without source data. In Proceedings of the AAAI Conference on Artificial Intelligence, volume 35, pages 8474–8481, 2021b. Zhizhong Li and Derek Hoiem. Learning without forgetting. IEEE transactions on pattern analysis and machine intelligence, 40(12):2935–2947, 2017. Jian Liang, Dapeng Hu, Ran He, and Jiashi Feng. Distill and fine-tune: Effective adaptation from a black-box source model. arXiv preprint arXiv:2104.01539, 1(3), 2021. Jian Liang, Dapeng Hu, Jiashi Feng, and Ran He. Dine: Domain adaptation from single and multiple black-box predictors. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 8003–8013, 2022. 12Published as a conference paper at ICLR 2024 Yong Lin, Shengyu Zhu, Lu Tan, and Peng Cui. Zin: When and how to learn invariance without environment partition? Advances in Neural Information Processing Systems, 35:24529–24542, 2022. Xiaofeng Liu, Fangxu Xing, Chao Yang, Georges El Fakhri, and Jonghye Woo. Adapting off-the- shelf source segmenter for target medical image segmentation. In Medical Image Computing and Computer Assisted Intervention–MICCAI 2021: 24th International Conference, Strasbourg, France, September 27–October 1, 2021, Proceedings, Part II 24, pages 549–559. Springer, 2021a. Xinyu Liu and Yixuan Yuan. A source-free domain adaptive polyp detection framework with style diversification flow. IEEE Transactions on Medical Imaging, 41(7):1897–1908, 2022. Yuang Liu, Wei Zhang, Jun Wang, and Jianyong Wang. Data-free knowledge transfer: A survey. arXiv preprint arXiv:2112.15278, 2021b. Yuejiang Liu, Parth Kothari, Bastien Van Delft, Baptiste Bellot-Gurlet, Taylor Mordan, and Alexandre Alahi. Ttt++: When does self-supervised test-time training fail or thrive? Advances in Neural Information Processing Systems, 34:21808–21820, 2021c. Mingsheng Long, Yue Cao, Jianmin Wang, and Michael Jordan. Learning transferable features with deep adaptation networks. In International conference on machine learning, pages 97–105. PMLR, 2015. David Lopez-Paz and Marc’Aurelio Ranzato. Gradient episodic memory for continual learning. Advances in neural information processing systems, 30, 2017. Chaochao Lu, Yuhuai Wu, José Miguel Hernández-Lobato, and Bernhard Schölkopf. Invariant causal representation learning for out-of-distribution generalization. In International Conference on Learning Representations, 2021. Xinhong Ma, Junyu Gao, and Changsheng Xu. Active universal domain adaptation. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 8968–8977, 2021. Haitao Mao, Lun Du, Yujia Zheng, Qiang Fu, Zelin Li, Xu Chen, Shi Han, and Dongmei Zhang. Source free unsupervised graph domain adaptation. arXiv preprint arXiv:2112.00955, 2021. Christoforos Mavrogiannis, Francesca Baldini, Allan Wang, Dapeng Zhao, Pete Trautman, Aaron Steinfeld, and Jean Oh. Core challenges of social robot navigation: A survey. ACM Transactions on Human-Robot Interaction, 12(3):1–39, 2023. Zachary Nado, Shreyas Padhy, D Sculley, Alexander D’Amour, Balaji Lakshminarayanan, and Jasper Snoek. Evaluating prediction-time batch normalization for robustness under covariate shift. arXiv preprint arXiv:2006.10963, 2020. Munan Ning, Donghuan Lu, Dong Wei, Cheng Bian, Chenglang Yuan, Shuang Yu, Kai Ma, and Yefeng Zheng. Multi-anchor active domain adaptation for semantic segmentation. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 9112–9122, 2021. Shuaicheng Niu, Jiaxiang Wu, Yifan Zhang, Yaofo Chen, Shijian Zheng, Peilin Zhao, and Mingkui Tan. Efficient test-time model adaptation without forgetting. In International conference on machine learning, pages 16888–16905. PMLR, 2022. Shuaicheng Niu, Jiaxiang Wu, Yifan Zhang, Zhiquan Wen, Yaofo Chen, Peilin Zhao, and Mingkui Tan. Towards stable test-time adaptation in dynamic wild world. InThe Eleventh International Con- ference on Learning Representations, 2023. URL https://openreview.net/forum?id=g2YraF75Tj. Sinno Jialin Pan, Ivor W Tsang, James T Kwok, and Qiang Yang. Domain adaptation via transfer component analysis. IEEE transactions on neural networks, 22(2):199–210, 2010. Adam Paszke, Sam Gross, Francisco Massa, Adam Lerer, James Bradbury, Gregory Chanan, Trevor Killeen, Zeming Lin, Natalia Gimelshein, Luca Antiga, et al. Pytorch: An imperative style, high-performance deep learning library. Advances in neural information processing systems, 32, 2019. 13Published as a conference paper at ICLR 2024 Vishal M Patel, Raghuraman Gopalan, Ruonan Li, and Rama Chellappa. Visual domain adaptation: A survey of recent advances. IEEE signal processing magazine, 32(3):53–69, 2015. Judea Pearl. Causality. Cambridge university press, 2009. Fabian Pedregosa, Gaël Varoquaux, Alexandre Gramfort, Vincent Michel, Bertrand Thirion, Olivier Grisel, Mathieu Blondel, Peter Prettenhofer, Ron Weiss, Vincent Dubourg, et al. Scikit-learn: Machine learning in python. the Journal of machine Learning research, 12:2825–2830, 2011. Jonas Peters, Peter Bühlmann, and Nicolai Meinshausen. Causal inference by using invariant prediction: identification and confidence intervals. Journal of the Royal Statistical Society: Series B (Statistical Methodology), 78(5):947–1012, 2016. Jonas Peters, Dominik Janzing, and Bernhard Schölkopf. Elements of causal inference: foundations and learning algorithms. The MIT Press, 2017. Viraj Prabhu, Arjun Chandrasekaran, Kate Saenko, and Judy Hoffman. Active domain adaptation via clustering uncertainty-weighted embeddings. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 8505–8514, 2021. Elan Rosenfeld, Pradeep Ravikumar, and Andrej Risteski. The risks of invariant risk minimization. arXiv preprint arXiv:2010.05761, 2020. Shiori Sagawa, Pang Wei Koh, Tatsunori B Hashimoto, and Percy Liang. Distributionally robust neural networks for group shifts: On the importance of regularization for worst-case generalization. arXiv preprint arXiv:1911.08731, 2019. Shibani Santurkar, Dimitris Tsipras, Andrew Ilyas, and Aleksander Madry. How does batch normal- ization help optimization? Advances in neural information processing systems, 31, 2018. Akanksha Saran, Safoora Yousefi, Akshay Krishnamurthy, John Langford, and Jordan T. Ash. Streaming active learning with deep neural networks. In Andreas Krause, Emma Brunskill, Kyunghyun Cho, Barbara Engelhardt, Sivan Sabato, and Jonathan Scarlett, editors, Proceedings of the 40th International Conference on Machine Learning, volume 202 of Proceedings of Machine Learning Research, pages 30005–30021. PMLR, 23–29 Jul 2023. URL https://proceedings.mlr. press/v202/saran23a.html. Harald Schafer, Eder Santana, Andrew Haden, and Riccardo Biasini. A commute in data: The comma2k19 dataset, 2018. Tobias Scheffer, Christian Decomain, and Stefan Wrobel. Active hidden markov models for informa- tion extraction. In Advances in Intelligent Data Analysis: 4th International Conference, IDA 2001 Cascais, Portugal, September 13–15, 2001 Proceedings 4, pages 309–318. Springer, 2001. Steffen Schneider, Evgenia Rusak, Luisa Eck, Oliver Bringmann, Wieland Brendel, and Matthias Bethge. Improving robustness against common corruptions by covariate shift adaptation. Advances in Neural Information Processing Systems, 33:11539–11551, 2020. Burr Settles. Active learning literature survey. 2009. Karen Simonyan and Andrew Zisserman. Very deep convolutional networks for large-scale image recognition. arXiv preprint arXiv:1409.1556, 2014. Jong-Chyi Su, Yi-Hsuan Tsai, Kihyuk Sohn, Buyu Liu, Subhransu Maji, and Manmohan Chandraker. Active adversarial domain adaptation. In Proceedings of the IEEE/CVF Winter Conference on Applications of Computer Vision, pages 739–748, 2020. Baochen Sun and Kate Saenko. Deep coral: Correlation alignment for deep domain adaptation. In European conference on computer vision, pages 443–450. Springer, 2016. Yu Sun, Xiaolong Wang, Zhuang Liu, John Miller, Alexei Efros, and Moritz Hardt. Test-time training with self-supervision for generalization under distribution shifts. In International conference on machine learning, pages 9229–9248. PMLR, 2020. 14Published as a conference paper at ICLR 2024 Yi-Hsuan Tsai, Wei-Chih Hung, Samuel Schulter, Kihyuk Sohn, Ming-Hsuan Yang, and Manmohan Chandraker. Learning to adapt structured output space for semantic segmentation. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 7472–7481, 2018. Eric Tzeng, Judy Hoffman, Trevor Darrell, and Kate Saenko. Simultaneous deep transfer across domains and tasks. In Proceedings of the IEEE international conference on computer vision, pages 4068–4076, 2015. Eric Tzeng, Judy Hoffman, Kate Saenko, and Trevor Darrell. Adversarial discriminative domain adaptation. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 7167–7176, 2017. Hemanth Venkateswara, Jose Eusebio, Shayok Chakraborty, and Sethuraman Panchanathan. Deep hashing network for unsupervised domain adaptation. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 5018–5027, 2017. Sudheendra Vijayanarasimhan and Ashish Kapoor. Visual recognition and detection under bounded computational resources. In 2010 IEEE Computer Society Conference on Computer Vision and Pattern Recognition, pages 1006–1013. IEEE, 2010. Dan Wang and Yi Shang. A new active labeling method for deep learning. In 2014 International joint conference on neural networks (IJCNN), pages 112–119. IEEE, 2014. Dequan Wang, Evan Shelhamer, Shaoteng Liu, Bruno Olshausen, and Trevor Darrell. Tent: Fully test- time adaptation by entropy minimization. InInternational Conference on Learning Representations, 2021. URL https://openreview.net/forum?id=uXl3bZLkr3c. Mei Wang and Weihong Deng. Deep visual domain adaptation: A survey. Neurocomputing, 312: 135–153, 2018. Qin Wang, Olga Fink, Luc Van Gool, and Dengxin Dai. Continual test-time domain adaptation. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 7201–7211, 2022a. Rui Wang, Zuxuan Wu, Zejia Weng, Jingjing Chen, Guo-Jun Qi, and Yu-Gang Jiang. Cross-domain contrastive learning for unsupervised domain adaptation. IEEE Transactions on Multimedia , 2022b. Garrett Wilson and Diane J Cook. A survey of unsupervised deep domain adaptation. ACM Transactions on Intelligent Systems and Technology (TIST), 11(5):1–46, 2020. Binhui Xie, Longhui Yuan, Shuang Li, Chi Harold Liu, Xinjing Cheng, and Guoren Wang. Active learning for domain adaptation: An energy-based approach. InProceedings of the AAAI Conference on Artificial Intelligence, volume 36, pages 8708–8716, 2022. Zhao Xu, Kai Yu, V olker Tresp, Xiaowei Xu, and Jizhi Wang. Representative sampling for text classification using support vector machines. In Advances in Information Retrieval: 25th European Conference on IR Research, ECIR 2003, Pisa, Italy, April 14–16, 2003. Proceedings 25, pages 393–407. Springer, 2003. Baoyao Yang, Hao-Wei Yeh, Tatsuya Harada, and Pong C Yuen. Model-induced generalization error bound for information-theoretic representation learning in source-data-free unsupervised domain adaptation. IEEE Transactions on Image Processing, 31:419–432, 2021a. Guanglei Yang, Hao Tang, Zhun Zhong, Mingli Ding, Ling Shao, Nicu Sebe, and Elisa Ricci. Transformer-based source-free domain adaptation. arXiv preprint arXiv:2105.14138, 2021b. Jianfei Yang, Xiangyu Peng, Kai Wang, Zheng Zhu, Jiashi Feng, Lihua Xie, and Yang You. Divide to adapt: Mitigating confirmation bias for domain adaptation of black-box predictors. arXiv preprint arXiv:2205.14467, 2022. H Yao, Yuhong Guo, and Chunsheng Yang. Source-free unsupervised domain adaptation with surrogate data generation. In Proceedings of NeurIPS 2021 Workshop on Distribution Shifts: Connecting Methods and Applications, 2021. 15Published as a conference paper at ICLR 2024 Hao-Wei Yeh, Baoyao Yang, Pong C Yuen, and Tatsuya Harada. Sofa: Source-data-free feature alignment for unsupervised domain adaptation. InProceedings of the IEEE/CVF Winter Conference on Applications of Computer Vision, pages 474–483, 2021. Fuming You, Jingjing Li, and Zhou Zhao. Test-time batch statistics calibration for covariate shift. arXiv preprint arXiv:2110.04065, 2021. Hu Yu, Jie Huang, Yajing Liu, Qi Zhu, Man Zhou, and Feng Zhao. Source-free domain adaptation for real-world image dehazing. In Proceedings of the 30th ACM International Conference on Multimedia, pages 6645–6654, 2022. Haojian Zhang, Yabin Zhang, Kui Jia, and Lei Zhang. Unsupervised domain adaptation of black-box source models. arXiv preprint arXiv:2101.02839, 2021. Marvin Zhang, Sergey Levine, and Chelsea Finn. Memo: Test time robustness via adaptation and augmentation. Advances in Neural Information Processing Systems, 35:38629–38642, 2022a. Yifan Zhang, Xue Wang, Kexin Jin, Kun Yuan, Zhang Zhang, Liang Wang, Rong Jin, and Tieniu Tan. Adanpc: Exploring non-parametric classifier for test-time adaptation. In International Conference on Machine Learning, pages 41647–41676. PMLR, 2023. Yizhe Zhang, Shubhankar Borse, Hong Cai, and Fatih Porikli. Auxadapt: Stable and efficient test-time adaptation for temporally consistent video semantic segmentation. In Proceedings of the IEEE/CVF Winter Conference on Applications of Computer Vision, pages 2339–2348, 2022b. Bowen Zhao, Chen Chen, and Shu-Tao Xia. Delta: degradation-free fully test-time adaptation. arXiv preprint arXiv:2301.13018, 2023a. Hao Zhao, Yuejiang Liu, Alexandre Alahi, and Tao Lin. On pitfalls of test-time adaptation. In International Conference on Machine Learning (ICML), 2023b. Chunting Zhou, Xuezhe Ma, Paul Michel, and Graham Neubig. Examining and combating spurious features under distribution shift. In International Conference on Machine Learning, pages 12857– 12867. PMLR, 2021. 16Published as a conference paper at ICLR 2024 Active Test-Time Adaptation: Foundational Analyses and An Algorithm Supplementary Material A B ROADER IMPACTS The field of domain generalization primarily concentrates on enhancing a model’s generalization abilities by preparing it thoroughly before deployment. However, it is equally important for deep learning applications to have the capacity for real-time adaptation, as no amount of preparation can account for all possible scenarios. Consequently, domain generalization and test-time adaptation are complementary strategies: the former is more weighty and extensive, while the latter is more agile, lightweight and privacy-friendly. This work delves into the development of a real-time model adaptation strategy that can be applied to any pre-trained models, including large language models, to enhance their adaptive capabilities. Our research does not involve any human subjects or dataset releases, nor does it raise any ethical concerns. Since this work does not directly tie to specific applications, we do not foresee any immediate negative societal impacts. Nonetheless, we acknowledge that any technological advancement may carry potential risks, and we encourage the continued assessment of the broader impacts of real-time adaptation methodologies in various contexts. B FAQ & D ISCUSSIONS To facilitate the reviewing process, we summarize the answers to the questions that arose during the discussion of an earlier version of this paper. The major updates of this version are reorganized theoretical studies, incremental clustering details, experimental reorganization, and additional datasets and settings . We include more related field comparisons to distinguish different settings. We also cover the position of this paper in literature and the main claims of this paper. Finally, we will frankly acknowledge the limitations of this paper, explain and justify the scope of coverage, and provide possible future directions. Q1: What is the relationship between the proposed ATTA protocol and stream based active learning (Saran et al., 2023)? A: We would like to discuss the difference between our work and the referenced work. 1. Real-time Training Distinction: Saran et al. (2023) doesn’t operate in real-time capacity. This is evident from their experiments, where their model is trained only after completing a round. In contrast, our work involves training the model post each batch. This positions Saran et al. (2023)’s work as an intrinsic active learning technique, while our approach leans towards TTA methods. 2. Continual Training Nuance: Following the point above, Saran et al. (2023) stands out of the scope of continual training. As they mentioned ‘each time new data are acquired, the ResNet is reset to the ImageNet pre-trained weights before being updated‘, Saran et al. (2023) starts afresh with each iteration and is out of scope for CF discussions. Contrarily, our model is continuously trained on varying distributions, compelling us to address the CF issue while preserving advantages derived from various stored distributions. 3. Comparative Complexity: Given the aforementioned distinctions, it’s evident that our task presents a greater challenge compared to theirs. In addition, we have included comparisons with stronger active learning settings in Sec. 5.3. Q2: What are the insights from the theoretically foundational analysis? A: 1. It sets a well-defined formulation and grounded theoretical framework for the ATTA setting. 2. While entropy minimizations can cause CF, balancing the learning rate and number of high/low entropy samples is conversely the key solution to both distribution shifts and 17Published as a conference paper at ICLR 2024 CF by corresponding benefits. Though adding low-entropy data is intuitive, it is crucial in that this simple operation can make methods either too conservative or too aggressive without the correct balancing conditions. 3. The studies in Sec. 3.1 directly present a feasible and guaranteed solution for imple- menting ATTA to tackle shifts while avoiding CF. The aligned empirical validations of Sec. 3.2 also instruct the implementation of SimATTA. Q3: In test-time adaptation, one important issue is that the number of testing samples in a batch may be small, which means the sample size m will also be very small. May it affect the theorem and make them become very loose? A: We consider this issue jointly from theoretical and empirical validations. 1. It is true that the theoretical bounds can be loose given a small size of m unlabeled test samples. This situation of the error bound is mathematically ascribed to the quotient between the VC-dimension d of the hypothesis class and m. Under the VC-dimension theory, the ResNet18 model we adopt should have d ≫ m. However, practically we perform fine-tuning on pre-trained models instead of training from scratch, which significantly reduces the scale of parameter update. In this case, an assumption can be established that fine-tuning a model is roughly equivalent to learning a model with a relatively small d (Appx. H). This assumption is potentially underpinned by the empirical alignment of our validation experiments with the theoretical framework (Fig. 1). To this end, experiments indicate thatd and m are practically of similar scale for our settings. This prevents our theoretical bounds from being very loose and meaningless in reality. 2. Regarding cases that our assumption does not apply, this issue would appear inevitable, since it is rigorously inherent in the estimation error of our streaming and varying test distributions. The distribution of a test stream can be hardly monitored when only a limited batch is allowed, which we consider as a limitation of TTA settings. Moreover, this issue directly implies the necessity of using a buffer for unlabeled samples. A good practice is to maintain a relatively comparable sample buffer scale. Q4: What distribution shifts can ATTA solve? A: We would like to follow (but not limited to) the work (Zhao et al., 2023b) to discuss the distribution shifts ATTA can solve. 1. As elucidated in Sec. 3.1 and Sec. 5, ATTA can solve domain generalization shifts. Domain generalization shifts include complex shifts on the joint data distribution P(X, Y), given X as the covariates and Y as the label variable. Since P(X, Y) = P(X)P(Y |X), ATTA can handle covariate shift (P(X)), label shift (P(Y )), and conditional shift (P(Y |X)). The shifts on both covariate and conditional distributions can cover the shift on labels, but they (covariate + conditional shifts) are more complicated than pure label shifts, where only the marginal label distribution changes while the conditional distribution remains. Note that the conditional shifts are generally caused by spurious correlations, where the independent causal mechanism assumption (Pearl, 2009) holds or no concept drifts exist. 2. In our framework, the distribution support of X at different time steps can be different, but we don’t cover the situation where the support of Y changes, i.e., class-incremental problems. Q5: It is unclear how many samples are selected in each minibatch of testing samples. How the total budget is distributed across the whole testing data stream? A: The number of selected samples for each minibatch is decided jointly by the incremental clustering and the cluster centroid number NC (t). Intuitively, this sample selection is a dynamic process, with NC (t) restricting the budget and incremental clustering performing sample selection. For each batch, we increase applicable clustering centroids as a maximum limit, while the exact number of the selected samples is given by the incremental clustering by how many clusters are located in the scope of new distributions. e.g., if the incoming batch does not introduce new data distributions, then we select zero samples even with increased NC (t). In contrast, if the incoming batch contains data located in multiple new distributions, the incremental clustering tends to select more samples than the NC (t) limit, thus forcing to merging of multiple previous clusters into one new cluster. 18Published as a conference paper at ICLR 2024 The incremental clustering is detailed in Sec. 4.2, and NC (t) is naively increased by a constant hyper-parameter k. Therefore, the budget is adaptively distributed according to the data streaming distribution with budgets controlled by k, which is also the reason why we compare methods under a budget limit. Q6: Could compared methods have access to a few ground-truth labels as well? Making other algorithms be able to use the same amount of ground-truth labels randomly will produce fairer comparisons. A: 1. The enhanced TTA setting is exactly the setup we provide to produce fairer comparisons. See Tab. 3 and Tab. 5 for comparison results. 2. ATTA also compares to a stronger setting ADA which can access the whole test datasets multiple times. Table 5: The table demonstrates the comparisons on PACS where all enhanced TTA baselines have 300 budgets to randomly select labeled samples. The training steps of these labeled samples are the same as the original TTA method training steps. For accumulated sample selection, please refer to our ablation studies. Method Domain-wise data stream A VG Random data stream A VG P→ →A→ →C→ →S P A C S 1 2 3 4 P A C S Source onlyBN w/o adapt 99.70 59.38 28.03 42.91 99.70 59.38 28.03 42.91 43.44 43.44 43.44 43.44 99.70 59.38 28.03 42.91BN w/ adapt 98.74 68.07 64.85 54.57 98.74 68.07 64.85 54.57 62.50 62.50 62.50 62.50 98.74 68.07 64.85 54.57 TTA Tent (steps=1) N/A 70.07 68.43 64.42 97.72 74.17 72.61 68.92 61.20 62.36 66.59 67.32 98.14 74.37 70.26 66.07Tent (steps=10) N/A 76.27 63.78 49.35 59.46 38.62 48.46 55.03 56.20 53.22 52.55 55.55 58.32 47.56 60.75 58.00EATA N/A 69.53 66.94 61.42 98.56 69.38 66.60 64.83 60.34 59.81 64.38 65.02 98.68 73.78 68.30 59.74CoTTA N/A 66.55 63.14 59.91 90.12 61.67 66.68 67.68 57.26 57.36 63.46 65.64 92.22 71.53 70.44 62.41SAR (steps=1) N/A 66.60 63.78 50.34 98.38 67.87 64.04 49.48 57.21 56.06 56.78 57.14 98.38 68.80 64.59 53.02SAR (steps=10) N/A 69.09 66.55 49.07 96.23 62.50 59.34 46.53 49.76 52.74 48.51 49.06 95.39 57.13 54.61 38.76 Ours (B ≤300) N/A 76.86 70.90 75.39 98.80 84.47 82.25 81.52 69.47 76.49 82.45 82.22 98.98 84.91 83.92 86.00 Q7: What is the position of ATTA? A: Comparisons with different settings are challenging. In this work, the design of our experiments (Sec. 5) is to overcome this challenge by comparing both weaker settings and stronger settings. While the significant performance over weaker settings renders the necessity of extra information, the comparable performance with stronger settings provides the potential to relax restricted requirements. Intuitively, ATTA is the most cost-effective option in the consideration of both efficiency and effectiveness. We further provide the following ATTA summary: ATTA, which incorporates active learning in FTTA, is the light, real-time, source-free, widely applicable setting to achieve high generalization performances for test-time adaptation. 1. Necessity: From the causality perspective, new information is necessary (Lin et al., 2022; Pearl, 2009; Peters et al., 2017) to attain generalizable over distribution shifts which are insurmountable within the current TTA framework. 2. Effectiveness: Compared to FTTA methods, ATTA produces substantially better perfor- mances, on-par with the costly active domain adaptation (ADA) methods as shown in Table 3 in the paper. 3. Efficiency: Relative to ADA methods, ATTA possesses superior efficiency, similar to general FTTA methods, as shown in Tab. 3. 4. Applicability: ATTA is a model-agnostic setting. (1) Compared to domain generalization methods, ATTA do not require re-training and has the potential to apply to any pre-trained models. One interesting future direction is designing ATTA methods for large language models (LLMs), where re-trainings are extremely expensive and source data may be in- accessible. (2) Compared to FTTA methods, ATTA can protect model parameters from corrupting while learning new distributions by fine-tuning pre-trained models, rendering it more feasible and practical. In comparison with existing works, ATTA is motivated to mitigate the limitations of previous settings: 1. FTTA: Limited generalization performance. 19Published as a conference paper at ICLR 2024 2. TTT: Not source-free; limited generalization performance. 3. ADA & domain adaptation/generalization: Expensive re-trainings; limited applicability to pre-trained models. 4. Online active learning: It does not maintain and protect adaptation performances for multiple distributions in one model and does not consider the CF problem. Q8: What is the potential practical utility of ATTA? A: 1. Empirically, our method can generally finish a round of sample selection/training of 100 frames in 5s, i.e., 20 frames per sec, which is more than enough to handle multiple practical situations. Experiments on time complexity are provided in Tab. 3, where SimATTA has comparable time efficiency. 2. As a case analysis, the autopilot system (Hu et al., 2023; Chen et al., 2022a) presents an application scenario requiring high-speed low-latency adaptations, while these adaptations are largely underexplored. When entering an unknown environment, e.g., a construction section, a system of ATTA setting can require the driver to take over the wheel. During the period of manual operation when the driver is handling the wheel, steering signals are generated, and the in-car system quickly adaptations. The system doesn’t need to record 60 frames per second, since only the key steering operations and the corresponding dash cam frames are necessary, which can be handled by ATTA algorithms processing at 20 frames per sec. In this case, the human annotations are necessary and indirect. ATTA makes use of this information and adapts in the short term instead of collecting videos and having a long-round fine-tuning (Schafer et al., 2018). 3. In addition, many scenarios applicable for ATTA are less speed-demanding than the case above. One example is a personalized chatbot that subtly prompts and gathers user labels during user interaction. In a home decoration setting, applications can request that users scan a few crucial areas to ensure effective adaptation. Social robots (Mavrogiannis et al., 2023), e.g., vacuum robots, often require users to label critical obstacles they’ve encountered. 4. Compared with ADA, ATTA stands out as the tailored solution for the above scenarios. It does not require intensive retraining or server-dependent fine-tuning, offering both speed and computational efficiency. Meanwhile, akin to other TTA methods, ATTA also ensures user privacy. While it might marginally exceed the cost of standard TTA methods, the superior generalization ability makes it a compelling choice and justifies the additional expense. Q9: What can be covered by this paper? A: This paper endeavors to establish the foundational framework for a novel setting referred to as ATTA. We target (1) positioning the ATTA setting, (2) solving the two major and basic challenges of ATTA,i.e., the mitigation of distribution shifts and the avoidance of catastrophic forgetting (CF). We achieve the first goal by building the problem formulation and analyses, and further providing extensive qualitative and well-organized experimental comparisons with TTA, enhanced TTA, and ADA settings. These efforts position ATTA as the most cost-effective option between TTA and ADA, where ATTA inherits the efficiency of TTA and the effectiveness of ADA. With our theoretical analyses and the consistent algorithm design, we validate the success of our second goal through significant empirical performances. Q10: What are not covered by this paper? A: Constructing a new setting involves multifaceted complexities. Although there are various potential applications discussed above including scaling this setting up for large models and datasets, we cannot cover them in this single piece of work. There are three main reasons. First, the topics covered by a single paper are limited. Formally establishing ATTA setting and addressing its major challenges of ATTA takes precedence over exploring practical applications. Secondly, given the interrelations between ATTA and other settings, our experimental investigations are predominantly comparative, utilizing the most representative datasets from TTA and domain adaptation to showcase persuasive results. Thirdly, many practical applications necessitate task-specific configurations, rendering them unsuitable for establishing a universal learning setting. While the current focus is on laying down the foundational aspects of ATTA, the exploration of more specialized applications remains a prospective avenue for future work in the ATTA domain. 20Published as a conference paper at ICLR 2024 C R ELATED WORKS The development of deep learning witnesses various applications (He et al., 2016; Gui et al., 2020). To tackle OOD problem, various domain generalization works emerge (Krueger et al., 2021; Sagawa et al., 2019). C.1 U NSUPERVISED DOMAIN ADAPTATION Unsupervised Domain Adaptation (UDA) (Pan et al., 2010; Patel et al., 2015; Wilson and Cook, 2020; Wang and Deng, 2018) aims at mitigating distribution shifts between a source domain and a target domain, given labeled source domain samples and unlabeled target samples. UDA methods generally rely on feature alignment techniques to eliminate distribution shifts by aligning feature distributions between source and target domains. Typical feature alignment techniques include discrepancy minimization (Long et al., 2015; Sun and Saenko, 2016; Kang et al., 2019) and adversarial training (Ganin and Lempitsky, 2015; Tsai et al., 2018; Ajakan et al., 2014; Ganin et al., 2016; Tzeng et al., 2015; 2017). Nevertheless, alignments are normally not guaranteed to be correct, leading to the alignment distortion problem as noted by Ning et al. (2021). Source-free Unsupervised Domain Adaptation (SFUDA) (Fang et al., 2022; Liu et al., 2021b) algorithms aim to adapt a pre-trained model to unlabeled target domain samples without access to source samples. Based on whether the algorithm can access model parameters, these algorithms are categorized into white-box and black-box methods. White-box SFUDA typically considers data recovery (generation) and fine-tuning methods. The former focuses on recovering source- like data (Ding et al., 2022; Yao et al., 2021), e.g., training a Generative Adversarial Network (GAN) (Kurmi et al., 2021; Li et al., 2020), while the latter employs various techniques (Mao et al., 2021), such as knowledge distillation (Chen et al., 2022b; Liu and Yuan, 2022; Yang et al., 2021b; Yu et al., 2022), statistics-based domain alignment (Ishii and Sugiyama, 2021; Liu et al., 2021a; Fan et al., 2022; Eastwood et al., 2021), contrastive learning (Huang et al., 2021; Wang et al., 2022b), and uncertainty-based adaptation (Gawlikowski et al., 2021; Fleuret et al., 2021; Chen et al., 2021; Li et al., 2021b). Black-box SFUDA cannot access model parameters and often relies on self-supervised knowledge distillation (Liang et al., 2022; 2021), pseudo-label denoising (Zhang et al., 2021; Yang et al., 2022), or generative distribution alignment (Yeh et al., 2021; Yang et al., 2021a). C.2 T EST-TIME ADAPTATION Test-time Adaptation (TTA), especially Fully Test-time Adaptation (FTTA) algorithms (Wang et al., 2021; Iwasawa and Matsuo, 2021; Karani et al., 2021; Nado et al., 2020; Schneider et al., 2020; Wang et al., 2022a; Zhao et al., 2023a; Niu et al., 2022; Zhang et al., 2022a; Niu et al., 2023; You et al., 2021; Zhang et al., 2022b), can be considered as realistic and lightweight methods for domain adaptation. Built upon black-box SFUDA, FTTA algorithms eliminate the requirement of a pre-collected target dataset and the corresponding training phase. Instead, they can only access an unlabeled data stream and apply real-time adaptation and training. In addition to FTTA, Test-time Training (TTT) (Sun et al., 2020; Liu et al., 2021c) often relies on appending the original network with a self-supervised task. TTT methods require retraining on the source dataset to transfer information through the self-supervised task. Although they do not access the source dataset during the test-time adaptation phase, TTT algorithms are not off-the-shelf source-free methods. TTA is a promising and critical direction for real-world applications, but current entropy minimization-based methods can be primarily considered as feature calibrations that require high-quality pseudo-labels. This requirement, however, can be easily violated under larger distribution shifts. Current TTA algorithms, inheriting UDA drawbacks, cannot promise good feature calibration results, which can be detrimental in real-world deployments. For instance, entropy minimization on wrongly predicted target domain samples with relatively low entropy can only exacerbate spurious correla- tions (Chen et al., 2020). Without extra information, this problem may be analogous to applying causal inference without intervened distributions, which is intrinsically unsolvable (Peters et al., 2016; Pearl, 2009). This paper aims to mitigate this issue with minimal labeled target domain samples. To minimize the cost, we tailor active learning techniques for TTA settings. It is worth noting that a recent work AdaNPC (Zhang et al., 2023) is essentially a domain gener- alization method with a TTA phase attached, while our ATTA is built based on the FTTA setting. Specifically, Current FTTA methods and our work cannot access the source domain. In contrast, 21Published as a conference paper at ICLR 2024 AdaNPC accesses source data to build its memory bank, circumventing the catastrophic forgetting problem. Furthermore, AdaNPC requires multiple source domains and training before performing TTA. Thus AdaNPC uses additional information on domain labels and retraining resources for its memory bank, undermining the merits of FTTA. Regarding theoretical bounds, their target domain is bounded by source domain error and model estimations (in big-O expression), while we consider active sample learning and time variables for varying test distributions. C.3 C ONTINUAL DOMAIN ADAPTATION Many domain adaptation methods focus on improving target domain performance, neglecting the performance on the source domain, which leads to the CF problem (Kemker et al., 2018; Kirkpatrick et al., 2017; Li and Hoiem, 2017; Lopez-Paz and Ranzato, 2017; De Lange et al., 2021; Wang et al., 2022a; Niu et al., 2022). This issue arises when a neural network, after being trained on a sequence of domains, experiences a significant degradation in its performance on previously learned domains as it continues to learn new domains. Continual learning, also known as lifelong learning, addresses this problem. Recent continual domain adaptation methods have made significant progress by employing gradient regularization, random parameter restoration, buffer sample mixture, and more. Although the CF problem is proposed in the continual learning field, it can occur in any source-free OOD settings since the degradation caused by CF is attributed to the network’s parameters being updated to optimize performance on new domains, which may interfere with the representations learned for previous domains. C.4 A CTIVE DOMAIN ADAPTATION Active Domain Adaptation (ADA) (Prabhu et al., 2021; Ning et al., 2021; Su et al., 2020; Ma et al., 2021; Xie et al., 2022) extends semi-supervised domain adaptation with active learning strate- gies (Cohn et al., 1996; Settles, 2009), aiming to maximize target domain performance with a limited annotation budget. Therefore, the key challenge of active learning algorithms is selecting the most informative unlabeled data in target domains (Kapoor et al., 2007). Sample selection strategies are of- ten based on uncertainty (Lewis and Catlett, 1994; Scheffer et al., 2001), diversity (Jain and Grauman, 2016; Hoi et al., 2009), representativeness (Xu et al., 2003), expected error minimization (Vijaya- narasimhan and Kapoor, 2010), etc. Among these methods, uncertainty and diversity-based methods are simple and computationally efficient, making them the most suitable choices to tailor for TTA settings. Adapting these strategies is non-trivial because, compared to typical active domain adaptation, our proposed Active Test-time Adaptation (ATTA) setting does not provide access to source data, model parameters, or pre-collected target samples. This requirement demands that our active sample selection algorithm select samples for annotation during data streaming. Consequently, this active sampling selection process is non-regrettable, i.e., we can only meet every sample once in a short period. To avoid possible confusion, compared to the recent Source-free Active Domain Adaptation (SFADA) method SALAD (Kothandaraman et al., 2023), we do not require access to model parameter gradients, training additional neural networks, or pre-collected target datasets. Therefore, our ATTA setting is quite different, much lighter, and more realistic than ADA and SFADA. C.5 A CTIVE ONLINE LEARNING The most related branch of active online learning (AOL) (Cacciarelli and Kulahci, 2023) is active online learning on drifting data stream (Zhou et al., 2021; Baier et al., 2021; Li et al., 2021a). Generally, these methods include two components, namely, detection and adaptation. Compared with ATTA, there are several distinctions. First, this line of studies largely focuses on the distribution shift detection problem, while ATTA focuses on multi-domain adaptations. Second, AOL on drifting data stream aims to detect and adapt to one current distribution in the stream, without considering preserving the adaptation abilities of multiple past distributions by maintaining and fine-tuning the original pre-trained models. In contrast, ATTA’s goal is to achieve the OOD generalization optimums adaptable across multiple source and target distributions, leading to the consideration of CF problems. Third, while AOL requires one-by-one data input and discard, ATTA maintains a buffer for incoming data before selection decisions. This is because ATTA targets maintaining the original model without corrupting and replacing it, such that making statistically meaningful and high-quality decisions is 22Published as a conference paper at ICLR 2024 critical for ATTA. In contrast, AOL allows resetting and retraining new models, whose target is more lean to cost saving and one-by-one manner. D F URTHER THEORETICAL STUDIES In this section, we refine the theoretical studies with supplement analysis and further results. We use the H-divergence and H∆H-distance definitions following (Ben-David et al., 2010). Definition 2 (H-divergence). For a function class H and two distributions D1 and D2 over a domain X, the H-divergence between D1 and D2 is defined as dH(D1, D2) = sup h∈H |Px∼D1 [h(x) = 1] − Px∼D2 [h(x) = 1]|. The H∆H-distance is defined base on H-divergence. We use the H∆H-distance definition follow- ing (Ben-David et al., 2010). Definition 3 (H∆H-distance). For two distributions D1 and D2 over a domain X and a hypothesis class H, the H∆H-distance between D1 and D2 w.r.t. H is defined as dH∆H(D1, D2) = sup h,h′∈H Px∼D1 [h(x) ̸= h′(x)] + Px∼D2 [h(x) ̸= h′(x)]. (9) The H∆H-distance essentially provides a measure to quantify the distribution shift between two distributions. It measures the maximum difference of the disagreement between two hypotheses in H for two distributions, providing a metrics to quantify the distribution shift between D1 and D2. H-divergence and H∆H-distance have the advantage that they can be applied between datasets, i.e., estimated from finite samples. Specifically, let S1, S2 be unlabeled samples of size m sampled from D1 and D2; then we have estimated H∆H-distance ˆdH(S1, S2). This estimation can be bounded based on Theorem 3.4 of Kifer et al. (2004), which we state here for completeness. Theorem 5. Let A be a collection of subsets of some domain measure space, and assume that the VC-dimension is some finite d. Let P1 and P2 be probability distributions over that domain and S1, S2 finite samples of sizes m1, m2 drawn i.i.d. according P1, P2 respectively. Then Pm1+m2 [|ϕA(S1, S2) − ϕA(P1, P2)| > ϵ] ≤ (2m)de−m1ϵ2/16 + (2m)de−m2ϵ2/16, (10) where Pm1+m2 is the m1 + m2’th power of P - the probability that P induces over the choice of samples. Theorem 5 bounds the probability for relativized discrepancy, and its applications in below lemmas and Theorem 1 help us bound the quantified distribution shifts between domains. The probability, according to a distribution D, that an estimated hypothesis h disagrees with the true labeling function g : X → {0, 1} is defined as ϵ(h(t), g) = E(x)∼D[|h(x, t) − g(x)|], which we also refer to as the error or risk ϵ(h(t)). While the source domain dataset is inaccessible under ATTA settings, we consider the existence of the source dataset DS for the purpose of accurate theoretical analysis. Thus, we initialize Dtr(0) as DS, i.e., Dtr(0) = DS. For every time step t, the test and training data can be expressed as Ute(t) and Dtr(t) = DS ∪ Dte(1) ∪ Dte(2) ∪ ··· ∪Dte(t). (11) We use N to denote the total number of samples in Dtr(t) and λ = (λ0, λ1, ··· , λt) to represent the ratio of sample numbers in each component subset. In particular, we have |DS| |Dtr(t)| = λ0, |Dte(1)| |Dtr(t)| = λ1, ··· , |Dte(t)| |Dtr(t)| = λt, (12) where Pt i=0 λi = 1. Therefore, at time step t, the model has been trained on labeled data Dtr(t), which contains t + 1 components consisting of a combination of data from the source domain and multiple test-time domains. For each domain the model encounters, DS, Ute(1), Ute(2), ··· , Ute(t), let ϵj(h(t)) denote the error of hypothesis h at time t on the jth domain. Specifically, ϵ0(h(t)) = ϵS(h(t)) represents the error of h(t) on the source data DS, and ϵj(h(t)) for j ≥ 1 denotes the error of h(t) on test data Ute(j). Our optimization minimizes a convex combination of training error over the labeled samples from all domains. Formally, given the vector w = (w0, w1, ··· , wt) of domain error 23Published as a conference paper at ICLR 2024 weights with Pt j=0 wj = 1 and the sample number from each component Nj = λjN, we minimize the empirical weighted error of h(t) as ˆϵw(h(t)) = tX j=0 wjˆϵj(h(t)) = tX j=0 wj Nj X Nj |h(x, t) − g(x)|. (13) Note that w, λ and N are also functions of t, which we omit for simplicity. We now establish two lemmas as the preliminary for Theorem 1. In the following lemma, we bound the difference between the weighted error ϵw(h(t)) and the domain error ϵj(h(t)). Lemma 6. Let H be a hypothesis space of VC-dimension d. At time step t, let the ATTA data domains be DS, Ute(1), Ute(2), ··· , Ute(t), and Si be unlabeled samples of size m sampled from each of the t + 1 domains respectively. Then for any δ ∈ (0, 1), for every h ∈ Hminimizing ϵw(h(t)) on Dtr(t), we have |ϵw(h(t)) − ϵj(h(t))| ≤ tX i=0,i̸=j wi  1 2 ˆdH∆H(Si, Sj) + 2 s 2d log(2m) + log 2 δ m + γi  , with probability of at least 1 − δ, where γi = minh∈H{ϵi(h(t)) + ϵj(h(t))}. In the following lemma, we provide an upper bound on the difference between the true and empirical weighted errors ϵw(h(t)) and ˆϵw(h(t)). Lemma 7. Let H be a hypothesis class. For Dtr(t) = DS ∪ Dte(1) ∪ ··· ∪Dte(t) at time t, if the total number of samples in Dtr(t) is N, and the ratio of sample numbers in each component is λj, then for any δ ∈ (0, 1) and h ∈ H, with probability of at least 1 − δ, we have P[|ϵw(h(t)) − ˆϵw(h(t))| ≥ϵ] ≤ 2 exp   −2Nϵ2/( tX j=0 w2 j λj ) ! . Thus, as wj deviates from λj, the feasible approximation ˆϵw(h(t)) with a finite number of labeled samples becomes less reliable. The proofs for both lemmas are provided in Appx. E. Building upon the two preceding lemmas, we proceed to derive bounds on the domain errors under the ATTA setting when minimizing the empirical weighted error using the hypothesis h at time t. Lemma 6 bounds the difference between the weighted error ϵw(h(t)) and the domain error ϵj(h(t)), which is majorly influenced by the estimatedH∆H-distance and the quality of discrepancy estimation. During the ATTA process, the streaming test data can form multiple domains and distributions. However, if we consider all data during the test phase as a single test domain,i.e., St i=1 Ute(i), we can simplify Lemma 6 to obtain an upper bound for the test error ϵT as |ϵw(h(t)) − ϵT (h(t))| ≤w0  1 2 ˆdH∆H(S0, ST ) + 2 s 2d log(2m) + log 2 δ m + γ  , (14) where γ = min h∈H{ϵ0(h(t)) + ϵT (h(t))}, and ST is sampled from St i=1 Ute(i). To understand Lamma 7, we need to understand Hoeffding’s Inequality, which we state below as a Proposition for completeness. Proposition 8 (Hoeffding’s Inequality). Let X be a set, D1, . . . , Dt be probability distributions on X, and f1, . . . , ft be real-valued functions on X such that fi : X → [ai, bi] for i = 1, . . . , t. Then for any ϵ >0, P  \f\f\f\f\f 1 t tX i=1 fi(x) − 1 t tX i=1 Ex∼Di[fi(x)] \f\f\f\f\f ≥ ϵ ! ≤ 2 exp   − 2t2ϵ2 Pt i=1(bi − ai)2 ! (15) where E[fi(x)] is the expected value of fi(x). Lamma 7 provides an upper bound on the difference between the true and empirical weighted errors ϵw(h(t)) and ˆϵw(h(t)). Thus, as wj deviates from λj, the feasible approximation ˆϵw(h(t)) with a finite number of labeled samples becomes less reliable. Building upon the two preceding lemmas, we proceed to derive bounds on the domain errors under the ATTA setting when minimizing the empirical weighted error using the hypothesis h at time t. Theorem 1 essentially bounds the performance of ATTA on the source and each test domains. The adaptation performance on a test domain is majorly 24Published as a conference paper at ICLR 2024 bounded by the composition of (labeled) training data, estimated distribution shift, and ideal joint hypothesis performance, which correspond to C, ˆdH∆H(Si, Sj), and γi, respectively. The ideal joint hypothesis error γi gauges the inherent adaptability between domains. If we consider the multiple data distributions during the test phase as a single test domain, i.e., St i=1 Ute(i), Theorem 1 can be reduced into bounds for the source domain error ϵS and test domain error ϵT . With the optimal test/source hypothesis h∗ T (t) = arg min h∈H ϵT (h(t)) and h∗ S(t) = arg minh∈H ϵS(h(t)), |ϵT (ˆh(t)) − ϵT (h∗ T (t))| ≤w0A + s w2 0 λ0 + (1 − w0)2 1 − λ0 B, (16a) |ϵS(ˆh(t)) − ϵS(h∗ S(t))| ≤(1 − w0)A + s w2 0 λ0 + (1 − w0)2 1 − λ0 B, (16b) where the distribution divergence termA = ˆdH∆H(S0, ST )+4 q 2d log(2m)+log 2 δ m +2γ, the empirical gap term B = 2 q d log(2N)−log(δ) 2N , ST is sampled from St i=1 Ute(i), and γ = minh∈H{ϵ0(h(t)) + ϵT (h(t))}. Our learning bounds demonstrates the trade-off between the small amount of budgeted test-time data and the large amount of less relevant source data. Next, we provide an approximation of the condition necessary to achieve optimal adaptation performance, which is calculable from finite samples and can be readily applied in practical ATTA scenarios. Following Eq. (16.a), with approximately B = c1 p d/N, the optimal value w∗ 0 to tighten the test error bound is a function of λ0 and A: w∗ 0 = λ0 − s A2N c2 1d − A2Nλ0(1 − λ0), for λ 0 ≥ 1 − d A2N , (17) where c1 is a constant. Note that λ0 ≥ 1 − d A2N should be the satisfied condition in practical ATTA settings, where the budget is not sufficiently big while the source data amount is relatively large. When the budget is sufficiently large or the source data amount is not sufficiently large compared to the distribution shift A, the optimal w∗ 0 for the test error bound is w∗ 0 = 0, i.e., using no source data since possible error reduction from the data addition is always less than the error increase caused by large divergence between the source data and the test data. Theorem 2 offers a direct theoretical guarantee that ATTA reduces the error bound on test domains in comparison to TTA without the integration of active learning. Following Theorem 1, when no active learning is included during TTA,i.e., w0 = λ0 = 1, the upper boundw0A+ q w2 0 λ0 + (1−w0)2 1−λ0 B ≥ A+B; when enabling ATTA, withw0 = λ0 ̸= 1, we can easily achieve an upper bound w0A + B < A+ B. Therefore, the incorporation of labeled test instances in ATTA theoretically enhances the overall performance across test domains, substantiating the significance of the ATTA setting in addressing distribution shifts. Entropy quantifies the amount of information contained in a probability distribution. In the context of a classification model, lower entropy indicates that the model assigns high probability to one of the classes, suggesting a high level of certainty or confidence in its prediction. When a model assigns low entropy to a sample, this high confidence can be interpreted as the sample being well-aligned or fitting closely with the model’s learned distribution. In other words, the model “recognizes” the sample as being similar to those it was trained on, hence the high confidence in its prediction. While entropy is not a direct measure of distributional distance, it can be used as an indicator of how closely a sample aligns with the model’s learned distribution. This interpretation is more about model confidence and the implied proximity rather than a strict mathematical measure of distributional distance. The pre-trained model is well-trained on abundant source domain data, and thus the model distribution is approximately the source distribution. Selecting low-entropy samples using essentially provides an estimate of sampling from the source dataset. Thus, Dϕ,S(t), based on well-aligned with the model’s learned distribution is an approximation of DS. When we consider the CF problem and feasibly include the source-like dataset Dϕ,S(t) into the ATTA training data in place of the inaccessible DS in Eq. (11), we can also derive bounds on the domain errors under this practical ATTA setting when minimizing the empirical weighted errorϵ′ w(h(t)) using the hypothesis h at time t, similar to Theorem 1. Let H be a hypothesis class of VC-dimension d. At time step t, for ATTA data domainsDϕ,S(t), Ute(1), Ute(2), ··· , Ute(t), Si are unlabeled samples of size m sampled from each of the t + 1 domains respectively. The total number of samples in Dtr(t) is 25Published as a conference paper at ICLR 2024 N and the ratio of sample numbers in each component is λi. If ˆh(t) ∈ Hminimizes the empirical weighted error ˆϵ′ w(h(t)) with the weight vector w on Dtr(t), and h∗ j (t) = arg minh∈H ϵj(h(t)) is the optimal hypothesis on the jth domain, then for any δ ∈ (0, 1), we have ϵj(ˆh(t)) ≤ ϵj(h∗ j (t)) + 2 tX i=0,i̸=j wi  1 2 ˆdH∆H(Si, Sj) + 2 s 2d log(2m) + log 2 δ m + γi   + 2C with probability of at least 1 − δ, where C = r\u0010Pt i=0 w2 i λi \u0011\u0010 d log(2N)−log(δ) 2N \u0011 and γi = minh∈H{ϵi(h(t)) + ϵj(h(t))}. Other derived results following Theorem 1 also apply for this practical ATTA setting. Further empirical validations for our theoretical results are provided in Appx. H. E P ROOFS This section presents comprehensive proofs for all the lemmas, theorems, and corollaries mentioned in this paper, along with the derivation of key intermediate results. Lemma 6. Let H be a hypothesis space of VC-dimension d. At time step t, let the ATTA data domains be DS, Ute(1), Ute(2), ··· , Ute(t), and Si be unlabeled samples of size m sampled from each of the t + 1 domains respectively. Then for any δ ∈ (0, 1), for every h ∈ Hminimizing ϵw(h(t)) on Dtr(t), we have |ϵw(h(t)) − ϵj(h(t))| ≤ tX i=0,i̸=j wi  1 2 ˆdH∆H(Si, Sj) + 2 s 2d log(2m) + log 2 δ m + γi  , with probability of at least 1 − δ, where γi = minh∈H{ϵi(h(t)) + ϵj(h(t))}. Proof. First we prove that given unlabeled samples of size m S1, S2 sampled from two distributions D1 and D2, we have dH∆H(D1, D2) ≤ ˆdH∆H(S1, S2) + 4 s 2d log(2m) + log 2 δ m . (18) We start with Theorem 3.4 of Kifer et al. (2004): Pm1+m2 [|ϕA(S1, S2) − ϕA(P1, P2)| > ϵ] ≤ (2m)de−m1ϵ2/16 + (2m)de−m2ϵ2/16. (19) In Eq. 19, ’d’ is the VC-dimension of a collection of subsets of some domain measure space A, while in our case, d is the VC-dimension of hypothesis space H. Following (Ben-David et al., 2010), the H∆H space is the set of disagreements between every two hypotheses inH, which can be represented as a linear threshold network of depth 2 with 2 hidden units. Therefore, the VC-dimension of H∆H is at most twice the VC-dimension of H, and the VC-dimension of our domain measure space is 2d for Eq. 19 to hold. Given δ ∈ (0, 1), we set the upper bound of the inequality to δ, and solve for ϵ: δ = (2m)2de−m1ϵ2/16 + (2m)2de−m2ϵ2/16. We rewrite the inequality as δ (2m)2d = e−m1ϵ2/16 + e−m2ϵ2/16; taking the logarithm of both sides, we get log δ (2m)2d = −m1 ϵ2 16 + log(1 +e−(m1−m2) ϵ2 16 ). 26Published as a conference paper at ICLR 2024 Assuming m1 = m2 = m and defining a = ϵ2 16 , we have log δ (2m)2d = −ma + log 2; rearranging the equation, we then get ma + log(δ/2) = 2d log(2m). Now, we can solve for a: a = 2d log(2m) + log 2 δ m . Recall that a = ϵ2 16 , so we get: ϵ = 4√a ϵ = 4 s 2d log(2m) + log 2 δ m . With probability of at least 1 − δ, we have |ϕA(S1, S2) − ϕA(P1, P2)| ≤4 s 2d log(2m) + log 2 δ m ; therefore, dH∆H(D1, D2) ≤ ˆdH∆H(S1, S2) + 4 s 2d log(2m) + log 2 δ m . (20) Now we prove Lemma 6. We use the triangle inequality for classification error in the derivation. For the domain error of hypothesis h at time t on the jth domain ϵj(h(t)), given the definition of ϵw(h(t)), |ϵw(h(t)) − ϵj(h(t))| = | tX i=0 wiϵi(h(t)) − ϵj(h(t))| ≤ tX i=0 wi|ϵi(h(t)) − ϵj(h(t))| ≤ tX i=0 wi(|ϵi(h(t)) − ϵi(h(t), h∗ i (t))| + |ϵi(h(t), h∗ i (t)) − ϵj(h(t), h∗ i (t))| + |ϵj(h(t), h∗ i (t)) − ϵj(h(t))|) ≤ tX i=0 wi(ϵi(h∗ i (t)) + |ϵi(h(t), h∗ i (t)) − ϵj(h(t), h∗ i (t))| + ϵj(h∗ i (t))) ≤ tX i=0 wi(γi + |ϵi(h(t), h∗ i (t)) − ϵj(h(t), h∗ i (t))|), where γi = minh∈H{ϵi(h(t)) + ϵj(h(t))}. By the definition of H∆H-distance and our proved Eq. 20, |ϵi(h(t), h∗ i (t)) − ϵj(h(t), h∗ i (t))| ≤sup h,h′∈H |ϵi(h(t), h′(t)) − ϵj(h(t), h′(t))| = sup h,h′∈H Px∼Di[h(x) ̸= h′(x)] + Px∼Dj [h(x) ̸= h′(x)] = 1 2dH∆H(Di, Dj) ≤ 1 2 ˆdH∆H(Si, Sj) + 2 s 2d log(2m) + log 2 δ m , 27Published as a conference paper at ICLR 2024 where Di, Dj denote the ith and jth domain. Therefore, |ϵw(h(t)) − ϵj(h(t))| ≤ tX i=0 wi(γi + |ϵi(h(t), h∗ i (t)) − ϵj(h(t), h∗ i (t))|) ≤ tX i=0 wi(γi + 1 2dH∆H(Di, Dj)) ≤ tX i=0 wi(γi + 1 2 ˆdH∆H(Si, Sj) + 2 s 2d log(2m) + log 2 δ m ). Since ϵi(h(t)) − ϵj(h(t)) = 0 when i = j, we derive |ϵw(h(t)) − ϵj(h(t))| ≤ tX i=0,i̸=j wi  1 2 ˆdH∆H(Si, Sj) + 2 s 2d log(2m) + log 2 δ m + γi  , with probability of at least 1 − δ, where γi = minh∈H{ϵi(h(t)) + ϵj(h(t))}. This completes the proof. Lemma 7. Let H be a hypothesis class. For Dtr(t) = DS ∪ Dte(1) ∪ ··· ∪Dte(t) at time t, if the total number of samples in Dtr(t) is N, and the ratio of sample numbers in each component is λj, then for any δ ∈ (0, 1) and h ∈ H, with probability of at least 1 − δ, we have P[|ϵw(h(t)) − ˆϵw(h(t))| ≥ϵ] ≤ 2 exp   −2Nϵ2/( tX j=0 w2 j λj ) ! . Proof. We apply Hoeffding’s Inequality in our proof: P  \f\f\f\f\f 1 t tX i=1 fi(x) − 1 t tX i=1 Ex∼Di[fi(x)] \f\f\f\f\f ≥ ϵ ! ≤ 2 exp   − 2t2ϵ2 Pt i=1(bi − ai)2 ! . (21) In the jth domain, there are λjN samples. With the true labeling function g(x), for each of the λjN samples x, let there be a real-valued function fi(x) fi(x) = wj λj |h(x, t) − g(x)|, where fi(x) ∈ [0, wj λj ]. Incorporating all the domains, we get ˆϵw(h(t)) = tX j=0 wjˆϵj(h(t)) = tX j=0 wj λjN X λjN |h(x, t) − g(x)| = 1 N tX j=0 λjNX i=1 fi(x), which corresponds to the 1 t Pt i=1 fi(x) part in Hoeffding’s Inequality. Due to the linearity of expectations, we can calculate the sum of expectations as 1 N tX j=0 λjNX i=1 E[fi(x)] = 1 N ( tX j=0 λjN wj λj ϵj(h(t))) = tX j=0 wjϵj(h(t)) = ϵw(h(t)), which corresponds to the 1 t Pt i=1 Ex∼Di[fi(x)] part in Hoeffding’s Inequality. Therefore, we can apply Hoeffding’s Inequality as P[|ϵw(h(t)) − ˆϵw(h(t))| ≥ϵ] ≤ 2 exp   −2N2ϵ2/( NX i=0 range2(fi(x))) ! = 2 exp   −2N2ϵ2/( tX j=0 λjN(wj λj )2) ! = 2 exp   −2Nϵ2/( tX j=0 w2 j λj ) ! . This completes the proof. 28Published as a conference paper at ICLR 2024 Theorem 1. Let H be a hypothesis class of VC-dimension d. At time step t, for ATTA data domains DS, Ute(1), Ute(2), ··· , Ute(t), Si are unlabeled samples of size m sampled from each of the t + 1 domains respectively. The total number of samples in Dtr(t) is N and the ratio of sample numbers in each component is λi. If ˆh(t) ∈ Hminimizes the empirical weighted error ˆϵw(h(t)) with the weight vector w on Dtr(t), and h∗ j (t) = arg minh∈H ϵj(h(t)) is the optimal hypothesis on the jth domain, then for any δ ∈ (0, 1), with probability of at least 1 − δ, we have ϵj(ˆh(t)) ≤ ϵj(h∗ j (t)) + 2 tX i=0,i̸=j wi  1 2 ˆdH∆H(Si, Sj) + 2 s 2d log(2m) + log 2 δ m + γi   + 2C, where C = r\u0010Pt i=0 w2 i λi \u0011\u0010 d log(2N)−log(δ) 2N \u0011 and γi = minh∈H{ϵi(h(t)) + ϵj(h(t))}. For future test domains j = t + k (k >0), assuming k′ = argmink′∈{0,1,...t} dH∆H(D(k′), Ute(t + k)) and min dH∆H (D(k′), Ute(t + k)) ≤ δD, where 0 ≤ δD ≪ +∞, then ∀δ, with probability of at least 1 − δ, we have ϵt+k(ˆh(t)) ≤ ϵt+k(h∗ t+k(t)) + tX i=0 wi  ˆdH∆H(Si, Sk′ ) + 4 s 2d log(2m) + log 2 δ m + δD + 2γi   + 2C. Proof. First we prove that for any δ ∈ (0, 1) and h ∈ H, with probability of at least 1 − δ, we have |ϵw(h(t)) − ˆϵw(h(t))| ≤ vuut  tX i=0 w2 i λi !\u0012d log(2N) − log(δ) 2N \u0013 . (22) We apply Theorem 3.2 of Kifer et al. (2004) and Lemma 7, P[|ϵw(h(t)) − ˆϵw(h(t))| ≥ϵ] ≤ (2N)d exp   −2Nϵ2/( tX j=0 w2 j λj ) ! . Given δ ∈ (0, 1), we set the upper bound of the inequality to δ, and solve for ϵ: δ = (2N)d exp   −2Nϵ2/( tX j=0 w2 j λj ) ! . We rewrite the inequality as δ (2N)d = e −2Nϵ2/(Pt j=0 w2 j λj ) , taking the logarithm of both sides, we get log δ (2N)d = −2Nϵ2/( tX j=0 w2 j λj ). Rearranging the equation, we then get ϵ2 = ( tX j=0 w2 j λj )d log(2N) − log(δ) 2N . Therefore, with probability of at least 1 − δ, we have |ϵw(h(t)) − ˆϵw(h(t))| ≤ vuut  tX i=0 w2 i λi !\u0012d log(2N) − log(δ) 2N \u0013 . (23) 29Published as a conference paper at ICLR 2024 Based on Eq. 23, we now prove Theorem 1. For the empirical domain error of hypothesis h at time t on the jth domain ϵj(ˆh(t)), applying Lemma 6, Eq. 23, and the definition of h∗ j (t), we get ϵj(ˆh(t)) ≤ ϵw(ˆh(t)) + tX i=0,i̸=j wi  1 2 ˆdH∆H(Si, Sj) + 2 s 2d log(2m) + log 2 δ m + γi   ≤ ˆϵw(ˆh(t)) + vuut  tX i=0 w2 i λi !\u0012d log(2N) − log(δ) 2N \u0013 + tX i=0,i̸=j wi  1 2 ˆdH∆H(Si, Sj) + 2 s 2d log(2m) + log 2 δ m + γi   ≤ ˆϵw(h∗ j (t)) + vuut  tX i=0 w2 i λi !\u0012d log(2N) − log(δ) 2N \u0013 + tX i=0,i̸=j wi  1 2 ˆdH∆H(Si, Sj) + 2 s 2d log(2m) + log 2 δ m + γi   ≤ ϵw(h∗ j (t)) + 2 vuut  tX i=0 w2 i λi !\u0012d log(2N) − log(δ) 2N \u0013 + tX i=0,i̸=j wi  1 2 ˆdH∆H(Si, Sj) + 2 s 2d log(2m) + log 2 δ m + γi   ≤ ϵj(h∗ j (t)) + 2 vuut  tX i=0 w2 i λi !\u0012d log(2N) − log(δ) 2N \u0013 + 2 tX i=0,i̸=j wi  1 2 ˆdH∆H(Si, Sj) + 2 s 2d log(2m) + log 2 δ m + γi   = ϵj(h∗ j (t)) + 2 tX i=0,i̸=j wi  1 2 ˆdH∆H(Si, Sj) + 2 s 2d log(2m) + log 2 δ m + γi   + 2C with probability of at least 1 − δ, where C = r\u0010Pt i=0 w2 i λi \u0011\u0010 d log(2N)−log(δ) 2N \u0011 and γi = minh∈H{ϵi(h(t)) + ϵj(h(t))}. For future test domains j = t + k where k > 0, we have the assumption that k′ = argmink′∈{0,1,...t} dH∆H(D(k′), Ute(t + k)) and min dH∆H(D(k′), Ute(t + k)) ≤ δD. Here, we slightly abuse the notation D(k′) to represent Ds if k′ = 0 and Ute(k′) if k′ > 0. Then we get ϵt+k(ˆh(t)) ≤ ϵw(ˆh(t)) + tX i=0 wi  1 2 ˆdH∆H(Si, St+k) + 2 s 2d log(2m) + log 2 δ m + γi   ≤ ϵw(ˆh(t)) + tX i=0 wi  1 2( ˆdH∆H(Si, Sk′ ) + ˆdH∆H(Sk′ , St+k)) + 2 s 2d log(2m) + log 2 δ m + γi   ≤ ϵw(ˆh(t)) + tX i=0 wi  1 2 ˆdH∆H(Si, Sk′ ) + 1 2δD + 2 s 2d log(2m) + log 2 δ m + γi   ≤ ˆϵw(ˆh(t)) + vuut  tX i=0 w2 i λi !\u0012d log(2N) − log(δ) 2N \u0013 + tX i=0 wi  1 2 ˆdH∆H(Si, Sk′ ) + 1 2δD + 2 s 2d log(2m) + log 2 δ m + γi   30Published as a conference paper at ICLR 2024 ≤ ˆϵw(h∗ t+k(t)) + vuut  tX i=0 w2 i λi !\u0012d log(2N) − log(δ) 2N \u0013 + tX i=0 wi  1 2 ˆdH∆H(Si, Sk′ ) + 1 2δD + 2 s 2d log(2m) + log 2 δ m + γi   ≤ ϵw(h∗ t+k(t)) + 2 vuut  tX i=0 w2 i λi !\u0012d log(2N) − log(δ) 2N \u0013 + tX i=0 wi  1 2 ˆdH∆H(Si, Sk′ ) + 1 2δD + 2 s 2d log(2m) + log 2 δ m + γi   ≤ ϵt+k(h∗ t+k(t)) + 2 vuut  tX i=0 w2 i λi !\u0012d log(2N) − log(δ) 2N \u0013 + 2 tX i=0 wi  1 2 ˆdH∆H(Si, Sk′ ) + 1 2δD + 2 s 2d log(2m) + log 2 δ m + γi   = ϵt+k(h∗ t+k(t)) + tX i=0 wi  ˆdH∆H(Si, Sk′ ) + 4 s 2d log(2m) + log 2 δ m + δD + 2γi   + 2C. with probability of at least 1−δ, where C = r\u0010Pt i=0 w2 i λi \u0011\u0010 d log(2N)−log(δ) 2N \u0011 , γi = minh∈H{ϵi(h(t))+ ϵt+k(h(t))}, and 0 ≤ δD ≪ +∞. This completes the proof. Theorem 2. Let H be a hypothesis class of VC-dimension d. For ATTA data domains DS, Ute(1), Ute(2), ··· , Ute(t), considering the test-time data as a single test domain St i=1 Ute(i), if ˆh(t) ∈ H minimizes the empirical weighted error ˆϵw(h(t)) with the weight vector w on Dtr(t), let the test error be upper-bounded with |ϵT (ˆh(t)) − ϵT (h∗ T (t))| ≤EBT (w, λ, N, t). Let w′ and λ′ be the weight and sample ratio vectors when no active learning is included, i.e., w′ and λ′ s.t. w′ 0 = λ′ 0 = 1 and w′ i = λ′ i = 0 for i ≥ 1, then for any λ ̸= λ′, there exists w s.t. EBT (w, λ, N, t) < EBT (w′, λ′, N, t). (24) Proof. From Theorem 1, we can derive the bound for the test error where the test-time data are considered as a single test domain: |ϵT (ˆh(t)) − ϵT (h∗ T (t))| ≤EBT (w, λ, N, t) = w0( ˆdH∆H(S0, ST ) + 4 s 2d log(2m) + log 2 δ m + 2γ) + 2 s w2 0 λ0 + (1 − w0)2 1 − λ0 r d log(2N) − log(δ) 2N ; and we simplify the above equation as |ϵT (ˆh(t)) − ϵT (h∗ T (t))| ≤w0A + s w2 0 λ0 + (1 − w0)2 1 − λ0 B, (25) where the distribution divergence termA = ˆdH∆H(S0, ST )+4 q 2d log(2m)+log 2 δ m +2γ, the empirical gap term B = 2 q d log(2N)−log(δ) 2N , ST is sampled from St i=1 Ute(i), and γ = minh∈H{ϵ0(h(t)) + ϵT (h(t))}. Since we have s w2 0 λ0 + (1 − w0)2 1 − λ0 = s (w0 − λ0)2 λ0(1 − λ0) + 1 ≥ 1, (26) 31Published as a conference paper at ICLR 2024 where Formula 26 obtains the minimum value if and only if w0 = λ0; when enabling ATTA with any λ0 ̸= 1, we can get EBT (w, λ, N, t) = w0A + s w2 0 λ0 + (1 − w0)2 1 − λ0 B ≥ w0A + B, (27) where the minimum value EBT (w, λ, N, t)min = w0A + B can be obtained with condition w0 = λ0 ̸= 1. When no active learning is included, i.e., for weight and sample ratio vectors w′ and λ′, w′ 0 = λ′ 0 = 1 and w′ i = λ′ i = 0 for i ≥ 1, we have EBT (w′, λ′, N, t) = w′ 0A + s w′2 0 λ′ 0 + (1 − w′ 0)2 1 − λ′ 0 B = A + B. (28) Since for EBT (w, λ, N, t)min = w0A + B, w0 < 1 and A, B >0 hold, we derive EBT (w, λ, N, t)min = w0A + B < A+ B = EBT (w′, λ′, N, t). (29) This completes the proof. Corollary 3. At time step t, for ATTA data domains Dϕ,S(t), Ute(1), Ute(2), ··· , Ute(t), Si are unla- beled samples of size m sampled from each of the t + 1 domains respectively, and SS is unlabeled samples of size m sampled from DS. If ˆh(t) ∈ Hminimizes ˆϵ′ w(h(t)) while other conditions remain identical to Theorem 1, then ϵS(ˆh(t)) ≤ ϵS(h∗ S(t)) + tX i=0 wi  ˆdH∆H(Si, SS) + 4 s 2d log(2m) + log 2 δ m + 2γi   + 2C, with probability at least 1 − δ, where C follows Theorem 1 and γi = minh∈H{ϵi(h(t)) + ϵS(h(t))}. Proof. For the empirical source error on DS of hypothesis h at time t, similar to Theorem 1, we apply Lemma 6, Eq. 23 to get ϵS(ˆh(t)) ≤ ϵw(ˆh(t)) + tX i=0 wi  1 2 ˆdH∆H(Si, SS) + 2 s 2d log(2m) + log 2 δ m + γi   ≤ ˆϵw(ˆh(t)) + vuut  tX i=0 w2 i λi !\u0012d log(2N) − log(δ) 2N \u0013 + tX i=0 wi  1 2 ˆdH∆H(Si, SS) + 2 s 2d log(2m) + log 2 δ m + γi   ≤ ˆϵw(h∗ S(t)) + vuut  tX i=0 w2 i λi !\u0012d log(2N) − log(δ) 2N \u0013 + tX i=0 wi  1 2 ˆdH∆H(Si, SS) + 2 s 2d log(2m) + log 2 δ m + γi   ≤ ϵw(h∗ S(t)) + 2 vuut  tX i=0 w2 i λi !\u0012d log(2N) − log(δ) 2N \u0013 + tX i=0 wi  1 2 ˆdH∆H(Si, SS) + 2 s 2d log(2m) + log 2 δ m + γi   ≤ ϵS(h∗ S(t)) + 2 vuut  tX i=0 w2 i λi !\u0012d log(2N) − log(δ) 2N \u0013 + 2 tX i=0 wi  1 2 ˆdH∆H(Si, SS) + 2 s 2d log(2m) + log 2 δ m + γi   32Published as a conference paper at ICLR 2024 = ϵS(h∗ S(t)) + tX i=0 wi  ˆdH∆H(Si, SS) + 4 s 2d log(2m) + log 2 δ m + 2γi   + 2C with probability of at least 1 − δ, where C = r\u0010Pt i=0 w2 i λi \u0011\u0010 d log(2N)−log(δ) 2N \u0011 and γi = minh∈H{ϵi(h(t)) + ϵS(h(t))}. This completes the proof. Corollary 4. At time step t, for ATTA data domains Dϕ,S(t), Ute(1), Ute(2), ··· , Ute(t), suppose that ˆh(t) ∈ Hminimizes ˆϵw′(h(t)) under identical conditions to Theorem 2. Let’s denote the source error upper bound with |ϵS(ˆh(t)) − ϵS(h∗ S(t))| ≤EBS(w, λ, N, t). Let w′ and λ′ be the weight and sample ratio vectors when Dϕ,S(t) is not included, i.e., w′ and λ′ s.t. w′ 0 = λ′ 0 = 0 . If ˆdH∆H(DS, Dϕ,S(t)) < ˆdH∆H(DS, St i=1 Ute(i)), then for any λ ̸= λ′, there exists w s.t. EBS(w, λ, N, t) < EBS(w′, λ′, N, t). (30) Proof. From Theorem 1, considering the test-time data as a single test domain, we can derive the bound for the source error on DS: |ϵS(ˆh(t)) − ϵS(h∗ S(t))| ≤EBS(w, λ, N, t) = w0( ˆdH∆H(S0, SS) + 4 s 2d log(2m) + log 2 δ m + 2γ) + (1 − w0)( ˆdH∆H(SS, ST ) + 4 s 2d log(2m) + log 2 δ m + 2γ′) + 2 s w2 0 λ0 + (1 − w0)2 1 − λ0 r d log(2N) − log(δ) 2N , where ST is sampled fromSt i=1 Ute(i), γ = minh∈H{ϵ0(h(t))+ϵS(h(t))}, and γ′ = minh∈H{ϵT (h(t))+ ϵS(h(t))}. We have s w2 0 λ0 + (1 − w0)2 1 − λ0 = s (w0 − λ0)2 λ0(1 − λ0) + 1 ≥ 1, (31) where the equality and the minimum value are obtained if and only if w0 = λ0. When Dϕ,S(t) is not included,i.e., with the weight and sample ratio vectorsw′ and λ′ s.t. w′ 0 = λ′ 0 = 0, using the empirical gap term B = 2 q d log(2N)−log(δ) 2N , we have EBS(w′, λ′, N, t) = ˆdH∆H(SS, ST ) + 4 s 2d log(2m) + log 2 δ m + 2γ′ + s w2 0 λ0 + (1 − w0)2 1 − λ0 B = ˆdH∆H(SS, ST ) + 4 s 2d log(2m) + log 2 δ m + 2γ′ + B. When Dϕ,S(t) is included with λ0 ̸= 0, EBS(w, λ, N, t) = w0( ˆdH∆H(S0, SS) + 4 s 2d log(2m) + log 2 δ m + 2γ) + (1 − w0)( ˆdH∆H(SS, ST ) + 4 s 2d log(2m) + log 2 δ m + 2γ′) + s w2 0 λ0 + (1 − w0)2 1 − λ0 B ≤ w0( ˆdH∆H(S0, SS) + 4 s 2d log(2m) + log 2 δ m + 2γ) + (1 − w0)( ˆdH∆H(SS, ST ) + 4 s 2d log(2m) + log 2 δ m + 2γ′) + B, 33Published as a conference paper at ICLR 2024 Algorithm 2 INCREMENTAL CLUSTERING (IC) Require: Given previously selected anchors, new unlabeled samples, and the cluster budget as Danc, Unew, and NC . Global anchor weights wanc = (wanc 1 , . . . , wanc |Danc|)⊤. 1: For simplicity, we consider anchor weights wanc as a global vector. 2: function IC(Danc, Unew, NC ) 3: wsp ← Concat(wanc, 1⊤ |Unew|) ▷ Assign all new samples with weight 1. 4: Φ ← Extract the features from the penultimate layer of model f on x ∈ Danc ∪ Unew in order. 5: clusters ← Weighted-K-Means(Φ, wsp, NC) 6: new_clusters ← {clusteri | ∀clusteri ∈ clusters, ∀x ∈ Danc, x /∈ clustersi} 7: Xnew_anchors ← {the closest sample x to the centroid of clusteri | ∀clusteri ∈ new_clusters} 8: Xanchors ← {x ∈ Danc} ∪Xnew_anchors 9: wanc ← Concat(wanc, 0⊤ |Xnew_anchors|) ▷ Initialize new anchor weights. 10: for wanc i ∈ wanc, wanc i ← wanc i + # sample of clusterj # anchor in clusterj , wanc i ∈ clusterj ▷ Weight accumulation. 11: Return Xanchors 12: end function where the minimum value can be obtained with condition w0 = λ0 ̸= 0. In practical learning scenarios, we generally assume adaptation tasks are solvable; therefore, there should be a prediction function that performs well on two distinct domains. In this case, γ and γ′ should be relatively small, so we can assume γ ≈ γ′. If ˆdH∆H(S0, SS) < ˆdH∆H(SS, ST ), then we have EBS(w, λ, N, t)min = w0( ˆdH∆H(S0, SS) + 4 s 2d log(2m) + log 2 δ m + 2γ) + (1 − w0)( ˆdH∆H(SS, ST ) + 4 s 2d log(2m) + log 2 δ m + 2γ′) + B < ˆdH∆H(SS, ST ) + 4 s 2d log(2m) + log 2 δ m + 2γ′ + B = EBS(w′, λ′, N, t). Therefore, we derive EBS(w, λ, N, t)min < EBS(w′, λ′, N, t). (32) This completes the proof. F I NCREMENTAL CLUSTERING F.1 A LGORITHM DETAILS We provide the detailed algorithm for incremental clustering as Alg. 2. F.2 V ISUALIZATION To better illustrate the incremental clustering algorithm, we provide visualization results on PACS to demonstrate the process. As shown in Fig. 3, the initial step of IC is a normal K-Means clustering step, and ten anchors denoted as \"X\" are selected. The weights of all samples in a clusters is aggregated into the corresponding anchor’s weight. Therefore, these ten samples (anchors) are given larger sizes visually (i.e., larger weights) than that of other new test samples in the first IC step (Fig. 4). During the first IC step, several distributions are far away from the existed anchors and form clusters 1,7,9 and 10, which leads to 4 new selected anchors. While the number of cluster centroid is only increased by 1, 4 of the existing anchors are clustered into the same cluster 8 (purple). Thus IC produces 4 new anchors instead of 1. Similarly, in the second IC step (Fig. 5), the new streaming-in test samples introduce a new distribution; IC produces 3 new clusters (4, 8, and 11) and the corresponding number of anchors to cover them. The number of centroid is only increased by 1, which implies that there are two original-cluster-merging events. More IC step visualization results are provided in Fig. 6 and 7. 34Published as a conference paper at ICLR 2024 Figure 3: Initial IC step: normal clustering. Left: Clustering results. Right: Selecting new anchors. Figure 4: The first IC step. Left: Weighted clustering results. Right: Selecting new anchors. Figure 5: The second IC step. Left: Weighted clustering results. Right: Selecting new anchors. 35Published as a conference paper at ICLR 2024 Figure 6: The third IC step. Left: Weighted clustering results. Right: Selecting new anchors. Figure 7: The fourth IC step. Left: Weighted clustering results. Right: Selecting new anchors. 36Published as a conference paper at ICLR 2024 G E XPERIMENT DETAILS In this section, we provide more experimental details including the details of the datasets and training settings. G.1 D ETAILS ABOUT THE DATASETS We adopt datasets PACS, VLCS, and Office-Home from DomainBed (Gulrajani and Lopez-Paz, 2020) with the same domain splits. All available licenses are mentioned below. • PACS (Li et al., 2017) includes four domains: art, cartoons, photos, and sketches. PACS is a 7-class classification dataset with 9,991 images of dimension (3, 224, 224). • VLCS (Fang et al., 2013) contains photographic domains: Caltech101, LabelMe, SUN09, and VOC2007. This dataset includes 10,729 images of dimension (3, 224, 224) with 5 classes. • Office-Home (Venkateswara et al., 2017) is a 65-class dataset, including domains: art, clipart, product, and real. VLCS includes 10,729 images of dimension (3, 224, 244). (License) • Tiny-ImageNet-C is a 200-class dataset, including 15 corrupt types. Tiny-ImageNet-C includes 150,000 images of dimension (3, 224, 244). Since the class number 200 is less than ImageNet (1000), the model’s last layer classifier needs to be adapted. In this work, we use the brightness corruption domain to adapt. In the source pretraining phase, we adopt the most ImageNet-like domain as our source domain. For PACS and Office-Home, we use domains \"photos\" and \"real\" as the source domains, respectively, while for VLCS, Caltech101 is assigned to apply the source pretraining. We freeze the random seeds to generate the sample indices order for the two test data streams, namely, the domain-wise data stream and the random data stream. For PACS, the domain-wise data stream inputs samples from domain art, cartoons, to sketches, while we shuffle all samples from these three domains in the random data stream. For VLCS, we stream the domains in the order: LabelMe, SUN09, and VOC2007, as the domain-wise data stream. For Office-Home, the domain-wise data stream order becomes art, clipart, and product. G.2 T RAINING AND OPTIMIZATION SETTINGS In this section, we extensively discuss the model architectures, optimization settings, and method settings. G.2.1 A RCHITECTURES PACS & VLCS. We adopt ResNet-18 as our model encoder followed by a linear classifier. The initial parameters of ResNet-18 are ImageNet pre-trained weights. In our experiment, we remove the Dropout layer since we empirically found that using the Dropout layer might degrade the optimization process when the sample number is small. The specific implementation of the network is closely aligned with the implementation in DomainBed (Gulrajani and Lopez-Paz, 2020). Office-Home. We employ ResNet-50 as our model encoder for Office-Home. Except for the architecture, the other model settings are aligned with the ResNet-18. Tiny-ImageNet-C ResNet-18 is adapted from ImageNet to Tiny-ImageNet-C by training the last linear layer. G.2.2 T RAINING & OPTIMIZATION In this section, we describe the training configurations for both the source domain pre-training and test-time adaptation procedures. Source domain pre-training. For the PACS and VLCS datasets, models are fine-tuned on the selected source domains for 3,000 iterations. The Adam optimizer is utilized with a learning rate 37Published as a conference paper at ICLR 2024 of 10−4. In contrast, for the Office-Home dataset, the model is fine-tuned for a longer duration of 10,000 iterations with a slightly adjusted learning rate of 5 × 10−5. Test-time adaptation. For test-time adaptation across PACS and VLCS, the pre-trained source model is further fine-tuned using the SGD optimizer with a learning rate of 10−3. While on Office-Home and Tiny-ImageNet-C, a learning rate of 10−4 is adopted. For all TTA baselines, barring specific exceptions, we faithfully adhere to the original implementation settings. A noteworthy exception is the EATA method, which requires a cosine similarity threshold. The default threshold of the original EATA implementation was not suitable for the three datasets used in our study, necessitating an adjustment. We empirically set this threshold to 0.5 for training. Unlike Tent and SAR, which only require the optimization of batch normalization layers (Santurkar et al., 2018), SimATTA allows the training of all parameters in the networks. In experiments, we use a tolerance count (tol) to control the training process. SimATTA will stop updating once the loss does not descrease for more than 5 steps. However, for Tiny-ImageNet-C, SimATTA uses ‘steps=10‘ for time comparisons since other methods apply at most 10 steps. G.2.3 M ETHOD SETTINGS Tent. In our experiments, we apply the official implementation of Tent1. Specifically, we evaluate Tent with 1 test-time training step and 10 steps, respectively. EATA.Our EATA implementation follows its official code2. In our experiments, EATA has 2000 fisher training samples, E0 = 0.4 × log(# class), ϵ <0.5. CoTTA. For CoTTA, we strictly follow all the code and settings from its official implementation3. SAR. With SAR’s official implementation4, we set E0 = 0 .4 × log(# class) and e0 = 0 .1 in our experiments. ADA baselines. For ADA baselines, we follow the architecture of the official implementation of CLUE (Prabhu et al., 2021)5. SimATTA Implementation. Our implementation largely involves straightforward hyperparameter settings. The higher entropy bound eh = 10−2 should exceed the lower entropy bound el, but equal values are acceptable. Empirically, the lower entropy bound el can be set to 10−3 for VLCS and Office-Home, or 10−4 for PACS. The choice of el is largely dependent on the number of source-like samples obtained. A lower el may yield higher-accuracy low-entropy samples, but this could lead to unstable training due to sample scarcity. Though experimentation with different hyperparameters is encouraged, our findings suggest that maintaining a non-trivial number of low-entropy samples and setting an appropriateλ0 are of primary importance. If λ0 < 0.5, CF may ensue, which may negate any potential improvement. Regarding the management of budgets, numerous strategies can be adopted. In our experiments, we utilized a simple hyperparameter k, varying from 1 to 3, to regulate the increasing rate of budget consumption. This strategy is fairly elementary and can be substituted by any adaptive techniques. G.3 S OFTWARE AND HARDWARE We conduct our experiments with PyTorch (Paszke et al., 2019) and scikit-learn (Pedregosa et al., 2011) on Ubuntu 20.04. The Ubuntu server includes 112 Intel(R) Xeon(R) Gold 6258R CPU @2.70GHz, 1.47TB memory, and NVIDIA A100 80GB PCIe graphics cards. The training process costs graphics memory less than 10GB, and it requires CPU computational resources for scikit-learn K-Means clustering calculations. Our implementation also includes a GPU-based PyTorch K-Means method for transferring calculation loads from CPUs to GPUs. However, for consistency, the results of our experiments are obtained with the original scikit-learn K-Means implementation. 1https://github.com/DequanWang/tent 2https://github.com/mr-eggplant/EATA 3https://github.com/qinenergy/cotta 4https://github.com/mr-eggplant/SAR 5https://github.com/virajprabhu/CLUE 38Published as a conference paper at ICLR 2024 Figure 8: Target loss surface on 2000 samples without source pre-training. The red points denote the loss minimum for a fixed λ0. The orange line denote the place where w0 = λ0. Figure 9: Target loss surface on 2000 samples with source pre-training. H E MPIRICAL VALIDATIONS FOR THEORETICAL ANALYSIS In this section, we undertake empirical validation of our learning theory, which encompasses multiple facets awaiting verification. In contemporary computer vision fields, pre-trained models play a pivotal role, and performance would significantly decline without the use of pre-trained features. The learning theory suggests that given the vast VC-dimension of complete ResNets, without substantial data samples, the training error cannot be theoretically tight-bounded. However, we show empirically in the following experiments that fine-tuning pre-trained models is behaviorally akin to training a model with a low VC-dimension. Training on 2000 Samples Without Source Domain Pre-training. For an ImageNet pre-trained ResNet-18 model, we trained it using 2000 samples from the PACS dataset. To ascertain the optimal value w∗ 0 in Equation 4, we trained multiple models for different w0 and λ0 pairings. For each pair, we derived the target domain loss (from art, cartoons, and sketches) post-training and plotted this loss on the z-axis. With w0 and λ0 serving as the xy-axes, we drafted the target domain loss ϵT surface in Figure 8. As the results show, given a λ0, the optimal w∗ 0 typically aligns with the line λ0 = w0, with a slight downward shift, which aligns with Equation 4. 39Published as a conference paper at ICLR 2024 Figure 10: Target loss surface on 500 samples with source pre-training. Figure 11: Source loss surface on 500 samples with source pre-training. 40Published as a conference paper at ICLR 2024 Figure 12: Target and source loss surface on 500 samples with source pre-training. Table 6: TTA comparisons on Office-Home. This table includes the two data stream settings mentioned in the dataset setup and reports performances in accuracy. Results that outperform all TTA baselines are highlighted in bold font. N/A denotes the adaptations are not applied on the source domain. Office-Home Domain-wise data stream Post-adaptation Random data stream Post-adaptation R →A→ →C→ →P R A C P 1 2 3 4 R A C P BN w/o adapt 93.78 42.93 37.62 59.90 93.78 42.93 37.62 59.90 46.82 46.82 46.82 46.82 93.78 42.93 37.62 59.90BN w/ adapt 92.38 49.69 39.43 63.53 92.38 49.69 39.43 63.53 50.88 50.88 50.88 50.88 92.38 49.69 39.43 63.53 Tent (steps=1) N/A 49.61 39.31 63.87 92.47 49.57 39.89 63.89 49.95 50.27 50.23 52.06 92.40 49.24 39.68 63.98Tent (steps=10) N/A 49.61 39.04 61.41 87.08 44.79 38.37 60.49 50.05 49.31 48.74 47.79 85.31 42.85 37.89 58.71EATA N/A 49.65 39.04 63.53 91.60 49.61 38.65 63.48 49.73 50.27 49.45 51.07 91.05 49.11 38.26 62.99CoTTA N/A 49.61 38.76 61.84 87.81 44.95 35.92 59.04 49.84 49.84 48.95 50.43 86.99 43.68 34.73 57.56SAR (steps=1) N/A 49.65 39.24 63.53 92.45 49.73 39.36 63.69 49.84 50.05 49.91 51.67 92.38 49.57 39.50 63.87SAR (steps=10) N/A 49.53 38.81 61.50 88.94 46.15 37.04 59.41 50.09 50.30 49.77 49.22 89.14 46.23 36.31 59.45 SimATTA (B ≤300) N/A 56.20 48.38 71.66 95.75 60.07 52.62 74.70 58.57 60.88 62.91 63.67 95.89 62.01 54.98 74.70SimATTA (B ≤500) N/A 58.71 51.11 74.36 96.03 62.05 57.41 76.98 58.85 62.63 63.41 64.31 95.91 63.78 57.87 77.09 Training on 2000 Samples with Source Domain Pre-training. To further assess the effects of source pre-training, we repeated the same experiment on a source pre-trained ResNet-18. The results are depicted in Figure 9. This experiment provides empirical guidance on selecting w0 in source domain pre-trained situations. The findings suggest that the optimal w∗ 0 non-trivially shifts away from the line λ0 = w0 towards lower-value regions. Considering the source pre-training process as using a greater quantity of source domain samples, it implies that when the number of source samples greatly exceeds target samples, a lower w0 can enhance target domain results. Training on 500 Samples with Source Domain Pre-training. We proceed to fine-tune the source domain pre-trained ResNet-18 using only 500 samples, thereby simulating active TTA settings. We train models with various w0 and λ0 pairings, then graph the target domain losses, source domain losses, and the combined losses. As shown in Figure 10, the target losses still comply with our theoretical deductions where the local minima are close to the line λ0 = w0 and marginally shift towards lower values. Considering the challenge of CF, the source domain results in Figure 11 suggest a reverse trend compared to the target domain, where lower λ0 and w0 values yield superior target domain results but inferior source domain results. Thus, to curb CF, the primary strategy is to maintain a relatively higher λ0. When considering both target and source domains, a balance emerges as depicted in Figure 12. The global minimum is located in the middle region, demonstrating the trade-off between the target domain and source domain performance. I A DDITIONAL EXPERIMENT RESULTS In this section, we provide additional experiment results. The Office-Home results and ablation studies will be presented in a similar way as the main paper. In the full results Sec. I.3, we will post more detailed experimental results with specific budget numbers and intermediate performance during the test-time adaptation. 41Published as a conference paper at ICLR 2024 Table 7: Comparisons to ADA baselines on Office-Home. The source domain is denoted as \"(S)\" in the table. Results are average accuracies with standard deviations). Office-Home R (S) A C P Random (B = 300) 95.04 (0.20) 57.54 (1.16) 53.43 (1.17) 73.46 (0.97) Entropy (B = 300) 94.39 (0.49) 61.21 (0.71) 56.53 (0.71) 72.31 (0.28) Kmeans (B = 300) 95.09 (0.14) 57.37 (0.90) 51.74 (1.34) 71.81 (0.39) CLUE (B = 300) 95.20 (0.23) 60.18 (0.98) 58.05 (0.43) 73.72 (0.70) Ours (B ≤300) 95.82 (0.07) 61.04 (0.97) 53.80 (1.18) 74.70 (0.00) I.1 R ESULTS ON OFFICE -HOME We conduct experiments on Office-Home and get the test-time performances and post-adaptation performances for two data streams. As shown in Tab. 6, SimATTA can outperform all TTA baselines with huge margins. Compared to ADA baselines under the source-free settings, as shown in Tab. 7, SimATTA obtains comparable results. I.2 A BLATION STUDIES Figure 13: Ablation study on PACS and VLCS.\"IC=0\" denotes removing incremental clustering (IC) selection. \"LE=0\" denotes removing the low-entropy (LE) sample training. Domain-wise stream and random stream are applied on first and second rows, respectively. The accuracy values are averaged across all splits/domains. In this section, we explore three variations of our method to examine the individual impacts of its components. The first variant replaces the incremental clustering selection with entropy selection, 42Published as a conference paper at ICLR 2024 where only the samples with the highest entropy are chosen. The second variant eliminates low- entropy sample training. The third variation combines the first and second variants. We perform this ablation study on the PACS and VLCS as outlined in Fig. 13. We denote the use of incremental clustering (IC) and low-entropy training (LE) respectively as IC=1 and LE=1. The experiments essentially reveals the effectiveness of incremental clustering and low-entropy- sample training. As we have detailed in Sec. 3.2, these techniques are designed to to select informative samples, increase distribution coverage, and mitigate catastrophic forgetting. These designs appositely serve the ATTA setting where the oracle has costs and the budget is limited. Therefore, their effectiveness is prominent particularly when the budget is small. As the results show, when the budget B ≤100 or B ≤300, removing the components observably impairs performances. When B gets large, more active samples cover a larger distribution; thus the performance gap from random selection and informative selection gets smaller. In the extreme case where B → ∞, all samples are selected and thus the superiority of our meticulously-designed techniques are not manifested. Specifically, our analysis yields several insights. First, SimATTA (LE=1, IC=1) comprehensively outperforms other variants on both datasets, different streams, and different budgets. Second, variants without low-entropy training (LE=0, IC=0/1) easily fail to produce stable results (e.g., domain-wise stream in VLCS). Third, SimATTA’s performance surpasses this variant on PACS’s domain-wise stream clearly especially when the budgets are low. This indicates these variants fail to retrieve the most informative style shift (PACS’s shifts) samples, which implies the advantage of incremental clustering when the budget is tight. In addition, these results show that IC has its unique advantage on domain-wise streams where distributions change abruptly instead of random streams. Therefore, compared to PACS’s domain- wise stream results, the reason for the smaller performance improvement of SimATTA over the variant (LE=1, IC=0) on VLCS’s domain-wise stream is that images in VLCS are all photos that do not include those severe style shifts in PACS (i.e., art, cartoons, and sketches). That is, when the shift is not severe, we don’t need IC to cover very different distributions, and selecting samples using entropy can produce good results. In brief, IC is extraordinary for severe distribution shifts and quick adaptation. It is worth mentioning that low budget comparison is essential to show the informative sample retrieval ability, since as the budget increases, all AL techniques will tend to perform closely. I.3 C OMPLETE EXPERIMENT RESULTS We provide complete experimental results in this section. As shown in Tab. 8, we present the full results for two data streams. The test-time adaptation accuracies are shown in the \"Current domain\" row, while the \"Budgets\" row denotes the used budget by the end of the domain. The rest four rows denote the four domain test results by the end of the real-time adaptation of the current domain, where the first column results are the test accuracy before the test-time adaptation phase. N/A represents \"do not apply\". Table 8: Tent (steps=1) on PACS. Tent (steps=1) Domain-wise data stream Random data stream P →A→ → C→ → S 1 → → 2→ → 3→ → 4→ Current domain N/A 67.29 64.59 44.67 56.35 54.09 51.83 48.58 Budgets N/A N/A N/A N/A N/A N/A N/A N/A P 99.70 98.68 98.38 97.60 98.56 98.08 97.72 97.19 A 59.38 69.09 68.95 66.85 68.07 67.33 65.58 63.53 C 28.03 64.04 65.19 64.08 64.85 65.19 62.97 60.75 S 42.91 53.65 47.39 42.58 54.57 49.83 44.13 41.56 J C HALLENGES AND PERSPECTIVES Despite advancements, test-time adaptation continues to pose considerable challenges. As previously discussed, without supplementary information and assumptions, the ability to guarantee model generalization capabilities is limited. However, this is not unexpected given that recent progress 43Published as a conference paper at ICLR 2024 Table 9: Tent (steps=10) on PACS. Tent (steps=10) Domain-wise data stream Random data stream P →A→ → C→ → S 1 → → 2→ → 3→ → 4→ Current domain N/A 67.38 57.85 20.23 47.36 31.01 22.84 20.33 Budgets N/A N/A N/A N/A N/A N/A N/A N/A P 99.70 95.45 87.43 62.63 93.83 81.32 65.39 50.78 A 59.38 64.94 55.03 34.52 55.32 40.28 28.27 23.68 C 28.03 55.89 56.70 40.57 54.52 39.68 27.22 20.95 S 42.91 36.96 26.27 13.59 32.25 23.16 20.95 19.62 Table 10: EATA on PACS. EATA Domain-wise data stream Random data stream P →A→ → C→ → S 1 → → 2→ → 3→ → 4→ Current domain N/A 67.04 64.72 50.27 57.31 56.06 58.17 59.78 Budgets N/A N/A N/A N/A N/A N/A N/A N/A P 99.70 98.62 98.50 98.62 98.68 98.62 98.50 98.62 A 59.38 68.90 68.16 66.50 68.65 68.95 69.34 69.63 C 28.03 63.74 65.36 62.46 65.19 66.00 65.57 65.70 S 42.91 54.01 52.89 48.18 55.71 55.64 54.09 54.26 Table 11: CoTTA on PACS. CoTTA Domain-wise data stream Random data stream P →A→ → C→ → S 1 → → 2→ → 3→ → 4→ Current domain N/A 65.48 62.12 53.17 56.06 54.33 57.16 57.42 Budgets N/A N/A N/A N/A N/A N/A N/A N/A P 99.70 98.68 98.62 98.62 98.62 98.62 98.56 98.62 A 59.38 65.82 65.87 65.48 66.02 65.87 66.31 65.97 C 28.03 62.63 63.05 63.10 63.01 62.88 63.01 62.97 S 42.91 53.88 54.03 53.78 54.67 55.31 55.10 54.62 Table 12: SAR (steps=1) on PACS. SAR (steps=1) Domain-wise data stream Random data stream P →A→ → C→ → S 1 → → 2→ → 3→ → 4→ Current domain N/A 66.75 63.82 49.58 56.78 56.35 56.68 56.70 Budgets N/A N/A N/A N/A N/A N/A N/A N/A P 99.70 98.68 98.50 98.32 98.74 98.56 98.50 98.44 A 59.38 68.02 68.07 66.94 67.87 68.65 68.55 68.16 C 28.03 62.84 64.97 62.93 63.82 64.89 64.46 64.38 S 42.91 53.47 52.07 45.74 54.92 55.46 53.68 52.53 Table 13: SAR (steps=10) on PACS. SAR (steps=10) Domain-wise data stream Random data stream P →A→ → C→ → S 1 → → 2→ → 3→ → 4→ Current domain N/A 69.38 68.26 49.02 53.51 51.15 51.78 45.60 Budgets N/A N/A N/A N/A N/A N/A N/A N/A P 99.70 98.20 95.39 96.47 97.13 97.78 97.72 94.13 A 59.38 72.36 66.60 62.16 62.74 64.94 66.11 56.64 C 28.03 63.44 68.30 56.19 59.77 61.73 62.03 56.02 S 42.91 53.37 44.59 54.62 41.00 49.66 48.79 36.37 44Published as a conference paper at ICLR 2024 Table 14: SimATTA (B ≤300) on PACS. SimATTA (B ≤300) Domain-wise data stream Random data stream P →A→ → C→ → S 1 → → 2→ → 3→ → 4→ Current domain N/A 76.86 70.90 75.39 69.47 76.49 82.45 82.22 Budgets N/A 75 145 223 66 142 203 267 P 99.70 98.44 98.86 98.80 97.96 98.68 99.04 98.98 A 59.38 80.71 82.32 84.47 73.97 80.52 81.10 84.91 C 28.03 48.12 82.00 82.25 72.35 81.06 83.36 83.92 S 42.91 32.78 56.25 81.52 79.49 83.10 84.78 86.00 Table 15: SimATTA (B ≤500) on PACS. SimATTA (B ≤500) Domain-wise data stream Random data stream P →A→ → C→ → S 1 → → 2→ → 3→ → 4→ Current domain N/A 77.93 76.02 76.30 68.46 78.22 80.91 85.49 Budgets N/A 121 230 358 102 221 343 425 P 99.70 98.92 98.86 98.62 98.20 99.46 99.10 99.16 A 59.38 87.01 87.60 88.33 73.39 79.20 84.91 86.67 C 28.03 54.78 83.96 83.49 68.43 74.40 84.22 84.77 S 42.91 46.37 63.53 83.74 81.34 81.04 86.66 87.71 Table 16: Tent (steps=1) on VLCS. Tent (steps=1) Domain-wise data stream Random data stream C →L→ → S→ → V 1 → → 2→ → 3→ → 4→ Current domain N/A 38.55 34.40 53.88 44.85 44.29 47.38 44.98 Budgets N/A N/A N/A N/A N/A N/A N/A N/A C 100.00 84.81 85.44 84.73 84.95 85.16 85.80 85.30 L 33.55 40.02 43.11 43.86 39.68 41.98 43.11 43.49 S 41.10 33.39 35.41 33.61 36.29 37.90 38.27 37.81 V 49.08 53.20 54.06 53.11 53.76 54.18 53.76 53.35 Table 17: Tent (steps=10) on VLCS. Tent (steps=10) Domain-wise data stream Random data stream C →L→ → S→ → V 1 → → 2→ → 3→ → 4→ Current domain N/A 45.41 31.44 32.32 46.13 42.31 43.51 39.48 Budgets N/A N/A N/A N/A N/A N/A N/A N/A C 100.00 73.07 48.34 42.54 74.13 62.19 56.54 52.01 L 33.55 46.61 38.44 37.65 44.88 45.93 43.41 40.32 S 41.10 31.75 28.82 27.79 35.37 36.14 35.28 33.64 V 49.08 48.05 40.14 33.12 50.50 44.49 42.48 40.37 Table 18: EATA on VLCS. EATA Domain-wise data stream Random data stream C →L→ → S→ → V 1 → → 2→ → 3→ → 4→ Current domain N/A 37.24 33.15 52.58 43.77 42.48 43.34 41.55 Budgets N/A N/A N/A N/A N/A N/A N/A N/A C 100.00 85.16 85.02 84.10 84.73 84.52 84.10 83.32 L 33.55 37.16 37.24 37.69 37.09 36.78 36.90 36.67 S 41.10 33.39 33.49 32.39 33.33 32.54 31.84 31.47 V 49.08 51.87 52.16 52.49 52.07 52.43 52.64 52.55 45Published as a conference paper at ICLR 2024 Table 19: CoTTA on VLCS. CoTTA Domain-wise data stream Random data stream C →L→ → S→ → V 1 → → 2→ → 3→ → 4→ Current domain N/A 37.39 32.54 52.25 43.69 42.14 43.21 42.32 Budgets N/A N/A N/A N/A N/A N/A N/A N/A C 100.00 81.55 81.98 82.12 82.61 82.47 82.12 81.98 L 33.55 37.20 37.91 37.65 38.48 38.22 38.40 37.99 S 41.10 30.71 32.78 33.12 34.00 33.70 33.97 33.52 V 49.08 52.01 52.64 52.90 53.64 53.14 53.08 53.23 Table 20: SAR (steps=1) on VLCS. SAR (steps=1) Domain-wise data stream Random data stream C →L→ → S→ → V 1 → → 2→ → 3→ → 4→ Current domain N/A 36.18 34.43 52.46 43.64 43.04 44.20 41.93 Budgets N/A N/A N/A N/A N/A N/A N/A N/A C 100.00 84.31 84.17 83.96 85.09 85.23 85.23 85.09 L 33.55 35.62 38.29 39.72 38.55 39.34 40.21 40.70 S 41.10 33.24 36.41 36.53 34.37 35.62 36.29 36.44 V 49.08 51.75 52.61 52.37 52.90 52.75 53.05 53.02 Table 21: SAR (steps=10) on VLCS. SAR (steps=10) Domain-wise data stream Random data stream C →L→ → S→ → V 1 → → 2→ → 3→ → 4→ Current domain N/A 35.32 34.10 51.66 43.56 42.05 42.53 41.16 Budgets N/A N/A N/A N/A N/A N/A N/A N/A C 100.00 83.96 83.04 82.12 84.03 84.24 85.23 85.09 L 33.55 34.07 35.92 41.49 39.53 38.37 37.65 37.58 S 41.10 31.93 34.89 33.94 35.19 32.94 33.88 33.12 V 49.08 51.33 51.51 53.08 52.78 52.34 51.78 52.01 Table 22: SimATTA (B ≤300) on VLCS. SimATTA (B ≤300) Domain-wise data stream Random data stream C →L→ → S→ → V 1 → → 2→ → 3→ → 4→ Current domain N/A 62.61 65.08 74.38 62.33 69.33 73.20 71.93 Budgets N/A 79 175 272 71 135 208 262 C 100.00 99.51 98.52 99.93 99.86 99.79 100.00 99.93 L 33.55 68.11 69.92 69.50 62.61 66.64 68.45 69.43 S 41.10 55.24 68.89 66.67 65.54 69.29 71.79 72.46 V 49.08 66.08 70.94 77.34 73.79 76.87 78.82 80.39 Table 23: SimATTA (B ≤500) on VLCS. SimATTA (B ≤500) Domain-wise data stream Random data stream C →L→ → S→ → V 1 → → 2→ → 3→ → 4→ Current domain N/A 63.52 68.01 76.13 62.29 70.45 73.50 72.02 Budgets N/A 113 266 446 107 203 283 356 C 100.00 99.29 98.59 99.51 99.93 99.86 99.86 99.43 L 33.55 62.95 70.63 70.56 66.57 67.09 67.24 70.29 S 41.10 51.31 73.83 73.10 65.33 71.79 72.91 72.55 V 49.08 59.36 71.65 78.35 73.58 77.84 80.01 80.18 46Published as a conference paper at ICLR 2024 Table 24: Tent (steps=1) on Office-Home. Tent (steps=1) Domain-wise data stream Random data stream R →A→ → C→ → P 1 → → 2→ → 3→ → 4→ Current domain N/A 49.61 39.31 63.87 49.95 50.27 50.23 52.06 Budgets N/A N/A N/A N/A N/A N/A N/A N/A R 96.44 92.33 92.36 92.47 92.38 92.45 92.45 92.40 A 57.07 49.73 49.73 49.57 49.69 49.73 49.57 49.24 C 44.97 39.27 39.54 39.89 39.45 39.68 39.73 39.68 P 73.15 63.60 63.66 63.89 63.60 63.82 63.93 63.98 Table 25: Tent (steps=10) on Office-Home. Tent (steps=10) Domain-wise data stream Random data stream R →A→ → C→ → P 1 → → 2→ → 3→ → 4→ Current domain N/A 49.61 39.04 61.41 50.05 49.31 48.74 47.79 Budgets N/A N/A N/A N/A N/A N/A N/A N/A R 96.44 91.99 89.14 87.08 92.08 90.80 88.59 85.31 A 57.07 49.94 46.77 44.79 49.44 48.21 45.69 42.85 C 44.97 38.58 39.11 38.37 40.18 40.02 38.63 37.89 P 73.15 63.28 61.03 60.49 64.36 63.64 61.12 58.71 Table 26: EATA on Office-Home. EATA Domain-wise data stream Random data stream R →A→ → C→ → P 1 → → 2→ → 3→ → 4→ Current domain N/A 49.65 39.04 63.53 49.73 50.27 49.45 51.07 Budgets N/A N/A N/A N/A N/A N/A N/A N/A R 96.44 92.36 92.17 91.60 92.38 92.22 91.71 91.05 A 57.07 49.57 49.53 49.61 49.69 49.40 49.36 49.11 C 44.97 39.08 39.01 38.65 39.27 39.01 38.42 38.26 P 73.15 63.42 63.42 63.48 63.51 63.37 63.33 62.99 Table 27: CoTTA on Office-Home. CoTTA Domain-wise data stream Random data stream R →A→ → C→ → P 1 → → 2→ → 3→ → 4→ Current domain N/A 49.61 38.76 61.84 49.84 49.84 48.95 50.43 Budgets N/A N/A N/A N/A N/A N/A N/A N/A R 96.44 90.38 88.02 87.81 90.48 89.37 88.00 86.99 A 57.07 48.58 45.53 44.95 47.34 46.35 44.62 43.68 C 44.97 36.66 35.58 35.92 37.55 36.40 35.44 34.73 P 73.15 60.40 57.74 59.04 61.12 59.63 58.35 57.56 Table 28: SAR (steps=1) on Office-Home. SAR (steps=1) Domain-wise data stream Random data stream R →A→ → C→ → P 1 → → 2→ → 3→ → 4→ Current domain N/A 49.65 39.24 63.53 49.84 50.05 49.91 51.67 Budgets N/A N/A N/A N/A N/A N/A N/A N/A R 96.44 92.38 92.31 92.45 92.40 92.36 92.36 92.38 A 57.07 49.65 49.57 49.73 49.69 49.61 49.57 49.57 C 44.97 39.34 39.22 39.36 39.34 39.56 39.47 39.50 P 73.15 63.51 63.51 63.69 63.60 63.71 63.71 63.87 47Published as a conference paper at ICLR 2024 Table 29: SAR (steps=10) on Office-Home. SAR (steps=10) Domain-wise data stream Random data stream R →A→ → C→ → P 1 → → 2→ → 3→ → 4→ Current domain N/A 49.53 38.81 61.50 50.09 50.30 49.77 49.22 Budgets N/A N/A N/A N/A N/A N/A N/A N/A R 96.44 92.20 92.06 88.94 92.40 92.47 91.53 89.14 A 57.07 49.40 49.77 46.15 49.81 50.02 48.91 46.23 C 44.97 39.20 38.63 37.04 39.50 39.29 38.65 36.31 P 73.15 63.53 62.69 59.41 64.18 64.18 62.83 59.45 Table 30: SimATTA (B ≤300) on Office-Home. SimATTA (B ≤300) Domain-wise data stream Random data stream R →A→ → C→ → P 1 → → 2→ → 3→ → 4→ Current domain N/A 56.20 48.38 71.66 58.57 60.88 62.91 63.67 Budgets N/A 75 187 277 79 147 216 278 R 96.44 95.43 95.43 95.75 95.91 95.96 96.01 95.89 A 57.07 57.56 59.50 60.07 58.34 59.91 61.15 62.01 C 44.97 42.25 52.46 52.62 51.66 52.30 54.75 54.98 P 73.15 68.84 70.13 74.70 72.45 73.10 74.50 74.70 Table 31: SimATTA (B ≤500) on Office-Home. SimATTA (B ≤500) Domain-wise data stream Random data stream R →A→ → C→ → P 1 → → 2→ → 3→ → 4→ Current domain N/A 58.71 51.11 74.36 58.85 62.63 63.41 64.31 Budgets N/A 107 284 440 126 248 361 467 R 96.44 95.69 95.71 96.03 96.26 96.19 95.87 95.91 A 57.07 61.43 61.43 62.05 58.18 61.15 61.52 63.78 C 44.97 46.41 57.73 57.41 53.17 55.14 56.79 57.87 P 73.15 70.74 71.98 76.98 73.51 74.18 75.78 77.09 48Published as a conference paper at ICLR 2024 in deep learning heavily relies on large-scale data. Consequently, two promising paths emerge: establishing credible assumptions and leveraging additional information. Firstly, developing credible assumptions can lead to comprehensive comparisons across various stud- ies. Given that theoretical guarantees highlight the inherent differences between methods primarily based on the application limits of their assumptions, comparing these assumptions becomes critical. Without such comparative studies, empirical evaluations may lack precise guidance and explanation. Secondly, while we acknowledge the value of real-world data (observations), discussions surrounding the use of extra information remain pertinent. Considerations include the strategies to acquire this supplementary information and the nature of the additional data needed. Despite the myriad of works on domain generalization, domain adaptation, and test-time adaptation, a comprehensive survey or benchmark encapsulating the aforementioned comparisons remains an unmet need. Moreover, potential future directions for out-of-distribution generalization extend beyond domain generalization and test-time adaptation. One promising avenue is bridging the gap between causal inference and deep learning, for instance, through causal representation learning. In conclusion, our hope is that this work not only offers a novel practical setting and algorithm but also illuminates meaningful future directions and research methodologies that can benefit the broader scientific community. 49",
      "meta_data": {
        "arxiv_id": "2404.05094v1",
        "authors": [
          "Shurui Gui",
          "Xiner Li",
          "Shuiwang Ji"
        ],
        "published_date": "2024-04-07T22:31:34Z",
        "pdf_url": "https://arxiv.org/pdf/2404.05094v1.pdf"
      },
      "llm_extracted_info": {
        "main_contributions": "The paper introduces Active Test-Time Adaptation (ATTA), a novel problem setting that integrates active learning within the fully Test-Time Adaptation (TTA) framework to address significant distribution shifts for streaming test data. The main contributions include a learning theory analysis demonstrating that incorporating limited labeled test instances enhances overall performance with theoretical guarantees, and the proposal of SimATTA, a simple yet effective ATTA algorithm. SimATTA utilizes real-time sample selection techniques, including sample entropy balancing, to avoid catastrophic forgetting. Extensive experiments confirm the theoretical analyses, showing substantial performance improvements over TTA methods, comparable effectiveness to Active Domain Adaptation (ADA) methods, and efficiency similar to TTA.",
        "methodology": "The proposed ATTA framework formally defines the problem of optimizing a pre-trained model on streaming, continuously shifting unlabeled test data by actively selecting and labeling a limited number of informative instances. The core methodology of SimATTA involves partitioning incoming unlabeled test samples into high-entropy and low-entropy sets. Low-entropy samples are pseudo-labeled by the frozen source-pretrained model to form a 'source-like' dataset, primarily to mitigate catastrophic forgetting. High-entropy samples are selected for active labeling (by an oracle) to address distribution shifts, using a novel incremental clustering technique. This clustering method, based on weighted K-means, stores representative samples (anchors) from seen distributions, adapting to new distributions by adding new clusters and merging old ones when exceeding a cluster budget. The model is then fine-tuned using both the pseudo-labeled low-entropy data and the actively labeled high-entropy anchors, balancing their respective training weights and sample numbers as guided by theoretical analyses.",
        "experimental_setup": "The evaluation of ATTA and SimATTA involves comparisons with three settings: standard TTA, enhanced TTA (TTA baselines fine-tuned with random labeled samples), and Active Domain Adaptation (ADA). Datasets used include PACS, VLCS, Office-Home, and Tiny-ImageNet-C, covering various domain shifts and scales. Source domains were designated (e.g., 'photos' for PACS, 'Caltech101' for VLCS). Two data stream strategies were employed: domain-wise (processing one target domain completely before the next) and random (shuffling samples from all target domains into splits). Baselines for TTA included BN w/o adapt, BN w/ adapt, Tent, EATA, CoTTA, and SAR. ADA baselines included Random, Entropy, K-means, and CLUE. Models used were ResNet-18 (PACS, VLCS, Tiny-ImageNet-C) and ResNet-50 (Office-Home), initialized with ImageNet pre-trained weights. Optimization used Adam for source pre-training and SGD for test-time adaptation, with specific learning rates. SimATTA's label budget was limited (e.g., B ≤ 300 or B ≤ 500 for most datasets, around 4,000 for Tiny-ImageNet-C). Ablation studies were conducted on PACS and VLCS to evaluate the individual contributions of incremental clustering and low-entropy sample training.",
        "limitations": "The current approach relies on the quality of the pre-trained model for selective entropy minimization, meaning training on incorrectly pseudo-labeled low-entropy samples might reinforce errors. It might not be cost-effective to expend annotation budgets on low-entropy samples. The theoretical bounds established can be loose given small batch sizes, though fine-tuning pre-trained models is empirically argued to behave like learning with a smaller effective VC-dimension. The paper focuses on foundational aspects and major challenges of ATTA, thus not covering various potential applications, scaling up for very large models/datasets (e.g., LLMs), or extensive task-specific configurations.",
        "future_research_directions": "Potential future research directions include developing alternative methods to prevent catastrophic forgetting in ATTA scenarios, especially exploring more cost-effective strategies for annotation budgets on low-entropy samples, such as correcting incorrect pseudo-labels. Further exploration into scaling the ATTA setting up for large models and datasets, including large language models (LLMs), is also suggested. Additionally, bridging the gap between causal inference and deep learning, possibly through causal representation learning, is identified as a promising avenue for out-of-distribution generalization beyond existing domain generalization and test-time adaptation methods."
      }
    },
    {
      "title": "Test Time Adaptation via Conjugate Pseudo-labels",
      "abstract": "Test-time adaptation (TTA) refers to adapting neural networks to distribution\nshifts, with access to only the unlabeled test samples from the new domain at\ntest-time. Prior TTA methods optimize over unsupervised objectives such as the\nentropy of model predictions in TENT [Wang et al., 2021], but it is unclear\nwhat exactly makes a good TTA loss. In this paper, we start by presenting a\nsurprising phenomenon: if we attempt to meta-learn the best possible TTA loss\nover a wide class of functions, then we recover a function that is remarkably\nsimilar to (a temperature-scaled version of) the softmax-entropy employed by\nTENT. This only holds, however, if the classifier we are adapting is trained\nvia cross-entropy; if trained via squared loss, a different best TTA loss\nemerges. To explain this phenomenon, we analyze TTA through the lens of the\ntraining losses's convex conjugate. We show that under natural conditions, this\n(unsupervised) conjugate function can be viewed as a good local approximation\nto the original supervised loss and indeed, it recovers the best losses found\nby meta-learning. This leads to a generic recipe that can be used to find a\ngood TTA loss for any given supervised training loss function of a general\nclass. Empirically, our approach consistently dominates other baselines over a\nwide range of benchmarks. Our approach is particularly of interest when applied\nto classifiers trained with novel loss functions, e.g., the recently-proposed\nPolyLoss, where it differs substantially from (and outperforms) an\nentropy-based loss. Further, we show that our approach can also be interpreted\nas a kind of self-training using a very specific soft label, which we refer to\nas the conjugate pseudolabel. Overall, our method provides a broad framework\nfor better understanding and improving test-time adaptation. Code is available\nat https://github.com/locuslab/tta_conjugate.",
      "full_text": "Test-Time Adaptation via Conjugate Pseudo-labels Sachin Goyal⋆1 Mingjie Sun⋆1 Aditi Raghunathan1 Zico Kolter1,2 1Carnegie Mellon University, 2Bosch Center for AI {sachingo, mingjies, raditi, zkolter}@cs.cmu.edu Abstract Test-time adaptation (TTA) refers to adapting neural networks to distribution shifts, with access to only the unlabeled test samples from the new domain at test-time. Prior TTA methods optimize over unsupervised objectives such as the entropy of model predictions in TENT [50], but it is unclear what exactly makes a good TTA loss. In this paper, we start by presenting a surprising phenomenon: if we attempt to meta-learn the “best” possible TTA loss over a wide class of functions, then we recover a function that isremarkably similar to (a temperature-scaled version of) the softmax-entropy employed by TENT. This only holds, however, if the classiﬁer we are adapting is trained via cross-entropy loss; if the classiﬁer is trained via squared loss, a different “best” TTA loss emerges. To explain this phenomenon, we analyze test-time adaptation through the lens of the training losses’sconvex conjugate. We show that under natural conditions, this (unsupervised) conjugate function can be viewed as a good local approximation to the original supervised loss and indeed, it recovers the “best” losses found by meta-learning. This leads to a generic recipe that can be used to ﬁnd a good TTA loss for any given supervised training loss function of a general class. Empirically, our approach consistently dominates other TTA alternatives over a wide range of domain adaptation benchmarks. Our approach is particularly of interest when applied to classiﬁers trained with novel loss functions, e.g., the recently-proposed PolyLoss [25] function, where it differs substantially from (and outperforms) an entropy-based loss. Further, we show that our conjugate based approach can also be interpreted as a kind of self-training using a very speciﬁc soft label, which we refer to as the conjugate pseudo-label. Overall, our method provides a broad framework for better understanding and improving test-time adaptation. Code is available at https://github.com/locuslab/ tta_conjugate. 1 Introduction Modern deep networks perform exceeding well on new test inputs that are close to the training distribution. However, this performance dramatically decreases on test inputs drawn from a different distribution. While there is a large body of work on improving the robustness of models, most robust training methods are highly specialized to the setting they cater to. For e.g., they assume pre-speciﬁed perturbations, subpopulations, and spurious correlations, or access to unlabeled data from the target distribution, and most methods offer close to no improvement on general distribution shifts beyond what they were trained for [12, 21]. In practice, it is often cumbersome (or even impossible) to precisely characterize all possible distri- bution shifts a model could encounter and then train accordingly. Instead, a model already trained on some source data must be able to adapt at test-time to new inputs from a different domain. This setting of test-time adaptation (TTA) has gained interest in recent years [ 6, 47, 50, 54]. TTA is typically accomplished by updating the source model parameters via a few steps of optimization on an unsupervised objective involving the new test sample from the target distribution. The choice ⋆ Equal Contribution 36th Conference on Neural Information Processing Systems (NeurIPS 2022). arXiv:2207.09640v2  [cs.LG]  23 Nov 2022of this unsupervised objective, which we call the TTA loss, dictates the success of the adaptation procedure. [47] uses a self-supervised objective on the test sample, [50] uses the entropy of model predictions, and several follow-ups have proposed variants or alternatives [ 40, 54]. However, it remains unclear as to how to choose or guide the selection of this TTA loss, and thus far the choice of these losses has remained largely heuristic in nature. In this work, we begin by presenting a set of intriguing experiments where we attempt to learn the “best” TTA loss for a given source classiﬁer and distribution shift. We parameterize the TTA loss by another neural network whose parameters are learnt via meta-learning [ 3, 9] where we differentiate through the adaptation process to ﬁnd the TTA loss that achieves the best adaptation on distribution shifts. Surprisingly, we ultimately learn a TTA loss that looksremarkably similar to (a temperature-scaled version of) the softmax-entropy loss, which was already proposed by [50]. Why did we recover the commonly used softmax-entropy loss despite the fact that the procedure is capable of learning a very general class of losses and the meta-learning process could potentially specialize to both the source classiﬁer and the distribution shift of interest? Furthermore, we ﬁnd that this pattern only holds when the loss used to train the source classiﬁer is cross-entropy loss; when a different loss such as squared loss is used instead, the meta-learning procedure recovers a TTA loss that itself looks more like a negative squared error, and is very different from the softmax-entropy loss (Section 3). In order to explain this phenomenon, we propose to consider TTA through the lens of the convex conjugate function. Speciﬁcally, given a hypothesis function h(x) and label y, several common losses (cross-entropy and the squared loss amongst them, but not limited to these) can be written in the form L(h(x),y) = f(h(x)) −yTh(x) for some function f. In these cases, we show that “natural” TTA loss for such classiﬁers is precisely the (negation of) the convex conjugate evaluated at the gradient of h, LTTA(x) = −f∗(∇f(h(x)), where f∗is the convex conjugate of f. This framework not only recovers the results of our meta-learning experiments, but also justiﬁes why some speciﬁc choices of TTA loss in the previous literature work well (e.g., this framework recovers TENT’s choice of softmax-entropy for cross-entropy-trained classiﬁer). Moreover, it also provides a broad framework for what the TTA loss should be when the source model is trained using various different loss functions (for example the recently-proposed PolyLoss [25, 29]) as is becoming increasingly common in machine learning. Further, we show that our proposed conjugate adaptation loss is in fact a kind of self-training with pseudo-labels [42], a classic approach in machine learning. Various formulations of the pseudo-label have been proposed in the literature, and our conjugate analysis provides a general recipe for the “correct” choice of soft pseudo-labels given byˆy(x) = ∇f(h(x)). We thus refer to these as conjugate pseudo-labels (Conjugate PL’s), and believe our work provides a broad framework for understanding adaptation with unlabeled data in general. Finally, we empirically verify the effectiveness of our proposed conjugate adaptation loss across several datasets and training losses, such as cross-entropy and squared loss, along with the recently- proposed PolyLoss [ 25] (which itself has shown higher standard test accuracy on a wide range of vision tasks). Over all models, datasets and training losses, we ﬁnd our proposed conjugate pseudo-labeling consistently outperforms prior TTA losses and improves TTA performance over the current state of the art. 2 Background and preliminaries. Test-time adaptation. We are interested in mapping an input x∈Rd to a label y∈Y. We learn a model hθ : Rd ↦→R|Y|parameterized by θthat maps an input xto predictions hθ(x). We assume access to a trained source model and adapt at test-time over the test input, before making the ﬁnal prediction. This is the standard test-time adaptation (TTA) setting [47, 50]. During TTA, we update the model parameters on an unsupervised objective L(x,hθ). For example, in TENT [50], this loss is the entropy of the softmax-normalized predictions of the model. At each time step of adaptation, we observe a batch of test inputs and we take a gradient step towards optimizing the TTA loss on this test batch. As is standard, we measure the average online performance of models across all steps (number of test batch inputs seen) in the adaptation process. Meta learning the loss function. In order to explore the existence of different TTA losses, we employ the meta-learning procedure where we attempt to learn the TTA loss. We use a similar procedure as prior work on meta-learning loss functions [3, 37] and parameterize the loss function via a neural network mφ : R|Y| ↦→R that takes in the model predictions/logits and outputs a loss value. We want to learn parameter φsuch that when we update θvia the loss function mφ, our ﬁnal 2performance is optimal. In order to do so, let xbe the unlabeled test samples to adapt to, and ybe the corresponding labels. We update θand φalternatively as follows. θt+1 ←θt −α∂mφt(hθt(x)) ∂θt , φt+1 ←φt −β∂L(hθt+1 (x′),y′) ∂φt , (1) where Lis some supervised surrogate loss function such as cross-entropy. Please refer to Appendix A3 for further details regarding meta-learning setup. Note that the meta-learning process above assumes access to labels yof test inputs. In this paper, we do not propose meta-learning the TTA loss as an approach. Rather, we use meta-learning to explore what the “best” TTA losses look like. We discuss our ﬁndings from this exploration in the next section. 3 Test-time Adaptation via Meta-Learnt Losses The objective used in TENT is the softmax-entropy of the model predictions which essentially makes the classiﬁer more conﬁdent in its current predictions. The same can be achieved by various other loss formulations such as those mentioned in [40]. With so many possible choices for the loss function, what should we use for TTA? In this section, we attempt to answer this empirically and present some intriguing observations. (a)  (b) Figure 1: Visualization of meta loss (blue) by varying one input prediction score. (a) For cross-entropy loss trained model, the learnt meta loss can be approximated with a scaled softmax-entropy function (dashed red). (b) When the source model is trained with a squared loss for classiﬁcation, the learnt meta loss (blue) can be ﬁtted closely with a quadratic function (dashed red), shown in Figure 1b. The range (max/min) of the prediction score (logit) in x-axis is chosen to cover the empirical range of the predicted logits. Experiment 1. We learn the TTA loss parameterized by a neural network via meta-learning as described in Section 2. Our source classiﬁer is a ResNet-26 trained on CIFAR-10 and we adapt to distribution shifts in CIFAR-10-C. We use the 4 labeled validation noises in CIFAR-10-C to learn the meta-loss network parameters and we denote the resulting learnt loss function by meta-TTA loss. We then adapt the source classiﬁer to the test set of 15 corruptions by optimizing the meta-TTA loss. Observations. First, we ﬁnd that TTA using meta-TTA loss performs better than TENT (12.35% vs 13.14%), suggesting that there are better TTA losses than previous losses based on softmax-entropy. However, on examining this meta-TTA loss, we ﬁnd a surprising observation. Figure 1a (blue curve) visualizes the learnt meta-loss over model predictions as we vary a single class prediction with the rest ﬁxed. Qualitatively, the learnt meta-loss looks very similar to softmax-entropy in one dimension. In fact, we can ﬁt it closely with a scaled softmax-entropy function (dashed red curve): α·H(softmax(hθ(x)/T)), where αis a magnitude parameter and T is a temperature scaler. We want to test if the meta-loss is basically learning the softmax-entropy function. Hence, we perform test-time adaptation with the ﬁtted softmax-entropy function instead (dashed red curve) and achieve an error of 12.32%, essentially recovering the performance of meta-TTA. 3Despite the ability to represent many different loss functions and potentially specialize to the CIFAR- 10-C setting, the meta-loss procedure gave back the standard entropy objective.Do we always recover a loss that looks like softmax-entropy? Experiment 2. In an attempt to isolate when we get back the entropy objective, we vary several things. We tried different architectures for the source classiﬁer, different lossesLduring the meta- learning process (1) and different training losses for the source classiﬁer. Results. We observed that we consistently recovered the temperature scaled softmax-entropy function in all cases except when we varied the training loss for the source classiﬁer (Appendix A.10). On using the squared loss function [18], a strikingly different meta-TTA loss emerges. Figure 1b (blue curve) shows the learnt meta-loss (13.48% error) for this network. Here again, the meta-TTA loss outperforms entropy (14.57%) but it is not simply due to a scaling factor. The loss now looks like the negative squared error (red curve). Like previously, we tried ﬁtting a quadratic loss directly to the meta loss in Figure 1b, and this time we even slightly outperformed the meta-TTA loss. To summarize, we used a meta-learning procedure to search for the “best” TTA loss, where the loss itself was parameterized by a neural network that could potentially represent arbitrarily complex loss functions. However, we ended up with loss functions displaying remarkable structure: across different architectures and different variants of meta-learning, for a classiﬁer trained with cross-entropy, the meta-TTA loss was temperature scaled softmax-entropy and for a classiﬁer trained with squared loss, the meta-TTA loss was a negative squared loss. This is interesting from both a practical and conceptual standpoint where the “best” TTA loss depends on the loss used to train the source classiﬁer in a clean fashion. We attempt to understand and explain this phenomenon in the next section. 4 Conjugate Pseudo Labels Results in the previous section raise an obvious question: why does softmax-entropy as used in TENT seem to be the “best” possible test time adaptation loss for classiﬁers trained via cross-entropy (at least, best in the sense that meta-learning consistently recovers something which essentially mimics softmax-entropy, even though meta-loss is parameterized by a neural network and hence could learn much more complex functions speciﬁc to the model and the particular shift)? And why, alternatively, does a quadratic TTA loss seem to perform best when the classiﬁer is trained via squared loss? In this section, we offer an explanation of this phenomenon via the construct of the convex conjugate function [1]. As we will see, our method recovers softmax-entropy and quadratic loss as the “natural” objectives for classiﬁers trained via cross-entropy and squared loss respectively. Furthermore, for classiﬁers trained via other loss functions, as is becoming increasingly common in deep learning, our approach naturally suggests corresponding test-time adaptation losses, which we show in the next section to comparatively outperform alternatives. Thus, we argue that our framework overall provides a compelling recipe for specifying the “correct” method for TTA for a large class of possible losses. 4.1 Losses and the convex conjugate We begin by formally considering loss functions between a hypothesis outputhθ(x) (e.g., the logit outputs of a classiﬁer, or the direct prediction of a regressor) and targetythat take the following form L(hθ(x),y) = f(hθ(x)) −yThθ(x) (2) for some function f; when there is no risk of confusion, we will use hin place of hθ(x) for simplicity of notation. While not every loss can be expressed in such a form, this captures a wide variety of common losses (possibly scaled by a constant value). For example, cross-entropy loss corresponds to the choice f(h) = log ∑ iexp(hi) and where y denotes a one-hot encoding of the class label; similarly, squared loss corresponds to the choice f(h) = 1 2 ∥h∥2 2. When training an over-parameterized classiﬁer, we can roughly view the training process as (approxi- mately) attaining the minimum over hypotheses hfor each training example min θ 1 t t∑ i=1 L(hθ(xi),yi) ≈1 t t∑ i=1 min h L(h,yi) (3) 4where t is the number of training samples. However, in the case of losses in the form (2), the minimization over hin this form represents a very speciﬁc and well-known optimization problem: it is known as the convex conjugate [1] of the function f min h L(h,y) = min h {f(h) −yTh}= −f⋆(y) (4) where f⋆ denotes the convex conjugate of f. f⋆ is a convex function in y(and indeed, is convex regardless of whether or not f is convex). Furthermore, for the case that f is convex differentiable, the optimality condition of this minimization problem is given by ∇f(hopt) = y, so we also have that f⋆(y) = f⋆(∇f(hopt)) (5) where hopt refers to the optimal classiﬁer (used interchangeably with hθopt ). Putting this all together, we can state (admittedly, in a rather informal manner) that under the assumption that θopt is chosen so as to approximately minimize the empirical loss on the source data in the over-parameterized setting, we have that for tinputs 1 t t∑ i=1 L(hθopt (xi),yi) ≈1 t t∑ i=1 −f⋆(∇f(hθopt (xi))) (6) i.e., the empirical loss can be approximated by the (negative) conjugate applied to the gradient of the f, at least in a region close to the optimal θopt that minimizes the empirical loss. But the later expression has the notable beneﬁt that it does not require any label yi in order to compute the loss, and thus can be used as a basis for TTA on target domain of the hypothesis function hθopt . Deﬁnition 1 (conjugate adaptation loss) Consider a loss function that takes the form given in 2, used for training a hypothesis hθ in the over-parameterized regime. We deﬁne the conjugate adaptation loss Lconj(hθ(x)) : R|Y|↦→R as follows. Lconj(hθ(x)) = −f⋆(∇f(hθ(x))) = f(hθ(x)) −∇f(hθ(x))⊤hθ(x). (7) 4.2 Recovery of existing test-time adaptation strategies Cross-entropy The interesting aspect to this formalism is that when applied to classiﬁers trained with cross-entropy, it recovers exactly the TENT approach to TTA : minimizing the softmax-entropy of hθ(x). And indeed, this loss was also recovered when using meta-learning to learn the “optimal” test-time adaptation loss. To see this, note that for cross-entropy, we have thatf(h) = log ∑ iexp(hi), giving the optimality condition y= ∇f(hopt) = exp(hopt)∑ iexp(hopt i ) and the conjugate function f⋆(y) = { ∑ iyilog yi if ∑ iyi = 1 ∞ otherwise . (8) In other words, Lconj(hθ(x)) = −f⋆(∇f(hθ(x))) = − ∑ i exp(hi)∑ jexp(hj) log exp(hi)∑ jexp(hj) (9) i.e. softmax-entropy of the model prediction, which is exactly the TTA loss that TENT uses. Squared loss For the squared loss, we have thatf(h) = 1 2 ∥h∥2 2, leading to the optimality condition y = hand conjugate function f⋆(y) = 1 2 ∥y∥2 2. Hence, the adaptation loss in this case would be simply given by Lconj(hθ(x)) = −f⋆(∇f(hθ(x))) = −1 2 ∥h∥2 2 which is also what we observed in the meta-learning experiments discussed in Section 3. 4.3 Conjugate pseudo-labels We now emphasize that by the nature of our approximations, there is an additional simple interpre- tation of the conjugate loss: it is also equal to the original loss (2) applied to the “psuedo-labels” ˜yCPL θ (x) = ∇f(hθ(x)), where CPL refers to conjugate pseudo-labels, i.e., Lconj(hθ(x)) = −f⋆(∇f(hθ(x))) = f(hθ(x)) −∇f(hθ(x))Thθ(x) = L(hθ(x),∇f(hθ(x))). (10) 5This property is known as the Fenchel-Young inequality, that isf(x) + f⋆(u) ≥xTuholding with equality when u = ∇f(x). In other words, our conjugate adaptation loss is precisely equivalent to self-training under the speciﬁc soft pseudo-labels given by ˜yCPL = ∇f(hθ(x)). And indeed, for many cases, this may be a more convenient form to compute than explicitly computing the conjugate function at all. For this reason, we refer to our method as that of conjugate pseudo-labels. In the case of cross-entropy loss, this approach then corresponds exactly to self-training using labels given by the softmax applied to the current hypothesis. We must emphasize, however, that while our conjugate formulation indeed has this “simple” form for the case of cross-entropy loss, the real advantage comes in that it provides the “correct”pseudo-label for use with other losses, which may result in pseudo-labels different from the “common” softmax operation. Example: conjugate pseudo-labels for PolyLoss. PolyLoss [25] is a recently-proposed simple alternative to cross-entropy loss than has been shown to improve performance across a wide variety of compute tasks. This loss is given by the form Lpoly(hθ(x),y) = Lce(hθ(x),y) + ϵ·yT(1 −softmax(hθ(x))) (11) We note that this can be put exactly into our conjugate form (equation 2) by writing the loss in a slightly more involved fashion, which we refer to as the expanded conjugate form Lpoly(hθ(x),y) = f(hθ(x)) −yTg(hθ(x)). (12) where f is the log-sum-exp function as before, and g(h) = h−ϵ(1 −softmax(h)). In order to formally put this into the form of the previous loss function (equation 2), we can simply deﬁne an alternative hypothesis as the function h′ θ(x) = g(hθ(x)), and then deﬁne PolyLoss in the conjugate form as Lpoly(h′ θ(x),y) = f(g−1(h′ θ(x))) −yTh′ θ(x). (13) Typically, however, it is easier to simply operate on the expanded conjugate form, which yields the optimality condition for the pseudo-label ∇f(hopt) = Dg(hopt)˜yCPL θ (x), where D is the Jacobian operator. For the case of PolyLoss, this leads to the conjugate pseudo-label of the following form: ˜yCPL θ (x) = (I+ ϵdiag(z) −ϵzzT)−1z, z ≡softmax(hθ(x)). Test-time adaptation. Finally, we note that the above discussion doesn’t actually address any topics related to test-time adaptation to OOD data, but merely provides a generic characterization of a self- training procedure for generic loss functions of the form(2). However, the application toTTA on OOD data is fairly straightforward: as long as the learnt source parameters θis a reasonable approximation to the true optimal θopt on the shifted domain, self-training with the conjugate pseudo-labels provides a reasonable proxy for ﬁne-tuning the network on the true OOD loss. We emphasize that, common to most approaches for TTA , there are still some amount of design decisions that must be put in place; these are detailed in Section 5.1. In practice, we observe OOD generalization typically beneﬁts (across all baselines) from an additional “temperature” scaling, i.e., applying the TTA loss to hθ(x)/T for some ﬁxed temperature T, although it requires a held-out validation dataset for tuningT. However, we should emphasize that truly unsupervisedTTA would require making an informed guess for the value of these hyper-parameters. The full procedure for test time adaptation via conjugate pseudo-labels is shown in Algorithm 1. Algorithm 1 Conjugate pseudo-labeling (Conjugate PL) Input: Source classiﬁer θ0 trained using loss L(hθ(x),y) = f(hθ(x)) −hθ(x)⊤y. N batches of test data Dtest = [x1,x2,...,x N] Hyperparams: learning rate ηand temperature T. Let ¯hθ(x) def = hθ(x)/T be the temperature scaled predictor. Let ˜yCPL θ (x) denote the conjugate pseudo-label function ˜yCPL θ (x) = ∇(f(¯hθ(x))). for n= 0,1,...N −1 do θn+1 = θn −η∇L ( ¯hθ(xn),˜yCPL θ (xn) ) [Self-training with conjugate pseudo-labels] 65 Experiments In this section, we empirically evaluate the effectiveness and generality of the proposed conjugate pseudo-labeling procedure (Algorithm 1) for test-time adaptation on a variety of datasets. 5.1 Setup Datasets. We evaluate on the three common corruption benchmarks: adapting a classiﬁer trained on CIFAR-10 to CIFAR-10-C, CIFAR-100 to CIFAR-100-C and ImageNet to ImageNet-C [ 15]. Following the previous works [47, 50], we report the error averaged across corruptions at the highest severity for CIFAR-10/100-C and averaged across corruptions and severity level for ImageNet-C. We also evaluate on three domain adaptation datasets: adapting a classiﬁer trained on SVHN to MNIST, an ImageNet classiﬁer to ImageNet-R [16] and adapting from synthetic to real data in VISDA-C [38]. Models and Training losses. Following previous works on TTA[47, 50], we use ResNet-26 [14] as the source classiﬁer architecture for CIFAR-10/100 experiments, ResNet-18 for SVHN to MNIST and a ResNet-50 for ImageNet and source synthetic data on VisDA-C. We consider source classiﬁers trained via the following loss functions: the de-facto cross-entropy, recently proposed polyloss [25] and squared loss [18]. Baselines. Our proposed conjugate pseudo-label is the classic approach of self-training with a speciﬁc form of pseudo-labels. In self-training, we replace the label ywith a pseudo-label ˜y(x) and adapt by optimizing the loss function L(hθ(x),˜y(x)). Note that we could either instantaneously update the pseudo-labels using the current classiﬁer, or generate pseudo-labels once with just the source classiﬁer. Instantaneous updates have been shown to work better for domain adaptation [7, 40], and we perform instantaneous updates for all methods. While we propose using ˜yCPL(x) = ∇f(hθ(x)) (See Section 4.3), we compare to the standard pseudo-labels used in the literature: • (i) the “hard” pseudo-label (hard PL) where ˜y(x) = arg maxi ( hθ(x) ) i is the most likely class as predicted by hθ. As is common in the self-training literature, we perform conﬁdence thresholding. • (ii) The “soft” pseudo-label (soft PL) where ˜y(x) is obtained by applying a softmax function to the model predictions hθ(x). We also compare with the following recently proposed test-time adaptation methods. • Entropy Minimization (ENT) [50] minimizes the entropy of model predictions. • Robust Pseudo-Label [40] where we minimize a robust classiﬁcation loss, Lrpl = q−1(1 −p(i|x)q) where i= argmaxjp(j|x) and q∈[0,1]. • MEMO [54] minimizes entropy of a model’s outputs across different augmentations of a test input. We implement a batch version, where we see multiple test points at once, for fair comparisons. TTA methodology. Following [ 50] and [40], we ﬁne-tune by updating the learnable scale and shift parameters of the batch normalization layers across all adaptation losses. For each batch, batch normalization statistics is also updated, as suggested in [41]. We report performance at the end of one round of test-time adaptation over the entire test set. We tune the learning rate (LR) and temperature (T) on the validation noises in the corruption benchmark by grid-search. LR is selected from {1e−1,1e−2,... 1e−4}and T from {1,2 ... 5}. All the experiments have been performed on A6000 GPU’s. On domain adaptation benchmarks, where there is no held-out target domain, we set T to be 1 and use the LR suggested by [ 6, 50]. We use the same hyperparameter tuning protocol across all methods. We single out temperature as a very important hyperparameter, as we discuss in the results below. 5.2 Results on classiﬁers trained with cross-entropy We study the effectiveness of our proposed conjugate pseudo-labels when the source classiﬁer is trained via cross-entropy loss. In this case, baselines Softmax PL and ENT are the same as Conjugate PL. Thus we omit them in our results. Table 1, reports the performance of various TTA methods. When the source classiﬁer is trained via cross-entropy, our conjugate pseudo-label algorithm exactly corresponds to entropy minimization with an additional temperature scaling. Entropy minimization as 7Dataset Temperature (T) Hard PL Robust PL MEMO Conjugate PL (ENT) CIFAR-10-C \u0017 13.95 (±0.06) 13.97 ( ±0.04) 12.60(±0.04) 13.07 (±0.05) \u0013 13.95 (±0.06) 12.85 ( ±0.04) 12.51(±0.01) 12.51(±0.03) CIFAR-100-C \u0017 45.22 (±0.4) 39.80 ( ±0.18) 38.52(±0.16) 41.15 (±0.25) \u0013 45.22 (±0.4) 36.37 ( ±0.10) 37.38 ( ±0.06) 36.10(±0.07) ImageNet-C \u0017 45.43(±0.05) 45.68 ( ±0.01) 48.91( ±0.03) 45.82(±0.01) \u0013 45.43 (±0.05) 45.61 ( ±0.01) 48.91( ±0.04) 45.36(±0.01) Table 1: Mean errors when adapting to corruptions using a source classiﬁer trained via cross- entropy loss. Here, conjugate pseudo-labeling becomes softmax-entropy minimization. With the right temperature scaling, softmax-entropy minimization matches or outperforms other approaches. Prior reported gains of other methods over softmax-entropy minimization disappear when we use temperature scaling. For additional context, the source classiﬁer errors without adaptation are: CIFAR-10-C (29.54%), CIFAR-100-C (62.26%), ImageNet-C (61.89%) proposed in prior work [50] does not tune the temperature parameter, and some newer objectives such as robust PL or MEMO outperform vanilla entropy minimization. For example, on CIFAR-100-C, vanilla ENT obtaines 41.15% average error, while robust PL improves this to39.80% and MEMO to 38.52%. However, with the right temperature scaling, entropy minimization obtains 36.10% error which outperforms the newer objectives (with and without temperature scaling). A similar observation holds for CIFAR-10-C and ImageNet-C as well. Essentially, the gains over vanilla entropy minimization vanish when we do temperature scaling, and entropy minimization (i.e. conjugate pseudo-labeling corresponding to cross-entropy) turns out to be the best objective after all. 5.3 Results on classiﬁers trained with polyloss and squared loss In the case of cross-entropy, conjugate pseudo-labeling reduces to the familiar notion of entropy minimization. We now explore the performance of our method on different loss functions where the conjugate pseudo-labels differ substantially from entropy minimization (section 4.3). Table 2 presents the results on the corruption benchmarks and Table 3 presents the results on the other domain adaptation datasets for source classiﬁers trained with PolyLoss. Dataset T Hard PL Robust PL ENT MEMO Softmax PL Conjugate PL (Ours) CIFAR-10-C \u0017 13.81(±0.12) 14.23(±0.02) 13.46(±0.06) 13.23(±0.07) 14.64(±0.11) 13.02(±0.09) \u0013 13.81(±0.12) 12.45(±0.05) 12.23(±0.06) 12.33(±0.04) 12.26(±0.04) 12.08(±0.05) CIFAR-100-C\u0017 40.47(±0.05) 42.86(±0.11) 40.12(±0.08) 39.90(±0.05) 41.00(±0.11) 38.17(±0.17) \u0013 40.47(±0.05) 39.80(±0.08) 38.23(±0.05) 39.23(±0.04) 37.04(±0.06) 36.83(±0.08) ImageNet-C \u0017 45.44(±0.21) 46.27(±0.03) 46.10(±0.03) 48.21(±0.05) 44.63(±0.03) 44.01(±0.01) \u0013 45.44(±0.21) 46.27(±0.03) 45.50(±0.02) 48.21(±0.04) 44.45(±0.03) 44.01(±0.01) Table 2: Mean errors when adapting to corruptions using a source classiﬁer trained via recently proposed Poly-1 Loss [ 25]. Conjugate pseudo-labeling consistently outperforms all previous ap- proaches. For additional context, source classiﬁer errors without adaptation : CIFAR-10-C (30.22%), CIFAR-100-C (63.91%) and ImageNet-C (62.18%). First, we note that, across all datasets in Table 2 and Table 3, our conjugate PL approach outperforms all other TTA losses. With polyloss classiﬁers, entropy minimization is no longer the best method—on CIFAR-100-C, entropy minimization achieves38.23% error while our conjugate PL achieves36.83%. We see similar consistent gains on CIFAR-10-C, ImageNet-C, ImageNet-R and VisDA-C. On digit adaptation tasks from SVHN to MNIST/USPS/MNISTM, where there is a larger shift between source and target, the gains are especially pronounced. Figure 2 compares how the task loss (polyloss ϵ= 6) on the test data decreases as we adapt the model through conjugate PL and other baselines. We use CIFAR-10-C as an example. Observe that our proposed conjugate PL indeed reduces the task loss the most among other baselines. 8Dataset Source Error Hard PL Robust PL EntropySoftmax PL Conjugate PL Ours SVHN→MNIST 28.33 20.21 19.73 14.28 16.54 10.73 SVHN→USPS 31.58 23.32 26.12 23.12 24.07 21.62 SVHN→MNISTM61.69 50.73 51.35 49.33 50.47 47.59 ImageNet-R 64.19 58.52 59.46 58.25 56.62 55.63 VisDA-C 58.13 40.43 45.44 44.11 39.63 38.42 Table 3: Target error when adapting models trained via polyloss on source domains across different domain adaptation bench- marks. Conjugate pseudo-labeling offers consistent and substan- tial gains over previous approaches across three datasets. Figure 2: Task Loss (PolyLoss ϵ= 6) evaluated on CIFAR-10-C test data during test-time adaptation. Furthermore, on CIFAR-10-C and ImageNet-C, we ﬁnd that adapting polyloss classiﬁers via conjugate PL improves the performance over all methods applied to cross-entropy trained source classiﬁers. For e.g., on ImageNet-C, the performance improves from 45.34% to 44.01%. However, this is only true when using the proposed conjugate PL. If we just did softmax-entropy minimization (even with temperature scaling), the ﬁnal adapted performance of a polyloss classiﬁer (45.5%) is in fact worse than that of a cross-entropy classiﬁer (45.34%). Our results suggest that as we develop new training losses that improve the source classiﬁers, it is important to adapt via conjugate pseudo-labeling to reap the maximum gains. Similarly, we experiment with the case when the source classiﬁer is trained using squared loss on the CIFAR-10 and CIFAR-100 datasets, and observe consistent gains using the proposed conjugate pseudo-labels over the baselines. For example, on CIFAR-10-C, TTA using conjugate PL gives and error of 12.87%, outperforming baselines like ENT (13.24%) and Softmax PL (31.81%). Table 5 in Appendix A.7 shows the detailed results. Comparing Table 1 and Table 2, we see that the relative ordering between the various baselines differs. This is further evidence that the adaptation loss has to depend on the training loss, and we believe our conjugate pseudo-label approach captures this appropriately by offering consistent gains across the various settings we experimented with. 6 Related Works Test-time adaptation methods. In recent years, the setting of test-time adaptation has gained a lot of interest with a host of different approaches proposed in the literature. One family of TTA approaches update the source classiﬁer by minimizing an unsupervised loss on the target distribution [4, 6, 20, 22, 35, 36, 40, 43, 44, 50, 51, 54]. TENT [ 50] proposes to minimize the entropy of model predictions at test time. Several follow ups like [ 6, 35, 40, 44, 54] propose alternative TTA objectives, e.g. robust pseudo-labelling [40], likelihood ratio loss [35], entropy of marginal probability averaged across augmentations [54] and self-supervised contrastive losses [6, 49]. However, most of these objectives are heuristically designed or chosen. In this paper, we provide a principled approach of designing unsupervised objectives for TTA . Another family of approaches for test-time adaptation such as [ 2, 8, 13, 31, 34, 47] leverage an auxiliary self-supervised task (e.g. rotation prediction [ 47], masked autoencoders [10]) to update model parameters on each test sample. Crucially, these methods require modifying the source model training by augmenting the supervised training objective with an auxiliary self-supervised loss. Hence it cannot be applied to typical standard classiﬁers that are trained by minimizing a supervised loss on the source data. Source-free domain adaptation. A very related setting to test-time adaptation is source-free domain adaptation, where a trained source classiﬁer must be adapted to a target distribution of interest, although the entire target unlabeled data is available at once. SHOT [28] proposes to optimize the source hypothesis (i.e. feature extractor) with a combination of entropy minimization, diversity and self-training on pseudo-labels on the unlabeled target data. [53] promotes feature clustering on features from target distributions. [24, 26] use generative modeling to estimate the underlying source distributions for enforcing feature invariance. Such approaches typically require multiple epochs over the target data and cannot be easily adopted to work in an online fashion. 9Unsupervised domain adaptation. The most canonical setting of domain adaptation involves access to labeled source data and unlabeled target data, all during training. The availability of source and target data during training lends itself to approaches that “align” the source and target representations in some way: [ 32, 33, 45, 48] match distribution statistics, [ 11] uses a discriminator, [ 46] uses self-supervised learning. However, such approaches require access to source data which might not always be feasible due to data privacy and efﬁciency issues. Pseudo-labels and self-training. Self-training is a classic idea for leveraging unlabeled data, devel- oped ﬁrst for the semi-supervised setting. Self-training generates pseudo-labels on the unlabeled data, allowing us to use any “supervised” loss on this pseudo-labeled data. Self-training has shown promising results in various settings like semi-supervised learning [ 19] and improving adversarial robustness [ 5]. Self-training has also been gaining attention in the setting of unsupervised domain adaptation [28, 39], where pseudo-labels generated on the unlabeled data from target domain is used to supervise the adaptation process. [ 7, 23, 52] provide theoretical insights into how self-training with pseudo-labels can help under distribution shift. TENT [50] (i.e entropy minimization) can be viewed as a form of self-training with instantaneous softmax pseudo-labels. Our work provides a general framework for the choice of soft pseudo-labels based on the conjugate analysis of the source training objective. Some prior works like [7, 17, 27, 30, 55, 56] have documented the improvement in performance when using instantaneous pseudo-labels over pre-computed pseudo-labels, and thus lend further support to the beneﬁts of our proposed conjugate pseudo-labeling approach. The ex- periment results presented in this work supporting conjugate pseudo-labels suggest that conjugate pseudo-labels is a promising direction of pseudo-labeling in a broader context. 7 Conclusion, Limitations and Future Directions In this work, we proposed a general test-time adaptation loss, based on the convex conjugate formulation which in turn was motivated by the intriguing meta learning experiments. The fact that meta-learning recovers the proposed loss hints at some kind of optimality of the loss. In Section 4, we prove that for a broad set of loss functions, the proposed (unsupervised) conjugate loss is close to the oracle supervised loss. However, this still does not completely answer what the optimal test-time adaptation loss is and why. The meta-learning framework in this work was constrained to learn functions over the logits of each individual input. It can be expanded to more involved setups, where we consider functions over the intermediate representations too and also consider learning functions over a batch of input while accounting for their interactions. Beyond the choice of the adaptation loss itself, achieving good test-time adaptation generally involves several heuristics like updating only the batch norm parameters [50]. While our work was motivated by the loss function, via the meta-learning experiments, we discovered that temperature scaling is another important hyper-parameter that improves the performance of all previous baselines as well. At a high level, test-time adaptation has to be appropriately regularized to prevent the updates over batches from taking the model too far: updating only a few batch norm parameters is one way to do that, and perhaps temperature scaling provides a similar beneﬁcial regularization effect by making the network predictions on unlabeled inputs less conﬁdent. Understanding the role of these heuristics more concretely is an interesting direction for future work. It also remains an open problem to understand under what sort of real-world distribution shifts would self-training based approaches would help. Finally, it is also worth extending and applying the conjugate pseudo-labeling to other settings like semi-supervised learning. 8 Acknowledgments We thank Shubhang Bhatnagar and Asher Trockman for helping with running the ImageNet experi- ments. We thank Zhili Feng for useful feedback. Sachin Goyal and Mingjie Sun were supported by funding from the Bosch Center for Artiﬁcial Intelligence. Aditi Raghunathan was supported by an Open Philanthropy AI Fellowship. 10References [1] https://en.wikipedia.org/wiki/Convex_conjugate. [2] Pratyay Banerjee, Tejas Gokhale, and Chitta Baral. Self-supervised test-time learning for reading comprehension. In Annual Conference of the North American Chapter of the Association for Computational Linguistics, 2021. [3] Sarah Bechtle, Artem Molchanov, Yevgen Chebotar, Edward Grefenstette, Ludovic Righetti, Gaurav Sukhatme, and Franziska Meier. Meta-learning via learned loss. arXiv preprint arXiv:1906.05374, 2019. [4] Malik Boudiaf, Romain Mueller, Ismail Ben Ayed, and Luca Bertinetto. Parameter-free online test-time adaptation. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), 2022. [5] Yair Carmon, Aditi Raghunathan, Ludwig Schmidt, John C Duchi, and Percy S Liang. Un- labeled data improves adversarial robustness. In H. Wallach, H. Larochelle, A. Beygelzimer, F. d'Alché-Buc, E. Fox, and R. Garnett, editors, Advances in Neural Information Processing Systems, volume 32. Curran Associates, Inc., 2019. URL https://proceedings.neurips. cc/paper/2019/file/32e0bd1497aa43e02a42f47d9d6515ad-Paper.pdf. [6] Dian Chen, Dequan Wang, Trevor Darrell, and Sayna Ebrahimi. Contrastive test-time adaptation. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), 2022. [7] Yining Chen, Colin Wei, Ananya Kumar, and Tengyu Ma. Self-training avoids using spurious features under domain shift. In Advances in Neural Information Processing Systems, 2020. [8] Mohammad Zalbagi Darestani, Jiayu Liu, and Reinhard Heckel. Test-time training can close the natural distribution shift performance gap in deep learning based compressed sensing. In Proceedings of the 39th International Conference on Machine Learning (ICML), 2022. [9] Chelsea Finn, Pieter Abbeel, and Sergey Levine. Model-agnostic meta-learning for fast adap- tation of deep networks. In Proceedings of the 34th International Conference on Machine Learning (ICML), 2017. [10] Yossi Gandelsaman, Yu Sun, Xinlei Chen, and Alexei A. Efros. Test-time training with masked autoencoders. In Advances in Neural Information Processing Systems, 2022. [11] Yaroslav Ganin, Evgeniya Ustinova, Hana Ajakan, Pascal Germain, Hugo Larochelle, François Laviolette, Mario March, and Victor Lempitsky. Domain-adversarial training of neural networks. Journal of Machine Learning Research, 17(59):1–35, 2016. [12] Ishaan Gulrajani and David Lopez-Paz. In search of lost domain generalization. InInternational Conference on Learning Representations, 2021. [13] Nicklas Hansen, Rishabh Jangir, Yu Sun, Guillem Alenya, Pieter Abbeel, Alexei A. Efros, Lerrel Pinto, and Xiaolong Wang. Self-supervised policy adaptation during deployment. In International Conference on Learning Representations, 2021. [14] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image recognition. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), 2016. [15] Dan Hendrycks and Thomas Dietterich. Benchmarking neural network robustness to common corruptions and perturbations. In International Conference on Learning Representations, 2019. [16] Dan Hendrycks, Steven Basart, Norman Mu, Saurav Kadavath, Frank Wang, Evan Dorundo, Rahul Desai, Tyler Zhu, Samyak Parajuli, Mike Guo, Dawn Song, Jacob Steinhardt, and Justin Gilmer. The many faces of robustness: A critical analysis of out-of-distribution generalization. In In IEEE/CVF International Conference on Computer Vision (ICCV), 2021. [17] Yosuke Higuchi, Niko Moritz, Jonathan Le Roux, and Takaaki Hori. Advancing momentum pseudo-labeling with conformer and initialization strategy. In IEEE International Conference on Acoustics, Speech and Signal Processing, 2022. 11[18] Like Hui and Mikhail Belkin. Evaluation of neural architectures trained with square loss vs cross-entropy in classiﬁcation tasks. In International Conference on Learning Representations, 2021. [19] Dong hyun Lee. Pseudo-label: The simple and efﬁcient semi-supervised learning method for deep neural networks. [20] Yusuke Iwasawa and Yutaka Matsuo. Test-time classiﬁer adjustment module for model-agnostic domain generalization. In Advances in Neural Information Processing Systems, 2021. [21] Pang Wei Koh, Shiori Sagawa, Henrik Marklund, Sang Michael Xie, Marvin Zhang, Akshay Balsubramani, Weihua Hu, Michihiro Yasunaga, Richard Lanas Phillips, Irena Gao, Tony Lee, Etienne David, Ian Stavness, Wei Guo, Berton A. Earnshaw, Imran S. Haque, Sara Beery, Jure Leskovec, Anshul Kundaje, Emma Pierson, Sergey Levine, Chelsea Finn, and Percy Liang. Wilds: A benchmark of in-the-wild distribution shifts. In Proceedings of the 38th International Conference on Machine Learning (ICML), 2021. [22] Takeshi Kojima, Yutaka Matsuo, and Yusuke Iwasawa. Robustifying vision transformer without retraining from scratch by test-time class-conditional feature alignment. In International Joint Conference on Artiﬁcial Intelligence, 2022. [23] Ananya Kumar, Tengyu Ma, and Percy Liang. Understanding self-training for gradual domain adaptation. In Proceedings of the 37 th International Conference on Machine Learning (ICML), 2020. [24] Vinod K Kurmi, Venkatesh K Subramanian, and Vinay P Namboodiri. Domain impression: A source data free domain adaptation method. In IEEE Winter Conference on Applications of Computer Vision (WACV), 2021. [25] Zhaoqi Leng, Mingxing Tan, Chenxi Liu, Ekin Dogus Cubuk, Jay Shi, Shuyang Cheng, and Dragomir Anguelov. Polyloss: A polynomial expansion perspective of classiﬁcation loss functions. In International Conference on Learning Representations, 2022. [26] Rui Li, Qianfen Jiao, Wenming Cao, Hau-San Wong, and Si Wu. Model adaptation: Unsuper- vised domain adaptation without source data. In IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), 2020. [27] Xinzhe Li, Qianru Sun, Yaoyao Liu, Qin Zhou, Shibao Zheng, Tat-Seng Chua, and Bernt Schiele. Learning to self-train for semi-supervised few-shot classiﬁcation. In Advances in Neural Information Processing Systems, 2019. [28] Jian Liang, Dapeng Hu, and Jiashi Feng. Do we really need to access the source data? source hypothesis transfer for unsupervised domain adaptation. InProceedings of the 37th International Conference on Machine Learning (ICML), 2020. [29] Tsung-Yi Lin, Priya Goyal, Ross Girshick, Kaiming He, and Piotr Dollár. Focal loss for dense object detection. In IEEE/CVF International Conference on Computer Vision (ICCV), 2017. [30] Hong Liu, Jianmin Wang, and Mingsheng Long. Cycle self-training for domain adaptation. In Advances in Neural Information Processing Systems, 2021. [31] Yuejiang Liu, Parth Kothari, Bastien van Delft, Baptiste Bellot-Gurlet, Taylor Mordan, and Alexandre Alahi. Ttt++: When does self-supervised test-time training fail or thrive? In Advances in Neural Information Processing Systems, 2021. [32] Mingsheng Long, Jianmin Wang, Guiguang Ding, Jiaguang Sun, and Philip S. Yu. Transfer feature learning with joint distribution adaptation. In IEEE/CVF International Conference on Computer Vision (ICCV), 2013. [33] Mingsheng Long, Yue Cao, Jianmin Wang, and Michael I. Jordan. Learning transferable features with deep adaptation networks. In Proceedings of the 32nd International Conference on Machine Learning, 2015. [34] Xuan Luo, Jia-Bin Huang, Richard Szeliski, Kevin Matzen, and Johannes Kopf. Consistent video depth estimation. In SIGGRAPH, 2020. 12[35] Chaithanya Kumar Mummadi, Robin Hutmacher, Kilian Rambach, Evgeny Levinkov, Thomas Brox, and Jan Hendrik Metzen. Test-Time Adaptation to Distribution Shift by Conﬁdence Maximization and Input Transformation. arXiv preprint arXiv: 2106.14999, 2021. [36] Shuaicheng Niu, Jiaxiang Wu, Yifan Zhang, Yaofo Chen, Shijian Zheng, Peilin Zhao, and Mingkui Tan. Efﬁcient test-time model adaptation without forgetting. In Proceedings of the 39th International Conference on Machine Learning (ICML), 2022. [37] Junhyuk Oh, Matteo Hessel, Wojciech M. Czarnecki, Zhongwen Xu, Hado P van Hasselt, Satinder Singh, and David Silver. Discovering reinforcement learning algorithms. In Advances in Neural Information Processing Systems, 2020. [38] Xingchao Peng, Ben Usman, Neela Kaushik, Judy Hoffman, Dequan Wang, and Kate Saenko. Visda: The visual domain adaptation challenge, 2017. [39] Viraj Prabhu, Shivam Khare, Deeksha Kartik, and Judy Hoffman. Sentry: Selective entropy optimization via committee consistency for unsupervised domain adaptation. In Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV), 2021. [40] Evgenia Rusak, Steffen Schneider, George Pachitariu, Luisa Eck, Peter Vincent Gehler, Oliver Bringmann, Wieland Brendel, and Matthias Bethge. If your data distribution shifts, use self- learning, 2022. URL https://openreview.net/forum?id=1oEvY1a67c1. [41] Steffen Schneider, Evgenia Rusak, Luisa Eck, Oliver Bringmann, Wieland Brendel, and Matthias Bethge. Improving robustness against common corruptions by covariate shift adaptation. In Advances in Neural Information Processing Systems, 2020. [42] H. Scudder. Probability of error of some adaptive pattern-recognition machines. IEEE Transac- tions on Information Theory, 1965. [43] Manli Shu, Weili Nie, De-An Huang, Zhiding Yu, Tom Goldstein, Anima Anandkumar, and Chaowei Xiao. Test-time prompt tuning for zero-shot generalization in vision-language models. In Advances in Neural Information Processing Systems, 2022. [44] Prabhu Teja Sivaprasad and François Fleuret. Test time adaptation through perturbation robust- ness. arXiv preprint arXiv: 2110.10232, 2021. [45] Baochen Sun, Jiashi Feng, and Kate Saenko. Correlation alignment for unsupervised domain adaptation. arXiv preprint arXiv: 1612.01939, 2016. [46] Yu Sun, Eric Tzeng, Trevor Darrell, and Alexei A. Efros. Unsupervised domain adaptation through self-supervision. arXiv preprint arXiv:1909.11825, 2019. [47] Yu Sun, Xiaolong Wang, Zhuang Liu, John Miller, Alexei A. Efros, and Moritz Hardt. Test-time training with self-supervision for generalization under distribution shifts. In Proceedings of the 36th International Conference on Machine Learning (ICML), 2019. [48] Eric Tzeng, Judy Hoffman, Ning Zhang, Kate Saenko, and Trevor Darrell. Deep domain confusion: Maximizing for domain invariance. arXiv preprint arXiv:1412.3474, 2014. [49] Dequan Wang, Shaoteng Liu, Sayna Ebrahimi, Evan Shelhamer, and Trevor Darrell. On-target adaptation. arXiv preprint arXiv: 2109.01087, 2021. [50] Dequan Wang, Evan Shelhamer, Shaoteng Liu, Bruno Olshausen, and Trevor Darrell. Tent: Fully test-time adaptation by entropy minimization. In International Conference on Learning Representations, 2021. [51] Qin Wang, Olga Fink, Luc Van Gool, and Dengxin Dai. Continual test-time domain adaptation. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), 2022. [52] Sang Michael Xie, Ananya Kumar, Robbie Jones, Fereshte Khani, Tengyu Ma, and Percy Liang. In-n-out: Pre-training and self-training using auxiliary information for out-of-distribution robustness. In International Conference on Learning Representations, 2021. 13[53] Shiqi Yang, Yaxing Wang, Joost van de Weijer, Luis Herranz, and Shangling Jui. Generalized source-free domain adaptation. In Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV), 2021. [54] Marvin Zhang, Sergey Levine, and Chelsea Finn. Memo: Test time robustness via adaptation and augmentation. In Advances in Neural Information Processing Systems, 2022. [55] Yang Zou, Zhiding Yu, B. V . K. Vijaya Kumar, and Jinsong Wang. Domain adaptation for semantic segmentation via class-balanced self-training. European Conference on Computer Vision, 2018. [56] Yang Zou, Zhiding Yu, Xiaofeng Liu, B. V . K. Vijaya Kumar, and Jinsong Wang. Conﬁdence regularized self-training. In IEEE/CVF International Conference on Computer Vision (ICCV), 2019. 14A Appendix A.1 Conjugate Derivations Cross-Entropy Loss : L(h,y) = − c∑ i=1 yilog exp(hi)∑c j=1 exp(hj) = − c∑ i=1 yi ∗hi + log c∑ j=1 exp(hj) = f(h) −y⊤h, (14) where f(h) is log ∑c j=1 exp(hj) and the constraint that ∑c i=1 yi = 1. Now, the conjugate f⋆(y) is given by : f⋆(y) = −min h {f(h) −yTh}= −min h {log c∑ j=1 exp(hj) −yTh} (15) with the constraint ∑c i=1 yi = 1. At the optimality, yi = (∇f(h))i = exp(hi)∑ jexp(hj) (16) Then, f⋆(y) = −log c∑ j=1 exp(hj) + c∑ i=1 hi exp(hi)∑ jexp(hj) = ∑ i exp(hi)∑ jexp(hj) log exp(hi)∑ jexp(hj), (17) if the constraint ∑c i=1 yi = 1 is satisﬁed, otherwise f⋆(y) = ∞by duality. This in turn gives, the conjugate loss for cross-entropy (when the constraint is satisﬁed) : Lconj(h) = −f⋆(y) = −f⋆(∇f(h)) = − ∑ i exp(hi)∑ jexp(hj) log exp(hi)∑ jexp(hj) (18) Squared Loss : L(h,y) = 1 2||h−y||2 2 ≈1 2||h||2 2 −y⊤h [ignoring the constant term] = f(h) −y⊤h, (19) Now, the conjugate f⋆(y) is given by: f⋆(y) = −min h {f(h) −yTh}= −min h {1 2||h||2 2 −yTh} = −1 2||h||2 2 (20) A.2 Experiments on Binary Classiﬁcation with Exponential Loss Here we present the results on a binary classiﬁcation task over a synthetic dataset of 100 dimensional gaussian clusters. 15Dataset Creation For the binary classiﬁcation task, we create a synthetic dataset similar to [23]. Speciﬁcally, let the data X ∼ N(µ,Σ) ∈ R100 and labels Y ∈ {−1,+1}. We sample µ ∼ N(k,I100). For Σ, similar to [ 23], we sample a diagonal matrix D, where each entry is sampled uniformly from a speciﬁed range, and a rotation matrix U from a HAAR distribution, giving Σ = UDUT. For the source data, we sample µ−1 s ,µ+1 s ,Σ−1 s ,Σ+1 s as speciﬁed above with k= 0. Now to create a distribution shifted data of various severity, we sampleµ−1 t ,µ+1 t ,Σ−1 t ,Σ+1 t as speciﬁed above with k= 1, which are then used to sample the shifted data as follows : µ1 λ = λµ1 t + (1 −λ)µ1 s µ−1 λ = λµ−1 t + (1 −λ)µ−1 s Σ1 λ = λΣ1 t + (1 −λ)Σ1 s Σ−1 λ = λΣ−1 t + (1 −λ)Σ−1 s Xλ ∼N(µλ,Σλ) In the following experiments, easy shift refers to λ= 0.6, moderate shift to λ= 0.65 and hard shift to λ= 0.7. Exponential Loss for Binary Classiﬁcation Let zbe the classiﬁcation score hθ(x). For logistic training loss, conjugate adaptation loss would default to entropy with sigmoid probability. Thus, here we experiment with a different but also commonly used surrogate loss to 0/1 loss: exponential loss, which is deﬁned as: Lexp(z,y) = exp(−yz) (21) where y∈{−1,+1}. It can be rewritten in the expanded conjugate form of: Lexp(z,y) = 1 2 · ( ez + e−z) −1 2 ·y· ( ez −e−z) (22) For exponential loss, the conjugate pseudo-label function and the conjugate pseudo-label loss are: yCPL exp (z) = ez −e−z ez + e−z, LCPL exp (z) = 2 ez + e−z (23) The model is adapted on shifted gaussian clusters and we compare the conjugate loss with two baseline approaches: 1) Hard pseudo-labelling exp(−yhard pl ·z); 2) Entropy applied to sigmoid probability P(y= +1) = σ(z). The losses are compared on three degrees of shift (easy, moderate and hard), which is controlled by the drifted distance of Gaussian clusters. The results are shown in Figure 3, where we plot the accuracy curve with respect to adaptation iterations. With easy and moderate shift, conjugate loss (green) generalizes faster to shifted test data; with hard shift, only conjugate loss improves model accuracy on shifted test data while entropy (blue) deteriorates model performance. Figure 3: Test-time adaptation result on synthetic data with three shift levels ranging from easy, moderate and hard (detailed in section A.2). The source model is a linear classiﬁer trained with exponential loss Lexp = e−yhθ(x). Adaptation with the conjugate loss generalizes better compared to baseline losses. 16A.3 Meta Learning Experiment Details In section 3 we talked about learning the meta-loss function parameterized by a neural network mφ : R|Y|↦→R, that takes in the model predictions/logits and outputs a loss value. Here we discuss the architecture chosen and the implementation details. Further, in Appendix A.4 we empirically show that the learnt meta-loss is not affected by the choice of task loss / surrogate loss used in meta learning (Lin Equation 1). Note that the task loss / surrogate loss function is used to update the meta-loss mφ during meta-learning. The surrogate loss is calculated on updated source model’s predictions on labeled samples from test domain. The surrogate loss tries to update the meta-loss in the outer loop such that when meta-loss is later used to update the source model in the inner loop, the source model generalizes better to the test domain. Architecture and Implementation Details Figure 4 gives an overall schema for meta-learning the loss function and algorithm 2 gives the pseudo-code for meta-learning the loss function. Below we describe this in further detail. We use a transformer (denoted by T) with a MLP (denoted by P) over the output of transformer as the architecture for mφ, i.e. mφ(x) = P(T(x)). Speciﬁcally, for a given source trained model hθ and input x∼Dtest : 1. Let hθ(x) ∈R|Y|be the model predictions/logits, where |Y|denotes the number of classes. 2. Let hj θ(x) ∈R,∀j ∈|Y| be the prediction corresponding to class j. 3. The input to transformer is then given by z ∈R|Y|×(1+e), where zj ∈R1+e,∀j ∈|Y| is the concatenation of hj θ(x) and the learnable positional embedding pej ∈Re. 4. The transformer output is given by w= T(z) ∈Rd, where ddenotes the feed-forward dimension of the transformer. 5. The transformer output wis ﬁnally passed through a MLP to get the meta-loss valuemφ(hθ(x)) = P(w) ∈R 6. The source model is updated by optimizing over the meta-loss. θt+1 ←θt −α∂mφt(hθt(x)) ∂θt (24) 7. The updated source model is then used to update the meta-loss by optimizing over some supervised loss function Ltask. φt+1 ←φt −β∂Ltask(hθt+1 (x′),y′) ∂φt , where (x′,y′) ∼Dtest (25) Note that the last step assumes access to labels of test inputs. In this paper, we do not propose meta-learning the TTA loss as an approach. Rather, we use meta-learning to explore what the “best” TTA losses look like. We select the trasformer input embedding dimension (1 + e) from {16,32,64}and transformer feed-forward dimension dfrom {32,64,128}. The number of transformer layers and the hidden layers in MLP are selected from {1,2}. We use Adam optimizer with a learning rate of 1e−3 for learning the meta-loss (i.e. the transformer + MLP). We train the meta-loss for 100 epochs with a batch size of 200. A.4 Effect of Task Loss in Meta Learning In section 3, we show that the meta losses learned on different source classiﬁers differ substantially if the source classiﬁers are trained using different source loss functions. Here we further empirically verify that the learnt meta loss is not affected by the task loss used in meta learning (Lin Equation 1). Thus the learnt meta loss is determined by the source model. In Figure 5, we show the meta loss learnt on a ResNet-26 trained with Cross Entropy loss for two meta task losses: Cross Entropy Figure 5a and Squared Loss Figure 5b. We plot the meta loss as a function over one of its input prediction scores, while keeping other ﬁxed. We can see that the task loss barely affects the learnt meta loss. Similar observations can be made for the classiﬁer trained with squared loss Figure 6. 17Meta-Loss  Backpropogate  Figure 4: Meta-Loss learning procedure : The model predictions hθt(x) are passed through the parameterized loss function mφt, which outputs a loss value. We optimize φ such that when optimizing the source model over the loss mφt(hθt(x)), the updated θt+1 has a better performance on the test domain. To do this, we take one gradient step over the meta-loss to get the update source model parameters θt+1, and then update φby evaluating θt+1 on the labeled validation data using some task loss Ltask. Algorithm 2 Learning the Meta-Loss Input: Source trained classiﬁer hθ0 . Randomly initialized meta-loss mφ0 . Task loss / Surrogate loss Ltask like cross-entropy or squared loss for meta learning N batches of test data Dtest = [(x1,y1),..., (xN,yN)] Hyperparams: learning rates αand β. for epoch= 0,1,2,... do for n= 0,1,...N −1 do θt+1 ←θt −α ∂mφt(hθt(xn)) ∂θt Sample (xr,yr) ∼Dtest. φt+1 ←φt −β∂Ltask(hθt+1 (xr),yr) ∂φt A.5 Test-Time Adaptation Detail For completeness, we also give the test-time adaptation setup in Algorithm 3. A.6 ImageNet results on each severity level In continuation with results shown in Table 2 in Section 5.3, Table 4 shows the mean errors averaged across the 15 corruption types for each of the severity level on ImageNet-C, for a source classiﬁer trained with PolyLoss (ϵ= 8). A.7 Square Loss Trained Source Classiﬁer In Section 5.3, we brieﬂy discussed that similar to the other source training losses like cross-entropy and polyloss, our proposed conjugate loss outperforms the baselines when the source classiﬁer is 18(a)  (b) Figure 5: Visualizations of meta loss by varying one input dimension (prediction score). The source model is a ResNet-26 trained with Cross Entropy. Here we show meta loss trained by two different task losses: Cross Entropy Figure 5a and Squared Loss Figure 5b. (a)  (b) Figure 6: Visualizations of meta loss by varying one input dimension (prediction score). The source model is a ResNet-26 trained with Squared Loss. Here we show meta loss trained by two different task losses: Cross Entropy Figure 6a and Squared Loss Figure 6b. Algorithm 3 Test-Time Adaptation Input: Source classiﬁer θ0 trained using loss L(hθ(x),y), An unsupervised loss function for test-time adaptation Ltta(x), N batches of test data Dtest = [x1,...,x N] Hyperparams: learning rate η. for n= 0,1,...N −1 do θn+1 = θn −η∇Ltta(xn) ˆyn = hθn+1 (xn) [Predictions for the nth batch] 19Corrution Severity Temperature Robust PL Entropy MEMO Softmax PL Conjugate 1 \u0017 34.27 33.17 34.39 32.49 32.26 \u0013 34.27 32.84 34.39 32.70 32.26 2 \u0017 41.25 39.04 40.38 37.78 37.40 \u0013 41.25 38.50 40.38 37.75 37.40 3 \u0017 47.37 44.04 45.67 42.30 41.72 \u0013 47.37 43.33 45.67 42.14 41.72 4 \u0017 56.63 51.88 54.49 49.61 48.84 \u0013 56.63 51.03 54.49 49.39 48.84 5 \u0017 67.11 62.53 66.13 60.94 59.90 \u0013 67.11 61.80 66.13 60.30 59.90 Mean \u0017 49.32 46.13 48.21 44.62 44.02 \u0013 49.32 45.50 48.21 44.45 44.02 Table 4: Mean Errors across the 15 noises for various severity level on the ImageNet-C dataset, with source model trained using Poly-1 Loss. Note that Temperature scaling helped only in the case of Entropy and Softmax PL. trained using a squared loss. Table 5 shows a detailed comparison with the baselines. We note that for the conjugate of squared loss, the temperature scaling can be wrapped into the learning rate as shown in Section 4.2. Further, on the CIFAR-10-C dataset we observe temperature scaling doesn’t help any of the other baselines too, hence we do not include the temperature row in CIFAR-10-C. Dataset Temperature Hard PL Robust PL ENT MEMO Softmax PL Conjugate PL CIFAR-10-C \u0017 13.71 (±0.07) 13.06 (±0.05) 13.24 (±0.02) 13.22 (±0.04) 14.85 (±0.08)12.99(±0.04) CIFAR-100-C \u0017 50.82 (±0.31) 44.53 (±0.13) 43.55 (±0.12) 51.35 (±0.04) 51.99 (±0.03)43.39(±0.11) \u0013 50.82 (±0.31) 43.99 (±0.15)43.21(±0.08) 51.35 (±0.04) 51.99 (±0.03) 43.39 (±0.11) Table 5: Mean Errors on the common corruptions datasets for source classiﬁer trained using squared loss. We note that temperature scaling didn’t help on the CIFAR-10-C dataset. Source Classiﬁer Errors without adaptation : CIFAR-10-C (28.34%), CIFAR-100-C (68.79%) Dataset Temperature (T) Hard PL Robust PL MEMO Conjugate PL (ENT) CIFAR-10-C \u0017 SGD,1e−3, 1 SGD,1 e−3, 1 SGD,1 e−3, 1 SGD, 1e−3, 1 \u0013 SGD,1e−3, 1 SGD,1 e−2, 2 SGD,5 e−3, 3 Adam,1e−3, 2 CIFAR-100-C \u0017 SGD,1e−2, 1 SGD,1 e−2, 1 SGD,5 e−3, 1 SGD, 1e−2, 1 \u0013 SGD,1e−2, 1 SGD,1 e−2, 2 SGD,1 e−2, 2 SGD,1e−2, 2 ImageNet-C \u0017 SGD,1e−2, 1 SGD,2.5 e−3, 1 SGD,1 e−3, 1 SGD,2.5e−3, 1 \u0013 SGD,1e−2, 1 SGD,2.5e−3, 1.5 SGD,1e−3, 1 SGD,2.5e−3, 1.5 Table 6: Hyper-parameters (Optimizer, Learning Rate, Temperature) for the results in Table 1, where we showed the mean errors on the common corruptions dataset for a source classiﬁer trained using cross-entropy loss. A.8 Hyper-Parameters We share the exact hyper-parameters found using gridsearch over the 4 validation noises for the common corruptions dataset. 20Cross Entropy Classiﬁer Experiments In Section 5.2, Table 1 shows the results when adapting a cross entropy trained classiﬁer on various common corruptions dataset. Table 6 gives the optimizer, learning rate and optimal temperature for each of the baseline and our proposed conjugate loss. PolyLoss Classiﬁer Experiments In Section 5.3, Table 2 shows the results when adapting a polyloss trained classiﬁer on various common corruptions dataset. Table 7 gives the optimizer, learning rate and optimal temperature for each of the baseline and our proposed conjugate loss. Dataset T Hard PL Robust PL ENT MEMO Softmax PL Conjugate PL (Ours) CIFAR-10-C\u0017 SGD,1e−3, 1 SGD,1e−3, 1 SGD,1 e−3, 1 SGD,5 e−3, 1 SGD, 1e−3, 1 SGD, 1e−3, 1 \u0013 SGD,1e−3, 1 SGD,1e−2, 3 SGD,1 e−2, 3 SGD,5 e−3, 3 SGD, 1e−3, 2 SGD, 1e−3, 1.5 CIFAR-100-C\u0017 SGD,1e−2, 1 SGD,1e−2, 1 SGD,1 e−2, 1 SGD,1 e−2, 1 SGD, 1e−2, 1 SGD, 1e−2, 1 \u0013 SGD,1e−2, 1 Adam,1e−3, 3 SGD,1 e−2, 2 SGD,1 e−2, 2 SGD, 1e−2, 2.5 SGD, 1e−2, 1.5 ImageNet-C\u0017 SGD,1e−2, 1 SGD,2.5e−3, 1 SGD,2.5e−3, 1 SGD,5e−3, 1 SGD, 2.5e−3, 1 SGD, 2.5e−3, 1 \u0013 SGD,1e−2, 1 SGD,2.5e−3, 1 SGD,2.5e−3, 1.5 SGD,5e−3, 1 SGD, 2.5e−3, 2 SGD, 2.5e−3, 1 Table 7: Hyper-parameters (Optimizer, Learning Rate, Temperature) for the results in Table 2, where we showed the mean errors on the common corruptions dataset for a source classiﬁer trained using poly-loss. Squared Loss Classiﬁer Experiments In Section 5.3, we brieﬂy discussed the results when adapt- ing a squared loss trained classiﬁer on various common corruptions dataset. Table 8 gives the optimizer, learning rate and optimal temperature for each of the baseline and our proposed conjugate loss for the results in Table 5. Digit Adaptation Datasets For the experiments on digits adaptation tasks, we do not have any validation set. Hence, we don’t use temperature scaling here (T = 1) and ﬁx the optimizer and LR as Adam and 1e−2 respectively for all the baselines. A.9 Additional Experiments on Digit Adaptation Datasets Similar to the setting of Table 1, we perform additional experiments on digit adaptation datasets when the source classiﬁer is trained using the cross-entropy loss. Note that when the source classiﬁer is trained using cross-entropy loss, the conjugate loss is equal to the softmax-entropy. In the absence of validation dataset in digit adaptation benchmarks, we used a ﬁxed learning rate of 0.01 for all the baselines, optimizer as Adam and an informed temperature scaling guess of T=2. Table 9 compares softmax-entropy minimization with various baselines. Here, again we observe that on SVHN →MNIST benchmark, without temperature scaling, MEMO (10.67% error) outperforms softmax-entropy (14.41% error). However, similar to the observations in Table 1, with temperature scaling, softmax-entropy minimization (9.26% error) is able to match the performance of MEMO (9.36% error). Further, on the SVHN →USPS benchmark, softmax-entropy (conjugate) and MEMO perform similar even without temperature scaling. A.10 Additional Meta Learning the TTA Loss Experiments In Section 3, we tried to learn a test-time adaptation (TTA) loss via meta-learning for adapting a CIFAR10 trained ResNet26 to distribution shifts on CIFAR10 corruptions. Figure 1 showed that the learnt meta-loss looks like a temperature scaled softmax-entropy. In this section, we show the learnt meta loss across a range of settings as described below : 1. Digit Adaptation: Figure 7a and 7b show the learnt meta-loss when adapting a SVHN trained ResNet26 to MNIST dataset and USPS dataset respectively. We observe that the learnt meta-loss can be well approximated by a temperature scaled softmax-entropy. 2. Various Noise Types: In Figure 8, we show the learnt meta-loss when adapting a ResNet26 trained on CIFAR10 dataset using cross-entropy loss, to various noise types like speckle, gaussian, saturate and spatter. The severity level is kept ﬁxed at the maximum i.e. 5. 21Dataset T Hard PL Robust PL ENT MEMO Softmax PL Conjugate PL (Ours) CIFAR-10-C\u0017 SGD,1e−2, 1 SGD,1 e−2, 1 SGD,1 e−2, 1 SGD,1e−2, 1 SGD,1 e−4, 1 SGD,1e−2, 1 CIFAR-100-C\u0017 Adam,1e−3, 1 Adam,1e−3, 1 Adam,1e−3, 1 Adam,1e−3, 1 Adam, 1e−4, 1 Adam, 1e−3, 1 \u0013 Adam,1e−3, 1 Adam,1e−3, 0.5 Adam,1e−3, 2 Adam,1e−3, 2 Adam, 1e−4, 2.5 Adam, 1e−3, 1 Table 8: Hyper-parameters (Optimizer, Learning Rate, Temperature) for the results in Table 5, where we showed the mean errors on the common corruptions dataset for a source classiﬁer trained using squared loss. Dataset Temperature (T) Hard PL Robust PL MEMO Conjugate PL (ENT) SVHN→MNIST \u0017 21.54 27.44 10.67 14.41 \u0013 21.54 13.26 9.36 9.26 SVHN→USPS \u0017 26.06 26.81 22.72 22.57 \u0013 26.06 22.32 22.42 22.27 Table 9: Mean errors when adapting to digit adaptation benchmarks using a source classiﬁer trained via cross-entropy loss. Here, conjugate pseudo-labeling becomes softmax-entropy minimization. Again we observe that with the right temperature scaling, softmax-entropy minimization matches other approaches. For additional context, the source classiﬁer errors without adaptation are: SVHN →MNIST (34.17%), SVHN →USPS (31.84%). 20  10  0 10 20 prediction score 5 0 5 10loss value meta loss (error 10.44%) softmax entropy (error 14.41) fitted entropy (error 9.26) Meta Loss for SVHN -> MNIST (a) 20  10  0 10 20 prediction score 6 4 2 0 2 4 6 8 loss value meta loss (error 20.13%) softmax entropy (error 22.57) fitted entropy (error 22.22) Meta Loss for SVHN -> USPS adpatation (b) Figure 7: Visualizations of the learnt meta-loss by varying one input dimension (prediction score). The source model is a ResNet-26 trained with cross-entropy on the SVHN dataset. (a) The learnt meta-loss when adapting to the MNIST test dataset. (b) The learnt meta-loss when adapting to the USPS test dataset. 3. Various Severity Levels: In Figure 9, we vary the severity level of the noise, keeping the noise type ﬁxed. 4. Dataset and Architecture: In Figure 10, we compare the learnt meta-loss when adapting to speckle noise, for different source classiﬁer architectures (ResNet26 and ResNet50) and different source training dataset (CIFAR10 and CIFAR100). In all the cases, we again observe that the learnt meta-loss can be well approximated by a temperature scaled softmax-entropy. 5. Squared Loss : Finally, in Figure 11 we show the learnt meta-loss for classiﬁers trained with squared loss function instead of cross-entropy. We observe that in this case, the learnt meta loss mimics a quadratic function as expected from the conjugate formulation. 22For each of the learnt meta losses, we also show the values (α,T,C ) we use to ﬁt the meta loss with softmax entropy function: α·H(softmax(x/T)) −C. Note that although the learnt meta-loss can be approximated by the conjugate, the parameters α,T,C differ across the settings. In the case of classiﬁers trained with squared loss, we ﬁt the meta loss with a quadratic function∑K i=1(A·x2 i + C), where Kis the number of classes and xis the logit vector. Again, we also show the ﬁtted parameter value A,C. The meta loss follows the trend of a quadratic function. The ﬁtted quadratic function performs better or similar as the meta loss, while the parameters of the ﬁtted quadratic function remain different across the meta learning setup (base classiﬁer architectures and noise types). (a)  (b) (c)  (d) Figure 8: Visualization of meta loss (blue) learnt from various noise types in CIFAR-10-C validation set, where base classiﬁers are trained with cross-entropy loss. We show the error of meta loss, softmax entropy and ﬁtted entropy for test-time adaptation on the corresponding noise types. We also show the parameters (α,T,C ) in the ﬁtted entropy. 23(a)  (b) (c)  (d) Figure 9: Visualization of meta loss (blue) learnt on speckle noise with different severity level for CIFAR-10-C, where base classiﬁers are trained with cross-entropy loss. We show the error of meta loss, softmax entropy and ﬁtted entropy for test-time adaptation on the corresponding noise types. We also show the parameters (α,T,C ) in the ﬁtted entropy. 24(a)  (b) (c)  (d) Figure 10: Visualization of meta loss (blue) learnt across datasets (CIFAR-10-C/CIFAR-100-C) and base classiﬁer architectures (ResNet-26/ResNet-50), where base classiﬁers are trained with cross-entropy loss. We show the error of meta loss, softmax entropy and ﬁtted entropy for test-time adaptation on the corresponding noise types. We also show the parameters ( α,T,C ) in the ﬁtted entropy. (a)  (b) Figure 11: Visualization of meta loss (blue), where base classiﬁer is trained with quadratic loss. We show the error of meta loss, softmax entropy and ﬁtted quadratic function for test-time adaptation on the corresponding noise types. We also show the parameters ( A,B,C ) in the ﬁtted quadratic function. 25",
      "meta_data": {
        "arxiv_id": "2207.09640v2",
        "authors": [
          "Sachin Goyal",
          "Mingjie Sun",
          "Aditi Raghunathan",
          "Zico Kolter"
        ],
        "published_date": "2022-07-20T04:02:19Z",
        "pdf_url": "https://arxiv.org/pdf/2207.09640v2.pdf"
      },
      "llm_extracted_info": {
        "main_contributions": "The paper addresses the problem of designing effective test-time adaptation (TTA) losses for neural networks facing distribution shifts with only unlabeled test samples. It presents the surprising finding that meta-learning the \"best\" TTA loss recovers a temperature-scaled softmax-entropy for cross-entropy trained classifiers, but a negative squared error for squared-loss trained classifiers. The core contribution is a principled explanation for this phenomenon through the lens of the convex conjugate function of the supervised training loss. This analysis leads to a generic recipe for deriving a suitable TTA loss for a broad class of training losses, interpreted as self-training with \"conjugate pseudo-labels\". Empirically, the proposed conjugate pseudo-labeling consistently outperforms prior TTA alternatives across various domain adaptation benchmarks, especially for classifiers trained with novel losses like PolyLoss, providing a broader framework for understanding and improving TTA.",
        "methodology": "The methodology involves two main parts: an empirical exploration using meta-learning and a theoretical framework based on convex conjugates. For exploration, TTA loss is parameterized by a neural network and learned via meta-learning, differentiating through the adaptation process to find the loss achieving the best performance on distribution shifts. The core method proposes the conjugate adaptation loss, Lconj(hθ(x)) = f(hθ(x)) −∇f(hθ(x))⊤hθ(x), derived from the convex conjugate (f⋆) of the source training loss L(hθ(x),y) = f(hθ(x)) −y⊤hθ(x). This loss is shown to recover softmax-entropy for cross-entropy and negative squared error for squared loss. The approach is further interpreted as a self-training mechanism using specific soft pseudo-labels called \"conjugate pseudo-labels\" (˜yCPL θ (x) = ∇f(hθ(x))). The adaptation process involves updating the model parameters (specifically, scale and shift parameters of batch normalization layers) by optimizing this conjugate pseudo-label loss on temperature-scaled predictions.",
        "experimental_setup": "The evaluation was conducted on three common corruption benchmarks (CIFAR-10-C, CIFAR-100-C, ImageNet-C) and three domain adaptation datasets (SVHN to MNIST, ImageNet-R, VisDA-C). Source classifiers used ResNet-26 for CIFAR, ResNet-18 for SVHN, and ResNet-50 for ImageNet/VisDA-C. Models were trained with cross-entropy, PolyLoss, and squared loss. Baselines included hard pseudo-labeling, soft pseudo-labeling, Entropy Minimization (TENT), Robust Pseudo-Label, and MEMO. TTA involved fine-tuning batch normalization parameters (scale and shift) and updating batch normalization statistics per batch. Performance was reported as average error across corruptions (highest severity for CIFAR, all severities for ImageNet-C). Hyperparameters (learning rate and temperature) were tuned via grid-search on validation noises for corruption benchmarks, while fixed values (T=1) were used for domain adaptation tasks without validation sets.",
        "limitations": "The meta-learning framework used in this work was constrained to learning functions over the logits of individual inputs, which could be expanded to more complex setups involving intermediate representations or batch interactions. The paper does not fully resolve the question of what constitutes the optimal TTA loss and why, despite demonstrating the conjugate loss's proximity to the oracle supervised loss. Achieving good TTA still relies on several heuristics, such as updating only batch norm parameters, and the role of these heuristics (e.g., temperature scaling) requires a more concrete understanding. It remains an open problem to determine under which real-world distribution shifts self-training based approaches are most effective. Finally, truly unsupervised TTA would ideally not require a held-out validation dataset for hyperparameter tuning like temperature scaling.",
        "future_research_directions": "Future research could involve expanding the meta-learning framework to learn TTA loss functions over intermediate representations or a batch of inputs, accounting for their interactions. Another direction is to gain a more concrete understanding of the role of various heuristics currently employed in TTA, such as updating only batch normalization parameters and temperature scaling. Investigating under what types of real-world distribution shifts self-training-based approaches are most beneficial and robust is also a promising area. Additionally, extending and applying the proposed conjugate pseudo-labeling framework to other machine learning settings, such as semi-supervised learning, is suggested."
      }
    },
    {
      "title": "Improved Test-Time Adaptation for Domain Generalization",
      "abstract": "The main challenge in domain generalization (DG) is to handle the\ndistribution shift problem that lies between the training and test data. Recent\nstudies suggest that test-time training (TTT), which adapts the learned model\nwith test data, might be a promising solution to the problem. Generally, a TTT\nstrategy hinges its performance on two main factors: selecting an appropriate\nauxiliary TTT task for updating and identifying reliable parameters to update\nduring the test phase. Both previous arts and our experiments indicate that TTT\nmay not improve but be detrimental to the learned model if those two factors\nare not properly considered. This work addresses those two factors by proposing\nan Improved Test-Time Adaptation (ITTA) method. First, instead of heuristically\ndefining an auxiliary objective, we propose a learnable consistency loss for\nthe TTT task, which contains learnable parameters that can be adjusted toward\nbetter alignment between our TTT task and the main prediction task. Second, we\nintroduce additional adaptive parameters for the trained model, and we suggest\nonly updating the adaptive parameters during the test phase. Through extensive\nexperiments, we show that the proposed two strategies are beneficial for the\nlearned model (see Figure 1), and ITTA could achieve superior performance to\nthe current state-of-the-art methods on several DG benchmarks. Code is\navailable at https://github.com/liangchen527/ITTA.",
      "full_text": "Improved Test-Time Adaptation for Domain Generalization Liang Chen1 Yong Zhang2* Yibing Song3 Ying Shan2 Lingqiao Liu1∗ 1 The University of Adelaide 2 Tencent AI Lab 3 AI3 Institute, Fudan University {liangchen527, zhangyong201303, yibingsong.cv}@gmail.com yingsshan@tencent.com lingqiao.liu@adelaide.edu.au Abstract The main challenge in domain generalization (DG) is to handle the distribution shift problem that lies between the training and test data. Recent studies suggest that test-time training (TTT), which adapts the learned model with test data, might be a promising solution to the problem. Gen- erally, a TTT strategy hinges its performance on two main factors: selecting an appropriate auxiliary TTT task for up- dating and identifying reliable parameters to update during the test phase. Both previous arts and our experiments in- dicate that TTT may not improve but be detrimental to the learned model if those two factors are not properly consid- ered. This work addresses those two factors by proposing an Improved Test-Time Adaptation (ITTA) method. First, in- stead of heuristically defining an auxiliary objective, we pro- pose a learnable consistency loss for the TTT task, which con- tains learnable parameters that can be adjusted toward bet- ter alignment between our TTT task and the main prediction task. Second, we introduce additional adaptive parameters for the trained model, and we suggest only updating the adap- tive parameters during the test phase. Through extensive ex- periments, we show that the proposed two strategies are ben- eficial for the learned model (see Figure 1), and ITTA could achieve superior performance to the current state-of-the-art methods on several DG benchmarks. Code is available at https://github.com/liangchen527/ITTA. 1. Introduction Recent years have witnessed the rapid development of deep learning models, which often assume the training and test data are from the same domain and follow the same distribution. However, this assumption does not always hold in real-world scenarios. Distribution shift among the source and target domains is ubiquitous in related areas [35], such as autonomous driving or object recognition tasks, resulting *Corresponding authors. This work is done when L. Chen is an intern in Tencent AI Lab. 0.5 1.1 0.5 1.2 0.5 0.5 0.5 1.4 0.4 0.4 0.4 0.3 art cartoon photo sketch 79.9 75.4 94.4 75.8 83.3 76.0 94.4 76.7 84.7 78.0 94.5 78.2 Figure 1. Performance improvements from the proposed two strate- gies (i.e. introducing a learnable consistency loss and including additional adaptive parameters to improve TTT) for the baseline model (i.e. ResNet18 [30] with existing augmentation strategy [75]). Experiments are conducted on the PACS dataset [37] with the leave- one-out setting. Following [27], we use 60 sets of random seeds and hyper-parameters for each target domain. The reported average accuracy and error bars verify the effectiveness of our method. in poor performances for delicately designed models and hindering the further application of deep learning techniques. Domain generalization (DG) [2,8,16,23,24,31,38 –40,40, 44, 47, 51, 52, 69], designed to generalize a learned model to unseen target domains, has attracted a great deal of attention in the research community. The problem can be traced back to a decade ago [7], and various approaches have been pro- posed to push the DG boundary ever since. Those efforts in- clude invariant representation learning [28,47,49,58], adver- sarial learning [23,40,44,69], augmentation [9,41,42,66,75], or meta-learning [2, 16, 38, 39]. Despite successes on certain occasions, a recent study [27] shows that, under a rigorous evaluation protocol, most of these arts are inferior to the baseline empirical risk minimization (ERM) method [61]. This finding is not surprising, as most current arts strive to decrease the distribution shift only through the training data while overlooking the contributions from test samples. Recently, the test-time training (TTT) technique [60] has been gaining momentum for easing the distribution shift problem. TTT lies its success in enabling dynamic tuning of the pretrained model with the test samples via an auxil- iary TTT task, which seems to be a promising effort when arXiv:2304.04494v2  [cs.CV]  16 Apr 2023confronting data from different domains. However, TTT is not guaranteed to improve the performance. Previous arts [46, 63] indicate that selecting an appropriate auxiliary TTT task is crucial, and an inappropriate one that does not align with the main loss may deteriorate instead of improv- ing the performance. Meanwhile, it is pointed out in [63] that identifying reliable parameters to update is also essential for generalization, which is in line with our experimental findings in Sec. 5.3. Both of these two tasks are non-trivial, and there are limited efforts made to address them. This paper aims to improve the TTT strategy for better DG. First, different from previous works that empirically define auxiliary objectives and assume they are aligned with the main task, our work does not make such assumptions. Instead, we suggest learning an appropriate auxiliary loss for test-time updating. Specifically, encouraged by recent successes in multi-view consistency learning [13,26,29], we propose to augment the consistency loss by adding learn- able parameters based on the original implementation, where the parameters can be adjusted to assure our TTT task can be more aligned with the main task and are updated by en- forcing the two tasks share the same optimization direction. Second, considering that identifying reliable parameters to update is an everlasting job given the growing size of current deep models, we suggest introducing new adaptive param- eters after each block during the test phase, and we only tune the new parameters by the learned consistency loss while leaving the original parameters unchanged. Through extensive evaluations on the current benchmark [27], we illustrate that the learnable consistency loss performs more effectively than the self-supervised TTT tasks adopted in previous arts [60, 63], and by tuning only the new adaptive parameters, our method is superior to existing strategies that update all the parameters or part of them. This work aims to ease the distribution shift problem by improving TTT, and the main contributions are three-fold: • We introduce a learnable consistency loss for test-time adaptation, which can be enforced to be more aligned with the main loss by tuning its learnable parameters. • We introduce new adaptive parameters for the trained model and only update them during the test phase. • We conduct experiments on various DG benchmarks and illustrate that our ITTA performs competitively against current arts under the rigorous setting [27] for both the multi-source and single-source DG tasks. 2. Related Works 2.1. Domain Generalization. Being able to generalize to new environments while de- ploying is a challenging and practical requirement for cur- rent deep models. Existing DG approaches can be roughly categorized into three types. (1) Invariant representation learning: The pioneering work [5] theoretically proves that if the features remain invariant across different domains, then they are general and transferable to different domains. Guided by this finding, [47] uses maximum mean discrep- ancy (MMD) to align the learned features, and [25] proposes to use a multi-domain reconstruction auto-encoder to obtain invariant features. More recently, [58] suggests maximiz- ing the inner product of gradients from different domains to enforce invariance, and a similar idea is proposed in [52] where these gradients are expected to be similar to their mean values. (2) Optimization algorithms: Among the different optimization techniques adopted in DG, prevail- ing approaches resort to adversarial learning [23, 40, 44, 69] and meta-learning [2, 16, 38, 39]. Adversarial training is often used to enforce the learned features to be agnostic about the domain information. In [23], a domain-adversarial neural network (DANN) is implemented by asking the main- stream feature to maximize the domain classification loss. This idea is also adopted in [44], where adversarial training and an MMD constraint are employed to update an auto- encoder. Meanwhile, the meta-learning technique is used to simulate the distribution shifts between seen and unseen environments [2, 16, 38, 39], and most of these works are developed based on the MAML framework [20]. (3) Aug- mentation: Most augmentation skills applied in the general- ization tasks are operated in the feature level [34, 41, 48, 75] except for [11,66,68] which mix images [68] or its phase [66] to synthesize new data. To enable contrastive learning, we incorporate an existing augmentation strategy [75] in our framework. This method originated from AdaIN [32], which synthesizes new domain information by mixing the statistics of the features. Similar ideas can be found in [42, 48]. 2.2. Test-Time Training and Adaptation Test-Time Training (TTT) is first introduced in [60]. The basic paradigm is to employ a test-time task besides the main task during the training phase and update the pre- trained model using the test data with only the test-time objective before the final prediction step. The idea is empir- ically proved effective [60] and further developed in other related areas [3, 10, 12, 14, 21, 22, 43, 56, 63, 65, 73, 74]. Most current works focus on finding auxiliary tasks for updat- ing during the test phase, and the efforts derive from self- supervion [3, 10, 21, 22, 43, 60], meta-learning [65, 73, 74], information entropy [63], pseudo-labeling [12, 14], to name a few. However, not all empirically selected test-time tasks are effective. A recent study [46] indicates that only when the auxiliary loss aligns with the main loss can TTT improve the trained model. Inspired by that, we propose a learnable consistency loss and enforce alignment between the two ob- jectives. Results show that our strategy can be beneficial for the trained model (see Figure 1).subtract Figure 2. Training process of ITTA. We use x from the source domain as input for the feature extractor fθ(·) to obtain the repre- sentation z and its augmented version z′, where the augmentation skill from [75] is applied. The classifier fϕ(·) and weight subnet- work fw(·) are used to compute the main loss Lmain and learnable consistency loss Lwcont. Please refer to our text for details. Meanwhile, [63] suggests that auxiliary loss is not the only factor that affects the performance. Selecting reliable parameters to update is also crucial within the TTT frame- work. Given the large size of current models, correctly iden- tifying these parameters may require tremendous amounts of effort. To this end, instead of heuristically selecting candi- dates, we propose to include new adaptive parameters for up- dating during the test phase. Experimental results show that the proposed method can obtain comparable performances against existing skills. 3. Methodology In the task of DG, we are often given access to data from S (S ≥ 1) source domains Ds = {D1, D2, ..., DS} and expect a model to make good prediction on unseen target domains Dt = {D1, D2, ..., DT } (T ≥ 1). Our method aims to improve the test-time training (TTT) strategy for better DG. The improvements are two-fold. First, we pro- pose a learnable consistency loss for the TTT task, which could be enforced to align with the main objective by tuning its learnable weights. Second, we suggest including addi- tional adaptive parameters and only updating these adaptive parameters during the test phase. 3.1. A Learnable Consistency Loss for TTT The TTT strategies have shown promising performances when dealing with distribution shift problems [43, 63]. How- ever, their successes are depended on the empirically selected auxiliary TTT tasks, which may deteriorate the performances if chosen improperly. Motivated by the recent successes in multi-view consistency learning [13, 26, 29], we suggest adopting a consistency loss in our TTT task. Note that the naive consistency loss is still not guaranteed to be effective as prior art [46] indicates that only when the auxiliary loss aligns with the main loss, can TTT improves the perfor- mance. To this end, we propose to augment the auxiliary loss with learnable parameters that could be adjusted toward a better alignment between the TTT and main tasks. In our case, we make the adopted consistency loss learnable by introducing a weight subnetwork that allows flexible ways Algorithm 1 Pseudo code of the training phase of ITTA in a PyTorch-like style. # fθ, fϕ, fw: feature extractor, classifier, weight subnetwork # α, 0: weight paramter, all zero tensor # training process for x, yin training loader: # load a minibatch with N samples def forward process(x, y): z, z′ = fθ.forward(x) # computing losses Lmain = CrossEntropyLoss(fϕ.forward(z), y) Lmain+ =CrossEntropyLoss(fϕ.forward(z′), y) Lwcont = MSELoss(fw.forward(z − z′), 0) return Lmain, Lwcont # SGD update: feature extractor and classifier Lmain, Lwcont = forward process(x, y) ([fθ.params, fϕ.params]).zero grad() (Lmain + αLwcont).backward() update( \u0002 fθ.params, fϕ.params \u0003 ) # compute objectives for updating weight subnetwork Lmain, Lwcont = forward process(x, y) Lmain.backward() ˆgmain = fθ.params.grad.clone().normalize() fθ.params.zero grad() Lwcont.backward() ˆgwcont = fθ.params.grad.clone().normalize() # SGD update: weight subnetwork MSELoss(ˆgmain, ˆgwcont).backward() fw.params.zero grad() update(fw.params) to measure the consistency between two views of the same instance. We first introduce the pipeline of our training framework. Given the D dimensional representation z ∈ RD1 and its corresponding augmented version z′ that are obtained from a feature extractor (i.e. {z, z′} = fθ(x), where x is an input image from Ds, and fθ(·) is the feature extractor parame- terized by θ. In our implementation, we use the existing augmentation method [75] to obtain z′ by modifying the intermediate activation in fθ(x). We show in our supplemen- tary material that our framework can also thrive with other augmentation strategies), our learnable consistency loss is given by, Lwcont = ∥fw(z − z′)∥, (1) where ∥ · ∥denotes the L2 norm; fw(·) is the weight sub- network parameterized by w. To make the training process more stable and potentially achieve better performance, we apply a dimension-wise nonlinear function to map each di- mension of z − z′ before calculating the L2 norm. That is, ∀h ∈ RD, fw(h) is implemented by stacking layers of a nonlinear function: ReLU(a ∗ h + b), where a ∈ RD and b ∈ RD are the weight and bias from the nonlinear function, 1We omit the batch dimensions of the variables for simplicity.… … subtract Figure 3. Test adaptation process of ITTA. Different from that in the training stage, we include additional adaptive parameters fΘ after each block of the feature extractor fθ. For each test sample x, the intermediate representations zi and z′i obtained from fi θ are passed to fi Θ before going to the next block fi+1 θ . We use the learnable consistency loss Lwcont as the objective to update fΘ. Please refer to our text for details. and different layers of a, bform the parameter w in fw. In effect, this creates a piecewise-linear mapping function for h: depending on the value of h, the output could be 0, a constant, or a scaling-and-shifted version of h. More studies about the design of fw are provided in our supplementary material. Compared to the naive consistency learning with- out fw, our Lwcont can be more flexible with an adjustable fw, which we show in the following is the key for learning an appropriate loss in the improved TTT framework. Combining Lwcont with the main loss Lmain which applies the cross-entropy loss (CE) for both the origi- nal and augmented inputs ( i.e. Lmain = CE(fϕ(z), y) + CE(fϕ(z′), y), where fϕ is the classifier parameterized by ϕ, and y is the corresponding label), the objective for the feature extractor and classifier can be formulated into, min{θ,ϕ} Lmain + αLwcont, (2) where α is the weight parameter that balances the contri- butions from the two terms. A simple illustration of the workflow is shown in Figure 2. From Eq. (2), the expected gradients for the feature ex- tractor from Lmain and Lwcont can be represented as, \u001a gmain = ∇θ(CE(fϕ(z), y) + CE(fϕ(z′), y)), (3) gwcont = ∇θ∥fw(z − z′)∥. (4) We observe that the direction of gwcont is also determined by the weight subnetwork fw(·), which should be close with gmain to ensure alignment between Lmain and Lwcont [46, 60]. To this end, we propose a straightforward solution by enforcing equality between the normalized versions of gmain and gwcont, and we use this term as the objective for updating fw(·), which gives, min w Lalign, s.t. Lalign = ∥ˆgmain − ˆgwcont∥, (5) where ˆgmain = gmain−Egmain σgmain , and similar for ˆgwcont. In our implementation, we update {θ, ϕ} and w in an alternative manner. Pseudo code of the training process are shown in Algorithm 1. Algorithm 2 Pseudo code of the test phase of ITTA in a PyTorch-like style. # fθ, fϕ: feature extractor, classifier # fw, fΘ: weight subnetwork, additional adaptive blocks # m, 0: total number of blocks in fθ, all zero tensor # test process for x in test loader: # load a test batch def forward process(x): z1, z′1 = f1 Θ.forward((f1 θ .forward(x))) # first blocks for i in range(2, m + 1): # the following m − 1 blocks zi, z′i = fi θ.forward(zi−1), fi θ.forward(z′i−1) zi, z′i = fi Θ.forward(zi), fi Θ.forward(z′i) return zi, z′i # test adaptation phase: SGD update additional adaptive parameters z, z′ = forward process(x) Lwcont = MSELoss(fw.forward(z − z′), 0) fΘ.params.zero grad() Lwcont.backward() update(fΘ.params) # final prediction z, = forward process(x) result = fϕ.forward(z) 3.2. Including Additional Adaptive Parameters Selecting expressive and reliable parameters to update during the test phase is also essential in the TTT frame- work [63]. Some strategies decide to update all the parame- ters from the feature extractor [3, 43], while others use only the parameters from the specific layers for updating [63, 71]. Given the fact that the sizes of current deep models are often very large and still growing, exhaustively trying different combinations among the millions of candidates seems to be an everlasting job. As there are no consensuses on which parameter should be updated, we suggest another easy alter- native in this work. Specifically, assuming there are a total of m blocks in the pretrained feature extractor fθ(·), and the i-th block can be denoted as fi θ(·). Then the intermediate representation zi from fi θ(·) can be formulated as, zi = fi θ(zi−1), s.t. z1 = f1 θ (x). (6) We propose to include additional adaptive blockfΘ that is parameterized by Θ after each block of fθ during the test- time adaptation phase, which reformulates Eq. (6) into, zi = fi Θ(fi θ(zi−1)), s.t. z1 = f1 Θ(f1 θ (x)), (7) where fΘ(·) does not change the dimension and sizes of the intermediate representations. In our work, we use a structure similar to fw to implement fΘ. Note zm is simplified as z in this phase, and the same process is applied for obtaining z′. Then, in the test-time adaptation phase, we suggest only updating the new adaptive parameters via the learned con- sistency loss. The optimization process can be written as,Table 1. Multi sources domain generalization. Experiments are conducted on the DomainBed benchmark [27]. All methods are examined for 60 trials in each unseen domain. Top5 accumulates the number of datasets where a method achieves the top 5 performances. The score here accumulates the numbers of the dataset where a specific art obtains larger accuracy than ERM on account of the variance. Best results are colored as red. Among the 22 methods compared, less than a quarter outperforms ERM in most datasets (Score ≥ 3). PACS VLCS OfficeHome TerraInc DomainNet Avg. Top5↑ Score↑ MMD [40] 81.3 ± 0.8 74.9 ± 0.5 59.9 ± 0.4 42.0 ± 1.0 7.9 ± 6.2 53.2 1 2 RSC [33] 80.5 ± 0.2 75.4 ± 0.3 58.4 ± 0.6 39.4 ± 1.3 27.9 ± 2.0 56.3 0 1 IRM [1] 80.9 ± 0.5 75.1 ± 0.1 58.0 ± 0.1 38.4 ± 0.9 30.4 ± 1.0 56.6 0 1 ARM [72] 80.6 ± 0.5 75.9 ± 0.3 59.6 ± 0.3 37.4 ± 1.9 29.9 ± 0.1 56.7 0 0 DANN [23] 79.2 ± 0.3 76.3 ± 0.2 59.5 ± 0.5 37.9 ± 0.9 31.5 ± 0.1 56.9 1 1 GroupGRO [55] 80.7 ± 0.4 75.4 ± 1.0 60.6 ± 0.3 41.5 ± 2.0 27.5 ± 0.1 57.1 0 1 CDANN [44] 80.3 ± 0.5 76.0 ± 0.5 59.3 ± 0.4 38.6 ± 2.3 31.8 ± 0.2 57.2 0 0 VREx [36] 80.2 ± 0.5 75.3 ± 0.6 59.5 ± 0.1 43.2 ± 0.3 28.1 ± 1.0 57.3 1 1 CAD [53] 81.9 ± 0.3 75.2 ± 0.6 60.5 ± 0.3 40.5 ± 0.4 31.0 ± 0.8 57.8 1 2 CondCAD [53] 80.8 ± 0.5 76.1 ± 0.3 61.0 ± 0.4 39.7 ± 0.4 31.9 ± 0.7 57.9 0 1 MTL [6] 80.1 ± 0.8 75.2 ± 0.3 59.9 ± 0.5 40.4 ± 1.0 35.0 ± 0.0 58.1 0 0 ERM [61] 79.8 ± 0.4 75.8 ± 0.2 60.6 ± 0.2 38.8 ± 1.0 35.3 ± 0.1 58.1 1 - MixStyle [75] 82.6 ± 0.4 75.2 ± 0.7 59.6 ± 0.8 40.9 ± 1.1 33.9 ± 0.1 58.4 1 1 MLDG [38] 81.3 ± 0.2 75.2 ± 0.3 60.9 ± 0.2 40.1 ± 0.9 35.4 ± 0.0 58.6 1 1 Mixup [68] 79.2 ± 0.9 76.2 ± 0.3 61.7 ± 0.5 42.1 ± 0.7 34.0 ± 0.0 58.6 2 2 Fishr [52] 81.3 ± 0.3 76.2 ± 0.3 60.9 ± 0.3 42.6 ± 1.0 34.2 ± 0.3 59.0 2 2 SagNet [48] 81.7 ± 0.6 75.4 ± 0.8 62.5 ± 0.3 40.6 ± 1.5 35.3 ± 0.1 59.1 1 2 SelfReg [34] 81.8 ± 0.3 76.4 ± 0.7 62.4 ± 0.1 41.3 ± 0.3 34.7 ± 0.2 59.3 2 3 Fish [58] 82.0 ± 0.3 76.9 ± 0.2 62.0 ± 0.6 40.2 ± 0.6 35.5 ± 0.0 59.3 3 4 CORAL [59] 81.7 ± 0.0 75.5 ± 0.4 62.4 ± 0.4 41.4 ± 1.8 36.1 ± 0.2 59.4 2 3 SD [51] 81.9 ± 0.3 75.5 ± 0.4 62.9 ± 0.2 42.0 ± 1.0 36.3 ± 0.2 59.7 4 4 Ours 83.8 ± 0.3 76.9 ± 0.6 62.0 ± 0.2 43.2 ± 0.5 34.9 ± 0.1 60.2 4 4 min Θ ∥fw(z − z′)∥, s.t. {z, z′} = fΘ(fθ(x)). (8) Note that different from the training phase, x in this stage is from the target domain Dt, and we use the online setting in [60] for updating. A simple illustration of the test adaptation pipeline is shown in Figure 3. For the final step, we use the original representation ob- tained from the pretrained feature extractor and the adapted adaptive parameters for prediction. Pseudo code of the test stage are shown in Algorithm 2. 4. Experiments 4.1. Settings Datasets. We evalute ITTA on five benchmark datasets: PACS [37] which consists of 9,991 images from 7 cate- gories. This dataset is probably the most widely-used DG benchmark owing to its large distributional shift across 4 do- mains including art painting, cartoon, photo, and sketch; VLCS [18] contains 10,729 images of 5 classes from 4 different datasets (i.e. domains) including PASCAL VOC 2007 [17], LabelMe [54], Caltech [19], and Sun [64] where each dataset is considered a domain in DG;OfficeHome [62] is composed of 15,588 images from 65 classes in office and home environments, and those images can be categorized into 4 domains (i.e. artistic, clipart, product, and real world); TerraInc [4] has 24,788 images from 10 classes. Those images are wild animals taken from 4 different locations (i.e. domains) including L100, L38, L43, and L46; Domain- Net [50] which contains 586,575 images from 345 classes, and the images in it can be depicted in 6 styles (i.e. clipart, infograph, painting, quickdraw, real, and sketch). Implementation details. For all the experiments, we use the ImageNet [15] pretrained ResNet18 [30] backbone that with 4 blocks as the feature extractor fθ, which could en- large the gaps in DG compared to larger models [70]. Corre- spondingly, we also include 4 blocks of additional adaptive parameters (i.e. fΘ), and each block is implemented with 5 layers of learnable parameters with weight initialized as all ones and bias initialized as all zeros. For the weight subnet- work fw, we use 10 layers of learnable parameters with the initialization skill similar to that of fΘ. The classifier fϕ is an MLP layer provided by the Domainbed benchmark [27]. For the weight parameter α in Eq. (2), we set it to be 1 for all experiments (please refer to our supplementary material for analysis). The random seeds, learning rates, batch size, and augmentation skills are all dynamically set for all the compared arts according to [27].Table 2. Single source domain generalization. Experiments are conducted on the PACS dataset [37]. Here A, C, P, and S are the art, cartoon, photo, and sketch domains in PACS. A→C represents models trained on the art domain and tested on the cartoon domain, and similar for others. All methods are examined for 60 trials in each unseen domain. Best results are colored as red. A→C A →P A →S C →A C →P C →S P →A P →C P →S S →A S →C S →P Avg. RSC 66.3 ±1.3 88.2±0.6 57.2±3.1 65.8±1.5 82.4±0.6 68.7±2.5 60.5±2.0 41.3±6.0 53.1±2.8 53.8±1.6 65.9±0.7 48.4±1.9 62.6 Fish 67.1 ±0.5 89.2±1.8 57.0±0.2 66.7±1.0 85.6±0.4 64.5±3.6 55.1±2.1 33.9±2.3 51.2±4.2 59.1±3.2 67.1±0.9 58.4±1.2 62.9 CDANN 66.5±1.7 92.2±0.6 65.0±0.9 70.6±0.1 82.9±1.4 67.7±3.0 60.6±0.3 42.2±6.4 46.9±9.9 51.4±2.3 60.7±1.2 51.9±0.4 63.2 SelfReg 63.9±1.9 90.1±1.0 56.8±2.2 70.2±2.3 85.4±0.3 70.2±2.2 60.9±2.6 38.8±4.0 50.5±3.2 54.5±4.7 66.2±1.2 51.7±4.1 63.3 DANN 67.5 ±1.6 91.2±1.3 67.5±1.3 70.6±1.0 81.4±0.4 66.6±1.1 54.1±2.3 33.5±2.7 52.8±2.3 53.8±1.7 64.4±0.7 58.9±0.8 63.5 CAD 67.1 ±1.5 89.6±0.4 60.2±0.2 67.7±3.1 83.7±1.4 70.2±2.6 60.6±2.6 38.3±3.7 53.8±3.2 50.7±1.6 65.8±1.3 54.4±1.7 63.5 GroupGRO66.5±1.2 90.5±1.5 58.9±2.5 70.8±0.9 85.7±1.2 69.7±1.8 62.3±2.1 41.1±2.7 48.2±4.1 54.8±0.5 65.2±1.6 53.9±1.4 64.0 MTL 67.3 ±1.0 90.1±1.0 58.9±0.7 70.2±1.8 84.2±2.2 71.9±0.7 58.3±2.7 38.5±2.7 52.8±1.5 55.4±3.1 66.1±1.3 55.2±2.6 64.1 IRM 67.5 ±1.8 93.0±0.5 62.9±4.7 67.6±1.3 83.8±0.4 68.9±0.8 63.7±1.8 39.9±3.7 49.0±5.4 54.9±1.4 63.1±2.1 54.9±1.4 64.1 ARM 66.0 ±2.4 91.2±0.7 58.7±6.9 70.6±0.8 84.2±1.0 69.1±0.9 59.2±1.8 42.1±5.6 52.1±3.0 60.0±0.6 62.9±3.3 53.8±2.0 64.2 Mixup 65.5 ±0.8 87.8±0.3 57.2±1.0 71.4±1.1 83.1±1.8 68.0±3.0 59.6±1.7 37.2±2.7 56.5±3.8 55.0±2.2 66.2±1.5 62.7±4.2 64.2 CORAL 66.8±0.5 90.3±0.7 61.5±1.9 67.9±2.1 85.4±0.3 70.4±1.3 55.9±2.9 40.4±4.9 49.8±8.5 55.8±2.1 67.6±0.9 58.9±3.8 64.2 SD 67.1 ±1.3 91.7±1.2 63.7±4.1 70.3±0.9 84.4±0.7 69.4±2.3 57.5±2.5 42.6±0.8 47.7±1.7 55.9±2.4 65.7±0.8 55.8±2.1 64.3 MMD 67.1 ±1.4 88.0±0.8 63.6±1.6 70.0±1.1 83.6±0.2 70.2±1.0 58.8±2.6 40.3±1.0 52.3±2.4 57.4±1.9 68.7±0.9 52.7±3.7 64.4 MLDG 67.3±2.0 90.8±0.5 64.4±0.9 70.8±1.0 84.2±0.3 69.7±1.8 61.6±1.0 41.3±5.1 50.4±0.2 49.9±2.5 66.8±0.4 58.7±3.4 64.7 CondCAD66.9±1.4 92.3±0.7 60.8±4.5 71.0±0.6 84.7±1.1 72.6±0.5 61.2±1.5 40.7±3.6 55.7±1.6 52.3±1.7 64.2±0.4 55.3±1.2 64.8 ERM 67.3 ±0.7 91.7±0.9 60.1±4.7 70.4±0.6 82.3±2.7 68.1±0.9 59.6±1.8 44.7±2.8 56.5±2.7 52.8±2.3 68.1±0.7 58.4±0.9 65.0 VREx 67.1 ±1.5 91.0±1.0 62.6±3.5 71.1±2.4 84.1±0.9 71.7±1.3 62.4±3.1 37.7±3.3 53.6±2.3 60.6±1.6 66.7±0.8 57.5±1.4 65.5 Fishr 67.9 ±1.9 92.7±0.3 62.4±4.7 71.2±0.5 83.4±0.6 70.2±1.1 60.0±2.3 42.7±3.2 57.1±3.9 55.7±3.7 68.4±1.0 62.0±3.1 66.1 SagNet 67.6±1.4 92.3±0.5 59.5±1.7 71.8±0.3 82.8±0.6 69.9±1.8 62.5±2.5 45.2±2.5 64.1±2.0 55.8±1.1 65.7±1.4 55.9±3.5 66.1 MixStyle 68.5±2.0 91.2±1.6 65.1±0.7 73.2±1.3 85.0±0.8 71.7±1.5 63.6±1.7 46.3±1.1 51.6±3.7 54.2±1.5 67.0±3.4 58.3±1.4 66.3 Ours 68.9 ±0.6 92.4±0.1 62.5±0.6 75.3±0.4 85.9±0.3 70.2±1.4 66.5±1.1 52.2±2.7 63.8±1.1 57.6±3.7 68.0±1.3 57.9±2.0 68.4 Training and evaluation details. For all the compared methods, we conduct 60 trials on each source domain, and each with 5,000 iteration steps. During the training stage, we split the examples from training domains to 8:2 (train:val) where the training and validation samples are dynamically selected among different training trials. During test, we select the model that performs the best in the validation samples and test it on the target domains. The strategy is referred to as the “training-domain validate set” model selec- tion method in [27]. For each domain in different datasets, the final performance is the average accuracy from the 60 trials. 4.2. Multi-Source Generalization In these experiments, all five benchmark datasets afore- mentioned are used for evaluation, and the leave-one-out strategy is adopted for training (i.e. with S = |Ds ∪Dt|2 −1, and T = 1). Results are shown in Table 1. We note that ERM method obtains favorable performance against existing arts. In fact, as a strong baseline, ERM is superior to half of the methods in the term of average accuracy, and only 5 arts (i.e. SelfReg [34], Fish [58], CORAL [59], SD [51], and ours) among the compared 22 methods outperforms ERM in most datasets (i.e. with Score ≥ 3). In comparison, the proposed ITTA is more effective than all other models on average. In particular, ITTA achieves the best performances in 3 out of the 5 benchmarks (i.e. PACS, VLCS, and TerraInc datasets) and 4 in the top 5. Note that although our method does not obtain the best performances in the OfficeHome and DomainNet benchmarks, it still outperforms more than half 2We use | · |to denote the number of domains in the environment. of the existing models. The results validate the effectiveness of our method when tested in the multi-source setting. We present results of average accuracy in each domain from different datasets in the supplementary material. Please refer to it for details. 4.3. Single-Source Generalization In these experiments, we adopt the widely-used PACS [37] benchmark for evaluation, and the models are trained on one domain while tested on the remaining three (i.e. with S = 1, and T = 3). Although some approaches, such as MLDG [38] and Fishr [52], may require more than one domain information for their trainings, we can simu- late multi-domain information using only the source domain, and thus the experimental settings are still feasible for them. Compared to the multi-source generalization task, the single- source generalization is considered more difficult due to the limited domain information during the training phase. Evalu- ation results are presented in Table 2. We note that the ERM method outperforms most state-of-the-art models, and only 5 models, including VREx [36], Fishr [52], SagNet [48], MixStyle [75], and the proposed ITTA, can obtain better re- sults than ERM in the term of average accuracy. Meanwhile, our method achieves the best performances when trained in 5 out of the 12 source domain, and it obtains the best perfor- mance on average, leading more than 2% than the second best (i.e. MixStyle [75]) and 3% the ERM method. In line with the findings in [27], we notice that the naive ERM method [61] can indeed perform favorably against most existing models under rigorous evaluation protocol. As a matter of fact, the proposed method is the only one that consistently outperforms ERM in both the multi-sourceTable 3. Evaluations of different TTT-based models in the unseen domain from PACS [37]. The reported accuracies (%) and standard deviations are computed from 60 trials in each target domain. Model Target domain Avg.Art Cartoon Photo Sketch Baseline 79.9 ±0.5 75.4±1.1 94.4±0.5 75.8±1.2 81.4±0.5 TTT [60] 81.5±0.8 77.6±0.6 94.3±0.2 78.4±0.7 83.0±0.2 MT3 [3] 82.0 ±1.0 76.5±1.0 94.1±0.2 77.7±1.3 82.6±0.6 TENT [63] 80.2±0.9 77.2±0.8 94.4±0.2 77.4±0.1 82.3±0.5 Ours 84.7 ±0.4 78.0±0.4 94.5±0.4 78.2±0.3 83.8±0.3 and single-source settings. These results indicate that DG remains challenging for current efforts that aim to ease the distribution shift only through training data, and using the proposed improved TTT strategy may be a promising direc- tion for solving DG. 5. Analysis All experiments in this section are conducted on the widely-used PACS benchmark [37] with the leave-one-out strategy. The experimental settings are the same as that illus- trated in Sec. 4.1. Please refer to our supplementary material for more analysis. 5.1. Compared with Other TTT-Based Models Using test-time adaptation to ease the distribution shift problem has been explored in previous works, such as the original TTT method [60] and MT3 [3]. Their differences lie in that TTT uses a rotation estimation task for the test-time objective, and MT3 adopts a contrastive loss for the task and implements the overall framework using MAML [20]. There is also a recently proposed TENT [63] that aims to minimize the entropy of the final results by tuning the parameters from the batch normalization (BN) layers. To analyze the overall effectiveness of our method, we compare ITTA with these arts using the same baseline (i.e. ResNet18 [30] backbone with the existing augmentation skill [75]). Results are shown in Table 3. We observe that all the com- pared TTT-based methods can improve the baseline model in almost all target domains except for the “Photo” domain, which might be due to the ImageNet pretraining [67]. This phenomenon demonstrates that the TTT strategy may be a promising effort for easing the distribution shift problem. Meanwhile, we observe that the proposed ITTA is superior to all other approaches in most target domains and leads in the term of average accuracy. The main reason is that compared to the empirically designed TTT tasks adopted in previous works, the proposed learnable consistency loss is enforced to be more aligned with the main loss, thus more suitable for the test-time adaptation task [46]. Meanwhile, compared to the strategies that update the original param- eters from the trained model, the adaptation of the newly included parameters is also more effective for the overall (a) Input (b) Ours w/o fw (c) Ours (d) Main Figure 4. Grad-CAM [57] visualizations from different loss terms. We use images with varying class labels from the four target do- mains of PACS [37] as inputs (i.e. art, cartoon, photo, and sketch domains from top to bottom). Ours w/o fw is the naive consis- tency loss with fw disabled in Eq. (1). The proposed learnable consistency loss can align well with the main classification task. TTT framework. In the following, we provide more analysis to support these claims. 5.2. Effectiveness of the Learnable Consistency Loss To examine the effectiveness of our learnable consistency loss, we conduct ablation studies by comparing our method with the following variants. (1) Ours w/o fw: we disable fw when computing the learnable consistency loss in Eq. (1), which uses the naive consistency loss for the auxiliary TTT task. (2) Ours w/ Ent.: after training the model using the baseline settings (i.e. ResNet18 with the augmentation strat- egy [75]), we use the entropy minimization task in [63] for the TTT task. (3) Ours w/ Rot.: we use the rotation estimation task in [60] for the TTT task. To ensure fair com- parisons, we use the same baseline settings and include the same additional adaptive parameters for all the variants. Results are shown in the 4th to 6th rows Table 4. We find that the results from the naive consistency loss ( i.e. Ours w/o fw) are slightly better than that from the other two specially-designed objectives (i.e. Ours w/ Ent. and Ours w/ Rot.) on average. Besides the possibility of deteriorating the performance [46], our results indicate that empirically select- ing a TTT task may also be far from optimal. Meanwhile, we observe that when enabling fw, the proposed learnable consistency loss is superior to that withoutfw in all target do-Table 4. Comparison between different TTT tasks and parameter selecting strategies in the unseen domain from the PACS benchmark [37]. Here the “Ent.”, “Rot.”, and “Lwcont” denotes the entropy minimization task in [63], the rotation estimation task in [60], and the proposed learnable consistency objective, the “All”, “BN”, and “Ada.” are the strategies that update all the parameters, parameters from the batch normalization layer, and the proposed strategy that updates only the new additional adaptive parameters. The reported accuracies (%) and standard deviations are computed from 60 trials in each target domain. Model TTT tasks Param selectings Target domain Avg.Ent. Rot. Lwcont All BN Ada. Art Cartoon Photo Sketch Ours − − ✓ − − ✓ 84.7±0.4 78.0 ±0.4 94.5 ±0.4 78.2 ±0.3 83.8 ±0.3 Ours w/ofw − − − − − ✓ 83.1±0.4 74.6 ±0.6 94.0 ±0.5 78.0 ±0.8 82.5 ±0.1 Ours w/ Ent. ✓ − − − − ✓ 79.9±2.4 77.3 ±0.3 94.8 ±0.8 77.6 ±0.4 82.4 ±0.8 Ours w/ Rot. − ✓ − − − ✓ 81.1±1.0 75.2 ±0.5 94.9 ±0.3 77.3 ±0.6 82.1 ±0.3 Ours w/o TTT − − ✓ − − − 83.3±0.5 76.0 ±0.5 94.4 ±0.5 76.7 ±1.4 82.8 ±0.3 Ours w/ All − − ✓ ✓ − − 83.0±0.7 77.0 ±1.4 94.5 ±0.7 77.4 ±0.9 83.0 ±0.2 Ours w/ BN − − ✓ − ✓ − 81.8±0.5 75.6 ±0.3 94.4 ±0.3 77.9 ±1.1 82.4 ±0.5 mains, and it leads in the term of average accuracy among the variants compared, illustrating its advantage against other adopted TTT tasks. These results are not surprising. By comparing the Grad-CAM [57] visualizations from the main classification task with the learnable and naive consistency losses in Figure 4, we find that the proposed learnable objec- tive can well align with the main loss when fw is enabled as the hot zones activated by these two tasks are similar, which guarantees the improvement for the test-time adapta- tion [46, 60]. Please refer to our supplementary material for more visualizations. 5.3. Effectiveness of the Adaptive Parameters We compare ITTA with three variants to demonstrate the effectiveness of the proposed additional adaptive parameters. (1) Ours w/o TTT: we do not update any parameters during the test phase. This variant is used to verify whether TTT can improve the pretrained model. (2) Ours w/ ALL: similar to the updating strategy in the original TTT method [60], we update all the parameters from the feature extractor during the test phase. (3) Ours w/ BN: following the suggestion from TENT [63], only parameters from the BN layers of the feature extractor are updated. Note the same pretrained model is shared for all variants in these experiments, and the objectives during the test adaptation phase are to minimize the same learned consistency loss. We list the results in the last three rows in Table 4. We observe that when only updating parameters from the BN layers, the performance is inferior to the strategy without test-time adaptation, and updating all the parameters does not ensure improvements in all target domains. The observations are in line with the findings in [63] that selecting reliable parameters to update is essential in the TTT system and may also interact with the choice of the TTT task. In comparison, when including additional adaptive parameters for updating, the pretrained model can be boosted in all environments. The results validate that our adaptive parameters are more effective than that selected with existing strategies [60, 63] when applied with the proposed learnable test-time objective. 5.4. Limitation Although the proposed learned loss can bring satisfaction improvements, we are aware that the lunch is not free. When the weight subnetwork fw is disabled, updating the joint loss in Eq. (2) only costs 1 forward and 1 backward. However, in order to update fw, we have to compute the second-order derivative in Eq. (5), which will require 1 more forward and 3 more backward processes, bringing extra burden to the system. Our future efforts aim to simplify the overall optimization process and reduce the cost for ITTA. 6. Conclusion In this paper, we aim to improve the current TTT strategy for alleviating the distribution shift problem in DG. First, given that the auxiliary TTT task plays a vital role in the over- all framework, and an empirically selecting one that does not align with the main task may potentially deteriorate instead of improving the performance, we propose a learnable con- sistency loss that can be enforced to be more aligned with the main loss by adjusting its learnable parameters. This strategy is ensured to improve the model and shows favorable perfor- mance against some specially-designed objectives. Second, considering that selecting reliable and effective parameters to update during the test phase is also essential while exhaus- tively trying different combinations may require tremendous effort, we propose a new alternative by including new ad- ditional adaptive parameters for adaptation during the test phase. This alternative is shown to outperform some pre- vious parameter selecting strategies via our experimental findings. By conducting extensive experiments under a rig- orous evaluation protocol, we show that our method can achieve superior performance against existing arts in both the multi-source and single-source DG tasks. Acknowledgements. Liang Chen is supported by the ChinaScholarship Council (CSC Student ID 202008440331). References [1] Martin Arjovsky, L´eon Bottou, Ishaan Gulrajani, and David Lopez-Paz. Invariant risk minimization. arXiv preprint arXiv:1907.02893, 2019. 5, 15, 16, 17 [2] Yogesh Balaji, Swami Sankaranarayanan, and Rama Chel- lappa. Metareg: Towards domain generalization using meta- regularization. In NeurIPS, 2018. 1, 2, 14, 15 [3] Alexander Bartler, Andre B¨uhler, Felix Wiewel, Mario D¨obler, and Bin Yang. Mt3: Meta test-time training for self- supervised test-time adaption. In AISTATS, 2022. 2, 4, 7 [4] Sara Beery, Grant Van Horn, and Pietro Perona. Recognition in terra incognita. In ECCV, 2018. 5, 17 [5] Shai Ben-David, John Blitzer, Koby Crammer, and Fernando Pereira. Analysis of representations for domain adaptation. In NeurIPS, 2006. 2 [6] Gilles Blanchard, Aniket Anand Deshmukh, Urun Dogan, Gyemin Lee, and Clayton Scott. Domain generalization by marginal transfer learning. arXiv preprint arXiv:1711.07910, 2017. 5, 15, 16, 17 [7] Gilles Blanchard, Gyemin Lee, and Clayton Scott. Generaliz- ing from several related classification tasks to a new unlabeled sample. In NeurIPS, 2011. 1 [8] Chaoqi Chen, Jiongcheng Li, Xiaoguang Han, Xiaoqing Liu, and Yizhou Yu. Compound domain generalization via meta- knowledge encoding. In CVPR, 2022. 1 [9] Chaoqi Chen, Luyao Tang, Feng Liu, Gangming Zhao, Yue Huang, and Yizhou Yu. Mix and reason: Reasoning over se- mantic topology with data mixing for domain generalization. In NeurIPS, 2022. 1 [10] Dian Chen, Dequan Wang, Trevor Darrell, and Sayna Ebrahimi. Contrastive test-time adaptation. In CVPR, 2022. 2 [11] Liang Chen, Yong Zhang, Yibing Song, Lingqiao Liu, and Jue Wang. Self-supervised learning of adversarial example: Towards good generalizations for deepfake detection. In CVPR, 2022. 2 [12] Liang Chen, Yong Zhang, Yibing Song, Jue Wang, and Lingqiao Liu. Ost: Improving generalization of deepfake detection via one-shot test-time training. In NeurIPS, 2022. 2, 12 [13] Ting Chen, Simon Kornblith, Mohammad Norouzi, and Geof- frey Hinton. A simple framework for contrastive learning of visual representations. In ICML, 2020. 2, 3 [14] Sungha Choi, Seunghan Yang, Seokeon Choi, and Sungrack Yun. Improving test-time adaptation via shift-agnostic weight regularization and nearest source prototypes. In ECCV, 2022. 2 [15] Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li, and Li Fei-Fei. Imagenet: A large-scale hierarchical image database. In CVPR, 2009. 5 [16] Qi Dou, Daniel Coelho de Castro, Konstantinos Kamnitsas, and Ben Glocker. Domain generalization via model-agnostic learning of semantic features. In NeurIPS, 2019. 1, 2 [17] Mark Everingham, Luc Van Gool, Christopher KI Williams, John Winn, and Andrew Zisserman. The pascal visual object classes (voc) challenge. IJCV, 88(2):303–338, 2010. 5 [18] Chen Fang, Ye Xu, and Daniel N Rockmore. Unbiased metric learning: On the utilization of multiple datasets and web images for softening bias. In ICCV, 2013. 5, 16 [19] Li Fei-Fei, Rob Fergus, and Pietro Perona. Learning gener- ative visual models from few training examples: An incre- mental bayesian approach tested on 101 object categories. In CVPR worksho, 2004. 5 [20] Chelsea Finn, Pieter Abbeel, and Sergey Levine. Model- agnostic meta-learning for fast adaptation of deep networks. In ICML, 2017. 2, 7 [21] Francois Fleuret et al. Uncertainty reduction for model adap- tation in semantic segmentation. In CVPR, 2021. 2 [22] Yossi Gandelsman, Yu Sun, Xinlei Chen, and Alexei A Efros. Test-time training with masked autoencoders. In NeurIPS, 2022. 2 [23] Yaroslav Ganin, Evgeniya Ustinova, Hana Ajakan, Pascal Germain, Hugo Larochelle, Franc ¸ois Laviolette, Mario Marc- hand, and Victor Lempitsky. Domain-adversarial training of neural networks. JMLR, 17(1):2096–2030, 2016. 1, 2, 5, 15, 16, 17 [24] Muhammad Ghifary, David Balduzzi, W Bastiaan Kleijn, and Mengjie Zhang. Scatter component analysis: A unified framework for domain adaptation and domain generalization. IEEE TPAMI, 39(7):1414–1430, 2016. 1 [25] Muhammad Ghifary, W Bastiaan Kleijn, Mengjie Zhang, and David Balduzzi. Domain generalization for object recognition with multi-task autoencoders. In ICCV, 2015. 2 [26] Jean-Bastien Grill, Florian Strub, Florent Altch ´e, Corentin Tallec, Pierre Richemond, Elena Buchatskaya, Carl Doer- sch, Bernardo Avila Pires, Zhaohan Guo, Mohammad Ghesh- laghi Azar, et al. Bootstrap your own latent-a new approach to self-supervised learning. In NeurIPS, 2020. 2, 3 [27] Ishaan Gulrajani and David Lopez-Paz. In search of lost domain generalization. In ICLR, 2021. 1, 2, 5, 6, 14, 15, 16, 17 [28] Sivan Harary, Eli Schwartz, Assaf Arbelle, Peter Staar, Shady Abu-Hussein, Elad Amrani, Roei Herzig, Amit Alfassy, Raja Giryes, Hilde Kuehne, et al. Unsupervised domain general- ization by learning a bridge across domains. In CVPR, 2022. 1 [29] Kaiming He, Haoqi Fan, Yuxin Wu, Saining Xie, and Ross Girshick. Momentum contrast for unsupervised visual repre- sentation learning. In CVPR, 2020. 2, 3 [30] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image recognition. In CVPR, 2016. 1, 5, 7, 14 [31] Shoubo Hu, Kun Zhang, Zhitang Chen, and Laiwan Chan. Domain generalization via multidomain discriminant analysis. In UAI, 2020. 1 [32] Xun Huang and Serge Belongie. Arbitrary style transfer in real-time with adaptive instance normalization. In ICCV, 2017. 2 [33] Zeyi Huang, Haohan Wang, Eric P Xing, and Dong Huang. Self-challenging improves cross-domain generalization. In ECCV, 2020. 5, 15, 16, 17[34] Daehee Kim, Youngjun Yoo, Seunghyun Park, Jinkyu Kim, and Jaekoo Lee. Selfreg: Self-supervised contrastive regular- ization for domain generalization. In ICCV, 2021. 2, 5, 6, 15, 16, 17 [35] Pang Wei Koh, Shiori Sagawa, Henrik Marklund, Sang Michael Xie, Marvin Zhang, Akshay Balsubramani, Weihua Hu, Michihiro Yasunaga, Richard Lanas Phillips, Irena Gao, et al. Wilds: A benchmark of in-the-wild distribu- tion shifts. In ICML, 2021. 1 [36] David Krueger, Ethan Caballero, Joern-Henrik Jacobsen, Amy Zhang, Jonathan Binas, Dinghuai Zhang, Remi Le Priol, and Aaron Courville. Out-of-distribution generalization via risk extrapolation (rex). In ICML, 2021. 5, 6, 15, 16, 17 [37] Da Li, Yongxin Yang, Yi-Zhe Song, and Timothy M Hospedales. Deeper, broader and artier domain generalization. In ICCV, 2017. 1, 5, 6, 7, 8, 12, 13, 14, 15 [38] Da Li, Yongxin Yang, Yi-Zhe Song, and Timothy M Hospedales. Learning to generalize: Meta-learning for do- main generalization. In AAAI, 2018. 1, 2, 5, 6, 15, 16, 17 [39] Da Li, Jianshu Zhang, Yongxin Yang, Cong Liu, Yi-Zhe Song, and Timothy M Hospedales. Episodic training for domain generalization. In ICCV, 2019. 1, 2 [40] Haoliang Li, Sinno Jialin Pan, Shiqi Wang, and Alex C Kot. Domain generalization with adversarial feature learning. In CVPR, 2018. 1, 2, 5, 15, 16, 17 [41] Pan Li, Da Li, Wei Li, Shaogang Gong, Yanwei Fu, and Timothy M Hospedales. A simple feature augmentation for domain generalization. In ICCV, 2021. 1, 2, 12, 14 [42] Xiaotong Li, Yongxing Dai, Yixiao Ge, Jun Liu, Ying Shan, and Ling-Yu Duan. Uncertainty modeling for out- of-distribution generalization. In ICLR, 2022. 1, 2 [43] Yizhuo Li, Miao Hao, Zonglin Di, Nitesh Bharadwaj Gun- davarapu, and Xiaolong Wang. Test-time personalization with a transformer for human pose estimation. In NeurIPS, 2021. 2, 3, 4 [44] Ya Li, Xinmei Tian, Mingming Gong, Yajing Liu, Tongliang Liu, Kun Zhang, and Dacheng Tao. Deep domain generaliza- tion via conditional invariant adversarial networks. In ECCV, 2018. 1, 2, 5, 15, 16, 17 [45] Yiying Li, Yongxin Yang, Wei Zhou, and Timothy Hospedales. Feature-critic networks for heterogeneous do- main generalization. In ICML, 2019. 14, 15 [46] Yuejiang Liu, Parth Kothari, Bastien van Delft, Baptiste Bellot-Gurlet, Taylor Mordan, and Alexandre Alahi. Ttt++: When does self-supervised test-time training fail or thrive? In NeurIPS, 2021. 2, 3, 4, 7, 8, 12, 14, 15 [47] Krikamol Muandet, David Balduzzi, and Bernhard Sch¨olkopf. Domain generalization via invariant feature representation. In ICML, 2013. 1, 2 [48] Hyeonseob Nam, HyunJae Lee, Jongchan Park, Wonjun Yoon, and Donggeun Yoo. Reducing domain gap by reducing style bias. In CVPR, 2021. 2, 5, 6, 15, 16, 17 [49] Prashant Pandey, Mrigank Raman, Sumanth Varambally, and Prathosh Ap. Generalization on unseen domains via inference- time label-preserving target projections. In CVPR, 2021. 1 [50] Xingchao Peng, Qinxun Bai, Xide Xia, Zijun Huang, Kate Saenko, and Bo Wang. Moment matching for multi-source domain adaptation. In ICCV, 2019. 5, 17 [51] Mohammad Pezeshki, Oumar Kaba, Yoshua Bengio, Aaron C Courville, Doina Precup, and Guillaume Lajoie. Gradient star- vation: A learning proclivity in neural networks. In NeurIPS, 2021. 1, 5, 6, 15, 16, 17 [52] Alexandre Rame, Corentin Dancette, and Matthieu Cord. Fishr: Invariant gradient variances for out-of-distribution gen- eralization. In ICML, 2022. 1, 2, 5, 6, 15, 16, 17 [53] Yangjun Ruan, Yann Dubois, and Chris J Maddison. Optimal representations for covariate shift. In ICLR, 2022. 5, 15, 16, 17 [54] Bryan C Russell, Antonio Torralba, Kevin P Murphy, and William T Freeman. Labelme: a database and web-based tool for image annotation. IJCV, 77(1):157–173, 2008. 5 [55] Shiori Sagawa, Pang Wei Koh, Tatsunori B Hashimoto, and Percy Liang. Distributionally robust neural networks for group shifts: On the importance of regularization for worst- case generalization. In ICLR, 2020. 5, 15, 16, 17 [56] Steffen Schneider, Evgenia Rusak, Luisa Eck, Oliver Bring- mann, Wieland Brendel, and Matthias Bethge. Improving robustness against common corruptions by covariate shift adaptation. In NeurIPS, 2020. 2 [57] Ramprasaath R Selvaraju, Michael Cogswell, Abhishek Das, Ramakrishna Vedantam, Devi Parikh, and Dhruv Batra. Grad- cam: Visual explanations from deep networks via gradient- based localization. In ICCV, 2017. 7, 8, 11, 13 [58] Yuge Shi, Jeffrey Seely, Philip HS Torr, N Siddharth, Awni Hannun, Nicolas Usunier, and Gabriel Synnaeve. Gradient matching for domain generalization. In ICLR, 2021. 1, 2, 5, 6, 15, 16, 17 [59] Baochen Sun and Kate Saenko. Deep coral: Correlation alignment for deep domain adaptation. In ECCV, 2016. 5, 6, 15, 16, 17 [60] Yu Sun, Xiaolong Wang, Zhuang Liu, John Miller, Alexei Efros, and Moritz Hardt. Test-time training with self- supervision for generalization under distribution shifts. In ICML, 2020. 1, 2, 4, 5, 7, 8, 11, 12, 13 [61] Vladimir Vapnik. The nature of statistical learning theory . Springer science & business media, 1999. 1, 5, 6, 15, 16, 17 [62] Hemanth Venkateswara, Jose Eusebio, Shayok Chakraborty, and Sethuraman Panchanathan. Deep hashing network for unsupervised domain adaptation. In CVPR, 2017. 5, 16 [63] Dequan Wang, Evan Shelhamer, Shaoteng Liu, Bruno Ol- shausen, and Trevor Darrell. Tent: Fully test-time adaptation by entropy minimization. In ICLR, 2021. 2, 3, 4, 7, 8, 11, 12, 13 [64] Jianxiong Xiao, James Hays, Krista A Ehinger, Aude Oliva, and Antonio Torralba. Sun database: Large-scale scene recog- nition from abbey to zoo. In CVPR, 2010. 5 [65] Zehao Xiao, Xiantong Zhen, Ling Shao, and Cees GM Snoek. Learning to generalize across domains on single test samples. In ICLR, 2022. 2 [66] Qinwei Xu, Ruipeng Zhang, Ya Zhang, Yanfeng Wang, and Qi Tian. A fourier-based framework for domain generaliza- tion. In CVPR, 2021. 1, 2 [67] Zhenlin Xu, Deyi Liu, Junlin Yang, Colin Raffel, and Marc Niethammer. Robust and generalizable visual representation learning via random convolutions. In ICLR, 2021. 7[68] Shen Yan, Huan Song, Nanxiang Li, Lincan Zou, and Liu Ren. Improve unsupervised domain adaptation with mixup training. arXiv preprint arXiv:2001.00677, 2020. 2, 5, 15, 16, 17 [69] Fu-En Yang, Yuan-Chia Cheng, Zu-Yun Shiau, and Yu- Chiang Frank Wang. Adversarial teacher-student representa- tion learning for domain generalization. In NeurIPS, 2021. 1, 2 [70] Nanyang Ye, Kaican Li, Haoyue Bai, Runpeng Yu, Lanqing Hong, Fengwei Zhou, Zhenguo Li, and Jun Zhu. Ood-bench: Quantifying and understanding two dimensions of out-of- distribution generalization. In CVPR, 2022. 5 [71] Fuming You, Jingjing Li, and Zhou Zhao. Test-time batch statistics calibration for covariate shift. arXiv preprint arXiv:2110.04065, 2021. 4 [72] Marvin Zhang, Henrik Marklund, Nikita Dhawan, Abhishek Gupta, Sergey Levine, and Chelsea Finn. Adaptive risk mini- mization: A meta-learning approach for tackling group distri- bution shift. arXiv preprint arXiv:2007.02931, 2020. 5, 15, 16, 17 [73] Marvin Zhang, Henrik Marklund, Nikita Dhawan, Abhishek Gupta, Sergey Levine, and Chelsea Finn. Adaptive risk mini- mization: Learning to adapt to domain shift. NeurIPS, 2021. 2 [74] Tao Zhong, Zhixiang Chi, Li Gu, Yang Wang, Yuanhao Yu, and Jin Tang. Meta-dmoe: Adapting to domain shift by meta- distillation from mixture-of-experts. In NeurIPS, 2022. 2 [75] Kaiyang Zhou, Yongxin Yang, Yu Qiao, and Tao Xiang. Do- main generalization with mixstyle. In ICLR, 2021. 1, 2, 3, 5, 6, 7, 12, 15, 16, 17 Appendix In this supplementary material, we provide, 1. Resource usage for ITTA in Section 7. 2. Grad-CAM visualizations of different loss terms in Section 8. 3. Parameter analysis of ITTA in Section 9; 4. Using a different augmentation skill for ITTA in Sec- tion 10. 5. Using different updating steps or a strategy for ITTA during the test phase in Section 11. 6. Using different network structures for the learnable consistency loss and adaptive parameters in Section 12. 7. Comparisons with other related methods in Section 13. 8. Detailed experimental results in the DomainBed bench- mark in Section 14. 7. Resource Usage Comparisons Between ITTA and the Baseline Model Requiring extra resources for our ITTA is a common lim- itation for existing test-time-based arts. To further evaluate our method, in this section, we compare FLOPS, model size, and inference time in Table 5. We compare only with ERM as most existing methods utilize the same network during in- ferences. We note that compare to the baseline model, ITTA requires extra Flops and processing time, this is because the adaptation process uses extra forward and backward steps during the test phase. While the parameters between the two models are similar because the newly included adaptive blocks are much smaller in size compared to the original model. Table 5. Resource comparisons during testing. Here inc. and exc. columns in ITTA indicate to include and exclude the TTA phase. Model Flops (G) Params (M) Time (s) Baseline 1.82 11.18 0.004 ITTA (inc.| exc.) 6.12 | 1.83 14.95 | 14.94 0.021 | 0.005 8. Grad-CAM Visualizations of Different Self- Supervised Objectives In Section 5 of the manuscript, we provide Grad-CAM [57] visualizations of our learnable consistency and the main losses to illustrate their alignment. To further show the differences between several TTT tasks [60, 63], we present more visual examples in this section. Results are shown in Figure 5. We observe that the entropy minimization [63] and rotation estimation [60] objectives do not activate the same regions as the main loss. As shown in the first row, for the class label of giraffe, both the main loss and our learned loss can correctly locate the two giraffes in the image, while the rotation estimation task can only locate one target, the same observation can be found when the learned weightsare disabled in our loss term. Meanwhile, although the two objects can be found for the entropy minimization task, the corresponding hot region does not align with that of the main loss. Similar phenomena can be observed in other samples. These visual examples demonstrate that our learned objective can better align with the main task than the TTT tasks adopted in previous works [60, 63], explaining why using the proposed learnable consistency loss can better improve TTT. 9. Parameter Analysis In this section, we analyze the hyper-parameter used in ITTA. We use the weight parameterα to balance the contri- butions from the main loss and weighted consistency loss (i.e. Lmain + αLwcont in Eq. (2) of our manuscript). To analyze the sensitivity of ITTA regarding different values of α, we conduct ablation studies in the PACS benchmark [37]. Results are listed in Table 6. We observe that the proposed ITTA can obtain favorable performances when α is in the range of 0.1 to 10, and it performs the best on average when setting as 1. We thus fix the parameter as 1 in all experi- ments. 10. A Different Augmentation Skill for ITTA In our manuscript, we use the existing augmentation strat- egy from [75] to obtain the augmented feature. In this sec- tion, we replace this implementation with that from [41] to further verify if our ITTA can still thrive with another aug- mentation skill. Different from [75] that mixes the statics of the feature to synthesize new information, [41] uses an affine transformation to create new features, where the weight for the transformation is sampled from a normal distribution with the mean value of one and standard value of zero, and the bias for the transformation is sampled from a normal distribution with the mean and standard values both zero. Experiments are conducted on the PACS benchmark [37] with the leave-one-out strategy. We compare ITTA with several different variants. (1) Ours w/o fw & TTT: this variant is the baseline model which uses the naive consistency loss for training and does not include TTT during the test phase. (2) Ours w/o fw: we disable the fw in our consistency loss, which uses the naive consistency loss for the test-time updating. (3) Ours w/o TTT: we do not update any parameters during the test phase. This variant is used to verify whether TTT can improve the pretrained model when replacing the augmentation strategy. We also compare these variants with the ERM method to show their effectivenesses. Results are listed in Table 7. We observe that ERM per- forms favorably against the baseline model, indicating that this augmentation strategy may not be beneficial for the training process. Meanwhile, we observe that when fw is disabled, the performances seem to decrease in 3 out of 4 target domains, and the average accuracy is also inferior to the baseline (i.e. Ours w/o fw & TTT). This result is in line with the finding in [46] that an inappropriate TTT task may deteriorate the performance. In comparison, we note that the performances are both improved when fw is enabled (i.e. Ours w/o TTT and Ours), which once again demonstrates that the proposed learnable consistency loss can improve the trained model. Moreover, we can also observe that when combining fw and TTT, our model is superior to other vari- ants and the ERM method. These results demonstrate that the proposed two strategies can improve the current TTT framework despite a less effective augmentation strategy. 11. Different Updating Steps or Strategies for ITTA In the manuscript, we use one TTT step for ITTA before during the testing step. In this section, we conduct experi- ments to evaluate the performances of ITTA with different TTT steps. Experiments are conducted on the PACS bench- mark [37] with the leave-one-out strategy, and each target domain is examined with 60 sets of random seeds and hyper- parameter settings. Results are listed in Table 8. We observe that the average accuracies of using more TTT steps are not improved greatly while the computational times are propor- tional to the TTT steps. To this end, we use one TTT step for ITTA as a compromise between accuracy and efficiency. We use the online setting from TTT [60] for all arts, which assumes test samples arrive sequentially and updates the adaptive blocks based on the states optimized from a previous sample. In this section, we also test ITTA in an episodic manner (i.e. Epi) [12]. Results in Table 8 suggest that while the episodic updating strategy performs slightly worse than the current scheme, and it still outperforms the baseline. 12. Different Network Structures for the Learnable Consistency Loss and Adaptive Parameters In our implementation, we use 10 layers of learnable pa- rameters for fw, and we use 5 layers of learnable parameters for fΘ after each block. In this section, we evaluate our ITTA with different network structures for these two mod- ules. Specifically, we compare the original implementation with the variants that use 1, 5, and 15 layers for fw and 1, 10, and 15 layers for fΘ to evaluate the performances of dif- ferent structures. Similarly, we conduct experiments on the PACS benchmark [37] with the leave-one-out strategy, and each target domain is examined with 60 sets of random seeds and hyper-parameter settings. Evaluation results are listed in Table 9. We observe that their differences in the average accuracy are rather subtle on account of the variances. To(a) Input (b) Entropy (c) Rotation (d) Ours w/o fw (e) Ours (f) Main Figure 5. Grad-CAM [57] visualizations from different loss terms. We use images with varying class labels (i.e. giraffe, elephant, house, and horse from top to bottom) from the four target domains of PACS [37] as inputs (i.e. art, cartoon, photo, and sketch domains from top to bottom). “Entropy” and “Rotation” here denote the entropy minimization and rotation estimation tasks in [63] and [60]. Ours w/o fw is the learnable consistency loss in Eq. (1) in the manuscript (i.e. ∥fw(z − z′)∥) when fw is disabled. The proposed learnable consistency loss can align well with the main classification task. Table 6. Sensitivity analysis of ITTA regarding different values ofα in the unseen domain from PACS [37]. The reported accuracies (%) and standard deviations are computed from 60 trials in each target domain. Values Target domain Avg.Art Cartoon Photo Sketch α = 0.1 83.9 ± 0.7 76.2 ± 1.1 94.8 ± 0.2 78.8 ± 0.8 83.4 ± 0.2 α = 1 (Ours) 84.7 ± 0.4 78.0 ± 0.4 94.5 ± 0.4 78.2 ± 0.3 83.8 ± 0.3 α = 10 83.9 ± 0.5 77.4 ± 0.6 94.2 ± 0.7 77.3 ± 0.8 83.2 ± 0.3 α = 100 81.5 ± 1.2 77.0 ± 0.6 92.6 ± 0.7 78.9 ± 2.1 82.5 ± 0.9 this end, we use the original implementation with 10 layers of learnable parameters for fw and 5 layers of learnable pa- rameters for fΘ, which performs relatively better than other variants. Since the adaptive blocks fΘ are attached after each layer of the network, one may wonder how the varying locations of the adaptive blocks affect the performance of ITTA. To answer this question, we further conduct experiments by adding the adaptive blocks after different layers of the orig- inal network. Denoting as Loc = lan given the n layers in the original network, we note that the model performs less effectively when the adaptive block is placed after the 1st layer of the network, and using all four adaptive blocks (i.e. ours) is more effective than other alternatives. 13. Comparisons with Other Related Methods Apart from the proposed ITTA, some other works also propose to include learnable parameters in their auxiliaryTable 7. Performances of our method with another augmentation strategy from [41] in the unseen domain from PACS [37]. The reported accuracies (%) and standard deviations are computed from 60 trials in each target domain. Model Target domain Avg.Art Cartoon Photo Sketch ERM 78.0 ± 1.3 73.4 ± 0.8 94.1 ± 0.4 73.6 ± 2.2 79.8 ± 0.4 Ours w/o fw & TTT 74.9 ± 0.4 74.1 ± 0.8 90.6 ± 0.3 79.7 ± 0.7 79.8 ± 0.4 Ours w/o fw 77.1 ± 1.0 73.6 ± 1.1 89.9 ± 0.4 78.4 ± 0.8 79.7 ± 0.2 Ours w/o TTT 77.5 ± 0.3 73.2 ± 0.6 92.4 ± 0.4 78.0 ± 1.0 80.3 ± 0.3 Ours (w/ fw & TTT) 79.2 ± 0.8 74.9 ± 1.1 92.2 ± 0.3 76.9 ± 0.7 80.8 ± 0.4 Table 8. Evaluations of ITTA in the unseen domain from PACS [37] with different TTT steps and updating strategies during the testing phase. The reported accuracies (%) and standard deviations are computed from 60 trials in each target domain. The time consumption (TC) is computed using one image with the size of 224 × 224. Epi. denotes updating ITTA in an episodic manner. Steps Target domain Avg. TCArt Cartoon Photo Sketch 1 step (Ours) 84.7 ± 0.4 78.0 ± 0.4 94.5 ± 0.4 78.2 ± 0.3 83.8 ± 0.3 2.4 ms 2 step 84.2 ± 0.9 77.5 ± 0.6 94.4 ± 0.4 79.1 ± 1.0 83.8 ± 0.1 4.2 ms 3 step 84.5 ± 1.2 77.6 ± 0.6 94.0 ± 0.6 79.3 ± 0.1 83.9 ± 0.3 6.1 ms Epi. 83.6 ± 0.7 77.9 ± 0.5 95.2 ± 0.1 76.6 ± 0.5 83.3 ± 0.4 losses. Examples include MetaReg [2] and Feature-Critic [45] which both suggest using meta-learning to produce more general models. The main difference between these arts and ITTA is that parameters in the auxiliary loss from [2,45] are gradually refined by episode training, and they are updated via a gradient alignment step in ITTA (see Sec. 3.1 in the manuscript), which is much simpler. In this sec- tion, we compare ITTA with these two arts in the PACS dataset [37] using the same settings aforementioned. Be- cause MetaReg [2] does not release codes, we thus directly cite the data from their paper in the comparison. Different from others, the results in [2] are averaged by 5 trials accord- ing to their paper, which is much less than our experimental settings. Meanwhile, we also compare with TTT++ [46] which suggests storing the momentum of the features from the source domain and enforcing the similarity between mo- mentums of features from the source and target domains. We use the same setting in Section 5.1 from the manuscript to evaluate TTT++. Results are listed in Table 10. We observe that our method consistently outperforms that from [2,45,46] for both the cases with and without TTT, indicating that the proposed learnable consistency loss and updating method is not only simpler but also more effective than the losses in [2, 45]. 14. Detailed Results in the DomainBed Bench- mark [27] this section presents the average accuracy in each domain from different datasets. As shown in Table 11, 12, 13, 14, and 15, these results are detailed illustrations of the results in Table 2 in our manuscript. For all the experiments, we use the “training-domain validate set” as the model selection method. A total of 22 methods are examined for 60 trials in each unseen domain, and all methods are trained with the leave-one-out strategy using the ResNet18 [30] backbones.Table 9. Performances of our method with different network structures for the consistency loss (i.e. fw) and adaptive parameters (i.e. fΘ) in the unseen domain from PACS [37]. Here ‘Loc=lan’ locates the adaptive block after the n-th layer of the model (‘la4’ is the last layer). The reported accuracies (%) and standard deviations are computed from 60 trials in each target domain. Structures Target domain Avg.Art Cartoon Photo Sketch Structures offw 1 layer 83.5 ±1.2 76.0 ±1.0 95.3 ±0.2 78.7 ±1.5 83.4 ±0.4 5 layers 83.7 ±0.6 76.8 ±0.9 94.6 ±0.3 78.8 ±0.3 83.5 ±0.3 10 layers (Ours) 84.7 ±0.4 78.0 ±0.4 94.5 ±0.4 78.2 ±0.3 83.8 ±0.3 15 layers 84.1 ±0.4 75.8 ±0.2 94.3 ±0.3 79.5 ±0.4 83.4 ±0.2 Structures offΘ 1 layer 84.0 ±0.6 77.4 ±0.5 94.4 ±0.5 78.3 ±0.4 83.5 ±0.3 5 layers (Ours) 84.7 ±0.4 78.0 ±0.4 94.5 ±0.4 78.2 ±0.3 83.8 ±0.3 10 layers 84.8 ±0.3 76.0 ±0.6 94.1 ±0.5 78.3 ±0.1 83.3 ±0.3 15 layers 83.9 ±0.8 76.0 ±0.5 93.8 ±0.4 78.7 ±1.4 83.1 ±0.6 Locations offΘ Loc=la1 83.4±0.7 76.8 ±0.3 94.4 ±0.3 77.8 ±0.3 83.1 ±0.3 Loc=la2 83.4±0.6 77.7 ±0.6 94.2 ±0.5 78.0 ±0.5 83.3 ±0.3 Loc=la3 84.0±0.4 77.5 ±0.3 94.4 ±0.1 77.8 ±0.1 83.4 ±0.2 Loc=la4 84.1±0.7 77.8 ±0.5 94.8 ±0.2 76.9 ±1.5 83.4 ±0.4 Table 10. Compare with learnable losses in [2, 45] in the unseen domain from PACS [37]. The reported accuracies ( %) and standard deviations are computed from 60 trials in each target domain except for [2] where the numbers are directly cited from their paper. Model Target domain Avg.Art Cartoon Photo Sketch MetaReg [2] 83.7 ± 0.2 77.2 ± 0.3 95.5 ± 0.2 70.3 ± 0.3 81.7 Feture-Critic [45] 78.4 ± 1.6 75.4 ± 1.2 92.6 ± 0.5 73.3 ± 1.4 80.0 ± 0.3 TTT++ [46] 84.3 ± 0.1 78.4 ± 0.5 93.8 ± 1.3 73.2 ± 3.2 82.4 ± 1.1 Ours w/o TTT 83.3 ± 0.5 76.0 ± 0.5 94.4 ± 0.5 76.7 ± 1.4 82.8 ± 0.3 Ours 84.7 ± 0.4 78.0 ± 0.4 94.5 ± 0.4 78.2 ± 0.3 83.8 ± 0.3 Table 11. Average accuracies on the PACS [37] datasets using the default hyper-parameter settings in DomainBed [27]. art cartoon photo sketch Average ERM [61] 78.0 ± 1.3 73.4 ± 0.8 94.1 ± 0.4 73.6 ± 2.2 79.8 ± 0.4 IRM [1] 76.9 ± 2.6 75.1 ± 0.7 94.3 ± 0.4 77.4 ± 0.4 80.9 ± 0.5 GroupGRO [55] 77.7 ± 2.6 76.4 ± 0.3 94.0 ± 0.3 74.8 ± 1.3 80.7 ± 0.4 Mixup [68] 79.3 ± 1.1 74.2 ± 0.3 94.9 ± 0.3 68.3 ± 2.7 79.2 ± 0.9 MLDG [38] 78.4 ± 0.7 75.1 ± 0.5 94.8 ± 0.4 76.7 ± 0.8 81.3 ± 0.2 CORAL [59] 81.5 ± 0.5 75.4 ± 0.7 95.2 ± 0.5 74.8 ± 0.4 81.7 ± 0.0 MMD [40] 81.3 ± 0.6 75.5 ± 1.0 94.0 ± 0.5 74.3 ± 1.5 81.3 ± 0.8 DANN [23] 79.0 ± 0.6 72.5 ± 0.7 94.4 ± 0.5 70.8 ± 3.0 79.2 ± 0.3 CDANN [44] 80.4 ± 0.8 73.7 ± 0.3 93.1 ± 0.6 74.2 ± 1.7 80.3 ± 0.5 MTL [6] 78.7 ± 0.6 73.4 ± 1.0 94.1 ± 0.6 74.4 ± 3.0 80.1 ± 0.8 SagNet [48] 82.9 ± 0.4 73.2 ± 1.1 94.6 ± 0.5 76.1 ± 1.8 81.7 ± 0.6 ARM [72] 79.4 ± 0.6 75.0 ± 0.7 94.3 ± 0.6 73.8 ± 0.6 80.6 ± 0.5 VREx [36] 74.4 ± 0.7 75.0 ± 0.4 93.3 ± 0.3 78.1 ± 0.9 80.2 ± 0.5 RSC [33] 78.5 ± 1.1 73.3 ± 0.9 93.6 ± 0.6 76.5 ± 1.4 80.5 ± 0.2 SelfReg [34] 82.5 ± 0.8 74.4 ± 1.5 95.4 ± 0.5 74.9 ± 1.3 81.8 ± 0.3 MixStyle [75] 82.6 ± 1.2 76.3 ± 0.4 94.2 ± 0.3 77.5 ± 1.3 82.6 ± 0.4 Fish [58] 80.9 ± 1.0 75.9 ± 0.4 95.0 ± 0.4 76.2 ± 1.0 82.0 ± 0.3 SD [51] 83.2 ± 0.6 74.6 ± 0.3 94.6 ± 0.1 75.1 ± 1.6 81.9 ± 0.3 CAD [53] 83.9 ± 0.8 74.2 ± 0.4 94.6 ± 0.4 75.0 ± 1.2 81.9 ± 0.3 CondCAD [53] 79.7 ± 1.0 74.2 ± 0.9 94.6 ± 0.4 74.8 ± 1.4 80.8 ± 0.5 Fishr [52] 81.2 ± 0.4 75.8 ± 0.8 94.3 ± 0.3 73.8 ± 0.6 81.3 ± 0.3 Ours 84.7 ± 0.4 78.0 ± 0.4 94.5 ± 0.4 78.2 ± 0.3 83.8 ± 0.3Table 12. Average accuracies on the VLCS [18] datasets using the default hyper-parameter settings in DomainBed [27]. Caltech LabelMe Sun VOC Average ERM [61] 97.7 ± 0.3 62.1 ± 0.9 70.3 ± 0.9 73.2 ± 0.7 75.8 ± 0.2 IRM [1] 96.1 ± 0.8 62.5 ± 0.3 69.9 ± 0.7 72.0 ± 1.4 75.1 ± 0.1 GroupGRO [55] 96.7 ± 0.6 61.7 ± 1.5 70.2 ± 1.8 72.9 ± 0.6 75.4 ± 1.0 Mixup [68] 95.6 ± 1.5 62.7 ± 0.4 71.3 ± 0.3 75.4 ± 0.2 76.2 ± 0.3 MLDG [38] 95.8 ± 0.5 63.3 ± 0.8 68.5 ± 0.5 73.1 ± 0.8 75.2 ± 0.3 CORAL [59] 96.5 ± 0.3 62.8 ± 0.1 69.1 ± 0.6 73.8 ± 1.0 75.5 ± 0.4 MMD [40] 96.0 ± 0.8 64.3 ± 0.6 68.5 ± 0.6 70.8 ± 0.1 74.9 ± 0.5 DANN [23] 97.2 ± 0.1 63.3 ± 0.6 70.2 ± 0.9 74.4 ± 0.2 76.3 ± 0.2 CDANN [44] 95.4 ± 1.2 62.6 ± 0.6 69.9 ± 1.3 76.2 ± 0.5 76.0 ± 0.5 MTL [6] 94.4 ± 2.3 65.0 ± 0.6 69.6 ± 0.6 71.7 ± 1.3 75.2 ± 0.3 SagNet [48] 94.9 ± 0.7 61.9 ± 0.7 69.6 ± 1.3 75.2 ± 0.6 75.4 ± 0.8 ARM [72] 96.9 ± 0.5 61.9 ± 0.4 71.6 ± 0.1 73.3 ± 0.4 75.9 ± 0.3 VREx [36] 96.2 ± 0.0 62.5 ± 1.3 69.3 ± 0.9 73.1 ± 1.2 75.3 ± 0.6 RSC [33] 96.2 ± 0.0 63.6 ± 1.3 69.8 ± 1.0 72.0 ± 0.4 75.4 ± 0.3 SelfReg [34] 95.8 ± 0.6 63.4 ± 1.1 71.1 ± 0.6 75.3 ± 0.6 76.4 ± 0.7 MixStyle [75] 97.3 ± 0.3 61.6 ± 0.1 70.4 ± 0.7 71.3 ± 1.9 75.2 ± 0.7 Fish [58] 97.4 ± 0.2 63.4 ± 0.1 71.5 ± 0.4 75.2 ± 0.7 76.9 ± 0.2 SD [51] 96.5 ± 0.4 62.2 ± 0.0 69.7 ± 0.9 73.6 ± 0.4 75.5 ± 0.4 CAD [53] 94.5 ± 0.9 63.5 ± 0.6 70.4 ± 1.2 72.4 ± 1.3 75.2 ± 0.6 CondCAD [53] 96.5 ± 0.8 62.6 ± 0.4 69.1 ± 0.2 76.0 ± 0.2 76.1 ± 0.3 Fishr [52] 97.2 ± 0.6 63.3 ± 0.7 70.4 ± 0.6 74.0 ± 0.8 76.2 ± 0.3 Ours 96.9 ± 1.2 63.7 ± 1.1 72.0 ± 0.3 74.9 ± 0.8 76.9 ± 0.6 Table 13. Average accuracies on the OfficeHome [62] datasets using the default hyper-parameter settings in DomainBed [27]. art clipart product real Average ERM [61] 52.2 ± 0.2 48.7 ± 0.5 69.9 ± 0.5 71.7 ± 0.5 60.6 ± 0.2 IRM [1] 49.7 ± 0.2 46.8 ± 0.5 67.5 ± 0.4 68.1 ± 0.6 58.0 ± 0.1 GroupGRO [55] 52.6 ± 1.1 48.2 ± 0.9 69.9 ± 0.4 71.5 ± 0.8 60.6 ± 0.3 Mixup [68] 54.0 ± 0.7 49.3 ± 0.7 70.7 ± 0.7 72.6 ± 0.3 61.7 ± 0.5 MLDG [38] 53.1 ± 0.3 48.4 ± 0.3 70.5 ± 0.7 71.7 ± 0.4 60.9 ± 0.2 CORAL [59] 55.1 ± 0.7 49.7 ± 0.9 71.8 ± 0.2 73.1 ± 0.5 62.4 ± 0.4 MMD [40] 50.9 ± 1.0 48.7 ± 0.3 69.3 ± 0.7 70.7 ± 1.3 59.9 ± 0.4 DANN [23] 51.8 ± 0.5 47.1 ± 0.1 69.1 ± 0.7 70.2 ± 0.7 59.5 ± 0.5 CDANN [44] 51.4 ± 0.5 46.9 ± 0.6 68.4 ± 0.5 70.4 ± 0.4 59.3 ± 0.4 MTL [6] 51.6 ± 1.5 47.7 ± 0.5 69.1 ± 0.3 71.0 ± 0.6 59.9 ± 0.5 SagNet [48] 55.3 ± 0.4 49.6 ± 0.2 72.1 ± 0.4 73.2 ± 0.4 62.5 ± 0.3 ARM [72] 51.3 ± 0.9 48.5 ± 0.4 68.0 ± 0.3 70.6 ± 0.1 59.6 ± 0.3 VREx [36] 51.1 ± 0.3 47.4 ± 0.6 69.0 ± 0.4 70.5 ± 0.4 59.5 ± 0.1 RSC [33] 49.0 ± 0.1 46.2 ± 1.5 67.8 ± 0.7 70.6 ± 0.3 58.4 ± 0.6 SelfReg [34] 55.1 ± 0.8 49.2 ± 0.6 72.2 ± 0.3 73.0 ± 0.3 62.4 ± 0.1 MixStyle [75] 50.8 ± 0.6 51.4 ± 1.1 67.6 ± 1.3 68.8 ± 0.5 59.6 ± 0.8 Fish [58] 54.6 ± 1.0 49.6 ± 1.0 71.3 ± 0.6 72.4 ± 0.2 62.0 ± 0.6 SD [51] 55.0 ± 0.4 51.3 ± 0.5 72.5 ± 0.2 72.7 ± 0.3 62.9 ± 0.2 CAD [53] 52.1 ± 0.6 48.3 ± 0.5 69.7 ± 0.3 71.9 ± 0.4 60.5 ± 0.3 CondCAD [53] 53.3 ± 0.6 48.4 ± 0.2 69.8 ± 0.9 72.6 ± 0.1 61.0 ± 0.4 Fishr [52] 52.6 ± 0.9 48.6 ± 0.3 69.9 ± 0.6 72.4 ± 0.4 60.9 ± 0.3 Ours 54.4 ± 0.2 52.3 ± 0.8 69.5 ± 0.3 71.7 ± 0.2 62.0 ± 0.2Table 14. Average accuracies on the TerraInc [4] datasets using the default hyper-parameter settings in DomainBed [27]. L100 L38 L43 L46 Average ERM [61] 42.1 ± 2.5 30.1 ± 1.2 48.9 ± 0.6 34.0 ± 1.1 38.8 ± 1.0 IRM [1] 41.8 ± 1.8 29.0 ± 3.6 49.6 ± 2.1 33.1 ± 1.5 38.4 ± 0.9 GroupGRO [55] 45.3 ± 4.6 36.1 ± 4.4 51.0 ± 0.8 33.7 ± 0.9 41.5 ± 2.0 Mixup [68] 49.4 ± 2.0 35.9 ± 1.8 53.0 ± 0.7 30.0 ± 0.9 42.1 ± 0.7 MLDG [38] 39.6 ± 2.3 33.2 ± 2.7 52.4 ± 0.5 35.1 ± 1.5 40.1 ± 0.9 CORAL [59] 46.7 ± 3.2 36.9 ± 4.3 49.5 ± 1.9 32.5 ± 0.7 41.4 ± 1.8 MMD [40] 49.1 ± 1.2 36.4 ± 4.8 50.4 ± 2.1 32.3 ± 1.5 42.0 ± 1.0 DANN [23] 44.3 ± 3.6 28.0 ± 1.5 47.9 ± 1.0 31.3 ± 0.6 37.9 ± 0.9 CDANN [44] 36.9 ± 6.4 32.7 ± 6.2 51.1 ± 1.3 33.5 ± 0.5 38.6 ± 2.3 MTL [6] 45.2 ± 2.6 31.0 ± 1.6 50.6 ± 1.1 34.9 ± 0.4 40.4 ± 1.0 SagNet [48] 36.3 ± 4.7 40.3 ± 2.0 52.5 ± 0.6 33.3 ± 1.3 40.6 ± 1.5 ARM [72] 41.5 ± 4.5 27.7 ± 2.4 50.9 ± 1.0 29.6 ± 1.5 37.4 ± 1.9 VREx [36] 48.0 ± 1.7 41.1 ± 1.5 51.8 ± 1.5 32.0 ± 1.2 43.2 ± 0.3 RSC [33] 42.8 ± 2.4 32.2 ± 3.8 49.6 ± 0.9 32.9 ± 1.2 39.4 ± 1.3 SelfReg [34] 46.1 ± 1.5 34.5 ± 1.6 49.8 ± 0.3 34.7 ± 1.5 41.3 ± 0.3 MixStyle [75] 50.6 ± 1.9 28.0 ± 4.5 52.1 ± 0.7 33.0 ± 0.2 40.9 ± 1.1 Fish [58] 46.3 ± 3.0 29.0 ± 1.1 52.7 ± 1.2 32.8 ± 1.0 40.2 ± 0.6 SD [51] 45.5 ± 1.9 33.2 ± 3.1 52.9 ± 0.7 36.4 ± 0.8 42.0 ± 1.0 CAD [53] 43.1 ± 2.6 31.1 ± 1.9 53.1 ± 1.6 34.7 ± 1.3 40.5 ± 0.4 CondCAD [53] 44.4 ± 2.9 32.9 ± 2.5 50.5 ± 1.3 30.8 ± 0.5 39.7 ± 0.4 Fishr [52] 49.9 ± 3.3 36.6 ± 0.9 49.8 ± 0.2 34.2 ± 1.3 42.6 ± 1.0 Ours 51.7 ± 2.4 37.6 ± 0.6 49.9 ± 0.6 33.6 ± 0.6 43.2 ± 0.5 Table 15. Average accuracies on the DomainNet [50] datasets using the default hyper-parameter settings in DomainBed [27]. clip info paint quick real sketch Average ERM [61] 50.4 ± 0.2 14.0 ± 0.2 40.3 ± 0.5 11.7 ± 0.2 52.0 ± 0.2 43.2 ± 0.3 35.3 ± 0.1 IRM [1] 43.2 ± 0.9 12.6 ± 0.3 35.0 ± 1.4 9.9 ± 0.4 43.4 ± 3.0 38.4 ± 0.4 30.4 ± 1.0 GroupGRO [55] 38.2 ± 0.5 13.0 ± 0.3 28.7 ± 0.3 8.2 ± 0.1 43.4 ± 0.5 33.7 ± 0.0 27.5 ± 0.1 Mixup [68] 48.9 ± 0.3 13.6 ± 0.3 39.5 ± 0.5 10.9 ± 0.4 49.9 ± 0.2 41.2 ± 0.2 34.0 ± 0.0 MLDG [38] 51.1 ± 0.3 14.1 ± 0.3 40.7 ± 0.3 11.7 ± 0.1 52.3 ± 0.3 42.7 ± 0.2 35.4 ± 0.0 CORAL [59] 51.2 ± 0.2 15.4 ± 0.2 42.0 ± 0.2 12.7 ± 0.1 52.0 ± 0.3 43.4 ± 0.0 36.1 ± 0.2 MMD [40] 16.6 ± 13.3 0.3 ± 0.0 12.8 ± 10.4 0.3 ± 0.0 17.1 ± 13.7 0.4 ± 0.0 7.9 ± 6.2 DANN [23] 45.0 ± 0.2 12.8 ± 0.2 36.0 ± 0.2 10.4 ± 0.3 46.7 ± 0.3 38.0 ± 0.3 31.5 ± 0.1 CDANN [44] 45.3 ± 0.2 12.6 ± 0.2 36.6 ± 0.2 10.3 ± 0.4 47.5 ± 0.1 38.9 ± 0.4 31.8 ± 0.2 MTL [6] 50.6 ± 0.2 14.0 ± 0.4 39.6 ± 0.3 12.0 ± 0.3 52.1 ± 0.1 41.5 ± 0.0 35.0 ± 0.0 SagNet [48] 51.0 ± 0.1 14.6 ± 0.1 40.2 ± 0.2 12.1 ± 0.2 51.5 ± 0.3 42.4 ± 0.1 35.3 ± 0.1 ARM [72] 43.0 ± 0.2 11.7 ± 0.2 34.6 ± 0.1 9.8 ± 0.4 43.2 ± 0.3 37.0 ± 0.3 29.9 ± 0.1 VREx [36] 39.2 ± 1.6 11.9 ± 0.4 31.2 ± 1.3 10.2 ± 0.4 41.5 ± 1.8 34.8 ± 0.8 28.1 ± 1.0 RSC [33] 39.5 ± 3.7 11.4 ± 0.8 30.5 ± 3.1 10.2 ± 0.8 41.0 ± 1.4 34.7 ± 2.6 27.9 ± 2.0 SelfReg [34] 47.9 ± 0.3 15.1 ± 0.3 41.2 ± 0.2 11.7 ± 0.3 48.8 ± 0.0 43.8 ± 0.3 34.7 ± 0.2 MixStyle [75] 49.1 ± 0.4 13.4 ± 0.0 39.3 ± 0.0 11.4 ± 0.4 47.7 ± 0.3 42.7 ± 0.1 33.9 ± 0.1 Fish [58] 51.5 ± 0.3 14.5 ± 0.2 40.4 ± 0.3 11.7 ± 0.5 52.6 ± 0.2 42.1 ± 0.1 35.5 ± 0.0 SD [51] 51.3 ± 0.3 15.5 ± 0.1 41.5 ± 0.3 12.6 ± 0.2 52.9 ± 0.2 44.0 ± 0.4 36.3 ± 0.2 CAD [53] 45.4 ± 1.0 12.1 ± 0.5 34.9 ± 1.1 10.2 ± 0.6 45.1 ± 1.6 38.5 ± 0.6 31.0 ± 0.8 CondCAD [53] 46.1 ± 1.0 13.3 ± 0.4 36.1 ± 1.4 10.7 ± 0.2 46.8 ± 1.3 38.7 ± 0.7 31.9 ± 0.7 Fishr [52] 47.8 ± 0.7 14.6 ± 0.2 40.0 ± 0.3 11.9 ± 0.2 49.2 ± 0.7 41.7 ± 0.1 34.2 ± 0.3 Ours 50.7 ± 0.7 13.9 ± 0.4 39.4 ± 0.5 11.9 ± 0.2 50.2 ± 0.3 43.5 ± 0.1 34.9 ± 0.1",
      "meta_data": {
        "arxiv_id": "2304.04494v2",
        "authors": [
          "Liang Chen",
          "Yong Zhang",
          "Yibing Song",
          "Ying Shan",
          "Lingqiao Liu"
        ],
        "published_date": "2023-04-10T10:12:38Z",
        "pdf_url": "https://arxiv.org/pdf/2304.04494v2.pdf"
      },
      "llm_extracted_info": {
        "main_contributions": "The paper addresses the domain generalization (DG) problem by proposing an Improved Test-Time Adaptation (ITTA) method. It focuses on two key factors for Test-Time Training (TTT): selecting an auxiliary TTT task and identifying reliable parameters to update. The main contributions are: 1) Introducing a learnable consistency loss for the TTT task, which contains learnable parameters adjusted to align better with the main prediction task by enforcing the same optimization direction. 2) Introducing additional adaptive parameters for the trained model and suggesting only updating these new adaptive parameters during the test phase, leaving the original model parameters unchanged. Extensive experiments demonstrate that ITTA achieves superior performance over current state-of-the-art methods on several DG benchmarks for both multi-source and single-source DG tasks.",
        "methodology": "ITTA improves the TTT strategy in two ways. First, instead of empirically defining an auxiliary objective, a learnable consistency loss (Lwcont) is proposed for the TTT task. This loss, derived from multi-view consistency learning, incorporates a weight subnetwork (fw) with learnable parameters. Lwcont is combined with the main cross-entropy loss (Lmain), and fw is updated by minimizing the L2 norm between the normalized gradients of Lmain and Lwcont with respect to the feature extractor parameters, ensuring alignment. Second, additional adaptive parameters (fΘ) are introduced after each block of the pretrained feature extractor (fθ) during the test-time adaptation phase. Only these new adaptive parameters (Θ) are updated during testing, using the learned consistency loss as the objective. During training, fθ and the classifier fϕ are updated with a combined loss (Lmain + αLwcont), and fw is updated to align the gradients. During testing, fθ and fϕ are frozen, and fΘ is updated with the learned consistency loss on test samples. For final prediction, the original fθ and fϕ are used with the adapted fΘ.",
        "experimental_setup": "ITTA was evaluated on five benchmark datasets: PACS, VLCS, OfficeHome, TerraInc, and DomainNet. The model uses an ImageNet pretrained ResNet18 backbone with 4 blocks as the feature extractor (fθ). Four blocks of additional adaptive parameters (fΘ) were included, each with 5 layers of learnable parameters. The weight subnetwork (fw) consists of 10 layers of learnable parameters. The classifier (fϕ) is an MLP layer. The weight parameter α for balancing main and consistency losses was set to 1. All experiments followed the rigorous DomainBed benchmark protocol, including dynamic settings for random seeds, learning rates, batch sizes, and augmentation skills. Evaluations were conducted for both multi-source (leave-one-out strategy, 60 trials per unseen domain) and single-source (PACS dataset, trained on one domain, tested on three, 60 trials per unseen domain) domain generalization tasks. Model selection used the 'training-domain validate set' method, picking the best-performing model on validation samples.",
        "limitations": "The primary limitation lies in the computational burden introduced by the learnable consistency loss. Updating the weight subnetwork (fw) requires computing second-order derivatives, which translates to one extra forward pass and three extra backward passes during the training phase. This significantly increases the computational cost compared to using a simpler auxiliary loss. Additionally, while the paper uses one TTT step for efficiency, it notes that more TTT steps do not greatly improve accuracy but proportionally increase computational time, indicating a trade-off between performance and resource consumption.",
        "future_research_directions": "Future research efforts will focus on simplifying the overall optimization process of ITTA to reduce its computational cost, particularly addressing the burden of computing second-order derivatives for the learnable consistency loss. This could involve exploring more efficient gradient alignment strategies or alternative architectures for the weight subnetwork and adaptive parameters."
      }
    },
    {
      "title": "Robust Test-Time Adaptation in Dynamic Scenarios",
      "abstract": "Test-time adaptation (TTA) intends to adapt the pretrained model to test\ndistributions with only unlabeled test data streams. Most of the previous TTA\nmethods have achieved great success on simple test data streams such as\nindependently sampled data from single or multiple distributions. However,\nthese attempts may fail in dynamic scenarios of real-world applications like\nautonomous driving, where the environments gradually change and the test data\nis sampled correlatively over time. In this work, we explore such practical\ntest data streams to deploy the model on the fly, namely practical test-time\nadaptation (PTTA). To do so, we elaborate a Robust Test-Time Adaptation (RoTTA)\nmethod against the complex data stream in PTTA. More specifically, we present a\nrobust batch normalization scheme to estimate the normalization statistics.\nMeanwhile, a memory bank is utilized to sample category-balanced data with\nconsideration of timeliness and uncertainty. Further, to stabilize the training\nprocedure, we develop a time-aware reweighting strategy with a teacher-student\nmodel. Extensive experiments prove that RoTTA enables continual testtime\nadaptation on the correlatively sampled data streams. Our method is easy to\nimplement, making it a good choice for rapid deployment. The code is publicly\navailable at https://github.com/BIT-DA/RoTTA",
      "full_text": "Robust Test-Time Adaptation in Dynamic Scenarios Longhui Yuan Binhui Xie Shuang Li \f School of Computer Science and Technology, Beijing Institute of Technology {longhuiyuan,binhuixie,shuangli}@bit.edu.cn Abstract Test-time adaptation (TTA) intends to adapt the pre- trained model to test distributions with only unlabeled test data streams. Most of the previous TTA methods have achieved great success on simple test data streams such as independently sampled data from single or multiple distri- butions. However, these attempts may fail in dynamic sce- narios of real-world applications like autonomous driving, where the environments gradually change and the test data is sampled correlatively over time. In this work, we ex- plore such practical test data streams to deploy the model on the fly, namely practical test-time adaptation (PTTA). To do so, we elaborate a Robust Test-Time Adaptation (RoTTA) method against the complex data stream in PTTA. More specifically, we present a robust batch normalization scheme to estimate the normalization statistics. Meanwhile, a memory bank is utilized to sample category-balanced data with consideration of timeliness and uncertainty. Further, to stabilize the training procedure, we develop a time-aware reweighting strategy with a teacher-student model. Exten- sive experiments prove that RoTTA enables continual test- time adaptation on the correlatively sampled data streams. Our method is easy to implement, making it a good choice for rapid deployment. The code is publicly available at https://github.com/BIT-DA/RoTTA 1. Introduction In recent years, many machine learning problems have made considerable headway with the success of deep neu- ral networks [13, 22, 33, 38]. Unfortunately, the perfor- mance of deep models drops significantly when training data and testing data come from different distributions [59], which limits their utility in real-world applications. To re- duce the distribution shift, a handful of works focus on transfer learning field [56], in particular, domain adapta- tion (DA) [17, 42, 45, 48, 69, 72] or domain generalization (DG) [40, 41, 52, 71, 83], in which one or more different but \fCorresponding author Test data stream Continual TTANon-i.i.d.TTAPractical  TTACategoryDistribution Fully TTA Correlation samplingDistributionchanging Figure 1. We consider the practical test-time adaptation (TTA) setup and compare it with related ones. First, Fully TTA [70] adapts models on a fixed test distribution with an independently sampled test stream. Then, on this basis, Continual TTA [73] takes the continually changing distributions into consideration. Next, Non-i.i.d. TTA [19] tries to tackle the correlatively sampled test streams on a single test distribution, where the label distribution among a batch of data deviates from that of the test distribution. To be more practical, Practical TTA strives to connect both worlds: distribution changing and correlation sampling. related labeled datasets (a.k.a. source domain) are collected to help the model generalize well to unlabeled or unseen samples in new datasets (a.k.a. target domain). While both DA and DG have extensively studied the problem of distribution shifts, they typically assume acces- sibility to the raw source data. However, in many practical scenarios like personal consumption records, the raw data should not be publicly available due to data protection reg- ulations. Further, existing methods have to perform heavy backward computation, resulting in unbearable training costs. Test-time adaptation (TTA) [3,11,16,24,26,54,65,81] attempts to address the distribution shift online at test time with only unlabeled test data streams. Unequivocally, TTA has drawn widespread attention in a variety of applications, e.g., 2D/3D visual recognition [2, 29, 49, 65, 82], multi- modality [63, 64] and document understanding [15]. Prior TTA studies [7, 20, 70, 73] mostly concentrate on a simple adaptation scenario, where test samples are inde- pendently sampled from a fixed target domain. To name a few, Sun et al. [65] adapt to online test samples drawn from a constant or smoothly changing distribution with an auxil- iary self-supervised task. Wang et al. [70] adapt to a fixed arXiv:2303.13899v1  [cs.CV]  24 Mar 2023Table 1. Comparison between our proposed practical test-time adaptation (PTTA) and related adaptation settings. Setting Adaptation StageAvailable Data Test Data Stream Train Test Source Target Distribution Sampling Protocol Domain Adaptation ! % ! ! - - Domain Generalization ! % ! % - - Test-Time Training [65] ! ! ! ! stationary independently Fully Test-Time Adaptation [70] % ! % ! stationary independently Continual Test-Time Adaptation [73]% ! % ! continually changing independently Non-i.i.d. Test-Time Adaptation [5, 19]% ! % ! stationary correlatively Practical Test-Time Adaptation (Ours)% ! % ! continually changing correlatively target distribution by performing entropy minimization on- line. However, such an assumption is violated when the test environments change frequently [73]. Later on, Boudiaf et al. [5] and Gonget al. [19] consider the temporal correlation ship within test samples. For example, in autonomous driv- ing, test samples are highly correlated over time as the car will follow more vehicles on the highway or will encounter more pedestrians in the streets. More realistically, the data distribution changes as the surrounding environment alerts in weather, location, or other factors. In a word, distribution change and data correlation occur simultaneously in reality. Confronting continually changing distributions, tradi- tional algorithms like pseudo labeling or entropy minimiza- tion become more unreliable as the error gradients cumu- late. Moreover, the high correlation among test samples re- sults in the erroneous estimation of statistics for batch nor- malization and collapse of the model. Driven by this analy- sis, adapting to such data streams will encounter two major obstacles: 1) incorrect estimation in the batch normaliza- tion statistics leads to erroneous predictions of test samples, consequently resulting in invalid adaptation; 2) the model will easily or quickly overfit to the distribution caused by the correlative sampling. Thus, such dynamic scenarios are pressing for a new TTA paradigm to realize robust adapta- tion. In this work, we launch a more realistic TTA setting, where distribution changing and correlative sampling oc- cur simultaneously at the test phase. We call this Practical Test-Time Adaptation, or briefly,PTTA. To understand more clearly the similarities and differences between PTTA and the previous setups, we visualize them in Figure 1 and sum- marize them in Table 1. To conquer this challenging prob- lem, we propose a Robust Test-Time Adaptation (RoTTA) method, which consists of three parts: 1) robust statistics es- timation, 2) category-balanced sampling considering time- liness and uncertainty and 3) time-aware robust training. More concretely, we first replace the erroneous statistics of the current batch with global ones maintained by the expo- nential moving average. It is a more stable manner to esti- mate the statistics in BatchNorm layers. Then, we simulate a batch of independent-like data in memory with category- balanced sampling while considering the timeliness and un- certainty of the buffered samples. That is, samples that are newer and less uncertain are kept in memory with higher priority. With this batch of category-balanced, timely and confident samples, we can obtain a snapshot of the current distribution. Finally, we introduce a time-aware reweight- ing strategy that considers the timeliness of the samples in the memory bank, with a teacher-student model to perform robust adaptation. With extensive experiments, we demon- strate that RoTTA can robustly adapt in the practical setup, i.e., PTTA. In a nutshell, our contributions can be summarized as: • We propose a new test-time adaptation setup that is more suitable for real-world applications, namely practical test-time adaptation (PTTA). PTTA considers both distribution changing and correlation sampling. • We benchmark the performance of prior methods in PTTA and uncover that they only consider one aspect of the problem, resulting in ineffective adaptation. • We propose a robust test-time adaptation method (RoTTA), which has a more comprehensive considera- tion of PTTA challenges. Ease of implementation and effectiveness make it a practical deployment option. • We extensively demonstrate the practicality of PTTA and the effectiveness of RoTTA on common TTA benchmarks [23], i.e., CIFAR-10-C and CIFAR-100- C and a large-scale DomainNet [58] dataset. RoTTA obtains state-of-the-art results, outperforming the best baseline by a large margin (reducing the averaged classification error by over 5.9%, 5.5% and 2.2% on CIFAR-10-C, CIFAR-100-C and DomainNet, respec- tively). 2. Related Work Domain adaptation (DA) studies the problem of transfer- ring the knowledge learned from a labeled source dataset to an unlabeled target dataset [8, 17, 43, 51, 67, 68]. Represen- tative techniques include latent distribution alignment [48, 77], adversarial training [17, 62], or self-training [75, 85]. The limitation of this setting, however, is that an unlabeled test dataset (target domain) is needed at training time, in addition to a labeled training dataset (source domain). Ac- cordingly, it might fail to handle more practical scenariosFeature 𝐹Robust batch normalization (RBN)Update𝜇௚, 𝜎௚ଶNormalizeFeature𝐹′Update bank with current sample  Training lossℒ௥in Eq. (7) Teacher StudentAdaptation with RBNMemorybankEMA 𝑡A stream of online dataUpdateTest timeCorrelationsamplingStrong & weakaugmentation flowDistributionsCategoryTeacherMajor classhas highest ℋin majorRemoveAddWhen ℋ>ℋSamples to beadded& removed Figure 2. Framework overview. Firstly, we replace the batch normalization layer with RBN which robustly normalizes the feature map. During the inference of the online test stream of PTTA, we utilize the predictions of samples to maintain a memory bank by category- balanced sampling with timeliness and uncertainty. Finally, we use the category-balanced, timely and confident data in the memory bank combined with a robust loss to adapt the model at test time. like test-time adaptation. Our practical test-time adaptation setting can be viewed as performing correlatively sample adaptation on the fly. It is worth noting that standard domain adaptation techniques might collapse when only continual data streams from multiple target domains are accessible. Domain generalization (DG) assumes that multiple source domains are available for model training and tries to learn models that can generalize well to any unseen domains [4, 26,40,41,52,84]. A broad spectrum of methodologies based on data augmentation [78, 84], meta-learning [14, 40], or domain alignment [50,52] has made great progress. In con- trast, this work instead aims to improve the performance of source pre-trained models at the test time by using unla- beled online data streams from multiple continually chang- ing target domains. Continual learning (CL) (also known as incremental learning, life-long learning) addresses the problem of learn- ing a model for many tasks sequentially without forgetting knowledge obtained from the preceding tasks. [1, 6, 31, 37, 60]. CL methods can often be categorized into replay- based [60, 66] and regularization-based [31, 44] methods. Ideas from continual learning are also adopted for continu- ous domain adaptation approaches [34, 74] In our work, we share the same motivation as CL and point out that prac- tical test-time adaptation (PTTA) also suffers catastrophic forgetting (i.e., performance degradation on new test sam- ples due to correlation sampling), which makes test-time adaptation approaches are unstable to deploy. Test-time adaptation (TTA) focus on more challenging settings where only source model and unlabeled target data are available [9, 18, 27, 28, 35, 46, 61]. A similar paradigm is source-free domain adaptation (SFDA) [10, 36, 47, 79], which also requires no access to the training (source) data. To name a few, Liang et al . [45] fit the source hypoth- esis by exploiting the information maximization and self- supervised pseudo-labeling. Kundu et al. [35] formalize a unified solution that explores SFDA without any category- gap knowledge. To fully utilize any arbitrary pre-trained model, Sun et al. [65] propose conducting adaptation on the fly with an auxiliary self-supervised task. Later on, Wanget al. [70] take a source pre-trained model and adapt it to the test data by updating a few trainable parameters in Batch- Norm layers [25] using entropy minimization [21]. While standard TTA has been widely studied in many tasks [2, 20, 63, 64, 70, 82], the fact remains that both dis- tribution changing [73] and data correlation sampling [19] has only been considered in isolation. For example, Gong et al. [19] propose instance-aware batch normalization and prediction-balanced reservoir sampling to address the chal- lenges of correlatively sampled test streams, however, it does not consider unstable adaptation resulting from long- term adaptation on continually changing distributions. On the other hand, Wang et al. [73] assume that the target test data is streamed from a continually changing environment and continually adapt an off-the-shelf source pre-trained model to the current test data. In this work, we launch PTTA, a more practical TTA setting to connect both worlds: distribution changing and correlation sampling. 3. Method 3.1. Problem Definition and Motivation Given a model fθ0 with parameter θ0 pre-trained on source domain DS = {(xS, yS)}, the proposed practical test-time adaptation (PTTA) aims to adapt fθ0 to a stream of online unlabeled samples X0, X1, ...,XT , where Xt is a batch of highly correlated samples from the distribution Ptest that changes with time t continually. More specifi- cally, at test time, with time going on, the test distribution Ptest changes continually as P0, P1, ...,P∞. At time step t, we will receive a batch of unlabeled and correlated samplesmotion distribution changing snow time  Distributions and Labels of PTTA T est Stream uniform 10 1 0.1 0.01 0.001 Dirichlet Parameter  Figure 3. Illustration of the labels and distributions of the test stream of CIFAR10-C under the setup PTTA. And we adopt Dirichlet distribution to simulate the process of correlative sam- pling. It is clear that as the concentration parameter δ decreases, the correlation among sampled data increases, which is reflected in the increasing aggregation of categories. Xt from Ptest. Next, Xt is fed into the model fθt and the model needs to adapt itself to the current test data streams and make predictions fθt (Xt) on the fly. As a matter of fact, this setup is largely driven the prac- tical demands of deploying models in dynamic scenarios. Taking for example the case of autonomous driving men- tioned in § 1, test samples are highly correlated and the data distribution changes continually with the weather or loca- tion. Another example is the situation of intelligent moni- toring, the camera will continuously capture more people at certain times, such as after work, but fewer of them during work time. Meanwhile, the light condition changes con- tinually from day to night. The deployed model should be robustly adapted in such dynamic scenarios. In a word, dis- tribution change and data correlation often happen simul- taneously in the real world. For this reason, existing TTA methods [7,9,19,28,70,73,81] might become unstable when the test stream is sampled from such dynamic scenarios. To obtain the test stream of PTTA, we adopt Dirich- let Distribution with parameter δ to simulate the correla- tion among test samples. We present the test data streams corresponding to different values of δ on the CIFAR10-C dataset in Figure 3. We can observe that the smaller δ is, the higher the correlation will be. For the sake of unity, we set δ = 0.1 as the default for all experiments. In the follow- ing, we present a robust test-time adaptation framework for the practical test-time adaptation setup defined above. An overview of our RoTTA is illustrated in Figure 2. 3.2. Robust Test-Time Adaptation Motivated by the fact that the statistics of current batch data, which are commonly used in previous TTA meth- ods [7, 20, 65, 70, 73], become unreliable when they en- counter correlative test data streams, we first turn to the global robust statistics for normalization. Then, to effec- tively adapt to the current distribution, we maintain a mem- ory bank by category-balanced sampling with considering timeliness and uncertainty, which captures a more stable snapshot of the distribution. Finally, we utilize the teacher- student model and design a timeliness-based reweighting strategy to train the model robustly. Robust batch normalization (RBN). Batch Normaliza- tion (BN) [25] is a widely-used training technique as it can accelerate the training and convergence speed of networks and stabilize the training process by reducing the risk of gradient explosion and vanishing. Given the feature map F ∈ RB×C×H×W as the input for a BN layer when train- ing, the channel-wise mean µ ∈ RC and variance σ2 ∈ RC are calculated as follows: µc = 1 BHW BX b=1 HX h=1 WX w=1 F(b,c,h,w) , (1) σ2 c = 1 BHW BX b=1 HX h=1 WX w=1 (F(b,c,h,w) − µc)2 . (2) Then the feature map is normalized and refined in a channel-wise manner as BN (F(b,c,h,w); µ, σ2) =γc F(b,c,h,w) − µc √σ2c + ϵ + βc , (3) where γ, β∈ RC are learnable parameters in the layer and ϵ > 0 is a constant for numerical stability. Meanwhile, during training, the BN layer maintains a group of global running mean and running variance (µs, σ2 s) for inference. Due to the domain shift at test time, the global statis- tics (µs, σ2 s) normalize test features inaccurately, causing significant performance degradation. To tackle the prob- lem above, some methods [55, 70, 73] use the statistics of the current batch to perform normalization. Unfortunately, when the test samples have a high correlation under PTTA setup, the statistics of the current batch also fail to correctly normalize the feature map, as demonstrated in Figure 4c. Specifically, the performance of BN [53] decreases rapidly as the data correlation increases. Based on the analysis above, we propose a robust batch normalization (RBN) module, which maintains a group of global statistics (µg, σ2 g) to normalize the feature map ro- bustly. Before the whole test-time adaptation, (µg, σ2 g) is initialized as the running mean and variance (µs, σ2 s) of the pre-trained model. When adapting the model, we update the global statistics first by exponential moving average as µg = (1− α)µg + αµ , (4) σ2 g = (1− α)σ2 g + ασ2 , (5) where (µ, σ2) is the statistics of the buffered samples in the memory bank. Then we normalize and affine the feature as Eq. (3) with (µg, σ2 g). When inferring for test samples, we directly utilize (µg, σ2 g) to calculate the output as Eq (3). Al- though simple, RBN is effective enough to tackle the prob- lem of normalization on test streams of PTTA.Category-balanced sampling with timeliness and uncer- tainty (CSTU). In the PTTA setup, the correlation among test samples Xt at time t leads to a deviation between the observed distribution bPtest and the test distribution Ptest. Specifically, the marginal label distribution p(y|t) tends to differ from p(y). Continuously learning with Xt over time t can lead to model adaptation to an unreliable distribution bPtest, resulting in ineffective adaptation and an increased risk of model collapse. To address this issue, we propose a category-balanced memory bank M with a capacity of N, which takes into account the timeliness and uncertainty of samples when up- dating. In particular, we adopt the predictions of test sam- ples as pseudo labels to guide the update ofM. Meanwhile, to guarantee the balance among categories, we distribute the capacity of M equally to each category, and samples of the major categories will be replaced first (refer to lines 5-9 in Algorithm 1). Furthermore, due to the continually changing test distribution, old samples in M are limited in value, and could even impair the ability of the model to adapt to the current distribution. Additionally, samples of high uncer- tainty always produce erroneous gradient information that can hinder model adaptation, as suggested by [55]. With this in mind, we attach each sample in M with a group of heuristics (A, U), where A, initialized as 0 and in- creasing with time t, is the age of the sample, and U the un- certainty calculated as the entropy of the prediction. Next, we combine the timeliness and uncertainty to calculate a heuristic score, i.e., category-balanced sampling with time- liness and uncertainty (CSTU), as follows: H = λt 1 1 + exp(−A/N) + λu U log C , (6) where λt and λu make the trade-off between timeliness and uncertainty, and for simplicity, λt and λu are set to 1.0 for all experiments, andC is the number of categories. We sum- marize our sampling algorithm in Algorithm 1. With CSTU, we can obtain a robust snapshot of the current test distribu- tion Ptest, and effectively adapt the model to it. Robust training with timeliness. Actually, after replacing BN layers with our RBN and obtaining the memory bank selected via CSTU, we can directly adopt the widely used techniques like pseudo labeling or entropy minimization to perform test-time adaptation. However, we notice that too old or unreliable instances still have the opportunity to stay in M since keeping the category balance is assigned the top priority. In addition, too aggressive updates of the model will make the category balance ofM unreliable, resulting in unstable adaptation. Meanwhile, error accumulation caused by the distribution change also makes the aforementioned approaches unworkable. To further reduce the risk of error gradients information from old and unreliable instances and stabilize the adapta- tion, we turn to the robust unsupervised learning method Algorithm 1: CSTU for one test sample. 1 Input: a test sample x and the teacher model fθT . 2 Define: memory bank M and its capacity N, number of classes C, per class occupation O ∈RC, total occupation Ω, classes to pop instance D. 3 Infer as p(y|x) =Softmax(fθT (x)). 4 Calculate the predicted category of x as ˆy = arg maxc p(c|x), the uncertainty as Ux = −PC c=1 p(c|x) log(p(c|x)), the age as Ax = 0, and the heuristic score Hx of x with Eq (6) 5 if Oˆy < N C then 6 if Ω <N: Search range D = ∅. 7 else: Search range D = {j|j = arg maxc Oc} 8 else 9 Search range D = {ˆy} 10 if D is ∅ then 11 Add (x, ˆy, Hx, Ux) into M. 12 else 13 Find the instance (ˆx, yˆx, Aˆx, Uˆx) with the highest value in Eq (6) Hˆx among D. 14 if Hx < Hˆx then 15 Remove (ˆx, yˆx, Aˆx, Uˆx) from M. 16 Add (x, ˆy, Hx, Ux) into M. 17 else 18 Discard x. 19 Increase the age of all instances in M. teacher-student model and propose a timeliness reweight- ing strategy. In addition, for the sake of time efficiency and stability, only affine parameters in RBN are trained during adaptation. At time step t, after inferring for the correlated data Xt with the teacher model fθT t and updating the memory bank M with Xt, we begin updating the student model fθS t and the teacher model fθT t . Firstly, we update parameters of stu- dent model θS t → θS t+1 by minimizing the following loss: Lr = 1 Ω ΩX i=1 L(xM i , Ai; θT t , θS t ) , (7) where Ω = |M| is the total occupation of the memory bank, and xM i and Ai(i = 1, ..., Ω) are instances in the memory bank and their age respectively. Subsequently, the teacher model is updated by exponential moving average as θT t+1 = (1− ν)θT t + νθS t+1 . (8) To calculate the loss value of an instancexM i from the mem- ory bank, the timeliness reweighting term is computed as E(Ai) = exp(−Ai/N) 1 + exp(−Ai/N) , (9)where Ai is the age of xM i , and N is the capacity of the bank. And then we calculate the cross entropy between the soft-max prediction pS(y|x′′ i ) of the strong-augmented view x′′ i from the student model and that pT (y|x′ i) of the weak- augmented view 1 x′ i from the teacher model as follows: ℓ(x′ i, x′′ i ) =−1 C CX c=1 pT (c|x′ i) logpS(c|x′′ i ) . (10) Finally, equipped with Eq. (9) and Eq. (10), the right-hand side of Eq. (7) reduces to L(xM i , Ai; θT t , θS t ) =E(Ai)ℓ(x′ i, x′′ i ) . (11) To sum up, equipped with RBN, CSTU, and robust training with timeliness, our RoTTA is capable of effectively adapt- ing any pre-trained models in dynamic scenarios. 4. Experiments 4.1. Setup Datasets. CIFAR10-C and CIFAR100-C [23] are the com- monly used TTA benchmarks to testify the robustness un- der corruptions. Both of them are obtained by applying 15 kinds of corruption with 5 different degrees of severity on their clean test images of original datasets CIFAR10 and CIFAR100 respectively. CIFAR10/CIFAR100 [32] have 50,000/10,000 training/test images, all of which fall into 10/100 categories. DomainNet [58] is the largest and hard- est dataset to date for domain adaptation and consists of about 0.6 million images with 345 classes. It consists of six different domains including Clipart (clp), Infograph (inf), Painting (pnt), Quickdraw (qdr), Real (rel), and Sketch (skt). We first pre-train a source model on the train set in one of six domains and testify all baseline methods on the test set of the remaining five domains. Implementation details. All experiments are conducted with PyTorch [57] framework. In the case of robustness to corruption, following the previous methods [55, 70, 73], we obtain the pre-trained model from RobustBench bench- mark [12], including the WildResNet-28 [80] for CIFAR10 → CIFAR10-C, and the ResNeXt-29 [76] for CIFAR100 → CIFAR100-C. Then, we change the test corruption at the highest severity 5 one by one to simulate that the test distri- bution continually changes with time in PTTA. And in the case of generalization under the huge domain gap, we train a ResNet-101 [22] by standard classification loss for each domain in DomainNet and adapt them continually to differ- ent domains except the source domain. Meanwhile, we uti- lize the Dirichlet distribution to simulate the correlatively sampled test stream for all datasets. For optimization, we adopt Adam [30] optimizer with learning rate 1.0 × 10−3, 1Weak augmentation is ReSize+CenterCrop. Strong augmentation is a combination nine operations like Clip, ColorJitter, and RandomAffine. β = 0.9. For a fair comparison, we set the batch size for all methods as 64 and the capacity of the memory bank of RoTTA as N = 64. Concerning the hyperparameters, we adopt a unified set of values for RoTTA across all experi- ments including α = 0.05, ν = 0.001, λt = 1.0, λu = 1.0, and δ = 0.1. More details are provided in the appendix. 4.2. Comparisons with the State-of-the-arts Robustness under corruptions. The classification error on CIFAR10→CIFAR10-C and CIFAR100→CIFAR100-C are shown in Table 2 and Table 3 respectively. We change the type of the current corruption at the highest severity 5 as time goes on, and sample data correlatively for infer- ence and adaptation simultaneously. The same test stream is shared across all compared methods. From Table 2 and Table 3, we can see that RoTTA achieves the best performance compared to previous meth- ods. Moreover, RoTTA has a significant performance gain to the second-best method that 5.9% improvement on CIFAR10 →CIFAR10-C and 5.5% improvement on CIFAR100→CIFAR100-C respectively, verifying the effec- tiveness of RoTTA to adapt the model under PTTA. In more detail, we can observe that BN [53], PL [39], TENT [70] and CoTTA [73] negatively adapt the model to the test streams of both datasets compared to Source (−6.5 ∼ −46.4%). This is attributed to the fact that these methods overlook the issues posed by correlation sampling, which can result in highly correlated data within a batch. As a consequence, traditional normalization statistics may be ineffective in appropriately normalizing the feature maps. Equipped with RBN and CSTU, RoTTA no longer suffers from this issue. Meanwhile, in Table 3, if focus on the adaptation procedure, we can see that the performance of PL [39], TENT [70] and NOTE [19] becomes worse and worse, and eventually, the model even collapses (error rate > 97%). This reveals that the impact of error accumula- tion on long-term adaptation can be catastrophic. To tackle this problem, RoTTA turns to robustly adapt the model with timeliness reweighting and confident samples in the mem- ory bank, and superior performance throughout the adapta- tion process demonstrates its effectiveness. In addition, we find that although LAME [5] never tunes the parameters of the model, it is still a competi- tive baseline for example it achieves the second-best result on CIFAR100→CIFAR100-C. However, its performance is very dependent on the performance of the pre-trained model e.g. negligible improvement on difficult corruptions (shot, gaussian, pixelate). On the contrary, our RoTTA is more flexible and achieves better and more robust results. Generalization under domain shift. We also evalu- ate RoTTA under a more challenging dataset DomainNet, where we continually adapt a source pre-trained model to correlatively sampled test streams of the rest domains. AsTable 2. Average classification error of the task CIFAR10→ CIFAR10-C while continually adapting to different corruptions at the highest severity 5 with correlatively sampled test stream under the proposed setup PTTA. Time t− − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − → Method motionsnow fog shot defocuscontrastzoom brightnessfrost elasticglass gaussianpixelatejpeg impulse Avg. Source 34.8 25.1 26.0 65.7 46.9 46.7 42.0 9.3 41.3 26.6 54.3 72.3 58.5 30.3 72.9 43.5BN [53] 73.2 73.4 72.7 77.2 73.7 72.5 72.9 71.0 74.1 77.7 80.0 76.9 75.5 78.3 79.0 75.2PL [39] 73.9 75.0 75.6 81.0 79.9 80.6 82.0 83.2 85.3 87.3 88.3 87.5 87.5 87.5 88.2 82.9TENT [70] 74.3 77.4 80.1 86.2 86.7 87.3 87.9 87.4 88.2 89.0 89.2 89.0 88.3 89.7 89.2 86.0LAME [5] 29.5 19.0 20.3 65.3 42.4 43.4 36.8 5.4 37.2 18.6 51.2 73.2 57.0 22.6 71.3 39.5CoTTA [73]77.1 80.6 83.1 84.4 83.9 84.2 83.1 82.6 84.4 84.2 84.5 84.6 82.7 83.8 84.9 83.2NOTE [19] 18.0 22.1 20.6 35.6 26.9 13.6 26.5 17.3 27.2 37.0 48.3 38.8 42.6 41.9 49.7 31.1 RoTTA 18.1 21.3 18.8 33.6 23.6 16.5 15.1 11.2 21.9 30.7 39.6 26.8 33.7 27.8 39.5 25.2(+5.9) Table 3. Average classification error of the task CIFAR100 → CIFAR100-C while continually adapting to different corruptions at the highest severity 5 with correlatively sampled test stream under the proposed setup PTTA. Time t− − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − → Method motionsnow fog shot defocuscontrastzoom brightnessfrost elasticglass gaussianpixelatejpeg impulse Avg. Source 30.8 39.5 50.3 68.0 29.3 55.1 28.8 29.5 45.8 37.2 54.1 73.0 74.7 41.2 39.4 46.4BN [53] 48.5 54.0 58.9 56.2 46.4 48.0 47.0 45.4 52.9 53.4 57.1 58.2 51.7 57.1 58.8 52.9PL [39] 50.6 62.1 73.9 87.8 90.8 96.0 94.8 96.4 97.4 97.2 97.4 97.4 97.3 97.4 97.4 88.9TENT [70] 53.3 77.6 93.0 96.5 96.7 97.5 97.1 97.5 97.3 97.2 97.1 97.7 97.6 98.0 98.3 92.8LAME [5] 22.4 30.4 43.9 66.3 21.3 51.7 20.6 21.8 39.6 28.0 48.7 72.8 74.6 33.1 32.3 40.5CoTTA [73]49.2 52.7 56.8 53.0 48.7 51.7 49.4 48.7 52.5 52.2 54.3 54.9 49.6 53.4 56.2 52.2NOTE [19] 45.7 53.0 58.2 65.6 54.2 52.0 59.8 63.5 74.8 91.8 98.1 98.3 96.8 97.0 98.2 73.8 RoTTA 31.8 36.7 40.9 42.1 30.0 33.6 27.9 25.4 32.3 34.0 38.8 38.7 31.3 38.0 42.9 35.0(+5.5) Table 4. Average classification error of DomainNet while continually adapting to different domains with correlatively sampled test stream. Timet− − − − − − − − − − − − − − − − − − →Timet− − − − − − − − − − − − − − − − − − →Timet− − − − − − − − − − − − − − − − − − →Timet− − − − − − − − − − − − − − − − − − →Sourceclp inf pnt qdr rel sktAvg. BN clp inf pnt qdr rel sktAvg. PL clp inf pnt qdr rel sktAvg.TENTclp inf pnt qdr rel sktAvg. clp N/A 83.9 65.4 88.6 48.0 59.1 69.0clp N/A 88.6 70.7 90.5 65.4 67.0 76.5clp N/A 94.5 98.9 99.5 99.7 99.7 98.5clp N/A 87.5 71.9 94.2 96.2 98.9 89.7inf 61.8 N/A 66.9 96.0 50.0 70.6 69.1inf 68.6 N/A 74.2 96.2 69.9 76.8 77.1inf 82.6 N/A 99.2 99.6 99.7 99.3 96.1inf 68.6 N/A 75.0 97.3 95.9 98.7 87.1pnt 56.5 83.7 N/A 94.2 42.6 63.4 68.1pnt 60.8 87.9 N/A 94.3 62.3 68.7 74.8pnt 78.6 99.4 N/A 99.7 99.6 99.7 95.4pnt 61.7 87.1 N/A 96.4 95.3 98.8 87.8qdr 89.2 99.0 98.6 N/A 95.0 92.3 94.8qdr 80.3 97.7 92.6 N/A 88.7 88.1 89.5qdr 81.7 99.5 99.6 N/A 99.7 99.8 96.1qdr 78.9 97.1 91.6 N/A 89.2 88.7 89.1rel 49.4 80.4 51.5 93.4 N/A 63.3 67.6rel 57.9 87.1 63.1 94.3 N/A 70.8 74.6rel 73.5 99.4 99.2 99.6 N/A 99.7 94.3rel 57.8 86.4 68.1 96.9 N/A 96.7 81.2skt 47.5 88.2 62.9 87.1 51.8 N/A 67.5skt 50.4 87.6 64.6 89.6 63.1 N/A 71.1skt 64.8 99.2 99.4 99.7 99.7 N/A 92.6skt 51.9 87.2 69.1 95.3 97.3 N/A 80.1Avg.60.9 87.0 69.1 91.9 57.5 69.7 72.7Avg.63.6 89.8 73.0 93.0 69.9 74.3 77.3Avg.76.2 98.4 99.3 99.6 99.7 99.6 95.5Avg.63.8 89.0 75.1 96.0 94.8 96.4 85.8 Timet− − − − − − − − − − − − − − − − − − →Timet− − − − − − − − − − − − − − − − − − →Timet− − − − − − − − − − − − − − − − − − →Timet− − − − − − − − − − − − − − − − − − →LAMEclp inf pnt qdr rel sktAvg.COTTAclp inf pnt qdr rel sktAvg.NOTEclp inf pnt qdr rel sktAvg.RoTTAclp inf pnt qdr rel sktAvg. clp N/A 82.2 64.5 87.7 46.9 58.9 68.0clp N/A 90.6 77.9 89.3 76.3 72.7 81.4clp N/A 89.2 73.0 94.8 98.4 99.4 91.0clp N/A 85.5 62.0 82.0 49.3 59.8 67.7inf 60.1 N/A 65.7 95.4 48.5 69.4 67.8inf 74.5 N/A 82.0 95.7 80.2 81.5 82.8inf 75.4 N/A 78.7 98.7 98.1 99.5 90.1inf 61.8 N/A 63.7 91.5 52.5 67.6 67.4pnt 55.8 81.5 N/A 93.3 41.3 62.1 66.8pnt 66.3 89.8 N/A 93.4 74.0 75.4 79.8pnt 64.7 89.8 N/A 97.8 98.4 99.2 90.0pnt 53.3 84.1 N/A 89.1 47.3 61.4 67.0qdr 88.3 99.1 99.0 N/A 94.9 92.2 94.7qdr 82.3 98.2 94.6 N/A 92.5 90.1 91.5qdr 74.7 97.2 92.2 N/A 93.5 99.6 91.4qdr 77.5 97.0 89.8 N/A 80.3 82.2 85.3rel 48.0 79.3 50.1 91.6 N/A 60.2 65.8rel 64.0 90.3 73.2 93.5 N/A 77.6 79.7rel 61.3 89.2 68.9 98.8 N/A 99.2 83.5rel 49.1 82.3 50.3 88.0 N/A 61.1 66.2skt 45.6 87.1 59.5 83.9 49.9 N/A 65.2skt 56.1 89.2 71.9 89.2 73.5 N/A 76.0skt 55.2 89.7 70.1 96.9 98.3 N/A 82.0skt 42.6 83.7 54.4 80.9 47.5 N/A 61.8Avg.59.6 85.8 67.8 90.4 56.3 68.6 71.4Avg.68.6 91.6 79.9 92.2 79.3 79.5 81.9Avg.66.3 91.0 76.6 97.4 97.3 99.4 88.0Avg.56.8 86.5 64.0 86.3 55.4 66.469.2(+2.2) shown in Table 4, consistent with the previous analysis, most of the methods include BN [53], PL [39], TENT [70], CoTTA [73] and NOTE [19] even perform worse than the Source model ( −4.6 ∼ −22.8%). RoTTA consistently achieves the best performance and has 2.2% gain than the second method LAME [5], demonstrating RoTTA’s effec- tiveness again. 4.3. Ablation Study Effect of each component. To further investigate the effi- cacy of each component, we replace each part with the nor- mally used solutions to obtain three variants: (1) RoTTA w/o RBN, replace RBN with test-time BN in TENT [70]; (2) RoTTA w/o CSTU, directly adapt the model on test stream; (3) RoTTA w/o robust training (RT), directly adapt the model only with entropy minimization. As shown in Table 5, we can observe that significant performance degra- dation occurs for all variants, proving that every part of our proposed method is valid for PTTA. Take one com- ponent for a detailed example, without RBN robustly nor- malizing feature maps, the performance of RoTTA drops 50.2% and 16.3% on CIFAR10-C and CIFAR100-C respec- tively, proving that RBN is robust enough to tackle the prob- lem of normalization of correlatively sampled data streams. CSTU enables RoTTA to adapt to a more stable distribu- tion by maintaining a timely and confident snapshot of the test distribution. Meanwhile, robust training with timeliness greatly reduces the accumulation of errors. Every compo- nent behaves significantly to enable effective adaptation un- der PTTA. Effect of the distribution changing order. To exclude the effect of a fixed order of distribution changing, we con- ducted experiments on ten different sequences of changes on CIFAR10-C and CIFAR100-C with independently andBN PL TENT LAME CoTTA NOTE RoTTA0 10 20 30 40 50 60 70 80Classification error (%) Source CIFAR-10  CIFAR-10-C Independent Correlative (a) CIFAR10-C. BN PL TENT LAME CoTTA NOTE RoTTA0 20 40 60 80Classification error (%) Source CIFAR-100  CIFAR-100-C Independent Correlative (b) CIFAR100-C. uniform 10 1 0.1 0.01 0.001 30 40 50 60 70 80 90 100Classification error (%) Source BN PL TENT LAME CoTTA NOTE RoTTA (c) δ. 16 32 64 128 256 512 40 50 60 70 80 90 100Classification error (%) Source BN PL TENT LAME CoTTA NOTE RoTTA (d) Batch size. Figure 4. (a) & (b) we adapt the model continually to different corruptions of 10 different orders with independently and correlatively sampled test streams on CIFAR10-C and CFAR100-C respectively and report their average classification error. (c) & (d) we verify the effect of δ and batch size to different methods on CIFAR100-C respectively. Table 5. Classification error of different variants of our RoTTA. Variant CIFAR10-C CIFAR100-C Avg. RoTTA w/o RBN 75.4 51.3 63.4 RoTTA w/o CSTU 47.1 46.3 46.7 RoTTA w/o RT 78.2 95.0 81.6 RoTTA 25.2 35.0 30.1 correlatively sampled test streams respectively. As shown in Figure 4a and 4b, no matter what kind of setup, RoTTA can achieve excellent results. The detailed results on the correlatively sampled test streams are shown in Table 6, RoTTA achieves 4.3% and 4.7% progress on CIFAR10- C and CIFAR100-C respectively. This shows that RoTTA can adapt the model robustly and effectively in long-term scenarios where distribution continually changes and test streams are sampled either independently or correlatively, making it a good choice for model deployment. Effect of Dirichlet concentration parameter δ. We vary the value of δ on CIFAR100-C and compare RoTTA with other approaches in Figure 4c. As the value of δ increases, the performance of BN [53], PL [39], TENT [70] and CoTTA [73] drops quickly, because they never consider the increasing correlation among test samples. NOTE [19] is stable to correlatively sampled test streams but does not consider the distribution changing, causing ineffective adaptation. Meanwhile, the higher correlation between test samples will make the propagation of labels more accurate, which is why the result of LAME [5] slightly improves. Fi- nally, excellent and stable results once again prove the sta- bility and effectiveness of RoTTA. Effect of batch size. In real scenarios, considering deploy- ment environments may use different test batch sizes, we conduct experiments with different values of test batch sizes and results are shown in Figure 4d. For a fair comparison, we control the frequency of updating the model of RoTTA so that the number of samples involved in back-propagation is the same. As the batch size increases, we can see that all of the compared methods have a significant improvement except for lame which has a slight decrease. This is be- cause the number of categories in a batch increases with the Table 6. Average classification error of tasks CIFAR10 → CIFAR10-C and CIFAR100 → CIFAR100-C while continually adapting to different corruptions of 10 different orders at the high- est severity 5 with correlatively sampled test stream. Method CIFAR10-C CIFAR100-C Avg. Source 43.5 46.4 46.9 BN [53] 75.2 52.9 64.1 PL [39] 75.2 52.9 60.1 TENT [70] 82.3 93.2 87.8 LAME [5] 39.5 40.6 40.1 NOTE [19] 30.5 76.1 53.3 CoTTA [73] 83.1 52.8 67.9 RoTTA 26.2(+4.3) 35.9(+4.7) 31.1(+9.0) increasing batch size, causing the overall correlation to be- come lower but the propagation of labels to become more difficult. Most significantly, RoTTA achieves the best re- sults across different batch sizes, demonstrating its robust- ness in dynamic scenarios once again. 5. Conclusion This work proposes a more realistic TTA setting where distribution changing and correlative sampling occur si- multaneously at the test phase, namely Practical Test-Time Adaptation (PTTA). To tackle the problems of PTTA, we propose Robust Test-Time Adaptation (RoTTA) method against the complex data stream. More specifically, a group of robust statistics for the normalization of feature maps is estimated by robust batch normalization. Meanwhile, a memory bank is adopted to capture a snapshot of the test distribution by category-balanced sampling with consider- ing timeliness and uncertainty. Further, we develop a time- aware reweighting strategy with a teacher-student model to stabilize the adaptation process. Extensive experiments and ablation studies are conducted to verify the robustness and effectiveness of the proposed method. We believe this work will pave the way for thinking about adapting models into real-world applications by test-time adaptation algorithm. Acknowledgements. This paper was supported by National Key R&D Program of China (No. 2021YFB3301503), and also supported by the National Natural Science Foundation of China under Grant No. 61902028.References [1] Rahaf Aljundi, Min Lin, Baptiste Goujaud, and Yoshua Ben- gio. Gradient based sample selection for online continual learning. In NeurIPS, pages 11816–11825, 2019. 3 [2] Fatemeh Azimi, Sebastian Palacio, Federico Raue, J ¨orn Hees, Luca Bertinetto, and Andreas Dengel. Self-supervised test-time adaptation on video data. In WACV, pages 2603– 2612, 2022. 1, 3 [3] Mathilde Bateson, Herve Lombaert, and Ismail Ben Ayed. Test-time adaptation with shape moments for image segmen- tation. In MICCAI, pages 736–745, 2022. 1 [4] Gilles Blanchard, Gyemin Lee, and Clayton Scott. General- izing from several related classification tasks to a new unla- beled sample. In NeurIPS, pages 2178–2186, 2011. 3 [5] Malik Boudiaf, Romain Mueller, Ismail Ben Ayed, and Luca Bertinetto. Parameter-free online test-time adaptation. In CVPR, pages 8344–8353, 2022. 2, 6, 7, 8, 13, 14, 15, 16, 17 [6] Francisco M Castro, Manuel J Mar ´ın-Jim´enez, Nicol´as Guil, Cordelia Schmid, and Karteek Alahari. End-to-end incre- mental learning. In ECCV, pages 233–248, 2018. 3 [7] Dian Chen, Dequan Wang, Trevor Darrell, and Sayna Ebrahimi. Contrastive test-time adaptation. In CVPR, pages 295–305, 2022. 1, 4 [8] Yuhua Chen, Wen Li, Christos Sakaridis, Dengxin Dai, and Luc Van Gool. Domain adaptive faster r-cnn for object de- tection in the wild. In CVPR, pages 3339–3348, 2018. 2 [9] Zhixiang Chi, Yang Wang, Yuanhao Yu, and Jin Tang. Test- time fast adaptation for dynamic scene deblurring via meta- auxiliary learning. In CVPR, pages 9137–9146, 2021. 3, 4 [10] Boris Chidlovskii, St ´ephane Clinchant, and Gabriela Csurka. Domain adaptation in the absence of source domain data. In KDD, pages 451–460, 2016. 3 [11] Sungha Choi, Seunghan Yang, Seokeon Choi, and Sun- grack Yun. Improving test-time adaptation via shift-agnostic weight regularization and nearest source prototypes. In ECCV, pages 440–458, 2022. 1 [12] Francesco Croce, Maksym Andriushchenko, Vikash Se- hwag, Edoardo Debenedetti, Nicolas Flammarion, Mung Chiang, Prateek Mittal, and Matthias Hein. Robustbench: a standardized adversarial robustness benchmark. In Neurips, 2021. 6 [13] Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner, Mostafa Dehghani, Matthias Minderer, Georg Heigold, Syl- vain Gelly, Jakob Uszkoreit, and Neil Houlsby. An image is worth 16x16 words: Transformers for image recognition at scale. In ICLR, 2021. 1 [14] Ying-Jun Du, Jun Xu, Huan Xiong, Qiang Qiu, Xiantong Zhen, Cees G. M. Snoek, and Ling Shao. Learning to learn with variational information bottleneck for domain general- ization. In ECCV, pages 200–216, 2020. 3 [15] Sayna Ebrahimi, Sercan ¨O. Arik, and Tomas Pfister. Test- time adaptation for visual document understanding. CoRR, abs/2206.07240, 2022. 1 [16] Yossi Gandelsman, Yu Sun, Xinlei Chen, and Alexei A Efros. Test-time training with masked autoencoders. In NeurIPS, 2022. 1 [17] Yaroslav Ganin, Evgeniya Ustinova, Hana Ajakan, Pas- cal Germain, Hugo Larochelle, Franc ¸ois Laviolette, Mario Marchand, and Victor S. Lempitsky. Domain-adversarial training of neural networks. J. Mach. Learn. Res., 17:59:1– 59:35, 2016. 1, 2 [18] Yunhe Gao, Xingjian Shi, Yi Zhu, Hao Wang, Zhiqiang Tang, Xiong Zhou, Mu Li, and Dimitris N. Metaxas. Vi- sual prompt tuning for test-time domain adaptation. CoRR, abs/2210.04831, 2022. 3 [19] Taesik Gong, Jongheon Jeong, Taewon Kim, Yewon Kim, Jinwoo Shin, and Sung-Ju Lee. Robust continual test- time adaptation: Instance-aware BN and prediction-balanced memory. In NeurIPS, 2022. 1, 2, 3, 4, 6, 7, 8, 13, 14, 15, 16, 17 [20] Sachin Goyal, Mingjie Sun, Aditi Raghunathan, and J Zico Kolter. Test time adaptation via conjugate pseudo-labels. In NeurIPS, 2022. 1, 3, 4 [21] Yves Grandvalet and Yoshua Bengio. Semi-supervised learning by entropy minimization. In NeurIPS, pages 529– 536, 2004. 3 [22] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image recognition. In CVPR, pages 770–778, 2016. 1, 6 [23] Dan Hendrycks and Thomas G. Dietterich. Benchmarking neural network robustness to common corruptions and per- turbations. In ICLR, 2019. 2, 6 [24] Hengguan Huang, Xiangming Gu, Hao Wang, Chang Xiao, Hongfu Liu, and Ye Wang. Extrapolative continuous-time bayesian neural network for fast training-free test-time adap- tation. In NeurIPS, 2022. 1 [25] Sergey Ioffe and Christian Szegedy. Batch normalization: Accelerating deep network training by reducing internal co- variate shift. In ICML, pages 448–456, 2015. 3, 4 [26] Yusuke Iwasawa and Yutaka Matsuo. Test-time classifier ad- justment module for model-agnostic domain generalization. In NeurIPS, pages 2427–2440, 2021. 1, 3 [27] Vidit Jain and Erik Learned-Miller. Online domain adapta- tion of a pre-trained cascade of classifiers. In CVPR, pages 577–584, 2011. 3 [28] Minguk Jang and Sae-Young Chung. Test-time adaptation via self-training with nearest neighbor information. CoRR, abs/2207.10792, 2022. 3, 4 [29] Junho Kim, Inwoo Hwang, and Young Min Kim. Ev-tta: Test-time adaptation for event-based object recognition. In CVPR, pages 17724–17733, 2022. 1 [30] Diederik P. Kingma and Jimmy Ba. Adam: A method for stochastic optimization. In ICLR, 2015. 6 [31] James Kirkpatrick, Razvan Pascanu, Neil C. Rabinowitz, Joel Veness, Guillaume Desjardins, Andrei A. Rusu, Kieran Milan, John Quan, Tiago Ramalho, Agnieszka Grabska- Barwinska, Demis Hassabis, Claudia Clopath, Dharshan Ku- maran, and Raia Hadsell. Overcoming catastrophic forget- ting in neural networks. CoRR, abs/1612.00796, 2016. 3 [32] Alex Krizhevsky, Geoffrey Hinton, et al. Learning multiple layers of features from tiny images. 2009. 6[33] Alex Krizhevsky, Ilya Sutskever, and Geoffrey E. Hinton. Imagenet classification with deep convolutional neural net- works. In NeurIPS, pages 1097–1105, 2012. 1 [34] Ananya Kumar, Tengyu Ma, and Percy Liang. Understand- ing self-training for gradual domain adaptation. In ICML, pages 5468–5479, 2020. 3 [35] Jogendra Nath Kundu, Naveen Venkat, Rahul M. V ., and R. Venkatesh Babu. Universal source-free domain adapta- tion. In CVPR, pages 4543–4552, 2020. 3 [36] Vinod K Kurmi, Venkatesh K Subramanian, and Vinay P Namboodiri. Domain impression: A source data free do- main adaptation method. In WACV, pages 615–625, 2021. 3 [37] Matthias De Lange, Rahaf Aljundi, Marc Masana, Sarah Parisot, Xu Jia, Ales Leonardis, Gregory G. Slabaugh, and Tinne Tuytelaars. A continual learning survey: Defying for- getting in classification tasks. IEEE Trans. Pattern Anal. Mach. Intell., 44(7):3366–3385, 2022. 3 [38] Yann LeCun, Yoshua Bengio, and Geoffrey E. Hinton. Deep learning. Nat., 521(7553):436–444, 2015. 1 [39] Dong-Hyun Lee et al. Pseudo-label: The simple and effi- cient semi-supervised learning method for deep neural net- works. In Workshop on challenges in representation learn- ing, ICML, volume 3, page 896, 2013. 6, 7, 8, 12, 14, 15, 16, 17 [40] Da Li, Yongxin Yang, Yi-Zhe Song, and Timothy M. Hospedales. Learning to generalize: Meta-learning for do- main generalization. In AAAI, pages 3490–3497, 2018. 1, 3 [41] Haoliang Li, Sinno Jialin Pan, Shiqi Wang, and Alex C. Kot. Domain generalization with adversarial feature learning. In CVPR, pages 5400–5409, 2018. 1, 3 [42] Shuang Li, Binhui Xie, Qiuxia Lin, Chi Harold Liu, Gao Huang, and Guoren Wang. Generalized domain conditioned adaptation network. IEEE Trans. Pattern Anal. Mach. Intell., 44(8):4093–4109, 2022. 1 [43] Shuang Li, Mixue Xie, Kaixiong Gong, Chi Harold Liu, Yulin Wang, and Wei Li. Transferable semantic augmen- tation for domain adaptation. In CVPR, pages 11516–11525, 2021. 2 [44] Zhizhong Li and Derek Hoiem. Learning without forgetting. IEEE Trans. Pattern Anal. Mach. Intell., 40(12):2935–2947, 2018. 3 [45] Jian Liang, Dapeng Hu, and Jiashi Feng. Do we really need to access the source data? source hypothesis transfer for un- supervised domain adaptation. In ICML, pages 6028–6039, 2020. 1, 3 [46] Yuejiang Liu, Parth Kothari, Bastien van Delft, Baptiste Bellot-Gurlet, Taylor Mordan, and Alexandre Alahi. TTT++: when does self-supervised test-time training fail or thrive? In NeurIPS, pages 21808–21820, 2021. 3 [47] Yuang Liu, Wei Zhang, and Jun Wang. Source-free do- main adaptation for semantic segmentation. In CVPR, pages 1215–1224, 2021. 3 [48] Mingsheng Long, Yue Cao, Zhangjie Cao, Jianmin Wang, and Michael I. Jordan. Transferable representation learning with deep adaptation networks. IEEE Trans. Pattern Anal. Mach. Intell., 41(12):3071–3085, 2019. 1, 2 [49] Wenao Ma, Cheng Chen, Shuang Zheng, Jing Qin, Huimao Zhang, and Qi Dou. Test-time adaptation with calibration of medical image classification nets for label distribution shift. In MICCAI, pages 313–323, 2022. 1 [50] Divyat Mahajan, Shruti Tople, and Amit Sharma. Domain generalization using causal matching. In ICML, pages 7313– 7324, 2021. 3 [51] Yishay Mansour, Mehryar Mohri, and Afshin Rostamizadeh. Domain adaptation: Learning bounds and algorithms. In COLT, 2009. 2 [52] Krikamol Muandet, David Balduzzi, and Bernhard Sch¨olkopf. Domain generalization via invariant fea- ture representation. In ICML, pages 10–18, 2013. 1, 3 [53] Zachary Nado, Shreyas Padhy, D. Sculley, Alexander D’Amour, Balaji Lakshminarayanan, and Jasper Snoek. Evaluating prediction-time batch normalization for robust- ness under covariate shift. CoRR, abs/2006.10963, 2020. 4, 6, 7, 8, 12, 14, 15, 16, 17 [54] Shuaicheng Niu, Jiaxiang Wu, Yifan Zhang, Yaofo Chen, Shijian Zheng, Peilin Zhao, and Mingkui Tan. Efficient test- time model adaptation without forgetting. In ICML, pages 16888–16905, 2022. 1 [55] Shuaicheng Niu, Jiaxiang Wu, Yifan Zhang, Yaofo Chen, Shijian Zheng, Peilin Zhao, and Mingkui Tan. Efficient test- time model adaptation without forgetting. In ICML, volume 162, pages 16888–16905, 2022. 4, 5, 6 [56] Sinno Jialin Pan and Qiang Yang. A survey on transfer learning. IEEE Trans. Knowl. Data Eng., 22(10):1345–1359, 2010. 1 [57] Adam Paszke, Sam Gross, Francisco Massa, Adam Lerer, James Bradbury, Gregory Chanan, Trevor Killeen, Zeming Lin, Natalia Gimelshein, Luca Antiga, et al. Pytorch: An imperative style, high-performance deep learning library. In NeurIPS, pages 8024–8035, 2019. 6 [58] Xingchao Peng, Qinxun Bai, Xide Xia, Zijun Huang, Kate Saenko, and Bo Wang. Moment matching for multi-source domain adaptation. In ICCV, pages 1406–1415, 2019. 2, 6 [59] Joaquin Quinonero-Candela, Masashi Sugiyama, Anton Schwaighofer, and Neil D Lawrence. Dataset shift in ma- chine learning. 2008. 1 [60] Sylvestre-Alvise Rebuffi, Alexander Kolesnikov, Georg Sperl, and Christoph H. Lampert. icarl: Incremental classi- fier and representation learning. InCVPR, pages 5533–5542, 2017. 3 [61] Amelie Royer and Christoph H Lampert. Classifier adapta- tion at prediction time. In CVPR, pages 1401–1409, 2015. 3 [62] Kuniaki Saito, Kohei Watanabe, Yoshitaka Ushiku, and Tat- suya Harada. Maximum classifier discrepancy for unsuper- vised domain adaptation. In CVPR, pages 3723–3732, 2018. 2 [63] Inkyu Shin, Yi-Hsuan Tsai, Bingbing Zhuang, Samuel Schulter, Buyu Liu, Sparsh Garg, In So Kweon, and Kuk- Jin Yoon. MM-TTA: multi-modal test-time adaptation for 3d semantic segmentation. In CVPR, pages 16907–16916, 2022. 1, 3[64] Manli Shu, Weili Nie, De-An Huang, Zhiding Yu, Tom Goldstein, Anima Anandkumar, and Chaowei Xiao. Test- time prompt tuning for zero-shot generalization in vision- language models. In NeurIPS, 2022. 1, 3 [65] Yu Sun, Xiaolong Wang, Zhuang Liu, John Miller, Alexei Efros, and Moritz Hardt. Test-time training with self- supervision for generalization under distribution shifts. In ICML, pages 9229–9248, 2020. 1, 2, 3, 4 [66] Rishabh Tiwari, KrishnaTeja Killamsetty, Rishabh K. Iyer, and Pradeep Shenoy. GCR: gradient coreset based replay buffer selection for continual learning. In CVPR, pages 99– 108, 2022. 3 [67] Yi-Hsuan Tsai, Wei-Chih Hung, Samuel Schulter, Ki- hyuk Sohn, Ming-Hsuan Yang, and Manmohan Chandraker. Learning to adapt structured output space for semantic seg- mentation. In CVPR, pages 7472–7481, 2018. 2 [68] Eric Tzeng, Judy Hoffman, Trevor Darrell, and Kate Saenko. Simultaneous deep transfer across domains and tasks. In ICCV, pages 4068–4076, 2015. 2 [69] Eric Tzeng, Judy Hoffman, Kate Saenko, and Trevor Darrell. Adversarial discriminative domain adaptation. In CVPR, pages 2962–2971, 2017. 1 [70] Dequan Wang, Evan Shelhamer, Shaoteng Liu, Bruno A. Ol- shausen, and Trevor Darrell. Tent: Fully test-time adaptation by entropy minimization. In ICLR, 2021. 1, 2, 3, 4, 6, 7, 8, 12, 13, 14, 15, 16, 17 [71] Jindong Wang, Cuiling Lan, Chang Liu, Yidong Ouyang, Tao Qin, Wang Lu, Yiqiang Chen, Wenjun Zeng, and Philip Yu. Generalizing to unseen domains: A survey on domain generalization. IEEE Trans. Knowl. Data Eng., 2022. 1 [72] Mei Wang and Weihong Deng. Deep visual domain adapta- tion: A survey. Neurocomputing, 312:135–153, 2018. 1 [73] Qin Wang, Olga Fink, Luc Van Gool, and Dengxin Dai. Con- tinual test-time domain adaptation. In CVPR, pages 7191– 7201, 2022. 1, 2, 3, 4, 6, 7, 8, 13, 14, 15, 16, 17 [74] Markus Wulfmeier, Alex Bewley, and Ingmar Posner. Incre- mental adversarial domain adaptation for continually chang- ing environments. In ICRA, pages 4489–4495, 2018. 3 [75] Binhui Xie, Shuang Li, Mingjia Li, Chi Harold Liu, Gao Huang, and Guoren Wang. Sepico: Semantic-guided pixel contrast for domain adaptive semantic segmentation. IEEE Trans. Pattern Anal. Mach. Intell., pages 1–17, 2023. 2 [76] Saining Xie, Ross Girshick, Piotr Doll ´ar, Zhuowen Tu, and Kaiming He. Aggregated residual transformations for deep neural networks. In CVPR, pages 5987–5995, 2017. 6 [77] Ruijia Xu, Guanbin Li, Jihan Yang, and Liang Lin. Larger norm more transferable: An adaptive feature norm approach for unsupervised domain adaptation. In ICCV, pages 1426– 1435, 2019. 2 [78] Zhenlin Xu, Deyi Liu, Junlin Yang, Colin Raffel, and Marc Niethammer. Robust and generalizable visual representation learning via random convolutions. In ICLR, 2021. 3 [79] Shiqi Yang, Yaxing Wang, Joost van de Weijer, Luis Herranz, and Shangling Jui. Generalized source-free domain adapta- tion. In ICCV, pages 8978–8987, 2021. 3 [80] Sergey Zagoruyko and Nikos Komodakis. Wide residual net- works. In BMVC, 2016. 6 [81] Marvin Mengxin Zhang, Sergey Levine, and Chelsea Finn. MEMO: Test time robustness via adaptation and augmenta- tion. In NeurIPS, 2022. 1, 4 [82] Yizhe Zhang, Shubhankar Borse, Hong Cai, and Fatih Porikli. Auxadapt: Stable and efficient test-time adaptation for temporally consistent video semantic segmentation. In WACV, pages 2633–2642, 2022. 1, 3 [83] Kaiyang Zhou, Ziwei Liu, Yu Qiao, Tao Xiang, and Chen Change Loy. Domain generalization: A survey. IEEE Trans. Pattern Anal. Mach. Intell., 2022. 1 [84] Kaiyang Zhou, Yongxin Yang, Yu Qiao, and Tao Xiang. Do- main generalization with mixstyle. In ICLR, 2021. 3 [85] Yang Zou, Zhiding Yu, BVK Vijaya Kumar, and Jinsong Wang. Unsupervised domain adaptation for semantic seg- mentation via class-balanced self-training. In ECCV, pages 289–305, 2018. 26. Appendix 6.1. Discussion Societal impact. RoTTA enables adapting pre-trained models on continually changing distributions with correl- atively sampled test streams without any more raw data or label requirements. Thus, our work may have a positive im- pact on communities to effectively deploy and adapt models in various real-world scenarios, which is economically and environmentally friendly. And since no training data is re- quired, this protects data privacy and has potential commer- cial value. We carry out experiments on benchmark datasets and do not notice any societal issues. It does not involve sensitive attributes. Future work. Our work suggests a few promising direc- tions for future work. Firstly, the proposed RoTTA is a preliminary attempt to perform test-time adaptation for the more realistic test stream under the setup PTTA. One could experiment to improve the algorithm by replacing some parts of RoTTA. More importantly, we hope that with this work, we can open a path to the original goal of test-time adaptation, which is performing test-time adaptation in real- world scenarios. Thus, one could improve PTTA to make it more realistic. Limitations. RoTTA achieves excellent performance on various tasks under the setup PTTA as demonstrated in Sec- tion 4 in the main paper, but we still find some limitations of it. Firstly, the adopted robust batch normalization (RBN) is a naive solution to the normalization of the correlatively sampled batch of data. This requires careful design of the value of α in RBN. Secondly, we observe that during the adaptation procedure of some methods like PL [39] and TENT [70], the model collapse finally. Although we de- sign many strategies to stabilize the adaptation and model collapse never occurs in the experiments of RoTTA, we are still missing a way to recover the model from the collapse state as a remedy. Thirdly, category similarity is only one kind of correlation. Although we conduct experiments on different datasets with Dirichlet distribution to simulate cor- relatively sampled test streams, we still need to validate our approach in some real-world scenarios. 6.2. Sensitivity to different hyper-parameters In this section, we conduct a detailed sensitivity analy- sis of the hyperparameters involved in RoTTA. All experi- ments are conducted on CIFAR100→CIFAR100-C, and the corruptions changes as motion, snow, fog, shot, defocus, contrast, zoom, brightness, frost, elastic, glass, gaussian, pixelate, jpeg, and impulse, and test streams are sampled correlatively with the Dirichlet parameter δ = 0.1. When we investigate the sensitivity to a specific hyperparameter, other hyperparameters are fixed to the default values, i.e., λt = 1.0, λu = 1.0, α = 0.05, and ν = 0.001, for all experiments. Table 7. Classification error with different value of λt/λu. λt/λu 0.0/2.0 0.5/1.5 1.0/1.0 1.5/ 0.5 2.0/ 0.0 CIFAR100-C 57.5 36.9 35.0 35.9 38.9 Trade-off between timeliness and uncertainty. When updating the memory bank, we take the timeliness and uncertainty of samples into account simultaneously, and λt and λu will make a trade-off between them. In Table 7, we show the results of RoTTA with varying λt/λu, i.e., λt/λu ∈ {0.0/2.0, 0.5/1.5, 1.0/1.0, 1.5/0.5, 2.0/0.0}. When we consider both of them, the results are relatively stable (35.0-36.9%). When we only think about one side, the performance drops significantly. For example, when we set λt/λu = 0.0/2.0 which means only considering uncer- tainty, the performance drops 22.5%. That’s because some confident samples get stuck in the memory bank, making it not work the way we design it. Table 8. Classification error with varying α α 0.5 0.1 0.05 0.01 0.005 0.001 CIFAR100-C 39.0 36.0 35.0 36.0 38.1 41.5 Sensitivity to α. We show the results of RoTTA with vary- ing α, i.e., α ∈ {0.5, 0.1, 0.05, 0.01, 0.005, 0.001} in Ta- ble 8. A larger value of α means updating the global statis- tics faster and vice versa. We can see that RoTTA achieves competitive results (35.0 − 36.0%) at appropriate values of α, i.e., α ∈ {0.1, 0.05, 0.01}. Updating too aggressively or too gently can lead to unreliable estimates of statistics. Table 9. Classification error with varying ν ν 0.05 0.01 0.005 0.001 0.0005 0.0001 CIFAR100-C 44.8 39.1 37.1 35.0 37.6 43.6 Sensitivity to ν. We show the results of RoTTA with vary- ing ν, i.e., ν ∈ {0.05, 0.01, 0.005, 0.001, 0.0005, 0.0001} in Table 9. As we can see, the best performance is achieved at ν = 0.001. Updating the teacher model too quickly or too slowly can cause performance degradation. 6.3. Additional experiment details and results 6.3.1 Compared methods BN [53] utilizes statistics of the current batch of data to nor- malize their feature maps without tuning any parameters. PL [39] is based on BN [53], and adopts pseudo labels to train the affine parameters in BN layers.TENT [70] is the first to propose fully test-time adaptation. It adopts test-time batch normalization and utilizes entropy minimization to train the affine parameters of BN layers. We reimplement it following the released code https:// github.com/DequanWang/tent. LAME [5] adapts the output of the pre-trained model by optimizing a group of latent variables without tuning any in- ner parts of the model. We reimplement it following the re- leased code https://github.com/fiveai/LAME. CoTTA [73] considers performing test-time adapta- tion on continually changing distributions and pro- pose augmentation-averaged pseudo-labels and stochastic restoration to address error accumulation and catastrophic forgetting. We reimplement it following the released code https://github.com/qinenergy/cotta. NOTE [19] proposes instance-aware normalization and prediction-balanced reservoir sampling to stable the adapta- tion on temporally correlated test streams. We reimplement it following the released code https://github.com/ TaesikGong/NOTE. 6.3.2 Simulate correlatively sampling As we described in the scenarios of autonomous driving that the car will follow more vehicles on the highway or will en- counter more pedestrians on the sidewalk, so we use the same category to simulate correlation. From a macro point of view, the test distribution Ptest changes continually as P0, P1, ...,P∞. During the period when Ptest = Pt, we adopt Dirichlet distribution to simulate correlatively sam- pled test stream. More specifically, we consider dividing samples of C classes into T slots. Firstly, we utilize Dirich- let distribution with parameter γ to generate the partition criterion q ∈ RC×T . Then for each class c, we split samples into T parts according to qc and assign each part to each slot respectively. Finally, we concatenate all slots to sim- ulate the correlatively sampled test stream for Ptest = Pt. And as Ptest changes, we use the above method again to generate the test stream. 6.3.3 Detailed results of different orders We report the average classification error of ten different distribution changing orders in Table 6 of the main pa- per. And then we present the specific results here, includ- ing Table 10, 11, 12, 13, 14, 15, 16, 17, 18, and 19 for CIFAR10→CIFAR10-C and Table 20, 21, 22, 23, 24, 25, 26, 27, 28, and 29 for CIFAR100 →CIFAR100-C. We can see consistently superior performance of RoTTA. One thing to mention is that on DomainNet we use alphabetical order to determine the order of domain changes.Table 10. Average classification error of the task CIFAR10→ CIFAR10-C while continually adapting to different corruptions at the highest severity 5 with correlatively sampled test stream under the proposed setup PTTA. Time t− − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − → Method brightnesspixelategaussianmotionzoom glass impulsejpeg defocuselasticshot frost snow fog contrast Avg. Source 9.3 58.5 72.3 34.8 42.0 54.3 72.9 30.3 46.9 26.6 65.7 41.3 25.1 26.0 46.7 43.5BN [53] 71.1 75.2 76.8 74.2 73.7 80.1 79.3 77.5 73.8 77.7 77.2 73.3 73.8 72.7 71.7 75.2PL [39] 71.7 75.9 80.2 78.4 80.2 85.2 85.3 85.4 85.1 86.7 87.9 87.9 88.1 88.3 87.9 83.6TENT [70] 71.6 75.9 81.3 80.5 82.3 85.6 87.1 87.0 87.1 88.1 88.2 87.8 87.9 88.3 88.2 84.4LAME [5] 5.4 56.8 73.1 29.1 37.0 50.5 71.4 22.3 42.8 18.6 65.5 37.3 18.8 20.4 43.6 39.5CoTTA [73] 75.0 79.8 83.1 83.4 83.2 84.0 84.5 83.2 83.5 83.3 83.6 83.0 83.0 83.4 83.7 82.6NOTE [19] 10.1 29.9 47.1 23.4 28.4 48.4 46.1 41.8 26.9 36.1 37.5 25.0 25.0 23.2 14.2 30.9 RoTTA 10.4 26.6 37.5 23.9 17.0 40.9 39.7 30.1 18.0 29.9 30.1 23.6 21.7 17.6 19.0 25.7(+5.2) Table 11. Average classification error of the task CIFAR10→ CIFAR10-C while continually adapting to different corruptions at the highest severity 5 with correlatively sampled test stream under the proposed setup PTTA. Time t− − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − → Method jpeg shot zoom frost contrastfog defocuselasticgaussianbrightnessglass impulsepixelatesnow motion Avg. Source 30.3 65.7 42.0 41.3 46.7 26.0 46.9 26.6 72.3 9.3 54.3 72.9 58.5 25.1 34.8 43.5BN [53] 77.6 75.8 73.4 74.1 73.1 72.5 72.9 77.1 77.2 72.2 79.9 79.9 75.5 74.6 72.9 75.2PL [39] 77.6 77.1 76.6 78.3 77.5 79.8 82.0 84.8 86.1 83.5 87.8 87.1 86.5 85.6 85.7 82.4TENT [70] 78.5 78.2 79.2 81.8 84.8 84.8 86.4 87.3 87.9 86.7 87.3 87.8 87.2 87.5 87.1 84.8LAME [5] 22.5 65.2 37.0 37.1 44.0 20.3 41.7 18.7 72.8 5.2 51.2 71.5 57.0 19.0 29.4 39.5CoTTA [73]78.5 81.0 82.8 84.1 84.9 83.4 83.5 83.5 84.5 83.3 84.7 84.6 83.0 84.4 83.4 83.3NOTE [19]35.4 36.1 22.1 21.3 11.6 24.8 24.5 36.0 37.7 18.4 49.0 47.4 43.9 30.4 29.2 31.2 RoTTA 33.2 33.3 19.8 24.1 24.9 20.5 16.2 31.7 28.4 11.8 43.1 36.9 32.5 20.7 20.6 26.5(+4.7) Table 12. Average classification error of the task CIFAR10→ CIFAR10-C while continually adapting to different corruptions at the highest severity 5 with correlatively sampled test stream under the proposed setup PTTA. Time t− − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − → Method contrastdefocusgaussianshot snow frost glass zoom elasticjpeg pixelatebrightnessimpulsemotion fog Avg. Source 46.7 46.9 72.3 65.7 25.1 41.3 54.3 42.0 26.6 30.3 58.5 9.3 72.9 34.8 26.0 43.5BN [53] 72.3 72.6 76.9 77.1 74.8 73.5 80.0 73.2 77.4 78.6 76.4 71.0 79.1 73.9 71.5 75.2PL [39] 72.4 75.3 80.7 82.6 83.3 83.5 86.6 85.7 86.6 88.4 87.5 86.6 88.3 88.2 86.8 84.1TENT [70] 73.5 77.9 85.5 86.9 87.6 87.8 88.3 87.7 88.6 89.2 88.5 88.5 89.3 88.6 88.6 86.4LAME [5] 43.5 42.3 73.1 65.3 19.2 37.3 51.1 36.8 18.5 22.5 56.9 5.5 71.1 29.1 20.5 39.5CoTTA [73]79.4 80.3 83.8 83.9 83.9 83.4 85.0 83.2 85.1 84.3 83.9 83.3 84.7 83.9 82.5 83.4NOTE [19] 9.6 21.8 40.1 31.0 25.5 22.6 44.8 22.8 33.2 39.4 33.2 18.1 50.0 28.3 29.8 30.0 RoTTA 18.4 17.9 38.4 31.9 23.3 19.8 40.7 17.4 31.4 29.8 27.8 11.3 43.8 19.7 18.8 26.0(+4.0) Table 13. Average classification error of the task CIFAR10→ CIFAR10-C while continually adapting to different corruptions at the highest severity 5 with correlatively sampled test stream under the proposed setup PTTA. Time t− − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − → Method shot fog glass pixelatesnow elasticbrightnessimpulsedefocusfrost contrastgaussianmotionjpeg zoom Avg. Source 65.7 26.0 54.3 58.5 25.1 26.6 9.3 72.9 46.9 41.3 46.7 72.3 34.8 30.3 42.0 43.5BN [53] 76.4 72.0 80.4 76.2 74.8 77.0 71.1 79.6 73.8 74.4 73.0 77.0 72.5 78.3 72.5 75.3PL [39] 77.0 73.3 82.4 79.8 81.0 82.3 79.5 84.4 82.7 83.5 83.5 85.5 84.8 87.0 84.5 82.1TENT [70]76.9 74.6 82.3 81.7 82.0 84.9 84.8 87.3 86.6 87.3 87.6 89.2 88.3 88.9 87.3 84.6LAME [5] 65.3 20.6 50.9 56.7 19.2 18.8 5.4 71.8 42.8 37.2 43.3 73.2 29.4 22.6 36.9 39.6CoTTA [73]77.4 77.6 83.8 81.9 82.2 82.6 80.4 83.3 82.3 81.5 82.7 82.6 81.1 82.9 81.0 81.6NOTE [19]34.0 20.9 43.1 36.6 24.0 36.4 12.1 48.0 25.9 23.9 13.4 38.1 25.0 43.2 24.2 29.9 RoTTA 35.0 21.1 43.9 29.2 22.1 29.7 10.8 44.6 25.3 22.7 24.6 29.4 26.9 34.4 16.1 27.7(+2.2) Table 14. Average classification error of the task CIFAR10→ CIFAR10-C while continually adapting to different corruptions at the highest severity 5 with correlatively sampled test stream under the proposed setup PTTA. Time t− − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − → Method pixelateglass zoomsnow fog impulsebrightnessmotionfrost jpeg gaussianshot contrastdefocus elastic Avg. Source 58.5 54.3 42.0 25.1 26.0 72.9 9.3 34.8 41.3 30.3 72.3 65.7 46.7 46.9 26.6 43.5BN [53] 76.0 79.6 73.3 75.2 72.9 79.8 71.1 73.5 74.1 78.6 77.4 76.1 72.0 73.8 76.4 75.3PL [39] 76.7 81.3 77.4 80.3 81.2 86.3 83.3 85.9 86.2 87.7 88.1 88.4 87.4 87.6 87.7 84.4TENT [70] 76.4 80.2 77.8 81.2 83.0 87.1 85.6 87.2 87.6 88.7 88.6 88.9 88.5 88.6 88.2 85.2LAME [5] 56.9 50.7 37.0 19.0 20.3 71.5 5.4 29.2 37.2 22.5 73.0 65.3 43.8 42.4 18.7 39.5CoTTA [73]77.1 83.6 84.1 84.8 84.4 85.2 84.0 84.3 84.9 84.9 85.0 84.7 85.3 84.4 84.3 84.1NOTE [19] 27.8 52.2 24.5 22.3 21.6 44.5 14.5 21.3 25.9 42.5 38.8 36.0 16.7 28.1 40.6 30.5 RoTTA 25.9 43.3 17.7 22.1 20.2 41.5 12.2 22.9 22.5 31.2 33.8 26.0 31.4 17.7 27.6 26.4(+4.1)Table 15. Average classification error of the task CIFAR10→ CIFAR10-C while continually adapting to different corruptions at the highest severity 5 with correlatively sampled test stream under the proposed setup PTTA. Time t− − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − → Method motionsnow fog shot defocuscontrastzoom brightnessfrost elasticglass gaussianpixelatejpeg impulse Avg. Source 34.8 25.1 26.0 65.7 46.9 46.7 42.0 9.3 41.3 26.6 54.3 72.3 58.5 30.3 72.9 43.5BN [53] 73.2 73.4 72.7 77.2 73.7 72.5 72.9 71.0 74.1 77.7 80.0 76.9 75.5 78.3 79.0 75.2PL [39] 73.9 75.0 75.6 81.0 79.9 80.6 82.0 83.2 85.3 87.3 88.3 87.5 87.5 87.5 88.2 82.9TENT [70] 74.3 77.4 80.1 86.2 86.7 87.3 87.9 87.4 88.2 89.0 89.2 89.0 88.3 89.7 89.2 86.0LAME [5] 29.5 19.0 20.3 65.3 42.4 43.4 36.8 5.4 37.2 18.6 51.2 73.2 57.0 22.6 71.3 39.5CoTTA [73]77.1 80.6 83.1 84.4 83.9 84.2 83.1 82.6 84.4 84.2 84.5 84.6 82.7 83.8 84.9 83.2NOTE [19] 18.0 22.1 20.6 35.6 26.9 13.6 26.5 17.3 27.2 37.0 48.3 38.8 42.6 41.9 49.7 31.1 RoTTA 18.1 21.3 18.8 33.6 23.6 16.5 15.1 11.2 21.9 30.7 39.6 26.8 33.7 27.8 39.5 25.2(+5.9) Table 16. Average classification error of the task CIFAR10→ CIFAR10-C while continually adapting to different corruptions at the highest severity 5 with correlatively sampled test stream under the proposed setup PTTA. Time t− − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − → Method frost impulsejpeg contrastzoom glass pixelatesnow defocusmotionbrightnesselasticshot fog gaussian Avg. Source 41.3 72.9 30.3 46.7 42.0 54.3 58.5 25.1 46.9 34.8 9.3 26.6 65.7 26.0 72.3 43.5BN [53] 73.8 79.1 77.9 73.0 73.7 80.1 75.7 74.4 73.7 74.0 71.7 77.0 75.9 72.8 76.2 75.3PL [39] 74.2 80.9 80.4 79.5 81.8 85.9 83.9 85.1 84.7 85.9 85.9 86.7 87.2 87.0 87.8 83.8TENT [70]73.9 80.3 81.8 81.6 83.6 86.3 85.6 85.7 86.4 87.7 87.4 88.8 88.8 88.5 88.4 85.0LAME [5] 37.4 71.8 22.4 43.5 37.0 50.5 57.0 19.0 42.8 29.1 5.4 18.7 65.2 20.4 72.9 39.5CoTTA [73]76.5 82.2 82.8 85.0 82.9 85.0 83.0 82.9 83.5 83.4 82.6 83.7 83.2 83.3 83.6 82.9NOTE [19]21.1 41.4 36.3 10.2 21.7 46.7 37.5 26.4 26.1 21.4 14.3 37.9 38.5 24.4 40.7 29.6 RoTTA 22.2 44.9 35.2 18.8 19.7 41.5 28.5 23.2 21.2 18.6 12.4 30.0 27.4 20.0 31.2 26.3(+3.3) Table 17. Average classification error of the task CIFAR10→ CIFAR10-C while continually adapting to different corruptions at the highest severity 5 with correlatively sampled test stream under the proposed setup PTTA. Time t− − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − → Method defocusmotionzoom shot gaussianglass jpeg fog contrastpixelatefrost snow brightnesselastic impulse Avg. Source 46.9 34.8 42.0 65.7 72.3 54.3 30.3 26.0 46.7 58.5 41.3 25.1 9.3 26.6 72.9 43.5BN [53] 72.8 72.7 73.3 77.2 77.3 80.0 77.6 72.6 73.3 76.6 73.8 74.1 70.3 77.5 79.0 75.2PL [39] 73.2 74.6 76.5 81.7 82.8 84.6 85.1 84.6 86.2 86.4 86.1 87.1 86.8 88.4 88.1 83.5TENT [70] 73.7 74.3 77.1 82.5 84.3 86.9 87.4 86.6 88.0 88.5 88.1 88.5 88.4 89.4 88.9 84.8LAME [5] 42.5 29.3 37.0 65.3 73.2 50.5 22.5 20.5 43.5 56.9 37.1 18.9 5.4 18.5 71.3 39.5CoTTA [73]76.3 79.8 82.4 83.3 83.8 84.5 83.1 82.7 84.7 82.9 83.0 83.3 81.4 83.8 83.8 82.6NOTE [19] 18.5 18.8 23.6 36.5 33.7 47.8 38.6 22.8 13.0 40.0 29.2 26.3 17.5 44.0 52.9 30.9 RoTTA 17.0 17.5 16.5 33.8 33.3 42.7 29.4 18.0 19.6 29.5 20.7 22.1 11.5 29.5 38.1 25.3(+5.6) Table 18. Average classification error of the task CIFAR10→ CIFAR10-C while continually adapting to different corruptions at the highest severity 5 with correlatively sampled test stream under the proposed setup PTTA. Time t− − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − → Method glass zoom impulsefog snow jpeg gaussianfrost shot brightnesscontrastmotionpixelatedefocus elastic Avg. Source 54.3 42.0 72.9 26.0 25.1 30.3 72.3 41.3 65.7 9.3 46.7 34.8 58.5 46.9 26.6 43.5BN [53] 79.7 72.3 79.8 73.2 74.7 77.7 76.6 73.2 77.1 72.2 73.0 73.3 75.5 73.8 76.4 75.2PL [39] 79.6 73.2 81.3 77.3 79.1 83.0 83.2 83.0 85.5 84.3 87.0 86.9 86.4 86.5 87.6 82.9TENT [70] 79.5 74.1 84.2 82.2 84.5 86.5 86.7 85.9 87.2 86.6 86.8 87.3 86.9 87.4 87.3 84.9LAME [5] 50.8 36.9 71.3 20.6 19.2 22.4 72.5 37.2 65.4 5.2 43.3 29.1 57.0 42.4 18.7 39.5CoTTA [73]81.5 79.4 85.2 84.1 84.5 84.2 84.8 84.0 84.8 83.2 85.2 83.8 83.2 84.6 83.6 83.7NOTE [19]45.0 21.2 42.3 21.0 21.6 38.4 36.4 21.4 33.1 16.7 14.6 25.4 43.5 29.1 38.5 29.9 RoTTA 42.6 17.6 48.1 23.9 21.9 32.6 32.1 20.7 30.2 12.0 21.9 20.0 33.7 16.4 28.1 26.8(+3.1) Table 19. Average classification error of the task CIFAR10→ CIFAR10-C while continually adapting to different corruptions at the highest severity 5 with correlatively sampled test stream under the proposed setup PTTA. Time t− − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − → Method contrastgaussiandefocuszoom frost glass jpeg fog pixelateelasticshot impulsesnow motion brightness Avg. Source 46.7 72.3 46.9 42.0 41.3 54.3 30.3 26.0 58.5 26.6 65.7 72.9 25.1 34.8 9.3 43.5BN [53] 72.4 76.2 73.2 73.7 73.6 80.0 77.6 72.6 76.4 77.7 77.2 79.9 73.8 73.9 70.0 75.2PL [39] 73.0 78.2 76.7 79.7 81.6 85.6 86.0 85.3 87.2 88.2 88.3 88.9 88.5 89.2 88.2 84.3TENT [70] 73.6 80.9 83.1 85.6 87.1 88.5 88.8 88.4 89.2 89.3 89.0 89.0 89.3 89.9 89.1 86.7LAME [5] 43.5 73.2 42.3 37.0 37.2 50.5 22.5 20.5 57.0 18.6 65.5 71.5 18.8 29.1 5.6 39.5CoTTA [73]79.5 81.4 83.4 83.6 83.9 85.0 84.0 82.8 84.8 84.8 84.5 84.7 84.1 84.4 82.8 83.6NOTE [19] 9.6 43.6 26.5 24.8 23.9 46.9 38.0 23.4 34.0 41.2 41.5 45.0 27.6 25.8 19.0 31.4 RoTTA 18.4 36.0 21.1 15.6 23.0 41.7 30.8 19.1 34.1 31.1 31.3 39.9 26.0 18.8 12.8 26.6(+4.8)Table 20. Average classification error of the task CIFAR100 → CIFAR100-C while continually adapting to different corruptions at the highest severity 5 with correlatively sampled test stream under the proposed setup PTTA. Time t− − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − → Method brightnesspixelategaussianmotionzoom glass impulsejpeg defocuselasticshot frost snow fog contrast Avg. Source 29.5 74.7 73.0 30.8 28.8 54.1 39.4 41.2 29.3 37.2 68.0 45.8 39.5 50.3 55.1 46.4BN [53] 46.5 52.0 58.6 47.4 47.4 57.6 58.2 56.9 47.0 53.4 56.0 52.5 53.1 57.7 49.1 52.9PL [39] 48.5 60.7 77.1 85.9 91.5 95.5 95.8 96.6 96.8 96.9 97.3 97.5 97.6 97.7 97.9 88.9TENT [70] 49.8 69.4 92.2 96.0 96.7 97.3 97.5 97.9 97.5 97.9 98.0 98.2 98.2 98.2 98.2 92.2LAME [5] 21.7 75.1 72.7 22.9 20.6 49.0 32.1 33.3 21.2 28.0 66.8 40.0 30.6 43.9 51.3 40.6CoTTA [73] 46.8 48.4 54.7 48.7 48.6 53.5 55.4 52.8 49.8 51.8 53.5 52.9 54.1 56.7 53.6 52.1NOTE [19] 42.6 53.0 69.9 52.1 53.3 70.4 73.1 76.7 80.8 96.0 97.7 97.1 96.6 97.2 95.8 76.8 RoTTA 28.4 37.3 44.6 31.9 28.3 41.8 43.6 39.9 28.0 35.2 38.2 33.7 33.0 39.5 31.0 35.6(+5.0) Table 21. Average classification error of the task CIFAR100 → CIFAR100-C while continually adapting to different corruptions at the highest severity 5 with correlatively sampled test stream under the proposed setup PTTA. Time t− − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − → Method jpeg shot zoom frost contrastfog defocuselasticgaussianbrightnessglass impulsepixelatesnow motion Avg. Source 41.2 68.0 28.8 45.8 55.1 50.3 29.3 37.2 73.0 29.5 54.1 39.4 74.7 39.5 30.8 46.4BN [53] 58.3 56.8 47.8 51.8 48.9 57.3 46.8 53.5 57.8 45.5 57.1 58.5 51.7 53.3 48.8 52.9PL [39] 59.4 66.3 74.9 87.5 94.2 95.5 96.2 97.1 97.4 97.2 97.5 97.7 98.0 98.2 98.2 90.4TENT [70] 62.0 79.3 91.7 95.8 96.9 97.0 97.4 97.7 97.6 97.7 97.9 97.9 98.0 97.9 97.9 93.5LAME [5] 33.6 66.7 21.1 39.9 50.6 43.9 21.0 28.6 72.5 21.6 48.6 32.5 74.5 30.6 22.5 40.6CoTTA [73]54.6 54.1 49.6 52.1 52.7 58.0 50.3 53.3 55.0 49.1 55.4 55.7 51.0 54.6 52.1 53.2NOTE [19]60.4 63.0 49.9 55.7 47.0 65.2 59.4 76.6 90.9 87.2 96.8 97.0 97.3 96.7 96.8 76.0 RoTTA 43.9 45.3 31.0 37.3 35.7 41.2 27.7 34.8 39.7 26.6 39.5 41.9 32.0 33.0 30.5 36.0(+4.6) Table 22. Average classification error of the task CIFAR100 → CIFAR100-C while continually adapting to different corruptions at the highest severity 5 with correlatively sampled test stream under the proposed setup PTTA. Time t− − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − → Method contrastdefocusgaussianshot snow frost glass zoom elasticjpeg pixelatebrightnessimpulsemotion fog Avg. Source 55.1 29.3 73.0 68.0 39.5 45.8 54.1 28.8 37.2 41.2 74.7 29.5 39.4 30.8 50.3 46.4BN [53] 49.4 47.2 58.6 56.2 52.7 52.0 57.9 46.1 54.4 57.7 50.5 46.2 58.2 47.6 58.5 52.9PL [39] 54.8 64.2 83.3 92.4 95.5 96.5 96.9 96.4 97.2 97.4 97.8 97.8 97.9 97.7 98.0 90.9TENT [70] 60.2 83.1 95.2 96.5 96.9 97.3 97.0 97.3 97.8 97.8 97.6 97.9 97.8 97.9 98.1 93.9LAME [5] 51.3 21.3 72.7 66.3 30.2 40.0 48.6 20.9 27.7 33.3 75.0 21.5 32.2 22.5 43.8 40.5CoTTA [73]52.1 48.6 55.1 52.7 53.4 51.9 55.9 49.2 53.2 52.8 49.2 49.7 56.2 50.7 58.1 52.6NOTE [19] 39.5 45.9 68.8 61.8 57.4 58.5 71.4 66.5 80.8 90.9 94.2 94.9 97.0 95.5 96.6 74.6 RoTTA 41.7 30.5 44.9 40.5 35.4 34.1 40.5 28.2 34.5 39.5 31.1 26.7 43.3 31.4 38.8 36.1(+4.4) Table 23. Average classification error of the task CIFAR100 → CIFAR100-C while continually adapting to different corruptions at the highest severity 5 with correlatively sampled test stream under the proposed setup PTTA. Time t− − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − → Method shot fog glass pixelatesnow elasticbrightnessimpulsedefocusfrost contrastgaussianmotionjpeg zoom Avg. Source 68.0 50.3 54.1 74.7 39.5 37.2 29.5 39.4 29.3 45.8 55.1 73.0 30.8 41.2 28.8 46.4BN [53] 57.5 58.6 58.5 50.5 52.7 53.1 45.9 57.9 47.0 51.5 47.8 58.2 48.2 57.1 47.7 52.8PL [39] 59.5 72.9 85.1 89.6 94.5 96.8 97.1 97.9 97.8 98.0 98.3 98.2 98.0 98.0 98.2 92.0TENT [70]60.3 81.4 95.0 96.6 97.0 97.3 97.3 97.7 97.7 97.7 97.8 97.7 97.6 97.6 97.9 93.8LAME [5] 66.4 43.2 49.0 75.2 30.2 28.5 21.6 32.5 21.2 39.5 52.0 72.8 22.3 33.1 20.5 40.5CoTTA [73]54.5 58.4 55.6 50.0 53.9 53.4 50.3 56.7 51.3 53.2 53.7 56.1 52.0 54.5 51.5 53.7NOTE [19]61.8 60.2 63.4 55.6 59.8 65.9 58.6 75.1 77.8 93.8 94.2 97.0 95.0 95.5 94.4 76.5 RoTTA 45.5 44.5 43.5 35.6 35.1 35.7 26.2 44.0 29.7 34.2 32.0 40.7 31.4 39.4 27.7 36.3(+4.2) Table 24. Average classification error of the task CIFAR100 → CIFAR100-C while continually adapting to different corruptions at the highest severity 5 with correlatively sampled test stream under the proposed setup PTTA. Time t− − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − → Method pixelateglass zoomsnow fog impulsebrightnessmotionfrost jpeg gaussianshot contrastdefocus elastic Avg. Source 74.7 54.1 28.8 39.5 50.3 39.4 29.5 30.8 45.8 41.2 73.0 68.0 55.1 29.3 37.2 46.4BN [53] 51.7 58.6 47.8 52.9 57.1 58.2 45.9 47.6 52.9 57.8 57.5 56.7 49.5 46.1 54.0 52.9PL [39] 52.4 68.0 73.4 87.9 93.7 96.1 95.7 96.0 96.5 96.7 97.5 97.7 97.7 97.3 97.7 89.6TENT [70] 53.5 77.8 91.1 96.0 97.0 97.6 97.4 97.6 97.9 98.1 98.1 98.0 98.1 97.9 98.1 92.9LAME [5] 74.8 48.2 21.1 30.6 43.4 32.5 21.6 23.0 39.6 33.3 72.7 66.5 51.5 20.7 27.5 40.5CoTTA [73]49.3 55.1 49.1 52.9 56.8 55.7 49.5 50.0 53.6 53.4 54.9 53.9 53.8 50.1 53.5 52.8NOTE [19] 52.2 64.9 47.5 57.0 61.9 67.3 60.4 67.8 77.4 90.6 97.1 96.8 92.8 95.9 96.6 75.1 RoTTA 36.4 44.4 29.7 36.5 41.0 44.1 26.8 29.5 33.0 40.3 40.3 38.2 33.9 28.5 34.9 35.8(+4.7)Table 25. Average classification error of the task CIFAR100 → CIFAR100-C while continually adapting to different corruptions at the highest severity 5 with correlatively sampled test stream under the proposed setup PTTA. Time t− − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − → Method motionsnow fog shot defocuscontrastzoom brightnessfrost elasticglass gaussianpixelatejpeg impulse Avg. Source 30.8 39.5 50.3 68.0 29.3 55.1 28.8 29.5 45.8 37.2 54.1 73.0 74.7 41.2 39.4 46.4BN [53] 48.5 54.0 58.9 56.2 46.4 48.0 47.0 45.4 52.9 53.4 57.1 58.2 51.7 57.1 58.8 52.9PL [39] 50.6 62.1 73.9 87.8 90.8 96.0 94.8 96.4 97.4 97.2 97.4 97.4 97.3 97.4 97.4 88.9TENT [70] 53.3 77.6 93.0 96.5 96.7 97.5 97.1 97.5 97.3 97.2 97.1 97.7 97.6 98.0 98.3 92.8LAME [5] 22.4 30.4 43.9 66.3 21.3 51.7 20.6 21.8 39.6 28.0 48.7 72.8 74.6 33.1 32.3 40.5CoTTA [73]49.2 52.7 56.8 53.0 48.7 51.7 49.4 48.7 52.5 52.2 54.3 54.9 49.6 53.4 56.2 52.2NOTE [19] 45.7 53.0 58.2 65.6 54.2 52.0 59.8 63.5 74.8 91.8 98.1 98.3 96.8 97.0 98.2 73.8 RoTTA 31.8 36.7 40.9 42.1 30.0 33.6 27.9 25.4 32.3 34.0 38.8 38.7 31.3 38.0 42.9 35.0(+5.5) Table 26. Average classification error of the task CIFAR100 → CIFAR100-C while continually adapting to different corruptions at the highest severity 5 with correlatively sampled test stream under the proposed setup PTTA. Time t− − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − → Method frost impulsejpeg contrastzoom glass pixelatesnow defocusmotionbrightnesselasticshot fog gaussian Avg. Source 45.8 39.4 41.2 55.1 28.8 54.1 74.7 39.5 29.3 30.8 29.5 37.2 68.0 50.3 73.0 46.4BN [53] 52.9 58.8 57.6 48.2 47.4 57.6 50.9 52.4 47.0 47.2 45.1 54.0 56.4 57.7 58.2 52.8PL [39] 56.9 73.3 86.7 94.4 95.8 97.3 97.2 97.4 97.6 97.4 97.7 97.6 97.8 98.3 98.1 92.2TENT [70]60.1 84.2 95.7 97.2 97.4 97.9 97.8 98.0 98.1 98.2 98.3 98.4 98.4 98.4 98.4 94.4LAME [5] 39.9 32.4 33.4 51.4 20.6 49.0 74.4 31.3 21.2 22.6 21.9 28.1 66.9 43.9 72.5 40.6CoTTA [73]51.5 55.3 54.3 51.8 49.4 55.3 50.7 54.2 51.4 50.6 49.5 53.6 55.0 57.1 55.8 53.0NOTE [19]51.6 60.9 60.3 45.4 54.3 70.8 68.8 75.0 75.7 87.1 94.7 95.6 96.7 96.4 97.2 75.4 RoTTA 40.0 46.3 42.8 36.4 29.2 42.3 33.2 34.4 28.4 29.2 26.4 34.5 38.5 39.8 39.3 36.0(+4.6) Table 27. Average classification error of the task CIFAR100 → CIFAR100-C while continually adapting to different corruptions at the highest severity 5 with correlatively sampled test stream under the proposed setup PTTA. Time t− − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − → Method defocusmotionzoom shot gaussianglass jpeg fog contrastpixelatefrost snow brightnesselastic impulse Avg. Source 29.3 30.8 28.8 68.0 73.0 54.1 41.2 50.3 55.1 74.7 45.8 39.5 29.5 37.2 39.4 46.4BN [53] 47.1 48.6 47.8 56.2 57.6 57.6 57.6 57.5 48.7 50.6 51.8 53.2 46.9 53.5 58.8 52.9PL [39] 48.8 58.7 69.9 88.0 95.1 96.6 96.7 96.9 97.4 97.4 98.2 98.2 98.2 98.3 98.5 89.1TENT [70] 51.0 67.6 85.8 95.9 97.2 97.5 97.2 97.7 98.1 97.9 97.7 97.7 98.0 98.0 98.2 91.7LAME [5] 21.2 22.8 21.1 66.3 72.8 49.0 33.3 44.8 51.7 74.9 39.8 31.2 21.3 27.3 32.3 40.6CoTTA [73]48.4 48.8 48.2 52.9 54.0 53.8 52.7 57.2 52.6 48.6 51.8 53.9 49.4 52.3 56.0 52.0NOTE [19] 45.1 46.7 49.1 67.3 65.5 69.4 75.5 80.3 83.8 96.0 97.6 97.1 96.1 97.9 98.7 77.7 RoTTA 29.6 31.3 28.8 43.9 41.5 41.3 40.9 39.8 32.1 32.6 33.1 33.0 26.5 34.5 42.9 35.4(+5.2) Table 28. Average classification error of the task CIFAR100 → CIFAR100-C while continually adapting to different corruptions at the highest severity 5 with correlatively sampled test stream under the proposed setup PTTA. Time t− − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − → Method glass zoom impulsefog snow jpeg gaussianfrost shot brightnesscontrastmotionpixelatedefocus elastic Avg. Source 54.1 28.8 39.4 50.3 39.5 41.2 73.0 45.8 68.0 29.5 55.1 30.8 74.7 29.3 37.2 46.4BN [53] 58.8 47.7 59.2 57.6 52.7 56.9 58.2 52.0 56.7 45.5 47.8 48.2 51.7 46.1 54.0 52.9PL [39] 60.1 59.5 75.1 85.7 91.5 94.6 96.5 97.1 97.4 97.3 98.0 97.7 97.9 97.8 97.7 89.6TENT [70] 61.6 71.5 91.0 95.9 96.6 97.1 96.9 97.3 97.4 97.2 97.9 98.0 98.1 97.9 97.8 92.8LAME [5] 48.6 20.6 32.3 44.4 30.2 33.6 72.4 40.0 66.3 21.6 52.0 22.8 74.6 20.7 27.5 40.5CoTTA [73]56.4 48.9 56.1 57.8 54.1 54.2 56.2 53.6 55.4 50.0 53.6 51.6 51.2 50.7 54.4 53.6NOTE [19]62.5 46.3 61.5 61.1 58.6 68.4 76.1 78.3 92.0 93.4 96.1 95.4 96.2 95.8 96.4 78.5 RoTTA 45.5 30.0 45.9 42.6 35.3 41.8 42.2 34.5 40.2 27.3 31.3 30.2 32.7 28.1 34.9 36.2(+4.3) Table 29. Average classification error of the task CIFAR100 → CIFAR100-C while continually adapting to different corruptions at the highest severity 5 with correlatively sampled test stream under the proposed setup PTTA. Time t− − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − → Method contrastgaussiandefocuszoom frost glass jpeg fog pixelateelasticshot impulsesnow motion brightness Avg. Source 55.1 73.0 29.3 28.8 45.8 54.1 41.2 50.3 74.7 37.2 68.0 39.4 39.5 30.8 29.5 46.4BN [53] 49.5 58.8 47.0 46.5 52.2 57.6 57.6 57.6 51.7 53.5 56.0 58.5 53.1 47.6 46.3 52.9PL [39] 53.6 70.4 76.0 85.1 91.2 95.2 96.0 97.0 96.9 97.3 97.3 97.6 97.5 97.6 97.7 89.8TENT [70] 60.2 89.1 95.0 96.2 96.9 97.0 96.5 97.0 97.0 97.2 97.6 97.8 97.5 97.9 97.7 94.0LAME [5] 51.3 72.5 21.5 21.0 39.6 49.0 33.3 44.8 74.8 28.0 66.8 32.5 30.6 22.5 21.4 40.6CoTTA [73]52.3 55.3 49.5 48.1 52.1 54.8 52.7 56.9 50.6 52.6 53.7 55.8 54.6 50.6 50.5 52.7NOTE [19] 39.1 64.7 48.9 50.6 59.1 70.1 71.7 75.0 85.2 95.7 96.9 98.4 96.0 95.9 94.9 76.1 RoTTA 41.4 46.2 30.5 28.5 36.0 40.9 40.5 39.6 33.0 35.0 38.2 43.1 33.9 30.7 27.1 36.3(+4.3)",
      "meta_data": {
        "arxiv_id": "2303.13899v1",
        "authors": [
          "Longhui Yuan",
          "Binhui Xie",
          "Shuang Li"
        ],
        "published_date": "2023-03-24T10:19:14Z",
        "pdf_url": "https://arxiv.org/pdf/2303.13899v1.pdf"
      },
      "llm_extracted_info": {
        "main_contributions": "The paper introduces Practical Test-Time Adaptation (PTTA), a new and more realistic Test-Time Adaptation (TTA) setup that considers both continually changing distributions and correlative sampling in test data streams, which is crucial for real-world applications like autonomous driving. It proposes Robust Test-Time Adaptation (RoTTA) as a comprehensive method to address PTTA challenges. The work benchmarks prior methods under PTTA, highlighting their ineffectiveness due to neglecting both aspects. RoTTA achieves state-of-the-art results, significantly outperforming baselines by reducing average classification error by over 5.9% on CIFAR-10-C, 5.5% on CIFAR-100-C, and 2.2% on DomainNet, demonstrating its effectiveness and ease of implementation for practical deployment.",
        "methodology": "The proposed Robust Test-Time Adaptation (RoTTA) method consists of three key components: 1) Robust Batch Normalization (RBN): It replaces standard batch normalization by maintaining global statistics (mean and variance) updated through exponential moving average, using statistics from a memory bank, to normalize feature maps robustly against correlative test data. 2) Category-balanced sampling with timeliness and uncertainty (CSTU): A memory bank is maintained to capture a stable snapshot of the current distribution. Samples are added based on category balance, prioritizing newer (lower age) and less uncertain (lower prediction entropy) samples, guided by pseudo-labels from the teacher model. 3) Robust training with timeliness: A teacher-student model is employed. The student model's affine parameters in RBN layers are updated by minimizing a cross-entropy loss between strong-augmented student predictions and weak-augmented teacher predictions. A time-aware reweighting strategy, `E(Ai) = exp(−Ai/N) / (1 + exp(−Ai/N))`, is applied to the loss, giving higher weight to newer samples from the memory bank. The teacher model's parameters are updated via exponential moving average of the student's parameters.",
        "experimental_setup": "Experiments were conducted on CIFAR10-C, CIFAR100-C, and DomainNet datasets. CIFAR10-C and CIFAR100-C, common TTA benchmarks, were used to test robustness under 15 types of corruptions at severity 5, with corruption types changing continually. DomainNet, a large-scale dataset, was used for generalization under huge domain shifts, adapting from one source domain to five target domains. Pre-trained models included WildResNet-28 for CIFAR10-C, ResNeXt-29 for CIFAR100-C (from RobustBench), and ResNet-101 for DomainNet (trained with standard classification loss). Correlative sampling in test streams was simulated using a Dirichlet distribution with parameter δ (defaulted to 0.1). Adam optimizer with a learning rate of 1.0 × 10−3 and a batch size of 64 were used across all methods. The memory bank capacity for RoTTA was N=64. RoTTA's hyperparameters (α=0.05, ν=0.001, λt=1.0, λu=1.0, δ=0.1) were unified across experiments. Validation involved comparing average classification error, ablation studies for each component, and sensitivity analyses for distribution changing order, Dirichlet parameter δ, and batch size.",
        "limitations": "The Robust Batch Normalization (RBN) used is considered a naive solution and requires careful design for its α value. The method currently lacks a way to recover the model from a collapse state, although RoTTA is designed to prevent such collapses. The simulation of correlative sampling primarily focuses on category similarity using Dirichlet distribution, and the approach needs further validation in more diverse real-world scenarios beyond this simulation.",
        "future_research_directions": "Future work could focus on improving the RoTTA algorithm by replacing or refining some of its existing components. There is also a direction to enhance the Practical Test-Time Adaptation (PTTA) setup itself to make it even more realistic for real-world scenarios. The authors hope this work paves the way for achieving the original goal of test-time adaptation, which is robust deployment of models in dynamic real-world applications."
      }
    },
    {
      "title": "Test-Time Training with Self-Supervision for Generalization under Distribution Shifts",
      "abstract": "In this paper, we propose Test-Time Training, a general approach for\nimproving the performance of predictive models when training and test data come\nfrom different distributions. We turn a single unlabeled test sample into a\nself-supervised learning problem, on which we update the model parameters\nbefore making a prediction. This also extends naturally to data in an online\nstream. Our simple approach leads to improvements on diverse image\nclassification benchmarks aimed at evaluating robustness to distribution\nshifts.",
      "full_text": "Test-Time Training with Self-Supervision for Generalization under Distribution Shifts Yu Sun1 Xiaolong Wang1 2 Zhuang Liu1 John Miller1 Alexei A. Efros1 Moritz Hardt1 Abstract In this paper, we propose Test-Time Training, a general approach for improving the performance of predictive models when training and test data come from different distributions. We turn a sin- gle unlabeled test sample into a self-supervised learning problem, on which we update the model parameters before making a prediction. This also extends naturally to data in an online stream. Our simple approach leads to improvements on di- verse image classiﬁcation benchmarks aimed at evaluating robustness to distribution shifts. 1. Introduction Supervised learning remains notoriously weak at generaliza- tion under distribution shifts. Unless training and test data are drawn from the same distribution, even seemingly minor differences turn out to defeat state-of-the-art models (Recht et al., 2018). Adversarial robustness and domain adapta- tion are but a few existing paradigms that try to anticipate differences between the training and test distribution with either topological structure or data from the test distribution available during training. We explore a new take on gener- alization that does not anticipate the distribution shifts, but instead learns from them at test time. We start from a simple observation. The unlabeled test sample xpresented at test time gives us a hint about the distribution from which it was drawn. We propose to take advantage of this hint on the test distribution by allowing the model parameters θto depend on the test sample x, but not its unknown label y. The concept of a variable decision boundary θ(x) is powerful in theory since it breaks away from the limitation of ﬁxed model capacity (see additional discussion in Section A1), but the design of a feedback mechanism from xto θ(x) raises new challenges in practice that we only begin to address here. 1University of California, Berkeley 2University of California, San Diego. Correspondence to: Yu Sun <yusun@berkeley.edu>. Proceedings of the 37 th International Conference on Machine Learning, Vienna, Austria, PMLR 119, 2020. Copyright 2020 by the author(s). Our proposed test-time training method creates a self- supervised learning problem based on this single test sample x, updating θat test time before making a prediction. Self- supervised learning uses an auxiliary task that automatically creates labels from unlabeled inputs. In our experiments, we use the task of rotating each input image by a multiple of 90 degrees and predicting its angle (Gidaris et al., 2018). This approach can also be easily modiﬁed to work outside the standard supervised learning setting. If several test samples arrive in a batch, we can use the entire batch for test-time training. If samples arrive in an online stream, we obtain further improvements by keeping the state of the parameters. After all, prediction is rarely a single event. The online version can be the natural mode of deployment under the additional assumption that test samples are produced by the same or smoothly changing distribution shifts. We experimentally validate our method in the context of object recognition on several standard benchmarks. These include images with diverse types of corruption at various levels (Hendrycks & Dietterich, 2019), video frames of moving objects (Shankar et al., 2019), and a new test set of unknown shifts collected by (Recht et al., 2018). Our algorithm makes substantial improvements under distribu- tion shifts, while maintaining the same performance on the original distribution. In our experiments, we compare with a strong baseline (labeled joint training) that uses both supervised and self- supervised learning at training-time, but keeps the model ﬁxed at test time. Recent work shows that training-time self- supervision improves robustness (Hendrycks et al., 2019a); our joint training baseline corresponds to an improved imple- mentation of this work. A comprehensive review of related work follows in Section 5. We complement the empirical results with theoretical inves- tigations in Section 4, and establish an intuitive sufﬁcient condition on a convex model of when Test-Time Training helps; this condition, roughly speaking, is to have correlated gradients between the loss functions of the two tasks. Project website: https://test-time-training.github.io/. arXiv:1909.13231v3  [cs.LG]  1 Jul 2020Test-Time Training with Self-Supervision for Generalization under Distribution Shifts 2. Method This section describes the algorithmic details of our method. To set up notation, consider a standard K-layer neural net- work with parameters θk for layer k. The stacked parameter vector θ = ( θ1,...,θ K) speciﬁes the entire model for a classiﬁcation task with loss function lm(x,y; θ) on the test sample (x,y). We call this the main task, as indicated by the subscript of the loss function. We assume to have training data (x1,y1),..., (xn,yn) drawn i.i.d. from a distribution P. Standard empirical risk minimization solves the optimization problem: min θ 1 n n∑ i=1 lm(xi,yi; θ). (1) Our method requires a self-supervised auxiliary task with loss function ls(x). In this paper, we choose the rotation prediction task (Gidaris et al., 2018), which has been demon- strated to be simple and effective at feature learning for convolutional neural networks. The task simply rotates x in the image plane by one of 0, 90, 180 and 270 degrees and have the model predict the angle of rotation as a four- way classiﬁcation problem. Other self-supervised tasks in Section 5 might also be used for our method. The auxiliary task shares some of the model parameters θe = ( θ1,...,θ κ) up to a certain κ ∈ {1,...,K }. We designate those κlayers as a shared feature extractor. The auxiliary task uses its own task-speciﬁc parameters θs = (θ′ κ+1,...,θ ′ K). We call the unshared parameters θs the self-supervised task branch, and θm = (θκ+1,...,θ K) the main task branch . Pictorially, the joint architecture is a Y-structure with a shared bottom and two branches. For our experiments, the self-supervised task branch has the same architecture as the main branch, except for the output dimensionality of the last layer due to the different number of classes in the two tasks. Training is done in the fashion of multi-task learning (Caru- ana, 1997); the model is trained on both tasks on the same data drawn fromP. Losses for both tasks are added together, and gradients are taken for the collection of all parameters. The joint training problem is therefore min θe,θm,θs 1 n n∑ i=1 lm(xi,yi; θm,θe) + ls(xi; θs,θe). (2) Now we describe the standard version of Test-Time Training on a single test sample x. Simply put, Test-Time Training ﬁne-tunes the shared feature extractor θe by minimizing the auxiliary task loss on x. This can be formulated as min θe ls(x; θs,θe). (3) Denote θ∗ e the (approximate) minimizer of Equation 3. The model then makes a prediction using the updated parameters θ(x) = (θ∗ e,θm). Empirically, the difference is negligible between minimizing Equation 3 over θe versus over both θe and θs. Theoretically, the difference exists only when optimization is done with more than one gradient step. Test-Time Training naturally beneﬁts from standard data augmentation techniques. On each test sample x, we per- form the exact same set of random transformations as for data augmentation during training, to form a batch only con- taining these augmented copies of xfor Test-Time Training. Online Test-Time Training. In the standard version of our method, the optimization problem in Equation 3 is al- ways initialized with parameters θ= (θe,θs) obtained by minimizing Equation 2. After making a prediction on x, θ∗ e is discarded. Outside of the standard supervised learning setting, when the test samples arrive online sequentially, the online version solves the same optimization problem as in Equation 3 to update the shared feature extractor θe. How- ever, on test sample xt, θis instead initialized with θ(xt−1) updated on the previous sample xt−1. This allows θ(xt) to take advantage of the distributional information available in x1,...,x t−1 as well as xt. 3. Empirical Results We experiment with both versions of our method (standard and online) on three kinds of benchmarks for distribution shifts, presented here in the order of visually low to high- level. Our code is available at the project website. Network details. Our architecture and hyper-parameters are consistent across all experiments. We use ResNets (He et al., 2016b), which are constructed differently for CIFAR-10 (Krizhevsky & Hinton, 2009) (26-layer) and Ima- geNet (Russakovsky et al., 2015) (18-layer). The CIFAR-10 dataset contains 50K images for training, and 10K images for testing. The ImageNet contains 1.2M images for train- ing and the 50K validation images are used as the test set. ResNets on CIFAR-10 have three groups, each containing convolutional layers with the same number of channels and size of feature maps; our splitting point is the end of the second group. ResNets on ImageNet have four groups; our splitting point is the end of the third group. We use Group Normalization (GN) instead of Batch Nor- malization (BN) in our architecture, since BN has been shown to be ineffective when training with small batches, for which the estimated batch statistics are not accurate (Ioffe & Szegedy, 2015). This technicality hurts Test-Time Training since each batch only contains (augmented) copies of a single image. Different from BN, GN is not dependent on batch size and achieves similar results on our baselines.Test-Time Training with Self-Supervision for Generalization under Distribution Shifts originalgauss shot impulsedefocus glass motion zoom snow frost fog bright contrastelasticpixelate jpeg 0 10 20 30 40 50Error (%) Object recognition task only Joint training (Hendrycks et al. 2019) TTT TTT-Online Figure 1.Test error (%) on CIFAR-10-C with level 5 corruptions.We compare our approaches, Test-Time Training (TTT) and its online version (TTT-Online), with two baselines: object recognition without self-supervision, and joint training with self-supervision but keeping the model ﬁxed at test time. TTT improves over the baselines and TTT-Online improves even further. We report results with BN in Section A4 of the appendix for completeness. We directly compare our architecture to that of Hendrycks et al. (2018) in subsection A4.5. Optimization details. For joint training (Equation 2), we use stochastic gradient descent with standard hyper- parameters as (Huang et al., 2016; He et al., 2016a). For Test-Time Training (Equation 3), we use stochastic gradient descent with the learning rate set to that of the last epoch during training, which is 0.001 in all our experiments. We set weight decay and momentum to zero during Test-Time Training, inspired by practice in (He et al., 2018; Liu et al., 2018). For the standard version of Test-Time Training, we take ten gradient steps, using batches independently gener- ated by the same image. For online version of Test-Time Training, we take only one gradient step given each new im- age. We use random crop and random horizontal ﬂip for data augmentation. See Section A2 of the appendix for computa- tional aspects of our method. In all the tables and ﬁgures, object recognition task onlyrefers to the plain ResNet model (using GN, unless otherwise speciﬁed); joint training refers to the model jointly trained on both the main task and the self-supervised task, ﬁxed at test time; this has been pro- posed as the method in Hendrycks et al. (2019a); Test-Time Training (TTT) refers to the standard version described sec- tion 2; and online Test-Time Training (TTT-Online)refers to the online version that does not discardθ(xt) for xt arriving sequentially from the same distribution. Performance for TTT-Online is calculated as the average over the entire test set; we always shufﬂe the test set before TTT-Online to avoid ordering artifacts. 3.1. Object Recognition on Corrupted Images Hendrycks & Dietterich (2019) propose to benchmark ro- bustness of object recognition with 15 types of corruptions from four broad categories: noise, blur, weather and digital. Each corruption type comes in ﬁve levels of severity, with level 5 the most severe (details and sample images in the ap- pendix). The corruptions are simulated to mimic real-world corruptions as much as possible on copies of the test set for both CIFAR-10 and ImageNet. The new test sets are named as CIFAR-10-C and ImageNet-C, respectively. In the pro- posed benchmark, training should be done on the original training set, and the diversity of corruption types should make it difﬁcult for any methods to work well across the board if it relies too much on corruption speciﬁc knowledge. For online Test-Time Training, we take the entire test set as a stream of incoming images, and update and test on each image in an online manner as it arrives. CIFAR-10-C. Our results on the level 5 corruptions (most severe) are shown in Figure 1. The results on levels 1-4 are shown in Section A4 in appendix. Across all ﬁve levels and 15 corruption types, both standard and online versions of Test-Time Training improve over the object recognition task only baseline by a large margin. The standard version always improves over joint training, and the online version often improves signiﬁcantly (>10%) over joint training and never hurts by more than 0.2%. Speciﬁcally, TTT-Online contributes >24% on the three noise types and 38% on pix- elation. For a learning problem with the seemingly unstable setup that abuses a single image, this kind of consistency is rather surprising. The baseline ResNet-26 with object recognition task only has error 8.9% on the original test set of CIFAR-10. The joint training baseline actually improves performance on the original to 8.1%. More surprisingly, unlike many other methods that trade off original performance for robustness, Test-Time Training further improves on the original test set by 0.2% consistently over multiple independent trials. This suggests that our method does not choose between speciﬁcity and generality.Test-Time Training with Self-Supervision for Generalization under Distribution Shifts originalgauss shot impulsedefocus glass motion zoom snow frost fog bright contrastelasticpixelate jpeg 0 20 40 60Accuracy (%) Object recognition task only Joint training (Hendrycks et al. 2019) TTT TTT-Online 0 20000 40000 Number of samples 60 62 64 66 68 70 72 74 76Accuracy (%) Original Sliding window average 0 20000 40000 Number of samples 12 15 18 21 24 27 30 33Accuracy (%) Gaussian Noise Sliding window average 0 20000 40000 Number of samples 16 18 20 22 24 26 28 30 32Accuracy (%) Defocus Blur Sliding window average 0 20000 40000 Number of samples 28 30 32 34 36 38Accuracy (%) Zoom Blur Sliding window average 0 20000 40000 Number of samples 33 36 39 42 45 48 51 54Accuracy (%) Fog Sliding window average 0 20000 40000 Number of samples 30 33 36 39 42 45 48 51Accuracy (%) Elastic Transform Sliding window average Figure 2.Test accuracy (%) on ImageNet-C with level 5 corruptions.Upper panel: Our approaches, TTT and TTT-Online, show signiﬁcant improvements in all corruption types over the two baselines. Lower panel: We show the accuracy of TTT-Online as the average over a sliding window of 100 samples; TTT-Online generalizes better as more samples are evaluated (x-axis), without hurting on the original distribution. We use accuracy instead of error here because the baseline performance is very low for most corruptions. Separate from our method, it is interesting to note that joint training consistently improves over the single-task baseline, as discovered by Hendrycks et al. (2019a). Hendrycks & Dietterich (2019) have also experimented with various other training methods on this benchmark, and point to Adversar- ial Logit Pairing (ALP) (Kannan et al., 2018) as the most effective approach. Results of this additional baseline on all levels of CIFAR-10-C are shown in the appendix, along with its implementation details. While surprisingly robust under some of the most severe corruptions (especially the three noise types), ALP incurs a much larger error (by a factor of two) on the original distribution and some corruptions (e.g. all levels of contrast and fog), and hurts performance signiﬁcantly when the corruptions are not as severe (espe- cially on levels 1-3); this kind of tradeoff is to be expected for methods based on adversarial training. ImageNet-C. Our results on the level 5 corruptions (most severe) are shown in Figure 2. We use accuracy instead of error for this dataset because the baseline performance is very low for most corruptions. The general trend is roughly the same as on CIFAR-10-C. The standard version of TTT always improves over the baseline and joint training, while the online version only hurts on the original by 0.1% over the baseline, but signiﬁcantly improves (by a factor of more than three) on many of the corruption types. In the lower panel of Figure 2, we visualize how the accu- racy (averaged over a sliding window) of the online version changes as more images are tested. Due to space constraints, we show this plot on the original test set, as well as every third corruption type, following the same order as in the original paper. On the original test set, there is no visible trend in performance change after updating on the 50,000 samples. With corruptions, accuracy has already risen sig- niﬁcantly after 10,000 samples, but is still rising towards the end of the 50,000 samples, indicating room for additional improvements if more samples were available. Without seeing a single label, TTT-Online behaves as if we were training on the test set from the appearance of the plots.Test-Time Training with Self-Supervision for Generalization under Distribution Shifts orig gauss shot impul defoc glass motn zoom snow frost fog brit contr elast pixel jpeg TTT-Online 8.2 25.8 22.6 30.6 14.6 34.4 18.3 17.1 20.0 18.0 16.9 11.2 15.6 21.6 18.1 21.2 UDA-SS 9.0 28.2 26.5 20.8 15.6 43.7 24.5 23.8 25.0 24.9 17.2 12.7 11.6 22.1 20.3 22.6 Table 1.Test error (%) on CIFAR-10-C with level 5 corruption.Comparison between online Test-Time Training (TTT-Online) and unsupervised domain adaptation by self-supervision (UDA-SS) (Sun et al., 2019) with access to the entire (unlabeled) test set during training. We highlight the lower error in bold. We have abbreviated the names of the corruptions, in order: original test set, Gaussian noise, shot noise, impulse noise, defocus blur, glass blue, motion blur, zoom blur, snow, frost, fog, brightness, contrast, elastic transformation, pixelation, and JPEG compression. The reported numbers for TTT-Online are the same as in Figure 1. See complete table in Table A2. 0 2000 4000 6000 8000 Number of samples 12 16 20 24 28 32 36 40 44 48Error (%) Gaussian Noise Joint training TTT TTT-Online UDA-SS 0 2000 4000 6000 8000 Number of samples 9 12 15 18 21 24 27 30 33 36Error (%) Shot Noise Joint training TTT TTT-Online UDA-SS 0 2000 4000 6000 8000 Number of samples 15 20 25 30 35 40 45 50Error (%) Impulse Noise Joint training TTT TTT-Online UDA-SS Figure 3.Test error (%) on CIFAR-10-C, for the three noise types, with gradually changing distribution.The distribution shifts are created by increasing the standard deviation of each noise type from small to large, the further we go on the x-axis. As the samples get noisier, all methods suffer greater errors the more we evaluate into the test set, but online Test-Time Training (TTT-Online) achieves gentler slopes than joint training. For the ﬁrst two noise types, TTT-Online also achieves better results over unsupervised domain adaptation by self-supervision (UDA-SS) (Sun et al., 2019). Comparison with unsupervised domain adaptation. Table 1 empirically compares online Test-Time Training (TTT-Online) with unsupervised domain adaptation through self-supervision (UDA-SS) (Sun et al., 2019), which is sim- ilar to our method in spirit but is designed for the setting of unsupervised domain adaptation (Section 5 provides a sur- vey of other related work in this setting). Given labeled data from the training distribution and unlabeled data from the test distribution, UDA-SS hopes to ﬁnd an invariant repre- sentation that extracts useful features for both distributions by learning to perform a self-supervised task, speciﬁcally rotation prediction, simultaneously on data from both. It then learns a labeling function on top of the invariant rep- resentation using the labeled data. In our experiments, the unlabeled data given to UDA-SS is the entire test set itself without the labels. Because TTT-Online can only learn from the unlabeled test samples that have already been evaluated on, it is given less information than UDA-SS at all times. In this sense, UDA- SS should be regarded as an oracle rather than a baseline. Surprisingly, TTT-Online outperforms UDA-SS on 13 out of the 15 corruptions as well as the original distribution. Our explanation is that UDA-SS has to ﬁnd an invariant representation for both distributions, while TTT-Online only adapts the representation to be good for the current test distribution. That is, TTT-Online has the ﬂexibility to forget the training distribution representation, which is no longer relevant. This suggests that in our setting, forgetting is not harmful and perhaps should even be taken advantage of. Gradually changing distribution shifts.In our previous experiments, we have been evaluating the online version under the assumption that the test inputs xt for t= 1...nare all sampled from the same test distribution Q, which can be different from the training distribution P. This assumption is indeed satisﬁed for i.i.d. samples from a shufﬂed test set. But here we show that this assumption can in fact be relaxed to allow xt ∼Qt, where Qt is close to Qt+1 (in the sense of distributional distance). We call this the assumption of gradually changing distribution shifts. We perform experiments by simulating such distribution shifts on the three noise types of CIFAR-10-C. For each noise type, xt is corrupted with standard deviation σt, and σ1,...,σ n interpolate between the standard deviation of level 1 and level 5. So xt is more severely corrupted as we evaluate further into the test set and t grows larger. As shown in Figure 3, TTT-Online still improves upon joint training (and our standard version) with this relaxed assumption, and even upon UDA-SS for the ﬁrst two noise types.Test-Time Training with Self-Supervision for Generalization under Distribution Shifts Accuracy (%) Airplane Bird Car Dog Cat Horse Ship Average Object recognition task only 67.9 35.8 42.6 14.7 52.0 42.0 66.7 41.4 Joint training (Hendrycks et al., 2019a) 70.2 36.7 42.6 15.5 52.0 44.0 66.7 42.4 TTT (standard version) 70.2 39.2 42.6 21.6 54.7 46.0 77.8 45.2 TTT-Online 70.2 39.2 42.6 22.4 54.7 46.0 77.8 45.4 Table 2.Class-wise and average classiﬁcation accuracy (%) on CIFAR classes in VID-Robust, adapted from (Shankar et al., 2019). Test-Time Training (TTT) and online Test-Time Training (TTT-Online) improve over the two baselines on average, and by a large margin on “ship” and “dog” classes where the rotation task is more meaningful than in classes like “airplane” (sample images in Figure A7). 3.2. Object Recognition on Video Frames The Robust ImageNet Video Classiﬁcation (VID-Robust) dataset was developed by Shankar et al. (2019) from the Ima- geNet Video detection dataset (Russakovsky et al., 2015), to demonstrate how deep models for object recognition trained on ImageNet (still images) fail to adapt well to video frames. The VID-Robust dataset contains 1109 sets of video frames in 30 classes; each set is a short video clip of frames that are similar to an anchor frame. Our results are reported on the anchor frames. To map the 1000 ImageNet classes to the 30 VID-Robust classes, we use the max-conversion function in Shankar et al. (2019). Without any modiﬁcations for videos, we apply our method to VID-Robust on top of the same ImageNet model as in the previous subsection. Our classiﬁcation accuracy is reported in Table 3. In addition, we take the seven classes in VID-Robust that overlap with CIFAR-10, and re-scale those video frames to the size of CIFAR-10 images, as a new test set for the model trained on CIFAR-10 in the previous subsection. Again, we apply our method to this dataset without any modiﬁcations. Our results are shown in Table 2, with a breakdown for each class. Noticing that Test-Time Training does not improve on the airplane class, we inspect some airplane samples (Figure A7), and observe black margins on two sides of most images, which provide a trivial hint for rotation prediction. In addition, given an image of airplanes in the sky, it is often impossible even for humans to tell if it is rotated. This shows that our method requires the self-supervised task to be both well deﬁned and non-trivial. 3.3. CIFAR-10.1: Unknown Distribution Shifts CIFAR-10.1 (Recht et al., 2018) is a new test set of size 2000 modeled after CIFAR-10, with the exact same classes and image dimensionality, following the dataset creation process documented by the original CIFAR-10 paper as closely as possible. The purpose is to investigate the distribution shifts present between the two test sets, and the effect on object recognition. All models tested by the authors suffer a large performance drop on CIFAR-10.1 comparing to CIFAR-10, even though there is no human noticeable difference, and Method Accuracy (%) Object recognition task only 62.7 Joint training (Hendrycks et al., 2019a) 63.5 TTT (standard version) 63.8 TTT-Online 64.3 Table 3.Test accuracy (%) on VID-Robust dataset (Shankar et al., 2019). TTT and TTT-Online improve over the baselines. Method Error (%) Object recognition task only 17.4 Joint training (Hendrycks et al., 2019a) 16.7 TTT (standard version) 15.9 Table 4.Test error (%) on CIFAR-10.1 (Recht et al., 2018). TTT is the ﬁrst method to improve the performance of an existing model on this new test set. both have the same human accuracy. This demonstrates how insidious and ubiquitous distribution shifts are, even when researchers strive to minimize them. The distribution shifts from CIFAR-10 to CIFAR-10.1 pose an extremely difﬁcult problem, and no prior work has been able to improve the performance of an existing model on this new test set, probably because: 1) researchers cannot even identify the distribution shifts, let alone describe them mathematically; 2) the samples in CIFAR-10.1 are only revealed at test time; and even if they were revealed during training, the distribution shifts are too subtle, and the sample size is too small, for domain adaptation (Recht et al., 2018). On the original CIFAR-10 test set, the baseline with only object recognition has error 8.9%, and with joint training has 8.1%; comparing to the ﬁrst two rows of Table 4, both suffer the typical performance drop (by a factor of two). TTT yields an improvement of 0.8% (relative improvement of 4.8%) over joint training. We recognize that this improve- ment is small relative to the performance drop, but see it as an encouraging ﬁrst step for this very difﬁcult problem.Test-Time Training with Self-Supervision for Generalization under Distribution Shifts 0 10 20 30 40 50 60 Gradient inner product 0 1 2 3 4 5Improvement (%) Level 5 Level 4 Level 3 Level 2 Level 1 0 10 20 30 40 50 60 Gradient inner product 0 5 10 15 20 25 30 35Improvement (%) Level 5 Level 4 Level 3 Level 2 Level 1 Figure 4.Scatter plot of the inner product between the gradients (on the shared feature extractor θe) of the main task lm and the self- supervised task le, and the improvement in test error (%) from Test-Time Training, for the standard (left) and online (right) version. Each point is the average over a test set, and each scatter plot has 75 test sets, from all 15 types of corruptions over ﬁve levels as described in subsection 3.1. The blue lines and bands are the best linear ﬁts and the 99% conﬁdence intervals. The linear correlation coefﬁcients are 0.93 and 0.89 respectively, indicating strong positive correlation between the two quantities, as suggested by Theorem 1. 4. Theoretical Results This section contains our preliminary study of when and why Test-Time Training is expected to work. For convex models, we prove that positive gradient correlation between the loss functions leads to better performance on the main task after Test-Time Training. Equipped with this insight, we then empirically demonstrate that gradient correlation governs the success of Test-Time Training on the deep learning model discussed in Section 3. Before stating our main theoretical result, we ﬁrst illustrate the general intuition with a toy model. Consider a regression problem where x∈Rd denotes the input, y1 ∈R denotes the label, and the objective is the square loss (ˆy−y1)2/2 for a prediction ˆy. Consider a two layer linear network parametrized by A∈Rh×d and v ∈Rh (where hstands for the hidden dimension). The prediction according to this model is ˆy= v⊤Ax, and the main task loss is lm(x,y1; A,v) = 1 2 ( y1 −v⊤Ax )2 . (4) In addition, consider a self-supervised regression task that also uses the square loss and automatically generates a label ys for x. Let the self-supervised head be parametrized by w∈Rh. Then the self-supervised task loss is ls(x,y2; A,w) = 1 2 ( y2 −w⊤Ax )2 . (5) Now we apply Test-Time Training to update the shared feature extractor Aby one step of gradient descent on ls, which we can compute with y2 known. This gives us A′←A−η ( y2 −w⊤Ax )( −wx⊤) , (6) where A′is the updated matrix and ηis the learning rate. If we set η= η∗where η∗= y1 −v⊤Ax (y2 −w⊤Ax) v⊤wx⊤x, (7) then with some simple algebra, it is easy to see that the main task loss lm(x,y1; A′,v) = 0. Concretely, Test-Time Training drives the main task loss down to zero with a single gradient step for a carefully chosen learning rate. In prac- tice, this learning rate is unknown since it depends on the unknown y1. However, since our model is convex, as long as η∗is positive, it sufﬁces to set η to be a small positive constant (see details in the appendix). If x̸= 0, one sufﬁ- cient condition for η∗to be positive (when neither loss is zero) is to have sign ( y1 −v⊤Ax ) = sign ( y2 −w⊤Ax ) (8) and v⊤w>0 . (9) For our toy model, both parts of the condition above have an intuition interpretation. The ﬁrst part says that the mistakes should be correlated, in the sense that predictions from both tasks are mistaken in the same direction. The second part, v⊤w>0, says that the decision boundaries on the feature space should be correlated. In fact, these two parts hold iff. ⟨∇lm(A),∇ls(A)⟩>0 (see a simple proof of this fact in the appendix). To summarize, if the gradients have positive correlation, Test-Time Training is guaranteed to reduce the main task loss. Our main theoretical result extends this to general smooth and convex loss functions.Test-Time Training with Self-Supervision for Generalization under Distribution Shifts Theorem 1. Let lm(x,y; θ) denote the main task loss on test instance x,y with parameters θ, and ls(x; θ) the self- supervised task loss that only depends onx. Assume that for all x,y, lm(x,y; θ) is differentiable, convex andβ-smooth in θ, and both ∥∇lm(x,y; θ)∥,∥∇ls(x,θ)∥≤ Gfor all θ. With a ﬁxed learning rate η= ϵ βG2 , for every x,y such that ⟨∇lm(x,y; θ),∇ls(x; θ)⟩>ϵ, (10) we have lm(x,y; θ) >lm(x,y; θ(x)), (11) where θ(x) = θ−η∇ls(x; θ) i.e. Test-Time Training with one step of gradient descent. The proof uses standard techniques in optimization, and is left for the appendix. Theorem 1 reveals gradient correlation as a determining factor of the success of Test-Time Training in the smooth and convex case. In Figure 4, we empirically show that our insight also holds for non-convex loss func- tions, on the deep learning model and across the diverse set of corruptions considered in Section 3; stronger gradient cor- relation clearly indicates more performance improvement over the baseline. 5. Related Work Learning on test instances. Shocher et al. (2018) pro- vide a key inspiration for our work by showing that image super-resolution could be learned at test time simply by try- ing to upsample a downsampled version of the input image. More recently, Bau et al. (2019) improve photo manipula- tion by adapting a pre-trained GAN to the statistics of the input image. One of the earlier examples of this idea comes from Jain & Learned-Miller (2011), who improve Viola- Jones face detection (Viola et al., 2001) by bootstrapping the more difﬁcult faces in an image from the more easily detected faces in that same image. The online version of our algorithm is inspired by the work of Mullapudi et al. (2018), which makes video segmentation more efﬁcient by using a student model that learns online from a teacher model. The idea of online updates has also been used in Kalal et al. (2011) for tracking and detection. A recent work in echocardiography (Zhu et al., 2019) improves the deep learning model that tracks myocardial motion and cardiac blood ﬂow with sequential updates. Lastly, we share the philosophy of transductive learning (Vapnik, 2013; Gam- merman et al., 1998), but have little in common with their classical algorithms; recent work by Tripuraneni & Mackey (2019) theoretically explores this for linear prediction, in the context of debiasing the LASSO estimator. Self-supervised learning studies how to create labels from the data, by designing various pretext tasks that can learn semantic information without human annotations, such as context prediction (Doersch et al., 2015), solving jig- saw puzzles (Noroozi & Favaro, 2016), colorization (Lars- son et al., 2017; Zhang et al., 2016), noise prediction (Bo- janowski & Joulin, 2017), feature clustering (Caron et al., 2018). Our paper uses rotation prediction (Gidaris et al., 2018). Asano et al. (2019) show that self-supervised learn- ing on only a single image, surprisingly, can produce low- level features that generalize well. Closely related to our work, Hendrycks et al. (2019a) propose that jointly training a main task and a self-supervised task (our joint training baseline in Section 3) can improve robustness on the main task. The same idea is used in few-shot learning (Su et al., 2019), domain generalization (Carlucci et al., 2019), and unsupervised domain adaptation (Sun et al., 2019). Adversarial robustness studies the robust risk RP,∆(θ) = Ex,y∼P maxδ∈∆ l(x + δ,y; θ), where l is some loss function, and ∆ is the set of perturbations; ∆ is often chosen as the Lp ball, for p ∈{1,2,∞}. Many popular algorithms formulate and solve this as a robust optimization problem (Goodfellow et al., 2014; Madry et al., 2017; Sinha et al., 2017; Raghunathan et al., 2018; Wong & Kolter, 2017; Croce et al., 2018), and the most well known technique is adversarial training. Another line of work is based on randomized smoothing (Cohen et al., 2019; Salman et al., 2019), while some other approaches, such as input transformations (Guo et al., 2017; Song et al., 2017), are shown to be less effective (Athalye et al., 2018). There are two main problems with the approaches above. First, all of them can be seen as smoothing the decision boundary. This establishes a theoretical tradeoff between accuracy and robustness (Tsipras et al., 2018; Zhang et al., 2019), which we also observe empirically with our adversarial training baseline in Section 3. Intuitively, the more diverse ∆ is, the less effective this one-boundary-ﬁts-all approach can be for a particular element of ∆. Second, adversarial methods rely heavily on the mathematical structure of ∆, which might not accurately model perturbations in the real world. Therefore, generalization remains hard outside of the ∆ we know in advance or can mathematically model, especially for non-adversarial distribution shifts. Empirically, Kang et al. (2019) shows that robustness for one ∆ might not transfer to another, and training on the L∞ball actually hurts robustness on the L1 ball. Non-adversarial robustness studies the effect of corrup- tions, perturbations, out-of-distribution examples, and real- world distribution shifts (Hendrycks et al., 2019b;a; 2018; Hendrycks & Gimpel, 2016). Geirhos et al. (2018) show that training on images corrupted by Gaussian noise makes deep learning models robust to this particular noise type, but does not improve performance on images corrupted by another noise type e.g. salt-and-pepper noise.Test-Time Training with Self-Supervision for Generalization under Distribution Shifts Unsupervised domain adaptation (a.k.a. transfer learn- ing) studies the problem of distribution shifts, when an unlabeled dataset from the test distribution (target domain) is available at training time, in addition to a labeled dataset from the training distribution (source domain) (Chen et al., 2011; Gong et al., 2012; Long et al., 2015; Ganin et al., 2016; Long et al., 2016; Tzeng et al., 2017; Hoffman et al., 2017; Csurka, 2017; Chen et al., 2018). The limitation of the problem setting, however, is that generalization might only be improved for this speciﬁc test distribution, which can be difﬁcult to anticipate in advance. Prior work try to anticipate broader distributions by using multiple and evolv- ing domains (Hoffman et al., 2018; 2012; 2014). Test-Time Training does not anticipate any test distribution, by chang- ing the setting of unsupervised domain adaptation, while taking inspiration from its algorithms. Our paper is a follow- up to Sun et al. (2019), which we explain and empirically compare with in Section 3. Our update rule can be viewed as performing one-sample unsupervised domain adaptation on the ﬂy, with the caveat that standard domain adaptation techniques might become ill-deﬁned when there is only one sample from the target domain. Domain generalization studies the setting where a meta distribution generates multiple environment distributions, some of which are available during training (source), while others are used for testing (target) (Li et al., 2018; Shankar et al., 2018; Muandet et al., 2013; Balaji et al., 2018; Ghifary et al., 2015; Motiian et al., 2017; Li et al., 2017a; Gan et al., 2016). With only a few environments, information on the meta distribution is often too scarce to be helpful, and with many environments, we are back to the i.i.d. setting where each environment can be seen as a sample, and a strong baseline is to simply train on all the environments (Li et al., 2019). The setting of domain generalization is limited by the inherent tradeoff between speciﬁcity and generality of a ﬁxed decision boundary, and the fact that generalization is again elusive outside of the meta distribution i.e. the actual P learned by the algorithm. One (few)-shot learning studies how to learn a new task or a new classiﬁcation category using only one (or a few) sample(s), on top of a general representation that has been learned on diverse samples (Snell et al., 2017; Vinyals et al., 2016; Fei-Fei et al., 2006; Ravi & Larochelle, 2016; Li et al., 2017b; Finn et al., 2017; Gidaris & Komodakis, 2018). Our update rule can be viewed as performing one-shot self- supervised learning and can potentially be improved by progress in one-shot learning. Continual learning (a.k.a. learning without forgetting) studies the setting where a model is made to learn a sequence of tasks, and not forget about the earlier ones while training for the later (Li & Hoiem, 2017; Lopez-Paz & Ranzato, 2017; Kirkpatrick et al., 2017; Santoro et al., 2016). In contrast, with Test-Time Training, we are not concerned about forgetting the past test samples since they have already been evaluated on; and if a past sample comes up by any chance, it would go through Test-Time Training again. In addition, the impact of forgetting the training set is minimal, because both tasks have already been jointly trained. Online learning (a.k.a. online optimization) is a well- studied area of learning theory (Shalev-Shwartz et al., 2012; Hazan et al., 2016). The basic setting repeats the following: receive xt, predict ˆyt, receive yt from a worst-case oracle, and learn. Final performance is evaluated using the regret, which colloquially translates to how much worse the online learning algorithm performs in comparison to the best ﬁxed model in hindsight. In contrast, our setting never reveals any yt during testing even for the online version, so we do not need to invoke the concept of the worst-case oracle or the regret. Also, due to the lack of feedback from the envi- ronment after predicting, our algorithm is motivated to learn (with self-supervision) before predicting ˆyt instead of after. Note that some of the previously covered papers (Hoffman et al., 2014; Jain & Learned-Miller, 2011; Mullapudi et al., 2018) use the term “online learning” outside of the learning theory setting, so the term can be overloaded. 6. Discussion The idea of test-time training also makes sense for other tasks, such as segmentation and detection, and in other ﬁelds, such as speech recognition and natural language process- ing. For machine learning practitioners with prior domain knowledge in their respective ﬁelds, their expertise can be leveraged to design better special-purpose self-supervised tasks for test-time training. Researchers for general-purpose self-supervised tasks can also use test-time training as an evaluation benchmark, in addition to the currently prevalent benchmark of pre-training and ﬁne-tuning. More generally, we hope this paper can encourage re- searchers to abandon the self-imposed constraint of a ﬁxed decision boundary for testing, or even the artiﬁcial division between training and testing altogether. Our work is but a small step toward a new paradigm where much of the learning happens after a model is deployed. Acknowledgements. This work is supported by NSF grant 1764033, DARPA and Berkeley DeepDrive. This paper took a long time to develop, and beneﬁted from con- versations with many of our colleagues, including Ben Recht and his students Ludwig Schmidt, Vaishaal Shanker and Becca Roelofs; Ravi Teja Mullapudi, Achal Dave and Deva Ramanan; and Armin Askari, Allan Jabri, Ashish Kumar, Angjoo Kanazawa and Jitendra Malik.Test-Time Training with Self-Supervision for Generalization under Distribution Shifts References Asano, Y . M., Rupprecht, C., and Vedaldi, A. Surprising effectiveness of few-image unsupervised feature learning. arXiv preprint arXiv:1904.13132, 2019. Athalye, A., Carlini, N., and Wagner, D. Obfuscated gradients give a false sense of security: Circumvent- ing defenses to adversarial examples. arXiv preprint arXiv:1802.00420, 2018. Balaji, Y ., Sankaranarayanan, S., and Chellappa, R. Metareg: Towards domain generalization using meta-regularization. In Advances in Neural Information Processing Systems, pp. 998–1008, 2018. Bau, D., Strobelt, H., Peebles, W., Wulff, J., Zhou, B., Zhu, J.-Y ., and Torralba, A. Semantic photo manipulation with a generative image prior. ACM Transactions on Graphics (TOG), 38(4):59, 2019. Bojanowski, P. and Joulin, A. Unsupervised learning by predicting noise. In Proceedings of the 34th International Conference on Machine Learning-Volume 70, pp. 517– 526. JMLR. org, 2017. Carlucci, F. M., D’Innocente, A., Bucci, S., Caputo, B., and Tommasi, T. Domain generalization by solving jigsaw puzzles. In Proceedings of the IEEE Conference on Com- puter Vision and Pattern Recognition , pp. 2229–2238, 2019. Caron, M., Bojanowski, P., Joulin, A., and Douze, M. Deep clustering for unsupervised learning of visual features. In Proceedings of the European Conference on Computer Vision (ECCV), pp. 132–149, 2018. Caruana, R. Multitask learning. Machine learning, 28(1): 41–75, 1997. Chen, M., Weinberger, K. Q., and Blitzer, J. Co-training for domain adaptation. In Advances in neural information processing systems, pp. 2456–2464, 2011. Chen, X., Sun, Y ., Athiwaratkun, B., Cardie, C., and Wein- berger, K. Adversarial deep averaging networks for cross- lingual sentiment classiﬁcation. Transactions of the Asso- ciation for Computational Linguistics, 6:557–570, 2018. Cohen, J. M., Rosenfeld, E., and Kolter, J. Z. Certiﬁed adversarial robustness via randomized smoothing. arXiv preprint arXiv:1902.02918, 2019. Croce, F., Andriushchenko, M., and Hein, M. Provable robustness of relu networks via maximization of linear regions. arXiv preprint arXiv:1810.07481, 2018. Csurka, G. Domain adaptation for visual applications: A comprehensive survey. arXiv preprint arXiv:1702.05374, 2017. Ding, G. W., Wang, L., and Jin, X. AdverTorch v0.1: An adversarial robustness toolbox based on pytorch. arXiv preprint arXiv:1902.07623, 2019. Doersch, C., Gupta, A., and Efros, A. A. Unsupervised visual representation learning by context prediction. In Proceedings of the IEEE International Conference on Computer Vision, pp. 1422–1430, 2015. Fei-Fei, L., Fergus, R., and Perona, P. One-shot learning of object categories. IEEE transactions on pattern analysis and machine intelligence, 28(4):594–611, 2006. Finn, C., Abbeel, P., and Levine, S. Model-agnostic meta- learning for fast adaptation of deep networks. In Proceed- ings of the 34th International Conference on Machine Learning-Volume 70, pp. 1126–1135. JMLR. org, 2017. Gammerman, A., V ovk, V ., and Vapnik, V . Learning by transduction. In Proceedings of the Fourteenth conference on Uncertainty in artiﬁcial intelligence , pp. 148–155. Morgan Kaufmann Publishers Inc., 1998. Gan, C., Yang, T., and Gong, B. Learning attributes equals multi-source domain generalization. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pp. 87–97, 2016. Ganin, Y ., Ustinova, E., Ajakan, H., Germain, P., Larochelle, H., Laviolette, F., Marchand, M., and Lempitsky, V . Domain-adversarial training of neural networks. The Journal of Machine Learning Research, 17(1):2096–2030, 2016. Geirhos, R., Temme, C. R., Rauber, J., Sch¨utt, H. H., Bethge, M., and Wichmann, F. A. Generalisation in humans and deep neural networks. In Advances in Neural Information Processing Systems, pp. 7538–7550, 2018. Ghifary, M., Bastiaan Kleijn, W., Zhang, M., and Balduzzi, D. Domain generalization for object recognition with multi-task autoencoders. In Proceedings of the IEEE international conference on computer vision, pp. 2551– 2559, 2015. Gidaris, S. and Komodakis, N. Dynamic few-shot visual learning without forgetting. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pp. 4367–4375, 2018. Gidaris, S., Singh, P., and Komodakis, N. Unsupervised rep- resentation learning by predicting image rotations. arXiv preprint arXiv:1803.07728, 2018. Gong, B., Shi, Y ., Sha, F., and Grauman, K. Geodesic ﬂow kernel for unsupervised domain adaptation. In2012 IEEE Conference on Computer Vision and Pattern Recognition, pp. 2066–2073. IEEE, 2012.Test-Time Training with Self-Supervision for Generalization under Distribution Shifts Goodfellow, I. J., Shlens, J., and Szegedy, C. Explain- ing and harnessing adversarial examples. arXiv preprint arXiv:1412.6572, 2014. Guo, C., Rana, M., Cisse, M., and van der Maaten, L. Coun- tering adversarial images using input transformations. arXiv preprint arXiv:1711.00117, 2017. Hazan, E. et al. Introduction to online convex optimization. Foundations and Trends® in Optimization, 2(3-4):157– 325, 2016. He, K., Zhang, X., Ren, S., and Sun, J. Deep residual learn- ing for image recognition. In Proceedings of the IEEE conference on computer vision and pattern recognition, pp. 770–778, 2016a. He, K., Zhang, X., Ren, S., and Sun, J. Identity mappings in deep residual networks. In European conference on computer vision, pp. 630–645. Springer, 2016b. He, K., Girshick, R., and Doll ´ar, P. Rethinking imagenet pre-training. arXiv preprint arXiv:1811.08883, 2018. Hendrycks, D. and Dietterich, T. Benchmarking neural network robustness to common corruptions and perturba- tions. arXiv preprint arXiv:1903.12261, 2019. Hendrycks, D. and Gimpel, K. A baseline for detecting misclassiﬁed and out-of-distribution examples in neural networks. arXiv preprint arXiv:1610.02136, 2016. Hendrycks, D., Mazeika, M., Wilson, D., and Gimpel, K. Using trusted data to train deep networks on labels cor- rupted by severe noise. InAdvances in neural information processing systems, pp. 10456–10465, 2018. Hendrycks, D., Lee, K., and Mazeika, M. Using pre-training can improve model robustness and uncertainty. arXiv preprint arXiv:1901.09960, 2019a. Hendrycks, D., Mazeika, M., Kadavath, S., and Song, D. Improving model robustness and uncertainty estimates with self-supervised learning. arXiv preprint, 2019b. Hoffman, J., Kulis, B., Darrell, T., and Saenko, K. Discover- ing latent domains for multisource domain adaptation. In European Conference on Computer Vision, pp. 702–715. Springer, 2012. Hoffman, J., Darrell, T., and Saenko, K. Continuous man- ifold based adaptation for evolving visual domains. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pp. 867–874, 2014. Hoffman, J., Tzeng, E., Park, T., Zhu, J.-Y ., Isola, P., Saenko, K., Efros, A. A., and Darrell, T. Cycada: Cycle- consistent adversarial domain adaptation. arXiv preprint arXiv:1711.03213, 2017. Hoffman, J., Mohri, M., and Zhang, N. Algorithms and theory for multiple-source adaptation. In Advances in Neural Information Processing Systems, pp. 8246–8256, 2018. Huang, G., Sun, Y ., Liu, Z., Sedra, D., and Weinberger, K. Q. Deep networks with stochastic depth. In European conference on computer vision, pp. 646–661. Springer, 2016. Ioffe, S. and Szegedy, C. Batch normalization: Accelerating deep network training by reducing internal covariate shift. arXiv preprint arXiv:1502.03167, 2015. Jain, V . and Learned-Miller, E. Online domain adaptation of a pre-trained cascade of classiﬁers. In CVPR 2011, pp. 577–584. IEEE, 2011. Kalal, Z., Mikolajczyk, K., and Matas, J. Tracking-learning- detection. IEEE transactions on pattern analysis and machine intelligence, 34(7):1409–1422, 2011. Kang, D., Sun, Y ., Brown, T., Hendrycks, D., and Steinhardt, J. Transfer of adversarial robustness between perturbation types. arXiv preprint arXiv:1905.01034, 2019. Kannan, H., Kurakin, A., and Goodfellow, I. Adversarial logit pairing. arXiv preprint arXiv:1803.06373, 2018. Kirkpatrick, J., Pascanu, R., Rabinowitz, N., Veness, J., Des- jardins, G., Rusu, A. A., Milan, K., Quan, J., Ramalho, T., Grabska-Barwinska, A., et al. Overcoming catastrophic forgetting in neural networks. Proceedings of the national academy of sciences, 114(13):3521–3526, 2017. Krizhevsky, A. and Hinton, G. Learning multiple layers of features from tiny images. Technical report, Citeseer, 2009. Larsson, G., Maire, M., and Shakhnarovich, G. Colorization as a proxy task for visual understanding. In CVPR, 2017. Li, D., Yang, Y ., Song, Y .-Z., and Hospedales, T. M. Deeper, broader and artier domain generalization. In Proceed- ings of the IEEE International Conference on Computer Vision, pp. 5542–5550, 2017a. Li, D., Zhang, J., Yang, Y ., Liu, C., Song, Y .-Z., and Hospedales, T. M. Episodic training for domain gen- eralization. arXiv preprint arXiv:1902.00113, 2019. Li, Y ., Tian, X., Gong, M., Liu, Y ., Liu, T., Zhang, K., and Tao, D. Deep domain generalization via conditional invariant adversarial networks. In Proceedings of the European Conference on Computer Vision (ECCV), pp. 624–639, 2018.Test-Time Training with Self-Supervision for Generalization under Distribution Shifts Li, Z. and Hoiem, D. Learning without forgetting. IEEE transactions on pattern analysis and machine intelligence, 40(12):2935–2947, 2017. Li, Z., Zhou, F., Chen, F., and Li, H. Meta-sgd: Learning to learn quickly for few-shot learning. arXiv preprint arXiv:1707.09835, 2017b. Liu, Z., Sun, M., Zhou, T., Huang, G., and Darrell, T. Re- thinking the value of network pruning. arXiv preprint arXiv:1810.05270, 2018. Long, M., Cao, Y ., Wang, J., and Jordan, M. I. Learn- ing transferable features with deep adaptation networks. arXiv preprint arXiv:1502.02791, 2015. Long, M., Zhu, H., Wang, J., and Jordan, M. I. Unsupervised domain adaptation with residual transfer networks. In Advances in Neural Information Processing Systems, pp. 136–144, 2016. Lopez-Paz, D. and Ranzato, M. Gradient episodic memory for continual learning. In Advances in Neural Information Processing Systems, pp. 6467–6476, 2017. Madry, A., Makelov, A., Schmidt, L., Tsipras, D., and Vladu, A. Towards deep learning models resistant to adversarial attacks. arXiv preprint arXiv:1706.06083 , 2017. Motiian, S., Piccirilli, M., Adjeroh, D. A., and Doretto, G. Uniﬁed deep supervised domain adaptation and gen- eralization. In Proceedings of the IEEE International Conference on Computer Vision, pp. 5715–5725, 2017. Muandet, K., Balduzzi, D., and Sch ¨olkopf, B. Domain generalization via invariant feature representation. In International Conference on Machine Learning, pp. 10– 18, 2013. Mullapudi, R. T., Chen, S., Zhang, K., Ramanan, D., and Fatahalian, K. Online model distillation for efﬁcient video inference. arXiv preprint arXiv:1812.02699, 2018. Noroozi, M. and Favaro, P. Unsupervised learning of visual representations by solving jigsaw puzzles. In European Conference on Computer Vision , pp. 69–84. Springer, 2016. Raghunathan, A., Steinhardt, J., and Liang, P. Certiﬁed defenses against adversarial examples. arXiv preprint arXiv:1801.09344, 2018. Ravi, S. and Larochelle, H. Optimization as a model for few-shot learning. IEEE transactions on pattern analysis and machine intelligence, 2016. Recht, B., Roelofs, R., Schmidt, L., and Shankar, V . Do cifar-10 classiﬁers generalize to cifar-10? arXiv preprint arXiv:1806.00451, 2018. Russakovsky, O., Deng, J., Su, H., Krause, J., Satheesh, S., Ma, S., Huang, Z., Karpathy, A., Khosla, A., Bernstein, M., Berg, A. C., and Fei-Fei, L. ImageNet Large Scale Visual Recognition Challenge. International Journal of Computer Vision (IJCV) , 115(3):211–252, 2015. doi: 10.1007/s11263-015-0816-y. Salman, H., Yang, G., Li, J., Zhang, P., Zhang, H., Razen- shteyn, I., and Bubeck, S. Provably robust deep learn- ing via adversarially trained smoothed classiﬁers. arXiv preprint arXiv:1906.04584, 2019. Santoro, A., Bartunov, S., Botvinick, M., Wierstra, D., and Lillicrap, T. Meta-learning with memory-augmented neu- ral networks. In International conference on machine learning, pp. 1842–1850, 2016. Shalev-Shwartz, S. et al. Online learning and online con- vex optimization. Foundations and Trends® in Machine Learning, 4(2):107–194, 2012. Shankar, S., Piratla, V ., Chakrabarti, S., Chaudhuri, S., Jyothi, P., and Sarawagi, S. Generalizing across domains via cross-gradient training. arXiv preprint arXiv:1804.10745, 2018. Shankar, V ., Dave, A., Roelofs, R., Ramanan, D., Recht, B., and Schmidt, L. Do image classiﬁers generalize across time? arXiv, 2019. Shocher, A., Cohen, N., and Irani, M. zero-shot super- resolution using deep internal learning. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pp. 3118–3126, 2018. Sinha, A., Namkoong, H., and Duchi, J. Certifying some dis- tributional robustness with principled adversarial training. arXiv preprint arXiv:1710.10571, 2017. Snell, J., Swersky, K., and Zemel, R. Prototypical networks for few-shot learning. In Advances in Neural Information Processing Systems, pp. 4077–4087, 2017. Song, Y ., Kim, T., Nowozin, S., Ermon, S., and Kushman, N. Pixeldefend: Leveraging generative models to understand and defend against adversarial examples. arXiv preprint arXiv:1710.10766, 2017. Su, J.-C., Maji, S., and Hariharan, B. Boosting supervi- sion with self-supervision for few-shot learning. arXiv preprint arXiv:1906.07079, 2019. Sun, Y ., Tzeng, E., Darrell, T., and Efros, A. A. Unsuper- vised domain adaptation through self-supervision. arXiv preprint, 2019.Test-Time Training with Self-Supervision for Generalization under Distribution Shifts Tripuraneni, N. and Mackey, L. Debiasing linear prediction. arXiv preprint arXiv:1908.02341, 2019. Tsipras, D., Santurkar, S., Engstrom, L., Turner, A., and Madry, A. Robustness may be at odds with accuracy. arXiv preprint arXiv:1805.12152, 2018. Tzeng, E., Hoffman, J., Saenko, K., and Darrell, T. Adver- sarial discriminative domain adaptation. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pp. 7167–7176, 2017. Vapnik, V .The nature of statistical learning theory. Springer science & business media, 2013. Vinyals, O., Blundell, C., Lillicrap, T., Wierstra, D., et al. Matching networks for one shot learning. In Advances in neural information processing systems, pp. 3630–3638, 2016. Viola, P., Jones, M., et al. Rapid object detection using a boosted cascade of simple features. CVPR (1), 1(511- 518):3, 2001. Wong, E. and Kolter, J. Z. Provable defenses against adver- sarial examples via the convex outer adversarial polytope. arXiv preprint arXiv:1711.00851, 2017. Zhang, H., Yu, Y ., Jiao, J., Xing, E. P., Ghaoui, L. E., and Jor- dan, M. I. Theoretically principled trade-off between ro- bustness and accuracy. arXiv preprint arXiv:1901.08573, 2019. Zhang, R., Isola, P., and Efros, A. A. Colorful image col- orization. In European conference on computer vision, pp. 649–666. Springer, 2016. Zhu, W., Huang, Y ., Vannan, M. A., Liu, S., Xu, D., Fan, W., Qian, Z., and Xie, X. Neural multi-scale self-supervised registration for echocardiogram dense tracking. arXiv preprint arXiv:1906.07357, 2019.Appendix: Test-Time Training with Self-Supervision for Generalization under Distribution Shifts A1. Informal Discussion on Our Variable Decision Boundary In the introduction, we claim that in traditional supervised learning θgives a ﬁxed decision boundary, while ourθgives a variable decision boundary. Here we informally discuss this claim. Denote the input space Xand output space Y. A decision boundary is simply a mapping f : X →Y. Let Θ be a model class e.g Rd. Now consider a family of parametrized functions gθ : X→Y , where θ∈Θ. In the context of deep learning, gis the neural network architecture and θcontains the parameters. We say that f is a ﬁxed decision boundary w.r.t. g and Θ if there exists θ ∈Θ s.t. f(x) = gθ(x) for every x ∈X , and a variable decision boundary if for every x∈X, there exists θ∈Θ s.t. f(x) = gθ(x). Note how selection of θcan depend on xfor a variable decision boundary, and cannot for a ﬁxed one. It is then trivial to verify that our claim is true under those deﬁnitions. A critical reader might say that with an arbitrarily large model class, can’t every decision boundary be ﬁxed? Yes, but this is not the end of the story. Let d = dim( X) × dim(Y), and consider the enormous model class Θ′= Rd which is capable of representing all possible mappings be- tween Xand Y. Let g′ θ′ simply be the mapping represented by θ′ ∈Θ′. A variable decision boundary w.r.t. g and Θ then indeed must be a ﬁxed decision boundary w.r.t. g′and Θ′, but we would like to note two things. First, without any prior knowledge, generalization in Θ′is impossible with any ﬁnite amount of training data; reasoning about g′and Θ′is most likely not productive from an algorithmic point of view, and the concept of a variable decision boundary is to avoid such reasoning. Second, selecting θbased on xfor a variable decision boundary can be thought of as “training” on all points x ∈Rd; however, “training” only happens when necessary, for the xthat it actually encounters. Altogether, the concept of a variable decision boundary is different from what can be described by traditional learning theory. A formal discussion is beyond the scope of this paper and might be of interest to future work. A2. Computational Aspects of Our Method At test time, our method is 2 × batch size × number of iterations times slower than regular test- ing, which only performs a single forward pass for each sample. As the ﬁrst work on Test-Time Training, this paper is not as concerned about computational efﬁciency as improving robustness, but here we provide two poten- tial solutions that might be useful, but have not been thor- oughly veriﬁed. The ﬁrst is to use the thresholding trick on ls, introduced as a solution for the small batches prob- lem in the method section. For the models considered in our experiments, roughly 80% of the test instances fall below the threshold, so Test-Time Training can only be performed on the other 20% without much effect on per- formance, because those 20% contain most of the sam- ples with wrong predictions. The second is to reduce the number of iterations of test-time updates. For the online version, the number of iterations is al- ready 1, so there is nothing to do. For the standard ver- sion, we have done some preliminary experiments setting number of iterations to 1 (instead of 10) and learn- ing rate to 0.01 (instead of 0.001), and observing results almost as good as the standard hyper-parameter setting. A more in depth discussion on efﬁciency is left for future works, which might, during training, explicitly make the model amenable to fast updates. A3. Proofs Here we prove the theoretical results in the main paper. A3.1. The Toy Problem The following setting applies to the two lemmas; this is simply the setting of our toy problem, reproduced here for ease of reference.Test-Time Training with Self-Supervision for Generalization under Distribution Shifts Consider a two layer linear network parametrized by A∈ Rh×d (shared) and v,w ∈Rh (ﬁxed) for the two heads, respectively. Denote x∈Rd the input and y1,y2 ∈R the labels for the two tasks, respectively. For the main task loss lm(A; v) = 1 2 ( y1 −v⊤Ax )2 , (12) and the self-supervised task loss ls(A; w) = 1 2 ( y2 −w⊤Ax )2 , (13) Test-Time Training yields an updated matrix A′←A−η ( y2 −w⊤Ax )( −wx⊤) , (14) where ηis the learning rate. Lemma 1. Following the exposition of the main paper, let η∗= (y1 −v⊤Ax) (y2 −w⊤Ax)v⊤wx⊤x. (15) Assume η∗∈[ϵ,∞) for some ϵ> 0. Then for any η∈(0,ϵ], we are guaranteed an improvement on the main loss i.e. lm(A′) <lm(A). Proof. From the exposition of the main paper, we know that lm(A−η∗∇lsA)) = 0, which can also be derived from simple algebra. Then by convexity, we have lm(A−η∇ls(A)) (16) = lm (( 1 − η η∗ ) A+ η η∗(A−η∗∇ls(A)) ) (17) ≤ ( 1 − η η∗ ) lm(A) + 0 (18) ≤ ( 1 −η ϵ ) lm(A) (19) <lm(A), (20) where the last inequality uses the assumption that lm(A) > 0, which holds because η∗>0. Lemma 2. Deﬁne ⟨U,V⟩= vec (U)⊤vec (V) i.e. the Frobenious inner product, then sign (η∗) = sign (⟨∇lm(A),∇ls(A)⟩) . (21) Proof. By simple algebra, ⟨∇lm(A),∇ls(A)⟩ = ⟨ ( y1 −v⊤Ax )( −vx⊤) , ( y2 −w⊤Ax )( −wx⊤) ⟩ = ( y1 −v⊤Ax )( y2 −w⊤Ax ) Tr ( xv⊤wx⊤) = ( y1 −v⊤Ax )( y2 −w⊤Ax ) v⊤wx⊤x, which has the same sign as η∗. A3.2. Proof of Theorem 1 For any η, by smoothness and convexity, lm(x,y; θ(x)) = lm(x,y; θ−η∇ls(x; θ)) ≤lm(x,y; θ) + η⟨∇lm(x,y; θ),∇ls(x,θ)⟩ + η2β 2 ∥∇ls(x; θ)∥2 . Denote η∗= ⟨∇lm(x,y; θ),∇ls(x,θ)⟩ β∥∇ls(x; θ)∥2 . Then Equation 22 becomes lm(x,y; θ−η∗∇ls(x; θ)) (22) ≤lm(x,y; θ) −⟨∇lm(x,y; θ),∇ls(x,θ)⟩2 2β∥∇ls(x; θ)∥2 . (23) And by our assumptions on the gradient norm and gradient inner product, lm(x,y; θ) −lm(x,y; θ−η∗∇ls(x; θ)) ≥ ϵ2 2βG2 . (24) Because we cannot observe η∗in practice, we instead use a ﬁxed learning rate η = ϵ βG2 , as stated in Theorem 1. Now we argue that this ﬁxed learning rate still improves performance on the main task. By our assumptions, η∗ ≥ ϵ βG2 , so η ∈(0,η∗]. Denote g= ∇ls(x; θ), then by convexity of lm, lm(x,y; θ(x)) = lm(x,y; θ−ηg) (25) = lm ( x,y; ( 1 − η η∗ ) θ+ η η∗(θ−η∗g) ) (26) ≤ ( 1 − η η∗ ) lm(x,y; θ) + η η∗lm(x,y; θ−η∗g) (27) Combining with Equation 24, we have lm(x,y; θ(x)) ≤ ( 1 − η η∗ ) lm(x,y; θ) + η η∗ ( lm(x,y; θ) − ϵ2 2βG2 ) = lm(x,y; θ) − η η∗ ϵ2 2βG2 Since η/η∗>0, we have shown that lm(x,y; θ) −lm(x,y; θ(x)) >0. (28)Test-Time Training with Self-Supervision for Generalization under Distribution Shifts A4. Additional Results on the Common Corruptions Dataset For table aethetics, we use the following abbreviations: B for baseline, JT for joint training, TTT for Test-Time Train- ing standard version, and TTT-Online for online Test-Time Training i.e. the online version. We have abbreviated the names of the corruptions, in order: original test set, Gaussian noise, shot noise, impulse noise, defocus blur, glass blue, motion blur, zoom blur, snow, frost, fog, brightness, contrast, elastic transformation, pixelation, and JPEG compression. A4.1. Results Using Batch Normalization As discussed in the results section, Batch Normalization (BN) is ineffective for small batches, which are the inputs for Test-Time Training (both standard and online version) since there is only one sample available when forming each batch; therefore, our main results are based on a ResNet using Group Normalization (GN). Figure A2 and Table A1 show results of our method on CIFAR-10-C level 5, with a ResNet using Batch Normalization (BN). These results are only meant to be a point of reference for the curious readers. In the early stage of this project, we have experimented with two potential solutions to the small batches problem with BN. The naive solution is to ﬁx the BN layers during Test-Time Training. but this diminishes the performance gains since there are fewer shared parameters. The better solution, adopted for the results below, is hard example mining: instead of updating on all inputs, we only update on inputs that incur large self-supervised task loss ls, where the large improvements might counter the negative effects of inaccurate statistics. Test-Time Training (standard version) is still very effective with BN. In fact, some of the improvements are quite dra- matic, such as on contrast (34%), defocus blue (18%) and Gaussian noise (22% comparing to joint-training, and 16% comparing to the baseline). Performance on the original distribution is still almost the same, and the original error with BN is in fact slightly lower than with GN, and takes half as many epochs to converge. We did not further experiment with BN because of two rea- sons: 1) The online version does not work with BN, because the problem with inaccurate batch statistics is exacerbated when training online for many (e.g. 10000) steps. 2) The baseline error for almost every corruption type is signiﬁ- cantly higher with BN than with GN. Although unrelated to the main idea of our paper, we make the interesting note that GN signiﬁcantly improves model robustness. A4.2. Additional Baseline: Adversarial Logit Pairing As discussed in the results section, Hendrycks & Dietterich (2019) point to Adversarial Logit Pairing (ALP) (Kannan et al., 2018) as an effective method for improving model robustness to corruptions and perturbations, even though it was designed to defend against adversarial attacks. We take ALP as an additional baseline on all benchmarks based on CIFAR-10 (using GN), following the training proce- dure in Kannan et al. (2018) and their recommended hyper- parameters. The implementation of the adversarial attack comes from the codebase of Ding et al. (2019). We did not run ALP on ImageNet because the two papers we reference for this method, Kannan et al. (2018) and Hendrycks & Di- etterich (2019), did not run on ImageNet or make any claim or recommendation. A4.3. Results on CIFAR-10-C and ImageNet-C, Level 5 Table A2 and Table A3 correspond to the bar plots in the results section. Two rows of Table A2 have been presented as Table 1 in the main text. A4.4. Results on CIFAR-10-C, Levels 1-4 The following bar plots and tables are on levels 1-4 of CIFAR-10-C. The original distribution is the same for all levels, so are our results on the original distribution. A4.5. Direct Comparison with Hendrycks et al. (2019a) The following comparison has been requested by an anony- mous reviewer for our ﬁnal version. Our joint training baseline is based on Hendrycks et al. (2019a), but also incor- porates some architectural changes (see below). We found these changes improved the robustness of our method, and felt that it was important to give the baseline the same ben- eﬁt. Note that our joint training baseline overall performs better than Hendrycks: Compare Table S2 to Figure 3 of Hendrycks et al. (2019a) (provided by the authors), our baseline has average error of 22.8% across all corruptions and levels, while their average error is 28.6%. Summary of architectural changes: 1) Group Normalization (GN) instead of Batch Normalization (BN). For complete- ness, the results with BN are provided in Table S1; c.f. GN results in Table S2 which signiﬁcantly improves robustness, with or without self-supervision. 2) We split after the sec- ond residual group, while they split after the third residual group right before the linear layer. This consistently gives about 0.5% - 1% improvement. 3) We use a ResNet-26, while they use a 40-2 Wide ResNet. But our baseline still performs better than their method even though our network is 4x smaller, due to the two tricks above.Test-Time Training with Self-Supervision for Generalization under Distribution Shifts Gaussian Noise  Shot Noise  Impulse Noise  Defocus Blur  Frosted Glass Blur Motion Blur  Zoom Blur  Snow  Frost  Fog Brightness  Contrast  Elastic  Pixelate  JPEG Figure A1.Sample images from the Common Corruptions Benchmark, taken from the original paper by Hendrycks & Dietterich (2019). originalgauss shot impulsedefocus glass motion zoom snow frost fog bright contrastelasticpixelate jpeg 0 20 40 60Error (%) Object recognition task only Joint training (Hendrycks et al. 2019) TTT Figure A2.Test error (%) on CIFAR-10-C, level 5, ResNet-26 with Batch Normalization. orig gauss shot impul defoc glass motn zoom snow frost fog brit contr elast pixel jpeg B 7.9 63.9 58.8 64.3 46.3 54.6 41.6 45.9 31.9 44.0 37.5 13.0 69.2 33.8 61.4 31.7 JT 7.5 70.7 65.6 67.2 43.1 55.4 40.9 42.7 30.3 44.5 42.5 12.7 58.6 30.7 62.6 31.9 TTT 7.9 47.9 45.2 54.8 27.6 50.4 31.5 30.9 28.7 34.3 26.9 12.6 35.2 30.6 51.2 31.3 Table A1.Test error (%) on CIFAR-10-C, level 5, ResNet-26 with Batch Normalization. orig gauss shot impul defoc glass motn zoom snow frost fog brit contr elast pixel jpeg B 8.9 50.5 47.2 56.1 23.7 51.7 24.3 26.3 25.6 34.4 28.1 13.5 25.0 27.4 55.8 29.8 JT 8.1 49.4 45.3 53.4 24.2 48.5 24.8 26.4 25.0 32.5 27.5 12.6 25.3 24.0 51.6 28.7 TTT 7.9 45.6 41.8 50.0 21.8 46.1 23.0 23.9 23.9 30.0 25.1 12.2 23.9 22.6 47.2 27.2 TTT-Online 8.2 25.8 22.6 30.6 14.6 34.4 18.3 17.1 20.0 18.0 16.9 11.2 15.6 21.6 18.1 21.2 UDA-SS 9.0 28.2 26.5 20.8 15.6 43.7 24.5 23.8 25.0 24.9 17.2 12.7 11.6 22.1 20.3 22.6 ALP 16.5 22.7 22.9 28.3 25.0 25.6 27.4 23.1 25.2 27.2 64.8 21.7 73.6 23.0 20.2 18.9 Table A2.Test error (%) on CIFAR-10-C, level 5, ResNet-26. orig gauss shot impul defoc glass motn zoom snow frost fog brit contr elast pixel jpeg B 68.9 1.3 2.0 1.3 7.5 6.6 11.8 16.2 15.7 14.9 15.3 43.9 9.7 16.5 15.3 23.4 JT 69.1 2.1 3.1 2.1 8.7 6.7 12.3 16.0 15.3 15.8 17.0 45.3 11.0 18.4 19.7 22.9 TTT 69.0 3.1 4.5 3.5 10.1 6.8 13.5 18.5 17.1 17.9 20.0 47.0 14.4 20.9 22.8 25.3 TTT-Online 68.8 26.3 28.6 26.9 23.7 6.6 28.7 33.4 35.6 18.7 47.6 58.3 35.3 44.3 47.8 44.3 Table A3.Test accuracy (%) on ImageNet-C, level 5, ResNet-18.Test-Time Training with Self-Supervision for Generalization under Distribution Shifts originalgauss shot impulsedefocus glass motion zoom snow frost fog bright contrastelasticpixelate jpeg 0 10 20 30 40 50Error (%) Object recognition task only Joint training (Hendrycks et al. 2019) TTT TTT-Online Figure A3.Test error (%) on CIFAR-10-C, level 4. See the results section for details. orig gauss shot impul defoc glass motn zoom snow frost fog brit contr elast pixel jpeg B 8.9 46.4 39.2 44.8 15.3 52.5 19.1 20.5 21.3 26.9 13.3 10.5 13.7 20.8 35.3 26.9 JT 8.1 45.0 38.3 42.2 16.4 50.2 20.7 20.5 21.1 25.4 14.1 10.0 14.7 19.0 33.2 25.1 TTT 7.9 41.5 35.4 39.8 15.0 47.8 19.1 18.4 20.1 24.0 13.5 10.0 14.1 17.7 29.4 24.5 TTT-Online 8.2 22.9 20.0 23.9 11.2 35.1 15.6 13.8 18.6 15.9 12.3 9.7 11.9 16.7 13.6 19.8 ALP 16.5 21.3 20.5 24.5 20.7 25.9 23.7 21.4 24.2 23.9 42.2 17.5 53.7 22.1 19.1 18.5 Table A4.Test error (%) on CIFAR-10-C, level 4, ResNet-26. originalgauss shot impulsedefocus glass motion zoom snow frost fog bright contrastelasticpixelate jpeg 0 10 20 30 40Error (%) Object recognition task only Joint training (Hendrycks et al. 2019) TTT TTT-Online Figure A4.Test error (%) on CIFAR-10-C, level 3. See the results section for details. orig gauss shot impul defoc glass motn zoom snow frost fog brit contr elast pixel jpeg B 8.9 42.2 35.1 30.7 12.2 41.7 18.6 17.5 19.0 25.3 10.8 9.7 11.6 15.3 21.7 24.6 JT 8.1 40.2 34.4 29.9 12.2 37.9 20.8 17.3 18.4 25.0 11.4 9.2 12.0 15.2 20.8 22.8 TTT 7.9 37.2 31.6 28.6 11.5 35.8 19.1 15.8 17.8 23.3 11.0 9.1 11.6 14.3 18.9 22.3 TTT-Online 8.2 21.3 17.7 17.9 9.0 23.4 15.3 12.5 16.4 15.8 10.9 9.0 10.7 12.8 12.2 18.7 ALP 16.5 20.0 19.3 20.5 19.2 21.2 24.0 20.5 20.9 24.2 30.1 16.6 39.6 20.9 17.8 18.0 Table A5.Test error (%) on CIFAR-10-C, level 3, ResNet-26.Test-Time Training with Self-Supervision for Generalization under Distribution Shifts originalgauss shot impulsedefocus glass motion zoom snow frost fog bright contrastelasticpixelate jpeg 0 10 20 30 40Error (%) Object recognition task only Joint training (Hendrycks et al. 2019) TTT TTT-Online Figure A5.Test error (%) on CIFAR-10-C, level 2. See the results section for details. orig gauss shot impul defoc glass motn zoom snow frost fog brit contr elast pixel jpeg B 8.9 31.7 22.6 24.3 9.9 42.6 14.9 14.7 21.7 18.4 9.8 9.1 10.0 13.1 17.1 22.4 JT 8.1 31.0 22.6 23.4 9.1 39.2 16.4 14.2 21.2 17.5 9.4 8.3 10.6 12.8 15.9 20.5 TTT 7.9 28.8 20.7 23.0 9.0 36.6 15.4 13.1 20.2 16.9 9.2 8.3 10.2 12.5 14.8 19.7 TTT-Online 8.2 16.8 13.8 15.5 8.5 23.4 13.3 11.5 16.8 12.7 9.4 8.4 9.7 12.4 11.5 17.0 ALP 16.5 18.0 17.2 19.0 17.8 20.7 21.2 19.3 19.0 20.1 22.4 16.3 29.2 20.3 17.4 17.8 Table A6.Test error (%) on CIFAR-10-C, level 2, ResNet-26. originalgauss shot impulsedefocus glass motion zoom snow frost fog bright contrastelasticpixelate jpeg 0 10 20 30 40Error (%) Object recognition task only Joint training (Hendrycks et al. 2019) TTT TTT-Online Figure A6.Test error (%) on CIFAR-10-C, level 1. See the results section for details. orig gauss shot impul defoc glass motn zoom snow frost fog brit contr elast pixel jpeg B 8.9 21.7 17.1 17.0 9.0 44.0 12.1 13.9 14.3 13.4 9.2 8.9 9.0 13.2 12.0 17.3 JT 8.1 20.4 16.6 16.9 8.2 40.5 12.2 13.0 13.1 12.3 8.4 8.1 8.5 12.9 11.3 15.9 TTT 7.9 19.1 15.8 16.5 8.0 37.9 11.7 12.2 12.8 11.9 8.2 8.0 8.3 12.6 11.1 15.5 TTT-Online 8.2 13.8 11.9 12.2 8.5 24.4 10.5 11.5 12.4 10.7 8.5 8.3 8.6 12.4 10.7 14.4 ALP 17.0 16.8 17.6 16.8 20.9 18.7 19.0 17.3 17.5 17.4 16.1 18.4 20.4 17.0 17.2 17.5 Table A7.Test error (%) on CIFAR-10-C, level 1, ResNet-26.Test-Time Training with Self-Supervision for Generalization under Distribution Shifts Figure A7.Sample Images from the VID-Robust dataset (Shankar et al., 2019) in the results section adapted to CIFAR-10. Each row shows eight sample images from one class. The seven classes shown are, in order: airplane, bird, car, dog, cat, horse, ship.",
      "meta_data": {
        "arxiv_id": "1909.13231v3",
        "authors": [
          "Yu Sun",
          "Xiaolong Wang",
          "Zhuang Liu",
          "John Miller",
          "Alexei A. Efros",
          "Moritz Hardt"
        ],
        "published_date": "2019-09-29T08:09:15Z",
        "pdf_url": "https://arxiv.org/pdf/1909.13231v3.pdf"
      },
      "llm_extracted_info": {
        "main_contributions": "The paper introduces Test-Time Training (TTT), a novel approach to enhance the generalization of predictive models when training and test data exhibit distribution shifts. The core idea is to convert a single unlabeled test sample into a self-supervised learning problem, allowing model parameters to be updated at test time before making a prediction. Key findings include substantial improvements in robustness on diverse image classification benchmarks (e.g., corrupted images, video frames, unknown shifts) without degrading performance on the original distribution. The method also extends naturally to online data streams by continuously updating parameters. Theoretically, the paper establishes that positive gradient correlation between the main and self-supervised task losses is a sufficient condition for TTT to improve performance.",
        "methodology": "Test-Time Training (TTT) works by fine-tuning a pre-trained model on individual unlabeled test samples. The model is initially trained in a multi-task learning fashion on both a main classification task and a self-supervised auxiliary task, sharing a feature extractor (ResNets with Group Normalization are used). For the self-supervised task, rotation prediction (predicting 0, 90, 180, or 270-degree rotations of the input image) is employed. At test time, for each incoming unlabeled sample, the shared feature extractor's parameters are updated by minimizing the self-supervised task loss on that sample (augmented with random crop and horizontal flip). The updated model then makes the main task prediction. An 'Online Test-Time Training' version extends this by initializing parameters for the current test sample with those updated from the previous sample in a stream, taking one gradient step per image. Theoretical analysis demonstrates that for convex models, a positive inner product between the gradients of the main and self-supervised tasks guarantees performance improvement.",
        "experimental_setup": "Experiments were conducted using ResNet-26 for CIFAR-10 and ResNet-18 for ImageNet, configured with Group Normalization. Datasets included CIFAR-10-C and ImageNet-C (benchmarking robustness to 15 types of common corruptions at 5 severity levels), VID-Robust (video frames for object recognition), and CIFAR-10.1 (a new test set simulating unknown distribution shifts for CIFAR-10). Baselines included a plain ResNet ('object recognition task only'), a model jointly trained on main and self-supervised tasks but fixed at test time ('joint training'), Unsupervised Domain Adaptation by Self-Supervision (UDA-SS), and Adversarial Logit Pairing (ALP). Optimization used Stochastic Gradient Descent (SGD). Standard TTT involved 10 gradient steps per test sample with a learning rate of 0.001, while online TTT used 1 step per sample. Performance was evaluated using test error or accuracy.",
        "limitations": "One significant limitation is the computational cost, as Test-Time Training is slower than regular inference (e.g., up to 20 times slower for standard TTT compared to a single forward pass). The method is also sensitive to the choice of normalization layer; Batch Normalization (BN) proved ineffective due to the small batch sizes (single image) used at test time, necessitating Group Normalization (GN) or specific workarounds that might reduce performance. Furthermore, the effectiveness of TTT relies on the self-supervised task being 'well defined and non-trivial' for the given test sample; if the self-supervised task provides trivial hints (e.g., rotation prediction on images with clear black margins, or on objects like airplanes where rotation is ambiguous even for humans), TTT yields no improvement.",
        "future_research_directions": "Future work could focus on improving the computational efficiency of Test-Time Training, potentially by exploring methods to make models amenable to faster updates during training, or by dynamically deciding when to apply TTT (e.g., using a thresholding trick based on self-supervised loss). Developing more effective and general-purpose self-supervised tasks, potentially leveraging specific domain knowledge, is another promising direction. The paper also suggests using TTT as a new evaluation benchmark for self-supervised tasks. Broader applications of TTT to other tasks (e.g., segmentation, detection) and fields (e.g., speech recognition, natural language processing) are envisioned. A more formal theoretical exploration of the 'variable decision boundary' concept introduced by TTT is also proposed. Finally, further progress in one-shot learning could potentially enhance the update rule used in TTT. The authors encourage researchers to reconsider the artificial division between training and testing, envisioning a paradigm where much of the learning occurs after model deployment."
      }
    },
    {
      "title": "Towards Stable Test-time Adaptation in Dynamic Wild World",
      "abstract": "Test-time adaptation (TTA) has shown to be effective at tackling distribution\nshifts between training and testing data by adapting a given model on test\nsamples. However, the online model updating of TTA may be unstable and this is\noften a key obstacle preventing existing TTA methods from being deployed in the\nreal world. Specifically, TTA may fail to improve or even harm the model\nperformance when test data have: 1) mixed distribution shifts, 2) small batch\nsizes, and 3) online imbalanced label distribution shifts, which are quite\ncommon in practice. In this paper, we investigate the unstable reasons and find\nthat the batch norm layer is a crucial factor hindering TTA stability.\nConversely, TTA can perform more stably with batch-agnostic norm layers, \\ie,\ngroup or layer norm. However, we observe that TTA with group and layer norms\ndoes not always succeed and still suffers many failure cases. By digging into\nthe failure cases, we find that certain noisy test samples with large gradients\nmay disturb the model adaption and result in collapsed trivial solutions, \\ie,\nassigning the same class label for all samples. To address the above collapse\nissue, we propose a sharpness-aware and reliable entropy minimization method,\ncalled SAR, for further stabilizing TTA from two aspects: 1) remove partial\nnoisy samples with large gradients, 2) encourage model weights to go to a flat\nminimum so that the model is robust to the remaining noisy samples. Promising\nresults demonstrate that SAR performs more stably over prior methods and is\ncomputationally efficient under the above wild test scenarios.",
      "meta_data": {
        "arxiv_id": "2302.12400v1",
        "authors": [
          "Shuaicheng Niu",
          "Jiaxiang Wu",
          "Yifan Zhang",
          "Zhiquan Wen",
          "Yaofo Chen",
          "Peilin Zhao",
          "Mingkui Tan"
        ],
        "published_date": "2023-02-24T02:03:41Z",
        "pdf_url": "https://arxiv.org/pdf/2302.12400v1.pdf"
      }
    },
    {
      "title": "Evaluation of Test-Time Adaptation Under Computational Time Constraints",
      "abstract": "This paper proposes a novel online evaluation protocol for Test Time\nAdaptation (TTA) methods, which penalizes slower methods by providing them with\nfewer samples for adaptation. TTA methods leverage unlabeled data at test time\nto adapt to distribution shifts. Although many effective methods have been\nproposed, their impressive performance usually comes at the cost of\nsignificantly increased computation budgets. Current evaluation protocols\noverlook the effect of this extra computation cost, affecting their real-world\napplicability. To address this issue, we propose a more realistic evaluation\nprotocol for TTA methods, where data is received in an online fashion from a\nconstant-speed data stream, thereby accounting for the method's adaptation\nspeed. We apply our proposed protocol to benchmark several TTA methods on\nmultiple datasets and scenarios. Extensive experiments show that, when\naccounting for inference speed, simple and fast approaches can outperform more\nsophisticated but slower methods. For example, SHOT from 2020, outperforms the\nstate-of-the-art method SAR from 2023 in this setting. Our results reveal the\nimportance of developing practical TTA methods that are both accurate and\nefficient.",
      "full_text": "Evaluation of Test-Time Adaptation Under Computational Time Constraints Motasem Alfarra 1 2 Hani Itani 1 Alejandro Pardo 1 Shyma Alhuwaider 1 Merey Ramazanova 1 Juan C. P´erez 1 Zhipeng Cai 2 Matthias M¨uller 2 Bernard Ghanem 1 Abstract This paper proposes a novel online evaluation protocol for Test Time Adaptation (TTA) meth- ods, which penalizes slower methods by provid- ing them with fewer samples for adaptation. TTA methods leverage unlabeled data at test time to adapt to distribution shifts. Although many effec- tive methods have been proposed, their impressive performance usually comes at the cost of signif- icantly increased computation budgets. Current evaluation protocols overlook the effect of this extra computation cost, affecting their real-world applicability. To address this issue, we propose a more realistic evaluation protocol for TTA meth- ods, where data is received in an online fashion from a constant-speed data stream, thereby ac- counting for the method’s adaptation speed. We apply our proposed protocol to benchmark sev- eral TTA methods on multiple datasets and sce- narios. Extensive experiments show that, when accounting for inference speed, simple and fast approaches can outperform more sophisticated but slower methods. For example, SHOT from 2020, outperforms the state-of-the-art method SAR from 2023 in this setting. Our results re- veal the importance of developing practical TTA methods that are both accurate and efficient1. 1. Introduction In recent years, Deep Neural Networks (DNNs) have demon- strated remarkable success in various tasks (He et al., 2016) thanks to their ability to learn from large datasets (Deng et al., 2009). However, a significant limitation of DNNs is their poor performance when tested on out-of-distribution 1King Abdullah University of Science and Technol- ogy (KAUST), Thuwal, Saudi Arabia 2Intel Labs, Munich, Germany. Correspondence to: Motasem Alfarra <mo- tasem.alfarra@kaust.edu.sa>. Proceedings of the 41 st International Conference on Machine Learning, Vienna, Austria. PMLR 235, 2024. Copyright 2024 by the author(s). 1Code: github/MotasemAlfarra/Online-Test-Time-Adaptation Current Evaluation Realistic Evaluation40 45 50 55 60 65 70 75Error Rate (%)  AdaBN 17  AdaBN 17  SHOT 20  SHOT 20  TENT 21  TENT 21  SAR 23  SAR 23 Figure 1: The trend of average error rate using offline evaluation vs our proposed online evaluation. In the offline setup, TTA methods demonstrate progress across time with a decreasing average error rate, e.g. from 68.5% using AdaBN to 56.2% using SAR. We propose a realistic evaluation protocol that accounts for the adaptation speed of TTA methods. Under this protocol, fast methods ( e.g. AdaBN) are unaffected, while slower (but more recent and sophisticated) methods (e.g. SAR) are penalized. data, which violates the i.i.d. assumption that the training and testing data are from the same distribution (Hendrycks et al., 2021; Hendrycks & Dietterich, 2019; Kar et al., 2022). Such failure cases are concerning, since distribu- tion shifts are common in real-world applications, e.g., im- age corruptions (Hendrycks & Dietterich, 2019), chang- ing weather conditions (Sakaridis et al., 2021), or security breaches (Goodfellow et al., 2014). Test Time Adaptation (TTA) (Saenko et al., 2010; Sun et al., 2020; Liu et al., 2021) has demonstrated promising results for solving the above problem. TTA leverages the unlabeled data that arrives at test time by adapting the forward pass of pre-trained DNNs according to some proxy task (Liang et al., 2020; Lee et al., 2013). Though recent methods have made significant progress at improving accuracy under dis- tribution shifts (Wang et al., 2020; Niu et al., 2022; Gao et al., 2022), many of them incur high computational over- head. For instance, some methods require self-supervised fine-tuning on the data (Chen et al., 2022), while others perform a diffusion process per input (Gao et al., 2022). The computational overhead of TTA methods decreases 1 arXiv:2304.04795v2  [cs.LG]  23 May 2024Evaluation of Test-Time Adaptation Under Computational Time Constraints their inference speed, which is a critical property in many real-world applications that require the TTA method to pro- duce predictions at the speed of the stream itself. This property, however, is overlooked in the current evaluation protocols for TTA methods. In particular, these protocols assume a setting, which neglects how events constantly un- fold regardless of the model’s speed, causing the model to miss incoming samples when it is busy processing previous ones. For TTA methods that adapt using test data, missing samples has a direct effect on the method’s accuracy, as it will have fewer samples for adaptation. That is, the slower the TTA method, the fewer samples it can leverage for adapt- ing to the distribution shift. Thus, the current protocol for evaluating TTA methods is not suitable for assessing their efficacy in real-world deployment. In this work, we propose a novel realistic evaluation proto- col that factors in inference speed to assess the real-world applicability of TTA methods. Our evaluation protocol is in- spired by Online Learning (Cai et al., 2021; Shalev-Shwartz et al., 2012) and mimics real-world scenarios by exposing all TTA methods to a constant-speed stream of data. In this setting, the performance of slow TTA methods is in- trinsically penalized, as the time spent adapting to a sample may lead to dropped samples that could have been useful for adaptation. Specifically, our protocol dictates that if a method gslow is k times slower than the stream, then it may only use every kth sample for adaptation. In contrast, a method gfast that is as fast as the stream is allowed to adapt to every sample. Figure 1 shows the effect of evaluating several methods under our proposed protocol, where slower methods (e.g., SAR (Niu14 et al., 2023)) are penalized and faster but simpler methods become better alternatives (e.g., SHOT (Liang et al., 2020) and AdaBN (Li et al., 2016)). We apply our proposed evaluation protocol to benchmark several TTA methods on multiple datasets, and provide a fair assessment of their performance subject to the realistic consequences of slower inference speeds. Our experimental results highlight the importance of developing TTA methods that adapt to distribution shifts with minimal impact on inference speed. Our contributions are two-fold: 1. We propose a realistic evaluation protocol for TTA methods that penalizes slower methods by providing them with fewer samples for adaptation. Our approach is effective at assessing TTA methods’ efficacy in sce- narios where data arrives as a constant-speed stream. 2. Following our proposed protocol, we provide a com- prehensive experimental analysis of 15 TTA methods evaluated on 3 large-scale datasets under 3 different evaluation scenarios. These scenarios consider adap- tation to a single domain and continual adaptation to several domains. Our analysis shows that, when in- ference speed is accounted for, simple (but faster) ap- proaches can benefit from adapting to more data, and thus outperform more sophisticated (but slower) meth- ods. Figure 1 demonstrates this for four TTA methods. We hope our evaluation scheme inspires future TTA methods to consider inference speed as a critical di- mension that affects their real-world performance. 2. Related Work Test Time Adaptation. The Test Time Adaptation (TTA) setup relaxes the “i.i.d” assumption between the training and testing distributions (Sun et al., 2020; Boudiaf et al., 2022). This relaxation is usually attained through a lifelong learning scheme on all received unlabeled data (Chen et al., 2022; Gong et al.). Earlier approaches such as TTT (Sun et al., 2020) and TTT++ (Liu et al., 2021), among others (Torralba & Efros, 2011; Tzeng et al., 2017), include a self-supervised loss (Gidaris et al., 2018) during training, which can then provide an error signal during adaptation. Despite their effectiveness, such approaches assume having control over how the model is trained. Fully Test Time Adaptation. Fully TTA methods are a subtype of TTA method that adapts at test time by modify- ing the model’s parameters (Liang et al., 2020; Lee et al., 2013; Mirza et al., 2022b; Mancini et al., 2018; Kojima et al., 2022) or its input (Gao et al., 2022) by using the incoming unlabeled data. Fully TTA methods are practi- cal, as they avoid assumptions on the training phase of a given model (Wang et al., 2020; Gao et al., 2022; Iwasawa & Matsuo, 2021). The first of these approaches adjusts the statistics of the Batch Normalization (BN) layers (Mirza et al., 2022a; Schneider et al., 2020; Li et al., 2016). For example, BN-adaptation (Schneider et al., 2020) leverages the statistics of the source data as a prior and infers the statis- tics for every received sample. On the other hand, AdaBN (Li et al., 2016) discards the statistics of the source domain and uses the statistics computed on the target domain. In line with light TTA methods, LAME (Boudiaf et al., 2022) proposes to only adapt the model’s output by finding the latent assignments that optimize a manifold-regularized like- lihood of the data. In this work, we found that such efficient methods preserve their accuracy under our proposed eval- uation. While fully TTA methods have been studied in the context of adversarial domain shifts (Alfarra et al., 2022; Croce et al., 2022; P´erez et al., 2021), in this work we focus on the context of natural shifts such as realistic image cor- ruptions (Hendrycks & Dietterich, 2019; Kar et al., 2022). Another line of work aims at adapting to distribution shifts by minimizing entropy. For instance, SHOT (Liang et al., 2020) adapts the feature extractor to minimize the entropy of individual predictions; while maximizing the entropy of the predicted classes. TENT (Wang et al., 2020) updates the learnable parameters of the BN layers to minimize the 2Evaluation of Test-Time Adaptation Under Computational Time Constraints Adapted SampleNon-AdaptedSampleTTA method Current evaluation . . . . . . Realistic evaluation . . . . . . Model Figure 2: Inference under the current and realistic evaluation protocols. The current evaluation setting (left) assumes that the incoming batches of stream S can wait until the adaptation process of a TTA method g finishes. This assumption is untenable in a real-time deployment scenario. Our proposed realistic evaluation (right) simulates a more realistic scenario where S reveals data at a constant speed. In this setup, slower TTA methods will adapt to a smaller portion of the stream. The remaining part of the stream will be predicted without adaptation by employing the most recent adapted model. We refer to the most recent adapted model as fθt+1 , with t denoting the time when the last sample was adapted to by g. When g is still adapting to a sample, the incoming sample is fed to fθt+1 to produce predictions. entropy of predictions. EATA (Niu et al., 2022) combines TENT with an active selection of reliable and non-redundant samples from the target domain and an anti-forgetting loss (Kirkpatrick et al., 2017). Further, SAR (Niu14 et al., 2023) equips TENT with an active sampling scheme that filters samples with noisy gradients. Other works use data-augmentation at test time (Ashukha et al., 2020). For example, MEMO (Zhang et al., 2021) adapts model parameters to minimize the entropy over a sample and multiple augmentations of it. CoTTA (Wang et al., 2022) uses augmentations to generate reliable pseudo- labels and then peform distillation. Finally, DDA (Gao et al., 2022) proposes to leverage a diffusion model (Ho et al., 2020) to restore corrupted inputs back to the source data distribution. These methods require multiple forward passes through the network or a diffusion model, leading to slower inference speeds. 3. Methodology In this section, we present our proposed Realistic TTA evalu- ation protocol. We first describe the current TTA evaluation protocol and its limitations Then, we introduce our Realistic TTA evaluation protocol, which addresses the shortcomings of the offline protocol. 3.1. Current Protocol TTA considers the practical setup, in which trained models are deployed in a target domain that exhibits distribution shifts to which they must adapt. Let fθ : X → Ybe a clas- sifier, parameterized by θ, that predicts the label y ∈ Yfor a given input x ∈ X. Before test time, fθ is assumed to have been trained on the dataset Dtrain ⊂ X × Y. At test time, i.e. when executing TTA,fθ is presented with a stream of data S, sampled from X, with potentially multiple distribution shifts w.r.t. Dtrain. Under this setup, a TTA method is a function g(θ, x) that sequentially adapts the model’s param- eters θ and/or the input x to enhance the performance under distributions shifts. Currently, TTA methods are evaluated in an offline setting. Formally, the Current TTA evaluation protocol simulates the interaction between the stream S and the TTA method g, at each time step t ∈ {0, 1, . . . ,∞}, as follows: Curr.1 S reveals a sample xt. Curr.2 g adapts xt to ˆxt, θt to ˆθt, generates prediction ˆyt, and updates parameters θt+1 = αθt + (1 − α)ˆθt.2 Note that all existing TTA methods can be modeled using this framework. For example, TENT (Wang et al., 2020) adapts network parameters to minimize entropy with α = 0, while leaving inputs unchanged, i.e. ˆxt = xt and θt+1 = ˆθt. DDA (Gao et al., 2022) adapts inputs via a diffusion process while preserving network parameters with α = 1, i.e. ˆxt = ˆxt and θt+1 = θt. CoTTA (Wang et al., 2022) applies knowledge distillation, and updates network parameters with an exponential moving average, i.e. setting 0 < α <1. Shortcomings of the Current TTA protocol.In the current protocol, the performance of a TTA method g is measured by comparing the ground truth labels yt with the predic- tions after adaptation ˆyt. An evaluation based only on this measure implicitly assumes that the stream is not constant 2Note that some methods abstain from adapting either xt or θt. 3Evaluation of Test-Time Adaptation Under Computational Time Constraints speed, but rather waits for g to adapt to xt (Curr.2) before revealing the next batch xt+1 (Curr.1). Figure 2 provides an illustration of this situation. This assumption results in the offline protocol favoring slower TTA methods, as the method’s performance is agnostic to its inference speed. However, in practical applications where the test data ar- rives at a constant speed, the offline protocol is not suitable for assessing a method’s performance. Next, we propose a remedy for this shortcoming. 3.2. Realistic Online Evaluation Protocol We propose a realistic evaluation of TTA methods that explicitly considers the relation between the speed of the method and the speed at which the stream reveals new data. This setup is more realistic, as it intrinsically penalizes the performance of slower TTA methods: long times spent in adaptation result in fewer samples to adapt to. A crucial aspect of our realistic TTA protocol is accounting for the implications of simulating a constant speed data stream S. For instance, consider a stream S that reveals data at a constant rate r samples per second. If a method gfast adapts to samples at speed r, then gfast will be able to adapt to every sample. On the other hand, if gslow adapts to samples at a speed r/2, then gslow will skip every other sample. We formalize the notion of the relation between the speed of the stream and the speed of a method g as the “relative adaptation speed of g”. This quantity, denoted by C(g) ∈ N, is simply the integer ratio of the speed of S to the speed of g. For instance, in the previous example, C(gfast) = 1, meaning gfast adjusts as fast as S reveals data, while C(gslow) = 2 , indicating S reveals its second batch while gslow is still adapting to the first one. Without loss of generality, we assume that fθ runs in real- time, i.e. that its speed is equal to r, and thus C(fθ) = 1 . This assumption allows us to suppose that the samples that are not processed by g can be processed by fθ. Under this setup, we define our realistic protocol by introducing the relative adaptation speed C(g) into the offline protocol. In particular, we simulate g’s availability by conditionally performing the adaptation step (Curr.2), depending on C(g). In this manner,g is only permitted to adapt when its previous adaptation step has finished. Formally, the realistic TTA evaluation protocol simulates the interaction between the constant speed stream S and the TTA method g, at each time step t ∈ {0, 1, . . . ,∞}, as follows: RTTA 1 S reveals a sample xt. RTTA 2 If (t mod C(g)) = 0, then g adapts xt to ˆxt, θt to ˆθt, generates a prediction ˆyt, and updates pa- rameters via θt+1 ← αθt + (1 − α)ˆθt. Otherwise, fθt generates a prediction ˆyt. Table 1: Average C(g(xt)). We report the average relative adaptation speed C(g) for 5 TTA methods. The higher C(g) is, the smaller the portion of data to which g adapts is. Method AdaBN TENT TTAC-NQ MEMO DDA C(g) 1 3 12 54 810 Here, “mod” represents the modulo operation. The above protocol assesses the performance of TTA methods by fac- toring in their speed. As such, faster methods are granted more adaptation steps and, conversely, slower methods are granted fewer (see Figure 2). Note that explicitly modeling the relative adaptation speeds allows us to evaluate TTA methods under different adaptation speeds by setting C(g) to arbitrary values. For instance, note that our realistic proto- col recovers the original offline protocol by settingC(g) = 1 for all methods. Next, we explain the calculation of C(g) for our realistic protocol. Online computation of C(g). In practice, estimating the relative adaptation speed C(g) can be a noisy process. The noise in this estimation essentially comes from two factors: hardware and input dependence. Hardware-induced noise applies to all methods, while input dependence applies to methods like ETA (Niu et al., 2022) which, upon receiving an input, may optionally abstain from adapting to it. This noise means that C(g) potentially varies across iterations. Our protocol accounts for this variability by conducting an online computation of C(g) on each revealed input. That is, instead of using a fixed value of C(g) at each itera- tion t, our protocol rather uses C (g(xt)). Formally, if we let R (g(x)) denote the speed at which g processes x, then the relative adaptation speed of g at x is defined as C (g(xt)) = ⌈r/R(g(x))⌉, where the ceiling function ac- counts for the stream’s discrete-time nature. Note that since we assumed C(fθ) = 1, then R (fθ(x)) = r. We report the empirical behavior of this online computation of C (g(xt)) for various TTA methods in Table 1, and leave the rest of the methods and the computation details to the Appendix. Next, we leverage our Realistic TTA protocol to conduct a comprehensive empirical study of several TTA methods. 4. Experiments We follow prior art (Wang et al., 2020; Niu14 et al., 2023; Gao et al., 2022) and focus on the task of image classifica- tion. In all our experiments, we assume that fθ is a ResNet- 50-BN3 (He et al., 2016) trained on ImageNet (Deng et al., 2009) (pretrained weights obtained from torchvision). We further assume that the stream S reveals batches of size 3SAR demonstrated the superiority of using batch independent normalization layers under batch size of 1. We leave this ablation to the Appendix along with experiments on other architectures. 4Evaluation of Test-Time Adaptation Under Computational Time Constraints Table 2: Episodic Error Rate on ImageNet-C. We report the error rate of different TTA methods on ImageNet-C benchmark under both the realistic and the current setup. A lower error rate indicates a better TTA method. The highlighted numbers indicate a better performance per method across setups. Episodic means the model will adapt to one corruption at a time. The model is reset back to the base model when moving to the next corruption. The current setup is merely the reproduction of every method. The first sub-table corresponds to methods that do not incur any or few extra computations, i.e. C(g) = 1. We show that methods generally perform worse in the realistic setup. The more computationally complex the TTA method is, the less data it will adapt to, and the worse is its performance. Noise Blur Weather DigitalMethod Realisticgauss. shot impul.defoc. glass motionzoom snow frost fog brigh. contr. elast. pixel. jpeg Avg. ∆ Source ✓ 97.8 97.1 98.1 82.1 90.2 85.2 77.5 83.1 76.7 75.6 41.1 94.6 83.0 79.4 68.4 82.0 - AdaBN ✓ 84.9 84.3 84.3 85.0 84.7 73.6 61.1 65.8 66.9 52.1 34.8 83.3 56.1 51.1 60.3 68.5 - LAME ✓ 98.3 97.6 98.6 82.4 90.9 86.1 78.1 84.5 77.5 77.3 41.4 94.8 84.8 80.0 68.9 82.7 - BN ✓ 84.6 83.9 83.8 80.1 80.2 71.7 60.4 65.4 65.2 51.6 34.6 76.3 54.4 49.7 59.2 66.7 - ✗ 73.4 70.2 73.0 76.6 75.5 59.8 53.8 54.2 63.4 44.7 35.5 79.3 46.9 43.2 49.7 59.9SHOT ✓ 73.6 69.0 71.1 74.6 74.8 60.0 52.9 54.1 61.3 44.1 34.1 77.8 46.8 43.1 49.2 59.1 (-0.8) ✗ 71.3 69.4 70.2 72.0 72.9 58.7 50.7 52.8 58.8 42.7 32.7 73.3 45.5 41.5 47.7 57.3TENT ✓ 75.7 78.3 75.2 76.3 77.3 64.6 55.6 57.3 61.4 45.9 33.5 77.1 50.1 44.2 51.4 61.6 (+4.3) ✗ 69.5 69.7 69.0 71.2 71.7 58.1 50.5 52.9 57.9 42.7 32.7 62.9 45.5 41.6 47.8 56.2SAR ✓ 79.4 78.5 78.1 79.9 79.3 67.5 56.1 60.5 63.1 47.4 34.0 75.3 51.7 46.6 53.8 63.4 (+7.2) ✗ 78.4 77.8 77.2 80.5 79.1 64.0 53.3 57.8 60.7 44.1 32.9 73.1 48.6 42.3 52.6 61.5CoTTA ✓ 82.9 81.6 81.9 87.4 85.6 75.6 61.1 63.1 64.9 49.9 34.8 91.2 54.0 48.8 56.6 68.0 (+6.5) ✗ 71.3 70.3 70.8 82.1 77.4 63.9 53.9 49.9 55.5 43.9 32.8 81.4 43.7 41.1 46.7 59.0TTAC-NQ ✓ 79.4 75.7 78.9 86.6 86.2 77.1 61.8 58.8 62.4 51.5 34.4 88.5 52.1 49.1 55.5 66.5 (+7.5) ✗ 65.5 62.4 63.5 66.6 67.2 52.0 47.3 48.2 54.1 39.9 32.1 55.0 42.3 39.2 44.8 52.0EATA ✓ 69.3 67.1 69.2 71.1 71.7 57.5 49.9 51.9 57.4 42.4 32.6 60.7 45.1 41.4 47.4 55.6 (+3.6) ✗ 92.5 91.3 91.0 84.0 87.0 79.3 72.4 74.6 71.3 67.9 39.0 89.0 76.2 67.0 62.4 76.3MEMO ✓ 97.7 97.0 98.0 82.1 90.1 85.1 77.4 83.0 76.6 75.4 41.0 94.5 82.9 79.2 68.2 81.9 (+5.6) ✗ 58.6 57.8 59.0 87.0 81.6 76.6 65.9 67.9 66.7 64.0 40.0 92.2 52.2 46.6 49.9 64.4DDA ✓ 97.8 97.0 98.1 82.1 90.2 85.2 77.5 83.1 76.7 75.6 41.1 94.6 83.0 79.4 68.3 82.0 (+17.6) 644, except for MEMO (Zhang et al., 2021), which pre- dicts on single images to incentivize prediction consistency over an input and its augmentations. Regarding datasets, we follow earlier works (Wang et al., 2020; Niu14 et al., 2023; Niu et al., 2022; Gao et al., 2022; Zhang et al., 2021), and thus evaluate on the ImageNet-C dataset (Hendrycks & Dietterich, 2019) with a corruption level of 5 for all 15 corruptions. We further extend our evaluation and consider CIFAR10-C, ImageNet-R (Hendrycks et al., 2021), and the more recent ImageNet-3DCC (Kar et al., 2022), which lever- ages depth estimates to construct more spatially-consistent corruptions. Our experiments compare the performance of the base- line model fθ (without test time adaptation) against 15 state-of-the-art TTA methods published in top-tier venues (e.g., CVPR, NeurIPS, and ICLR) between 2017 and 2023. In particular, we consider: BN (Schneider et al., 2020) and AdaBN (Li et al., 2016), which adjust the statistics of the batch normalization layers; SHOT (Liang et al., 2020) and SHOT-IM (Liang et al., 2020), which fine-tune the feature extractor to maximize mutual information; entropy mini- mization approaches such as TENT (Wang et al., 2020), 4This batch size is recommended by most baselines (Wang et al., 2020; Niu et al., 2022) ETA (Niu et al., 2022) (a more efficient version of TENT), and SAR (Niu14 et al., 2023), which trains the learnable parameters of the batch normalization layers; distillation approaches, such as CoTTA (Wang et al., 2022), Pseudo Labeling (PL) (Lee et al., 2013), and the very recent and efficient LAME (Boudiaf et al., 2022); EATA (Niu et al., 2022) and TTAC (Su et al., 2022) that assume access to the source training data; data-dependent approaches such as MEMO (Zhang et al., 2021) and the diffusion-based method DDA (Gao et al., 2022). For all methods, we use their official implementation with their recommended hyper- parameters. We report our experimental results on a subset of 12 baselines, while leaving ETA, SHOT-IM, and PL to the appendix due to space constraints and their similarity to SHOT and EATA. As mentioned in Section 3.2 , our protocol performs an online computation of the relative adaptation speed of g. In particular, for each batch revealed by the stream, we compute C (g(x)). Then, if C(g(xi)) = k, all the samples {xi+1, xi+2, . . . , xi+k} are processed by fθi without adap- tation. Otherwise, if C(g(xi)) = 1, then these samples are processed by g. For methods that accumulate parameter updates such as TENT (Wang et al., 2020), fθi is the most recent updated model g(fθi−1 ). We report all our main re- sults as the average across three seeds, and leave the detailed 5Evaluation of Test-Time Adaptation Under Computational Time Constraints SHOT TENT TTAC-NQ SAR EATA COTTA brigh.pixel.gauss.motionzoomglassimpul.jpegdefoc.elast.shotfrostsnowfog contr.clean 30 40 50 60 70 80 90 100Error Rate (%) (a) Current Continual TTA. brigh.pixel.gauss.motionzoomglassimpul.jpegdefoc.elast.shotfrostsnowfog contr.clean 30 40 50 60 70 80 90 100Error Rate (%)  (b) Realistic Continual TTA. Figure 3: Continual Error Rate on ImageNet-C. We report the continual error rate of several TTA methods on ImageNet-C benchmark under both realistic and current setups. A lower error rate indicates a better TTA method. Continual evaluation means the corruptions are presented in a sequence without resetting the model in between. We choose the same order as presented along the x-axis; starting with brightness and ending with clean validation set. In the current setup, we observe an increasing trend for SHOT, TENT, and TTAC-NQ. This is hypothesized to be due to overfitting on the early distribution shifts. This behavior is mitigated in the realistic setup due to adapting to fewer batches. EATA and SAR perform equally well in both realistic and current continual setups due to sample rejection. We report the standard deviation across 3 seeds. analysis to the Appendix. Throughout the experiments, we refer to our realistic evaluation protocol as “realistic/on- line”, and refer to the current protocol as “current/offline”. Next, we evaluate all methods on four different scenarios: (i) when domain shifts happen in an episodic manner, (ii) when domain shifts happen continually, i.e. one after the other, (iii) when the stream speed varies, (iii) when domain shifts happen continually with label correlation; practical evaluation (Yuan et al., 2023) ,and (v) when the baseline fθ is unavailable for evaluating the samples skipped by the TTA method g (left for the appendix). 4.1. Episodic Evaluation of TTA First, we consider an episodic evaluation of domain shifts, whereby S contains a single domain (e.g. one corruption) from ImageNet-C. We analyze this simple and most com- mon setup to assess the performance of TTA methods under real-time evaluation. We report the error rates on all corrup- tions in Table 2 and the average error rate across corruptions. We summarize the insights as follows: (i) The performance of TTA methods often degrades significantly under the realistic setup. Most methods induce a significant computational overhead, which prevents them from adapting to every sample from the test stream. For example, the error rate increases by 7.5% for TTAC- NQ and 4.3% for TENT, where C(gTTAC-NQ) = 12 and C(gTENT) = 3 (see Table 1). That is, TENT adapts to one- third of the batches revealed by the stream, while TTAC-NQ adapts to one every twelve batches. (ii) Very efficient methods, withC(g) = 1, such as LAME and BN, do not lose in performance. Evaluating such methods in offline or realistic setups is inconsequential, as their adaptation incurs negligible additional computation (since they adapt during the forward pass (Li et al., 2016; Schneider et al., 2020) or by adjusting the logits (Boudiaf et al., 2022) at a speed that pales in comparison to that of the stream). Interestingly, in our realistic evaluation, the simple BN (published in 2020) with an average error rate of 66.7% outperforms more recent and advanced methods such as SAR (published in 2023) by 1.7%. Furthermore, AdaBN (published in 2017) significantly outperforms the very recent diffusion-based DDA by a notable 13%. (iii) Data-dependent approaches, such as MEMO and DDA, are extremely inefficient. Despite the independence of MEMO and DDA on batch size, they incur a massive computational burden. For instance, C(gMEMO) = 54 and C(gDDA) = 810. Thus, both methods will be busy adapting for considerable portions of the stream, leaving most predic- tions to the non-adapted classifier. This phenomenon is the reason behind the reported performance of these methods being so close to that of fθ (i.e. around 82%). This result calls for future research to focus on increasing the efficiency of data-dependent adaptation methods. (iv) Sample rejection-oriented methods can perform well under the realistic protocol. EATA adapts efficiently due to its fast sample rejection algorithm, which relies solely on 6Evaluation of Test-Time Adaptation Under Computational Time Constraints the forward pass to admit samples for adaptation. EATA’s low error rate of 55.6%, combined with a small performance drop of less than 4%, positions it as the top performer under the realistic evaluation protocol on ImageNet-C. On the other hand, SAR does not benefit from sample rejection. SAR’s performance drop of 7.5% is due to its dependence on gradients for sample rejection, which reduces its speed. (v) SHOT benefits from the realistic protocol. Interest- ingly, we found that SHOT (and SHOT-IM in the Appendix), a fine-tuning-based approach, benefits from our realistic evaluation. In particular, we found that SHOT’s error rate decreases by 2% on fog corruption and by 0.8% on average. This observation could suggest that SHOT could potentially improve performance by disposing of fine-tuning on every batch. It is also worth mentioning that, under our realis- tic evaluation, SHOT (introduced in 2020) outperforms all methods except EATA. (vi) Performance changes are consistent across corrup- tions. Note that all methods that are somewhat efficient can improve the source model across all corruptions, in both the offline and realistic setups. Furthermore, the performance changes when comparing the offline and realistic setups are consistent across all corruptions. This finding suggests that the performance of these methods is independent of the do- main shift being considered. We further test this hypothesis by benchmarking these methods on two other datasets with other types of domain shifts in Section 4.4. 4.2. Continual Evaluation of TTA Next, we analyze the more challenging continual setup, fol- lowing (Wang et al., 2022; Niu et al., 2022). In particular, we construct the stream S by concatenating all corruptions from ImageNet-C. That is, we adapt TTA methods continu- ally on all corruptions followed by the clean validation set, without ever resetting the network weights. We introduce the notion of realistic adaptation to the continual setup to study the effects of a constant stream speed on the bench- mark. We report results in Figure 3 for both the offline and realistic protocols, where the horizontal-axis shows how cor- ruptions are ordered in the stream. We limit the experiments in this section to six TTA methods (SHOT, TENT, TTAC- NQ, COTTA, EATA, and SAR), and leave the remaining details for the Appendix. We observe: (i) Methods that do not perform sample rejection (SHOT, TENT, TTAC) scale poorly in the offline-continual setup. This phenomenon can be attributed to these methods over- fitting to early distributions. However, methods that do perform sample rejection (SAR and EATA) do not overfit as easily to corruptions, and can thus adapt to the rest of the stream. Even worse, such methods tend to even significantly degrade the performance on clean data. 1/16 1/8 1/4 1/2 1 η 52 55 58 61 64 67Error Rate (%) SHOT TENT TTAC-NQ SAR EATA Figure 4: Average Error Rate on ImageNet-C Under Slower Stream Speeds. We report the average error rate for several TTA methods on ImageNet-C under slower stream speeds. In our proposed realistic model evaluation, the stream speed r is normalized by the time needed for a for- ward pass using the base model. We evaluate different TTA methods under a stream with speed ηr with η ∈ (0, 1]. An η = 1/16 means the stream is 16 times slower than the forward pass of the base model. We report the standard deviation across 3 different random seeds. Different TTA methods degrade differently when varying η. (ii) In the realistic-continual setup, methods that do not perform sample rejection benefit from skipping adapta- tion on some batches, and become competitive with the methods that perform sample rejection. That is, while skipping parts of the stream deteriorated the performance of such methods in the episodic evaluation , this skipping actu- ally helped in preventing these methods from over-fitting in the continual setup. 4.3. Stream Speed Analysis In the previous experiments, we normalized the stream speed to be the same as that of fθ’s forward pass. That is, we assumed that the rate r at which S reveals new batches is equal to R (fθ(x)). However, some applications may enjoy a slower stream, giving TTA methods more time to adapt to samples. To explore this scenario, we vary the speed at which the stream reveals new data. In particular, let the new stream rate be η rwith η ∈ (0, 1]. Hence, as η → 0, the stream slows down and allows methods to adapt to all samples. Conversely, as η → 1, the stream speeds up, and at η = 1 we recover our realistic evaluation protocol. We experiment with the stream speed by setting η ∈ {1/16, 1/8, 1/4, 1/2, 1}, and evaluate five representative TTA methods (SHOT, TENT, TTAC-NQ, SAR, and EATA) in the episodic setup . Figure 4 summarizes our results by reporting the average error rate across all corruptions. We next list our observations: (i) The performance of TTA methods varies widely.For 7Evaluation of Test-Time Adaptation Under Computational Time Constraints Table 3: Episodic Error Rate on ImageNet-C with ViT. We report the error rate of three baselines (Source, Tent, SAR) on the 15 different corruptions on ImageNet-C when the backbone is ViT architecture pretrained on ImageNet. We observe that while generally better backbones yield smaller error rate, expensive methods perform worse under our realistic evaluation. The more expensive the method is (e.g. SAR compared to Tent), the more performance reduction it suffers. Noise Blur Weather DigitalMethodRealisticgauss. shot impul. defoc. glass motionzoom snow frost fog brigh. contr. elast. pixel. jpeg Avg. ∆ Source ✓ 90.5 93.3 91.8 71.0 76.6 66.1 72.9 84.1 73.5 52.8 45.3 55.9 69.5 55.5 52.2 70.1 - ✗ 69.9 95.9 68.9 55.8 62.0 52.3 57.9 57.2 53.6 41.8 28.9 40.7 59.1 39.7 42.0 55.0Tent ✓ 80.7 88.9 81.0 63.0 69.5 58.3 64.9 65.8 59.7 47.7 33.2 47.3 64.6 45.1 46.4 61.1 (-6.1) ✗ 55.5 56.9 55.1 47.5 50.4 44.3 48.7 42.4 47.3 33.6 25.4 35.6 44.8 33.5 36.4 43.8SAR ✓ 70.0 72.5 69.4 56.6 63.4 54.0 60.0 56.4 53.5 43.0 30.5 43.3 58.7 41.5 43.8 54.5 (-10.7) example, TTAC-NQ starts degrading faster (at η = 1/16) due to its slow adaptation speed. For other methods, the η at which they degrade varies. For instance, while TENT has a higher error rate than SAR in slow streams (η ≤ 1/8), TENT outperforms SAR in the regime of faster streams η ≤ 1/4. Interestingly, SHOT (Liang et al., 2020) ranks the worst at η ≤ 1/8, then ranks second when η ≥ 1/2, becoming a viable alternative. At last, the order of different methods significantly changes depending on the speed of the stream. For example, SAR changes from being second best at η ≤ 1/8 to third at η = 1/4 and then to fifth ( i.e. second worst) at η ≥ 1/2. (ii) EATA provides a good trade-off between speed and performance. In fact, EATA gives the best overall perfor- mance (lowest error rate) independent of the stream’s speed. This virtue is attributable to EATA’s combination of good performance and adaptation speed based on efficient sample rejection. Results on other datasets are in the Appendix. 4.4. Results on Other Benchmarks and Architectures We extend our evaluation protocol to cover ImageNet- 3DCC (Kar et al., 2022) and ImageNet-R (Hendrycks et al., 2021) datasets and ResNet-18 (results in the ap- pendix) and ViT (Kolesnikov et al., 2021) architectures. ImageNet-R contains rendition versions of ImageNet span- ning 200 classes. ImageNet-3DCC constructs more spatially-consistent corruptions than ImageNet-C by lever- aging depth estimates. For ViT, we conduct episodic evalu- ation on ImageNet-C in a similar setup to Section 4.1 and report the results in Table 3 for the non-adapted model, Tent, and SAR. For ImageNet-R and ImageNet-3DCC, we fix the architecture to ResNet-50 and experiment on the entire datasets and set the severity level to 5 in ImageNet-3DCC. Due to the space constraint, we limit our experiments to the episodic evaluation, and leave other results and analyses to the Appendix. We evaluate the effectiveness of 10 TTA methods in Table 4, where we report the average error rate across all corruptions. We observe that our results are consistent across all con- Table 4: Average Error Rate on ImageNet-R and ImageNet-3DCC. We report the average error rate of dif- ferent TTA methods on ImageNet-R and ImageNet-3DCC under both the realistic and current setups. A lower error rate indicates a better TTA method. The highlighted num- bers indicate a better performance per method across setups. We observe that methods generally perform worse in the more realistic realistic setup. The conclusions are consistent with what we observed on ImageNet-C (Table 2). Method ImageNet-R ImageNet-3DCC Current Realistic ∆ Current Realistic ∆ Source 63.8 63.8 - 73.9 73.9 - AdaBN 60.6 60.6 0 72.1 72.1 0 BN 60.0 60.0 0 70.5 70.5 0 LAME 60.5 60.5 0 72.1 72.1 0 SHOT 70.3 62.6 (+7.7) 69.2 67.0 (+2.2) TENT 58.1 59.1 (-1.0) 64.5 66.8 (-2.3) SAR 57.5 59.6 (-2.1) 63.5 71.4 (-7.9) CoTTA 57.3 61.5 (-4.5) 66.4 75.6 (-9.2) EATA 55.7 57.1 (-1.4) 60.9 63.1 (-2.2) TTAC-NQ 59.2 60.8 (-1.6) 65.7 73.6 (-7.9) sidered datasets and architectures. Similar to our results in Table 2, the more computationally involved SAR de- grades more than Tent when leveraging ViT architecture. Regarding other datasets, we find that simple methods that adapt during the forward pass are unaffected by the realis- tic setup. All the other methods, except SHOT, experience degradation in their results on both datasets. We observe again that, on these two datasets, while SHOT actually ben- efits from the realistic evaluation, EATA remains the best alternative on both ImageNet-R and ImageNet-3DCC. 4.5. Evaluation under Practical TTA Recently, (Yuan et al., 2023) extended the continual test- time adaptation evaluation to include label-imbalances; known as Practical Test-Time Adaptation (PTTA) setup. In this setting, the stream not only reveals a continual se- quence of distribution shifts, but also the revealed batches 8Evaluation of Test-Time Adaptation Under Computational Time Constraints Table 5: Episodic Error Rate on CIFAR10-C under Practical Evaluation (Yuan et al., 2023).We report the error rate of two baselines (Source, RoTTA (Yuan et al., 2023)) on the 15 different corruptions on CIFAR10-C when the backbone is ResNet-18. We observe that under our computational constrained evaluation, the only method tailored to this setting; RoTTA, performs worse than the non-adapted baseline. Noise Blur Weather DigitalMethodRealisticgauss. shot impul. defoc. glass motionzoom snow frost fog brigh. contr. elast. pixel. jpeg Avg. ∆ Source ✓ 72.3 65.7 72.9 46.9 54.3 34.8 42.0 25.1 41.3 26.0 9.3 46.7 26.6 58.5 30.3 43.5 - ✗ 36.9 34.9 45.8 16.6 44.2 19.9 16.53 21.6 22.4 18.8 9.8 20.6 28.4 27.1 34.5 26.5RoTTA ✓ 55.0 54.4 63.2 43.3 62.3 43.7 43.5 44.8 47.7 43.4 35.3 41.8 54.0 47.7 54.6 49.0 (-22.5) have significant label imbalances. To combat this combined challenge, the work of (Yuan et al., 2023) proposed to lever- age a balanced memory bank for adaptation. In this section, we extend our computational constrained evaluation to the PTTA setup and compare RoTTA (Yuan et al., 2023) with a non-adapted model on CIFAR10-C benchmark. Table 5 summarizes the results. We observe that while RoTTA indeed reduces the error rate under the PTTA setup on CIFAR10-C (17% below the non-adapted model), our realistic evaluation uncovers its computational limitation. We found that RoTTA’s error rate increases by over 22% surpassing the error rate of the non-adapted model. Note that RoTTA stores samples from the stream in a memory bank then adapts the model on sampled samples from the memory bank. Thus, the slower the adaptation of RoTTA, the less diverse the samples in the memory bank, hindering its adaptation. 4.6. Effect of Hyper-parameter Tuning The performance of different TTA methods heavily depends on their hyper-parameter settings (Zhao et al., 2023). Here, we assess the impact of our proposed evaluation on TTA methods when tuning their hyperparameters. For that regard, we conduct hyper parameter search for Tent (as a fundamen- tal baseline) and experiment with different learning rates (the only hyper-parameter for Tent). Table 6 summarizes the results under episodic evaluation for 4 different corruptions on ImageNet-C. We observe that while conducting hyper-parameter search indeed improves the performance of TENT, its error rate increases under our realistic evaluation across all hyperparameters. That is, while conducting hyper-parameter search might indeed result in a better performance for TTA methods, the insights obtained through our proposed evaluation scheme remains consistent: more efficient TTA methods will have a smaller performance drop under the realistic evaluation. 5. Conclusions In this work, we find that the performance of Test Time Adaptation (TTA) methods can vary depending on the con- Table 6: Effect of our evaluation under hyperparameter tuning. We report the error rate for Tent under different learning rates under both the current and our proposed real- istic evaluation. While carefully tuning the learning rate for Tent results in a better performance, our realistic evaluation causes a performance drop under all learning rates. lr Realisticgauss. motion fog pixel. Avg. ∆ ✗ 74.1 63.3 44.7 43.5 56.41×10−4 ✓ 79.7 69.0 47.8 46.8 60.8 (-4.4) ✗ 71.1 59.7 43.1 41.9 53.92×10−4 ✓ 77.6 66.1 46.0 45.0 58.7 (-4.7) ✗ 69.6 58.1 42.4 41.1 52.83×10−4 ✓ 74.9 64.0 45.0 44.0 57.0 (-4.2) ✗ 68.8 57.1 42.0 40.8 52.24×10−4 ✓ 73.7 62.3 44.5 43.2 55.9 (-3.7) text in which they are used. In the episodic evaluation, the efficiency of the method is the most important factor, with more efficient methods like AdaBN and BN showing consistent performance, while data-dependent approaches suffer. Sample rejection methods generally perform well, and fine-tuning approaches such as SHOT can even improve when adapting to fewer samples. In the continual evalua- tion, methods that do not perform sample rejection scale poorly in the offline-continual setup but benefit from skip- ping adaptation on some batches in the realistic-continual setup. Furthermore, our stream speed analysis shows that the performance of TTA methods can vary widely at differ- ent speeds. Our findings are consistent across corruptions and multiple datasets. They can help researchers and practi- tioners to better understand the strengths and weaknesses of different TTA methods, and to choose the most appropriate method for their specific use case. Acknowledgements This work was partially done during a research internship of the first author at Intel Labs. This work was supported by the King Abdullah University of Science and Technol- ogy (KAUST) Office of Sponsored Research (OSR) under Award No. OSR-CRG2021-4648. We would like to thank Yasir Ghunaim and Mattia Soldan for the helpful discussion. 9Evaluation of Test-Time Adaptation Under Computational Time Constraints Impact Statement Our work advances Machine Learning by proposing a re- alistic evaluation protocol for Test Time Adaptation meth- ods, prioritizing computational efficiency. This approach promotes the development of AI systems that are both ac- cessible in resource-limited settings and environmentally sustainable, by favoring simpler, faster methods. Such ad- vancements contribute to more inclusive and responsible AI deployment, aligning with ethical goals of broadening access and reducing environmental impacts References Alfarra, M., P´erez, J. C., Thabet, A., Bibi, A., Torr, P. H., and Ghanem, B. Combating adversaries with anti-adversaries. In Proceedings of the AAAI Conference on Artificial In- telligence, volume 36, pp. 5992–6000, 2022. Ashukha, A., Lyzhov, A., Molchanov, D., and Vetrov, D. Pitfalls of in-domain uncertainty estimation and ensem- bling in deep learning. arXiv preprint arXiv:2002.06470, 2020. Boudiaf, M., Mueller, R., Ben Ayed, I., and Bertinetto, L. Parameter-free online test-time adaptation. In Proceed- ings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 8344–8353, 2022. Cai, Z., Sener, O., and Koltun, V . Online continual learning with natural distribution shifts: An empirical study with visual data. In Proceedings of the IEEE/CVF Interna- tional Conference on Computer Vision, pp. 8281–8290, 2021. Chen, D., Wang, D., Darrell, T., and Ebrahimi, S. Con- trastive test-time adaptation. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 295–305, 2022. Croce, F., Gowal, S., Brunner, T., Shelhamer, E., Hein, M., and Cemgil, T. Evaluating the adversarial robustness of adaptive test-time defenses. In International Conference on Machine Learning, pp. 4421–4435. PMLR, 2022. Deng, J., Dong, W., Socher, R., Li, L.-J., Li, K., and Fei-Fei, L. Imagenet: A large-scale hierarchical image database. In 2009 IEEE conference on computer vision and pattern recognition, pp. 248–255. Ieee, 2009. Gao, J., Zhang, J., Liu, X., Darrell, T., Shelhamer, E., and Wang, D. Back to the source: Diffusion-driven test-time adaptation. arXiv preprint arXiv:2207.03442, 2022. Gidaris, S., Singh, P., and Komodakis, N. Unsupervised rep- resentation learning by predicting image rotations. arXiv preprint arXiv:1803.07728, 2018. Gong, T., Jeong, J., Kim, T., Kim, Y ., Shin, J., and Lee, S.-J. Note: Robust continual test-time adaptation against temporal correlation. In Advances in Neural Information Processing Systems. Goodfellow, I. J., Shlens, J., and Szegedy, C. Explain- ing and harnessing adversarial examples. arXiv preprint arXiv:1412.6572, 2014. He, K., Zhang, X., Ren, S., and Sun, J. Deep residual learn- ing for image recognition. In Proceedings of the IEEE conference on computer vision and pattern recognition, pp. 770–778, 2016. Hendrycks, D. and Dietterich, T. Benchmarking neural network robustness to common corruptions and pertur- bations. Proceedings of the International Conference on Learning Representations, 2019. Hendrycks, D., Basart, S., Mu, N., Kadavath, S., Wang, F., Dorundo, E., Desai, R., Zhu, T., Parajuli, S., Guo, M., Song, D., Steinhardt, J., and Gilmer, J. The many faces of robustness: A critical analysis of out-of-distribution generalization. ICCV, 2021. Ho, J., Jain, A., and Abbeel, P. Denoising diffusion proba- bilistic models. Advances in Neural Information Process- ing Systems, 33:6840–6851, 2020. Iwasawa, Y . and Matsuo, Y . Test-time classifier adjustment module for model-agnostic domain generalization. Ad- vances in Neural Information Processing Systems , 34: 2427–2440, 2021. Kar, O. F., Yeo, T., Atanov, A., and Zamir, A. 3d common corruptions and data augmentation. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 18963–18974, 2022. Kirkpatrick, J., Pascanu, R., Rabinowitz, N., Veness, J., Des- jardins, G., Rusu, A. A., Milan, K., Quan, J., Ramalho, T., Grabska-Barwinska, A., et al. Overcoming catastrophic forgetting in neural networks. Proceedings of the national academy of sciences, 114(13):3521–3526, 2017. Kojima, T., Matsuo, Y ., and Iwasawa, Y . Robustifying vision transformer without retraining from scratch by test- time class-conditional feature alignment. arXiv preprint arXiv:2206.13951, 2022. Kolesnikov, A., Dosovitskiy, A., Weissenborn, D., Heigold, G., Uszkoreit, J., Beyer, L., Minderer, M., Dehghani, M., Houlsby, N., Gelly, S., Unterthiner, T., and Zhai, X. An image is worth 16x16 words: Transformers for image recognition at scale. 2021. 10Evaluation of Test-Time Adaptation Under Computational Time Constraints Lee, D.-H. et al. Pseudo-label: The simple and efficient semi-supervised learning method for deep neural net- works. In Workshop on challenges in representation learning, ICML, volume 3, pp. 896, 2013. Li, Y ., Wang, N., Shi, J., Liu, J., and Hou, X. Revisit- ing batch normalization for practical domain adaptation. arXiv preprint arXiv:1603.04779, 2016. Liang, J., Hu, D., and Feng, J. Do we really need to access the source data? source hypothesis transfer for unsuper- vised domain adaptation. In International Conference on Machine Learning, pp. 6028–6039. PMLR, 2020. Liu, Y ., Kothari, P., Van Delft, B., Bellot-Gurlet, B., Mordan, T., and Alahi, A. Ttt++: When does self-supervised test-time training fail or thrive? Advances in Neural Information Processing Systems, 34:21808–21820, 2021. Mancini, M., Karaoguz, H., Ricci, E., Jensfelt, P., and Ca- puto, B. Kitting in the wild through online domain adap- tation. In 2018 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS), pp. 1103–1109. IEEE, 2018. Mirza, M. J., Micorek, J., Possegger, H., and Bischof, H. The norm must go on: dynamic unsupervised do- main adaptation by normalization. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 14765–14775, 2022a. Mirza, M. J., Soneira, P. J., Lin, W., Kozinski, M., Possegger, H., and Bischof, H. Actmad: Activation matching to align distributions for test-time-training, 2022b. URL https://arxiv.org/abs/2211.12870. Niu, S., Wu, J., Zhang, Y ., Chen, Y ., Zheng, S., Zhao, P., and Tan, M. Efficient test-time model adaptation with- out forgetting. In International conference on machine learning, pp. 16888–16905. PMLR, 2022. Niu14, S., Wu, J., Zhang, Y ., Wen, Z., Chen, Y ., Zhao, P., and Tan15, M. Towards stable test-time adaptation in dynamic wild world. International Conference on Learning Representations, 2023. P´erez, J. C., Alfarra, M., Jeanneret, G., Rueda, L., Thabet, A., Ghanem, B., and Arbel´aez, P. Enhancing adversarial robustness via test-time transformation ensembling. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pp. 81–91, 2021. Saenko, K., Kulis, B., Fritz, M., and Darrell, T. Adapting visual category models to new domains. In Computer Vision–ECCV 2010: 11th European Conference on Com- puter Vision, Heraklion, Crete, Greece, September 5-11, 2010, Proceedings, Part IV 11 , pp. 213–226. Springer, 2010. Sakaridis, C., Dai, D., and Van Gool, L. Acdc: The ad- verse conditions dataset with correspondences for seman- tic driving scene understanding. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pp. 10765–10775, 2021. Schneider, S., Rusak, E., Eck, L., Bringmann, O., Brendel, W., and Bethge, M. Improving robustness against com- mon corruptions by covariate shift adaptation. Advances in Neural Information Processing Systems, 2020. Shalev-Shwartz, S. et al. Online learning and online con- vex optimization. Foundations and Trends® in Machine Learning, 4(2):107–194, 2012. Su, Y ., Xu, X., and Jia, K. Revisiting realistic test-time training: Sequential inference and adaptation by anchored clustering. arXiv preprint arXiv:2206.02721, 2022. Sun, Y ., Wang, X., Liu, Z., Miller, J., Efros, A., and Hardt, M. Test-time training with self-supervision for generaliza- tion under distribution shifts. In International conference on machine learning, pp. 9229–9248. PMLR, 2020. Torralba, A. and Efros, A. A. Unbiased look at dataset bias. In CVPR 2011, pp. 1521–1528. IEEE, 2011. Tzeng, E., Hoffman, J., Saenko, K., and Darrell, T. Adver- sarial discriminative domain adaptation. In Proceedings of the IEEE conference on computer vision and pattern recognition, pp. 7167–7176, 2017. Wang, D., Shelhamer, E., Liu, S., Olshausen, B., and Darrell, T. Tent: Fully test-time adaptation by entropy minimiza- tion. arXiv preprint arXiv:2006.10726, 2020. Wang, Q., Fink, O., Van Gool, L., and Dai, D. Continual test- time domain adaptation. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 7201–7211, 2022. Yuan, L., Xie, B., and Li, S. Robust test-time adaptation in dynamic scenarios. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 15922–15932, 2023. Zhang, M., Levine, S., and Finn, C. Memo: Test time ro- bustness via adaptation and augmentation. arXiv preprint arXiv:2110.09506, 2021. Zhao, H., Liu, Y ., Alahi, A., and Lin, T. On pitfalls of test- time adaptation. International Conference on MAchine Learning, 2023. 11Evaluation of Test-Time Adaptation Under Computational Time Constraints A. Methodology A.1. Online Computation of C(g) Section 3.2 discussed the online evaluation protocol of TTA methods. Here, we give more details on the calcu- lation of C(g), the relative adaptation speed of g, during our online evaluation. First, we set R (g(x)) as the time recording function for g to perform a forward pass for a single batch. To ensure a reliable time calculation, we exe- cute torch.cuda.synchronize() before starting the timer and before ending it. This ensures all GPU operations are finished for the moment time is computed. To alleviate hardware dependence, we also calculate R(fθ(x)) for each evaluation step computing the relative adaptation complex- ity. It is worth mentioning that C(g) for SHOT, EATA, SAR, and COTTA are[3, 3, 8, 103] on average, respectively. B. Experiments B.1. Episodic Evaluation of TTA SHOT, PL, and ETA For completeness, we report the results on 3 baselines: Pseudo Label (Lee et al., 2013), SHOT-IM (Liang et al., 2020), and ETA (Niu et al., 2022) in Table 7. We follow the same setup as in the main paper. Our results are consistent with the findings of Section 4.1 and Table 2. In particular, SHOT-IM improves its perfor- mance under the online evaluation, similar to SHOT. Further, the performance of ETA and PL degrades under the online evaluation due to the additional computational burden. Nev- ertheless, ETA is similar to EATA in providing the best tradeoff between additional computational requirements and performance improvements. SAR with GN We equip our results to include ResNet50 with Group Normalization (GN) layers, following (Niu14 Figure 5: C(g) computation across iterations. We report our online calculations for the relative adaptation speed ofg, C(g), for SAR, SHOT, EATA, and TENT throughout a full evaluation episode. We observe that, overall, C(g) has a stable behavior throughout evaluation iterations. et al., 2023). We report the results in Table 7, where we observe that: (i) Under a relatively large batch size (64), ResNet50 with GN underperforms ResNet50 with Batch Normalization. In fact, the average error rate for SAR in- creases from 56.2% to 65.8%. (ii) The online evaluation penalizes SAR in both architecture choices with a perfor- mance degradation of 3.6% under the GN-based ResNet. Finally, it is worth mentioning that SAR with GN layers attains a similar performance under a batch size of 1. Ablating Batch Sizes In the experiments section, we fixed the batch size to 64 following the recommendations of ear- lier works (Wang et al., 2020; Niu et al., 2022). Here, we investigate the effect of our proposed online evaluation un- der different choices of batch sizes. To that end, we vary the batch size in {1, 16, 32, 128}, and report the results in Figure 6. We draw the following observations: Table 7: Episodic Error Rate on ImageNet-C. We report the error rate of different TTA methods on the ImageNet-C benchmark under both the online and offline setups. A lower error rate indicates a better TTA method. The highlighted numbers indicate a better performance per method across setups. Episodic means the model will adapt to one corruption at a time. The model is reset back to the base model when moving to the next corruption. The offline setup is merely the reproduction of every method. We show that methods generally perform worse in the more realistic online setup. The more computationally complex the TTA method is, the less data it will adapt to, and the worse its performance. SAR-GN represents SAR when deployed on ResNet50 with Group Normalization (GN) layers, following (Niu14 et al., 2023). Noise Blur Weather DigitalMethod Online gauss. shot impul. defoc. glass motionzoom snow frost fog brigh. contr. elast. pixel. jpeg Avg. ∆ ✗ 73.1 69.8 72.0 76.9 75.9 58.5 52.7 53.3 62.2 43.8 34.6 82.6 46.0 42.3 48.9 59.5SHOT-IM ✓ 71.1 68.6 70.7 73.2 73.6 59.1 51.9 52.8 60.5 43.7 33.6 77.3 45.7 42.1 48.6 58.2 (-0.3) ✗ 92.2 92.2 92.8 97.0 89.8 57.7 49.6 50.7 57.1 41.5 32.6 91.1 44.3 40.3 46.6 65.0PL ✓ 90.6 86.3 83.6 93.2 89.7 63.0 51.7 55.0 59.3 43.8 32.9 92.3 47.3 42.4 49.3 65.3 (+0.3) ✗ 64.9 62.7 63.6 66.4 66.3 52.4 47.3 48.2 54.1 40.2 32.2 54.8 42.3 39.2 44.7 52.0ETA ✓ 70.2 67.0 69.6 71.5 71.5 56.9 50.2 51.9 57.0 42.0 32.5 60.5 44.6 40.8 47.1 55.6 (+3.6) ✗ 71.8 69.0 70.3 81.5 81.0 69.6 69.5 57.1 56.6 94.3 29.2 56.0 84.8 51.4 44.7 65.8SAR-GN ✓ 82.0 80.2 82.1 80.2 88.6 78.5 75.1 59.6 53.9 66.9 30.7 63.3 81.3 71.3 47.5 69.4 (+3.6) 12Evaluation of Test-Time Adaptation Under Computational Time Constraints 1 16 32 128 Batch Size 50 60 70 80 90 100Avg. Error Rate (%) ADABN OFFLINE ONLINE 1 16 32 128 Batch Size 50 60 70 80 90 100  BN-ADAPTATION OFFLINE ONLINE 1 16 32 128 Batch Size 50 60 70 80 90 100 COTTA OFFLINE ONLINE 1 16 32 128 Batch Size 50 60 70 80 90 100Avg. Error Rate (%) EATA OFFLINE ONLINE 1 16 32 128 Batch Size 50 60 70 80 90 100 ETA OFFLINE ONLINE 1 16 32 128 Batch Size 50 60 70 80 90 100  LAME OFFLINE ONLINE 1 16 32 128 Batch Size 50 60 70 80 90 100Avg. Error Rate (%) PL OFFLINE ONLINE 1 16 32 128 Batch Size 50 60 70 80 90 100 SAR OFFLINE ONLINE 1 16 32 128 Batch Size 50 60 70 80 90 100 SHOT OFFLINE ONLINE 1 16 32 128 Batch Size 50 60 70 80 90 100Avg. Error Rate (%) SHOTIM OFFLINE ONLINE 1 16 32 128 Batch Size 50 60 70 80 90 100 TENT OFFLINE ONLINE 1 16 32 128 Batch Size 50 60 70 80 90 100 TTAC-NQ OFFLINE ONLINE Figure 6: Batch Size Analysis current vs. realistic setups for every method. We assess the performance variation of 12 different TTA methods under varying batch sizes. We experiment with batch sizes in{1, 16, 32, 128}. We do not include the baseline, MEMO, and DDA, since they are data-dependent approaches and are unaffected by batch size. All TTA methods, except LAME, are severely affected by smaller batch sizes. Nonetheless, the realistic evaluation degrades the performance of all methods, except SHOT and SHOT-IM. (i) Online evaluation improves the performance of SHOT and SHOT-IM. This result is consistent with the earlier observations in Table 2. Note that PL shares a similar trend as well. (ii) The performance of TTA methods degrades when switching from offline to online evaluation, regardless of the batch size. This result is highlighted in COTTA, ETA, EATA, SAR, TENT, and TTAC-NQ. (iii) Performance of TTA methods vastly varies when varying the batch size. This result is consistent with earlier findings in the literature (Gao et al., 2022; Niu14 et al., 2023), where most TTA methods fail with small batch sizes. At last, and to ease comparison across methods, we summa- rize all the plots for all methods in Figure 7. Consistency with 3 random seeds. For all of our exper- iments, we run each experiment with 3 random seeds. In most of our results, we found out that the standard deviation of performance across runs is very small. Our results in Figures 3 and 4 demonstrate this variation in the shaded area for 5 different TTA methods. B.2. Continual Evaluation of TTA We further explore another setup for the continual evalua- tion of TTA. In particular, we follow (Wang et al., 2022) in concatenating all corruptions in ImageNet-C with 11 differ- ent orders. We then report the average performance of each method across all runs and corruptions in Table 8. We run each experiment with 3 random seeds, and report our results with standard deviations. For the remaining implementation 13Evaluation of Test-Time Adaptation Under Computational Time Constraints 1 16 32 128 Batch Size 50 60 70 80 90 100Avg. Error Rate (%) OFFLINE 1 16 32 128 Batch Size 50 60 70 80 90 100 ONLINE ADABN BN-ADAPTATION COTTA EATA ETA LAME PL SAR SHOT SHOTIM TENT TTAC-NQ Figure 7: Summary of batch size analysis: current vs. realistic setups. Left: Current evaluation, i.e.,Section 3.1. Right: Realistic evaluation,i.e.,Section 3.2. While EATA achieves the lowest error rate under batch sizes≥ 32, SHOT becomes a very competitive baseline, outperforming EATA, at a batch size of 128. Table 8: Continual Error Rate on ImageNet-C. We report the average continual error rate for 11 different corruption orders, with 3 different seeds, under both the offline and online setups with a corruption severity level of 5. Continual refers to continually adapting after each corruption without resetting. This metric indicates the model’s capability to learn from previous corruptions. The offline setup refers to the performance of the model in a continual learning scheme, whereas the online setup refers to the performance of the model in a continual learning scheme, under our more realistic online setup. We show that the more complex a method is, the fewer samples it adapts to, achieving better performance in a continual learning scheme. Avg. Error (%) COTTA ETA TENT SAR EATA SHOT TTAC-NQ Offline 65.3 ± 5.9 56 .4 ± 2.3 84 .6 ± 16.0 59 .8 ± 3.0 56 .4 ± 2.3 88 .4 ± 11.4 81 .8 ± 11.4 Online 69.3 ± 2.8 57 .7 ± 2.0 65 .6 ± 5.0 60 .4 ± 1.8 57 .7 ± 1.9 78 .2 ± 7.7 65 .1 ± 3.8 details, we follow our setup in main paper. We observe that, similar to our conclusions in Section 4.2, online eval- uation helps methods that do not perform sample rejection (e.g.,TENT). Nonetheless, both ETA and EATA provide the best trade-off between performance and additional compu- tational burden. B.3. Stream Speed Analysis For completeness, we extend our stream speed analysis in Section 4.3 to cover the ImageNet-3DCC dataset. We preserve our experimental setup by varying the stream speed according to ηr, with η ∈ {1/16, 1/8, 1/4, 1/2, 1. Figure 8 summarizes our results for SHOT, TENT, TTAC-NQ, EATA, and SAR. We observe similar trends to the ones in Figure 4, where the performance of different TTA methods varies widely under different stream speeds. The large relative adaptation speed of TTAC-NQ degrades its performance under even slow streams (e.g.,η = 1/8), while SHOT reduces its error rate under faster streams. Furthermore, EATA is consistently outperforming all other considered approaches under different stream speeds. B.4. Evaluation on Other Benchmarks We report the error rates on all corruptions of ImageNet- 3DCC (Kar et al., 2022), along with the overall average error rate, in Table 9. The conclusions we draw for ImageNet- 3DCC (Kar et al., 2022) are very similar to the ones ob- served on ImageNet-C (Hendrycks & Dietterich, 2019) (in Section 4.1). We observe that efficient methods, with C(g) = 1, such as LAME and BN, maintain performance. Furthermore, the performance of some TTA methods (Wang et al., 2020; Niu14 et al., 2023; Niu et al., 2022; Wang et al., 2022) degrades in the online setup, while others that use pseudo labeling (Lee et al., 2013; Liang et al., 2020) actually improve. This degradation seems to be directly proportional to the amount of data a method misses according to its C(g). 14Evaluation of Test-Time Adaptation Under Computational Time Constraints Table 9: Episodic Error Rate on ImageNet-3DCommonCorruptions. We report the error rate of different TTA methods on ImageNet-3DCC (Kar et al., 2022) benchmark under both the realistic and offline setups. A lower error rate indicates a better TTA method. The highlighted numbers indicate a better performance per method across setups. Episodic means the model will adapt to one corruption at a time. The model is reset back to the base model when moving to the next corruption. The offline setup corresponds to reproducing the reported performance of every method. The first sub-table corresponds to methods that incur none or few additional computations, i.e.,C(g) = 1. We show that methods generally perform worse in the more realistic setup. The more computationally complex the TTA method is, the fewer data it will adapt to, and the worse its performance. Depth of field Noise LightingWeather Video Camera motionMethod RealisticNear focus Far focusColor quant. ISO noise Low lightFlash Fog 3DBit error H.265 ABR H.265 CRFXY-mot. blur Z-mot. blurAvg. ∆ Source ✓ 46.9 55.6 82.5 94.0 71.7 78.7 75.3 88.6 70.6 65.4 82.0 75.3 73.9 -AdaBN ✓ 45.2 55.0 71.8 76.8 64.1 80.8 75.0 91.8 80.9 76.7 79.1 67.5 72.1 -LAME ✓ 45.3 55.0 71.9 76.9 64.1 80.8 75.1 91.8 80.9 76.8 79.2 67.6 72.1 -BN ✓ 43.9 54.3 72.3 76.6 60.9 80.1 72.4 90.9 78.7 73.8 76.9 65.6 70.5 - PL ✗ 39.8 49.8 65.5 72.6 48.9 79.0 66.1 97.5 92.1 86.2 88.7 57.6 70.3(-1.6)✓ 41.0 51.3 66.5 71.5 52.8 77.4 68.1 95.6 86.0 78.7 77.0 59.2 68.7 SHOT ✗ 43.0 53.6 67.1 64.2 51.9 81.1 73.2 97.2 83.5 77.8 77.3 60.1 69.2(-2.2)✓ 41.7 51.4 64.4 63.8 51.6 77.5 71.6 95.1 79.9 74.6 73.7 58.5 67.0 SHOT-IM✗ 42.2 52.7 66.6 63.7 51.0 81.0 72.1 97.0 83.3 77.6 75.6 59.2 68.5(-1.9)✓ 41.2 51.2 64.4 63.3 51.3 77.5 70.9 94.9 79.4 74.1 72.3 58.3 66.6 TENT ✗ 39.9 49.6 62.4 62.2 50.7 75.6 68.5 91.6 75.7 70.2 70.4 57.0 64.5(+2.3)✓ 41.7 51.4 65.5 67.2 54.7 77.4 70.1 90.7 76.8 71.9 74.0 60.8 66.8 SAR ✗ 40.3 50.0 62.0 61.2 50.6 73.8 65.8 90.1 73.9 68.8 69.1 56.8 63.5(+6.9)✓ 44.9 54.7 71.1 75.4 62.6 80.3 73.8 91.7 80.5 76.1 78.6 66.9 71.4 ETA ✗ 38.7 47.9 59.1 56.7 46.8 71.0 62.1 90.6 72.8 67.3 64.7 52.9 60.9(+2.3)✓ 39.7 49.3 61.6 60.7 50.0 73.5 65.2 90.3 74.4 69.1 68.8 55.9 63.2 CoTTA ✗ 40.8 50.9 66.3 68.3 54.6 77.2 68.0 90.2 76.4 71.1 73.1 60.4 66.4(+9.2)✓ 55.4 63.1 74.1 77.0 64.7 83.4 78.1 93.7 84.0 80.3 81.7 71.9 75.6 TTAC-NQ✗ 40.7 50.5 61.0 61.1 51.5 72.8 66.6 93.8 81.1 74.7 75.7 59.1 65.7(+7.9)✓ 49.9 57.0 69.3 72.3 58.9 79.8 76.3 95.8 86.5 83.0 84.6 69.8 73.6 EATA ✗ 38.6 47.8 59.2 56.6 46.9 71.2 62.2 90.9 72.5 67.4 64.6 52.9 60.9(+2.2)✓ 39.8 49.3 61.6 60.5 49.9 73.5 64.8 90.6 73.7 69.1 68.6 55.7 63.1 C. Single Model Evaluation Scheme In Section 3.2, we assume fθt can generate predictions whenever g is occupied with adapting to a batch. This setup assumes the capacity to concurrently deploy two models. However, this assumption might be unfair to methods with C(g) = 1, since it allows expensive methods to skip batches without large penalties. We thus also study the case where only one model can be deployed. Studying this setup requires establishing a policy on how samples missed by the TTA method g are treated. That is, when g is busy adapting, all skipped samples still must be predicted without access to fθt . Depending on the applica- tion, this prediction could leverage prior knowledge about the problem e.g. temporal correlation across samples, or the bias of the distribution. In our setup, we consider the most strict scenario in which, whenever g is busy, a ran- dom classifier generates predictions for the incoming sam- ples. This naive design choice results from our evaluation on ImageNet-based datasets, which contain images whose classes display no bias nor temporal correlation. We conduct episodic evaluation, similar to Section 4.1, on ImageNet-C dataset. We average the error rates per corruption category (e.g. averaging error rates for gaussian, shot, and impulse noises) and present the results of this study in Table 10. We draw the following observation. Single model evaluation strongly favors methods with C(g) = 1. We observe that all models that are slower than the stream are heavily penalized to the point that using the original pre-trained model becomes a better alternative. However, methods that can be as fast as the stream, like AdaBN or BN, become the best alternative due to their speed. This result encourages more research toward devel- oping efficient TTA methods that have negligible additional computational overhead. D. Results on ResNet18 In our experiments in the main paper, we focused on the stan- dard ResNet18-architecture, following the common practice in the literature. Here, and for completeness, we extend our results to cover the smaller and more efficient ResNet18 architecture. Teble 11 summarizes the episodic evaluation of 6 TTA methods on ImageNet-C dataset. Similar to our conclusions in the episodic evaluation section in the main paper, more expensive adaptation methods degrade more under our realistic evaluation scheme. 15Evaluation of Test-Time Adaptation Under Computational Time Constraints Table 10: Per Corruption Category Average Error Rate Using Single Model Evaluation on ImageNet-C. We re- port the average error rate per corruption category of dif- ferent TTA methods under single model realistic evaluation mode on ImageNet-C. Single model mode assumes the de- ployment of a single modelg instead of two under a constant speed stream S. We assume the most extreme scenario, that is if a model g is occupied adapting to a batch, the incoming batch is fed to a random classifier. We observe that the best TTA methods to use in this scenario are AdaBN (Li et al., 2016) and BN (Schneider et al., 2020), which simply adapt the BN statistics. Method Realistic Noise Blur Weather Digital Avg. Source ✓ 97.7 83.8 69.1 81.4 82.0 AdaBN ✓ 84.5 76.1 54.9 62.7 68.5 BN ✓ 84.1 73.1 54.2 59.9 66.7 SHOT ✓ 92.6 91.3 87.0 88.5 89.7 TENT ✓ 91.9 89.4 83.0 85.0 87.0 SAR ✓ 95.6 94.0 90.1 91.3 92.6 EATA ✓ 89.4 87.6 82.0 83.2 85.3 TTAC-NQ ✓ 96.6 96.9 96.3 96.4 96.5 Table 11: Evaluating different TTA methods with ResNet- 18 architecture on ImageNet-C. We report the average error rate across all different types of corruptions (lower is bet- ter). TTA methods generally perform worse in the more realistic setup. The more computationally complex the TTA method is, the less data it will adapt to, and the worse is its performance. Method Basic BN SHOT Tent EATA SAR Current 85.4 70.1 64.4 64.9 59.7 63.8 Realistic 85.4 70.1 64.5 68.3 63.2 69.5 Diff - - 0.1 3.4 3.5 5.7 1/16 1/8 1/4 1/2 1 η 62 64 66 68 70 72 74Error Rate (%) SHOT TENT TTAC-NQ SAR EATA Figure 8: Average Error Rate on ImageNet-3DCC Under Slower Stream Speeds. We report the average error rate for several TTA methods on ImageNet-3DCC under slower stream speeds. In our proposed online model evaluation, the stream speed r is normalized by the time needed for a forward pass using the base model. We evaluate different TTA methods under a stream with speed ηr with η ∈ (0, 1]. An η = 1/16 means the stream is 16 times slower than the forward pass of the base model. We report the standard deviation across 3 random seeds. Different TTA methods degrade differently when varying η. 16",
      "meta_data": {
        "arxiv_id": "2304.04795v2",
        "authors": [
          "Motasem Alfarra",
          "Hani Itani",
          "Alejandro Pardo",
          "Shyma Alhuwaider",
          "Merey Ramazanova",
          "Juan C. Pérez",
          "Zhipeng Cai",
          "Matthias Müller",
          "Bernard Ghanem"
        ],
        "published_date": "2023-04-10T18:01:47Z",
        "pdf_url": "https://arxiv.org/pdf/2304.04795v2.pdf"
      },
      "llm_extracted_info": {
        "main_contributions": "This paper proposes a novel online evaluation protocol for Test-Time Adaptation (TTA) methods that explicitly accounts for the computational cost and adaptation speed of these methods. The current evaluation protocols overlook this crucial aspect, affecting real-world applicability. The new protocol penalizes slower TTA methods by providing them with fewer samples for adaptation from a constant-speed data stream. Through extensive benchmarking of 15 TTA methods across multiple datasets and scenarios, the paper demonstrates that, when inference speed is considered, simple and fast approaches can significantly outperform more sophisticated but slower state-of-the-art methods. This highlights the critical importance of developing TTA methods that are both accurate and computationally efficient for practical deployment.",
        "methodology": "The core of the methodology is the Realistic TTA Evaluation Protocol, which addresses the shortcomings of the conventional offline evaluation. The current protocol implicitly assumes the data stream waits for a TTA method's adaptation to complete, neglecting inference speed. The proposed realistic protocol simulates a constant-speed data stream and introduces the 'relative adaptation speed' C(g), defined as the integer ratio of the stream's speed (r) to the TTA method's processing speed (R(g(x))). If C(g) = k, the method g is allowed to adapt only every k-th sample. For skipped samples, the most recently adapted model or the initial pre-trained model (fθ) is used for prediction, assuming fθ operates in real-time (C(fθ) = 1). C(g) is computed online for each input batch using C(g(xt)) = ⌈r/R(g(x))⌉, accounting for hardware and input variations. The protocol ensures that slower methods intrinsically adapt to fewer samples, thereby penalizing their performance in a realistic online setting.",
        "experimental_setup": "The experiments primarily focus on the task of image classification. The baseline model used is a ResNet-50-BN3, pretrained on ImageNet, with additional experiments using ViT and ResNet-18 architectures. Data streams reveal batches of size 64, except for MEMO which processes single images. Benchmarking was conducted on several datasets: ImageNet-C (with corruption level 5 for all 15 corruptions), CIFAR10-C, ImageNet-R, and ImageNet-3DCC (level 5 severity). A total of 15 state-of-the-art TTA methods, published between 2017 and 2023, were evaluated (including BN, AdaBN, SHOT, TENT, SAR, CoTTA, EATA, MEMO, DDA, among others), using their official implementations and recommended hyperparameters. The evaluation encompassed multiple scenarios: episodic domain shifts (single corruption, model reset), continual domain shifts (concatenated corruptions, no reset), varying stream speeds (ηr with η ∈ {1/16, 1/8, 1/4, 1/2, 1}), and a practical TTA setup with label imbalances (PTTA) using RoTTA on CIFAR10-C. Hyperparameter tuning effects were also studied with TENT. All results are averaged across three random seeds.",
        "limitations": "The primary limitation identified by the paper is that existing TTA evaluation protocols overlook computational costs, making them unrealistic. An implicit limitation of the proposed realistic evaluation protocol, explored in the appendix, is its assumption of the capacity to concurrently deploy two models (the TTA method 'g' and a fast baseline model 'fθ') to handle samples skipped by 'g'. In a stricter 'single model evaluation' scenario where only one model can be deployed, and skipped samples are processed by a random classifier (due to lack of temporal correlation or bias in datasets like ImageNet-C), slower TTA methods are heavily penalized to the extent that the non-adapted pre-trained model becomes a better alternative. This suggests that the real-world applicability of slower TTA methods is highly dependent on the system's ability to process unadapted samples efficiently.",
        "future_research_directions": "The paper strongly advocates for future research to focus on developing practical TTA methods that are both accurate and computationally efficient. Specific directions include increasing the efficiency of data-dependent adaptation methods (e.g., MEMO and DDA), which currently incur massive computational burdens and perform poorly under realistic time constraints. The authors hope their proposed evaluation scheme inspires the TTA research community to consider inference speed as a critical dimension that significantly affects real-world performance. Furthermore, there is an implicit call for developing efficient TTA methods with negligible additional computational overhead, particularly for deployment scenarios where only a single model can be used and skipped samples cannot be efficiently processed by a separate baseline."
      }
    },
    {
      "title": "Learning to Adapt to Evolving Domains"
    },
    {
      "title": "Test-Time Adaptation for Depth Completion",
      "abstract": "It is common to observe performance degradation when transferring models\ntrained on some (source) datasets to target testing data due to a domain gap\nbetween them. Existing methods for bridging this gap, such as domain adaptation\n(DA), may require the source data on which the model was trained (often not\navailable), while others, i.e., source-free DA, require many passes through the\ntesting data. We propose an online test-time adaptation method for depth\ncompletion, the task of inferring a dense depth map from a single image and\nassociated sparse depth map, that closes the performance gap in a single pass.\nWe first present a study on how the domain shift in each data modality affects\nmodel performance. Based on our observations that the sparse depth modality\nexhibits a much smaller covariate shift than the image, we design an embedding\nmodule trained in the source domain that preserves a mapping from features\nencoding only sparse depth to those encoding image and sparse depth. During\ntest time, sparse depth features are projected using this map as a proxy for\nsource domain features and are used as guidance to train a set of auxiliary\nparameters (i.e., adaptation layer) to align image and sparse depth features\nfrom the target test domain to that of the source domain. We evaluate our\nmethod on indoor and outdoor scenarios and show that it improves over baselines\nby an average of 21.1%.",
      "full_text": "Test-Time Adaptation for Depth Completion Hyoungseob Park Yale Vision Lab hyoungseob.park@yale.edu Anjali Gupta Yale Vision Lab anjali.gupta@yale.edu Alex Wong Yale Vision Lab alex.wong@yale.edu Abstract It is common to observe performance degradation when transferring models trained on some (source) datasets to tar- get testing data due to a domain gap between them. Existing methods for bridging this gap, such as domain adaptation (DA), may require the source data on which the model was trained (often not available), while others, i.e., source-free DA, require many passes through the testing data. We pro- pose an online test-time adaptation method for depth com- pletion, the task of inferring a dense depth map from a single image and associated sparse depth map, that closes the per- formance gap in a single pass. We first present a study on how the domain shift in each data modality affects model per- formance. Based on our observations that the sparse depth modality exhibits a much smaller covariate shift than the image, we design an embedding module trained in the source domain that preserves a mapping from features encoding only sparse depth to those encoding image and sparse depth. During test time, sparse depth features are projected using this map as a proxy for source domain features and are used as guidance to train a set of auxiliary parameters (i.e., adap- tation layer) to align image and sparse depth features from the target test domain to that of the source domain. We eval- uate our method on indoor and outdoor scenarios and show that it improves over baselines by an average of 21.1%. Code available at github.com/seobbro/TTA-depth-completion. 1. Introduction Reconstructing the 3-dimensional (3D) structure of an envi- ronment can support a number of spatial tasks, from robotic navigation and manipulation to augmented and virtual reality. Most systems addressing these tasks are built for sensor plat- forms equipped with range (i.e., lidar or radar) or optics (i.e., camera or sensors). While range sensors can measure the 3D coordinates of the surrounding space, they often yield point clouds that are sparse. Likewise, these coordinates can also be estimated from images by means of Structure- from-Motion (SfM) or Visual Inertial Odometry (VIO). For the goal of dense mapping, depth completion is the task of recovering the dense depth of a 3D scene as observed from a sparse point cloud, which is often post-processed into a sparse depth map by projecting the points onto the image plane, and guided by a synchronized calibrated image. Training a depth completion model can be done in a su- pervised (using ground truth) or unsupervised (using SfM) manner. The former dominates in performance, but requires expensive annotations that are often unavailable; the latter uses unannotated images, but they must satisfy SfM assump- tions between frames, i.e., motion, covisibility, etc. Like most learning-based methods, models trained under both paradigms typically experience a performance drop when tested on a new dataset due to a covariate shift, i.e., domain gap. As we can only assume that a single pair of image and sparse depth map is available in the target domain for the depth completion, models belonging to either learning paradigms cannot easily be trained or adapted to the new domain even when given the testing data. We focus on test- time adaptation (TTA) for depth completion, where one is given access to the test data in a stream, i.e., one batch at a time, without being able to revisit previously-seen examples. The goal is to learn causally and to quickly adapt a set of pre-existing weights trained on a source domain to a target test domain, so one can reduce the performance gap. We begin with some motivating observations on the ef- fects of the domain gap: (i) Errors in target domain tend to be higher when feed both the image and sparse depth as input rather than sparse depth only, as shown in Fig. 1. This implies that the depth modality exhibits a smaller covariate shift between the source and target domains than the image modality, to the extent that forgoing the image altogether often yields superior results than using either both sparse depth and image or the image alone. (ii) Yet, when operating in the source domain, we observe the opposite effect – forgo- ing the image is detrimental to performance. Naturally, this begs the question: How should one leverage data modalities that are less sensitive to the domain shift (e.g., sparse depth) to support alignment between source and target domains for modalities that are more sensitive (e.g., RGB image)? To answer this question, we investigate a test-time adap- tation approach that learns an embedding for guiding the 1 arXiv:2402.03312v4  [cs.CV]  27 May 2024model parameter update by exploiting the data modality (sparse depth) that is less sensitive to the domain shift. The embedding module maps the latent features encoding sparse depth to the latent features encoding both image and sparse depth. The mapping is trained in the source domain and frozen when deployed to the target domain for adaptation. During test time, sparse depth is first fed through the en- coder and mapped, through the embedding module, to yield a proxy for image and sparse depth embeddings from the source domain – we refer to the embedded sparse depth fea- tures as proxy embeddings. Note: As the mapping is learned in the source domain, the proxy embeddings will also follow the distribution of source image and sparse depth embed- dings. Next, both image and sparse depth from the target test domain are fed as input to the encoder. By maximizing the similarity between test-time input embeddings and the proxy embeddings, we align the target distribution to that of the source to reduce the domain gap. In other words, our method exploits a proxy modality for guiding test-time adap- tation and we call the approach, ProxyTTA. When used in conjunction with typical loss functions to penalize discrepan- cies between predictions and input sparse depth, and abrupt depth transitions, i.e., Total Variation, the embeddings serve as regularization to guide the model parameter update and prevent excessive drift from those trained on the source data. Following test-time adaptation conventions, we assume limited computational resources, and that inputs arrive in a stream of small batches and must be processed within a time budget without access to the past data. To ensure fast model updates under these constraints, we deploy auxiliary parame- ters, or an adaptation layer, to be updated while freezing the rest of the network – thus achieving low-cost adaptation. We demonstrate our method in both indoor (VIO) and outdoor (lidar) settings across six datasets, where we not only target typical adaptation scenarios where the shift exists between real and synthetic data domains with similar scenes, i.e. from KITTI [45] to Virtual KITTI [12], but also between differ- ent scene layouts, i.e., from VOID [53] to NYUv2 [30] and SceneNet [29]. Our proxy embeddings consistently improve over baselines by an average of 21.09% across all methods and datasets. To the best of our knowledge, we are the first to introduce test-time adaptation for depth completion. 2. Related work Test Time Adaptation(TTA) aims to adapt a given model, pretrained on source data, to test data without access to the source training data. Related fields along this vein include unsupervised domain adaptation [ 13, 32], which utilizes source domain data (in practice, this may not be available) for adaptation, and source-free domain adaptation [22], which does not assume access to source data, but allows access to test data on multiple passes. In contrast, we focus on test-time adaptation where we do not have access to source data and must adapt to test data in a single pass. Previous studies have proposed strategies to select the source model’s component to be preserved, such as the class prototypes extracted from the source data [ 5, 26, 37], the subset of source model parameters [19, 49], and the discrim- inative feature from the self-supervised learning (SSL) [5]. For instance, [ 49] proposes TENT, a simple but effective batch-norm layer adaptation with entropy minimization for fully test-time adaptation. TTT [44] performs classification- layer adaptation by updating the last linear layer of the source model; [ 26] extends this with TTT++ and utilizes joint task-specific and model-specific information based on self-supervised learning. [19] presents T3A, an optimization- free classifier adjustment module. [ 5] uses shift-agnostic weight regularization (SWR) to prevent an effect from the erroneous signal in test time, jointly with the nearest source prototype classifier and a self-supervised proxy task. [ 2] proposes a contrastive learning with an online pseudo-label refinement while [37] proposes pseudo-label refinement and momentum update for 3D point cloud segmentation. [ 50] proposes continual test-time adaptation based on stochastic restoration and weight-averaged pseudo-labels. [ 41] uses efficient residual modules to realign the pretrained weights. The above methods largely focus on single-image-based tasks, i.e., classification [2, 5, 26] and semantic segmenta- tion [37], and rely on entropy constraints from [49]. Unlike prior work on classification [26, 44] and segmen- tation [37], depth completion is a regression problem; hence, existing methods using entropy-based objectives [49], which operate on logits, are not applicable in this task. Instead, we propose to minimize sparse depth reconstruction and lo- cal smoothness objectives – similar to that of some existing unsupervised methods [52, 53] – and to maximize cosine similarity between the proxy embeddings and the test time image and sparse depth embeddings. Depth Completionaims to output dense depth from a single image and synchronized point cloud, i.e., from lidar or tracked by VIO, projected onto a sparse depth map, by multimodal fusion [8, 46, 53, 58–60]. Unsupervised depth completion approaches rely on Structure-from-Motion (SfM) for training and require access to an auxiliary training dataset containing stereo image pairs [38, 51] or monocular videos [25, 28, 53–56] with synchro- nized sparse depth maps. Typically, they minimize a linear combination of photometric reprojection consistency, sparse depth reconstruction error, and local smoothness [28, 53–55]. These methods can support online training, but are limited by the need for stereo or monocular videos with sufficient parallax and co-visibility. In contrast, our approach does not rely on SfM and can be used in more general scenarios. Supervised depth completion trains the model by mini- mizing a loss with respect to ground truth. [ 3, 18] focus on network operations and designs to effectively deal with 2Source DomainTarget Domain Inputs Predictions and error maps with different inputs 0.1 0.0 0.1m 8.0m Ground truth ImageSparse depthImageSparse depth Image only Sparse depth only Image + sparse depth RMSE: 496.65RMSE: 597.91 RMSE: 1528.98RMSE: 1046.28 RMSE: 2601.01 RMSE: 2462.63 Figure 1. Model sensitivity to input modalities. While utilizing both sparse depth and image as input, the best performance is achieved in the source domain (VOID). Yet, forgoing the image in the test domain (NYUv2) often yields lower error than using both as input. sparse inputs. [20, 28, 61] propose early and late fusion of image and depth encoder features while [17] uses separate networks for each. [ 23] proposes a multi-scale cascaded hourglass network to enhance the depth encoder with im- age features. [4] proposes convolutional spatial propagation network; [31] extends it to non-local spatial propagation to refine an initial depth map based on confidence and learnable affinity; [24] further extends it to dynamic spatial propaga- tion. [9, 10, 34, 35] learn uncertainty of the depth estimates. [48] utilizes confidence maps to combine depth predictions while [33, 57, 63] use the surface normals to guide depth prediction. [21] incorporates cost volume for depth predic- tion. [ 40] used radar. [ 36] devises transformer architec- ture with cross-modal attention, and [62] proposes a hybrid convolution-transformer architecture for depth completion. While both unsupervised and supervised methods have demonstrated strong performance on benchmarks, they often fail to generalize to test datasets with large domain discrep- ancies. Moreover, obtaining ground truth is unrealistic for real-time applications, and accumulating sufficient parallax incurs large latencies – presenting significant challenges for online adaptation. Unlike past works, we do not assume access to ground truth nor data outside of the input. Unsupervised Domain Adaptation(UDA) addresses the discrepancy between labeled source data and unlabeled target data [6, 13, 32, 43]. The only existing UDA depth completion method [27] models the domain gap as the noise in sparse points and the appearance in images. Unlike most UDA approaches that require source data during adaptation, we are only given the inputs necessary for inference in a stream without the ability to revisit past data, and must up- date the model online under a limited computational budget. Our Contributions.We present (i) a study on how the domain shift in each data modality (e.g., image and sparse depth) affects model performance when transferring it from source to target test domain. This study motivates (ii) our approach to learn an embedding of sparse depth features (which are less sensitive to the domain shift) that serves as proxy to source features for guiding test-time adaptation. (iii) To the best of our knowledge, we are the first to propose test-time adaptation for the depth completion task, and (iv) will release code, models, and dataset benchmarking setup to make development accessible for the research community. 3. Method Formulation For ease of use, we assume access to a (source) pretrained depth completion model fθ that infers a dense depth map ˆd from a calibrated RGB image I ∈ RH×W×3 and its associ- ated sparse point cloud projected onto the image plane as a sparse depth map z ∈ RH×W + , i.e., fθ(I, z) → ˆd ∈ RH×W + . For simplicity, we assume that the model was trained to min- imize a supervised loss between prediction and ground truth d ∈ RH×W + on a source dataset Ds = {I(n) s , z(n) s , d(n)}Ns n=1, where Ns indicates the number of data samples. Following conventions in TTA, we assume access to the source domain dataset prior to deployment. During test-time adaptation, we follow the protocol of [26, 44], where we only have access to the target domain data Dt = {I(n) t , z(n) t }Nt n=1 and utilize an online procedure to adapt to unseen Nt target data samples. Note that we make no assumptions about supervision during test-time; hence, while we present results on supervised methods for controlled experiments, we see our method being applicable towards unsupervised methods as well. Our method, ProxyTTA, is split into three stages (Fig. 3): 30 800 1600 2400 MSG-CHNNLSPNCostDCNet MAE: VOID → NYUv2 0 1000 2000 3000 MSG-CHNNLSPNCostDCNet MAE: VOID →ScanNet 0 1000 2000 3000 MSG-CHNNLSPNCostDCNet RMSE: VOID →ScanNet 0 8000 16000 24000 MSG-CHNNLSPNCostDCNet MAE: KITTI →Waymo 0 9000 18000 27000 MSG-CHNNLSPNCostDCNet RMSE: KITTI →Waymo 0 6000 12000 18000 MSG-CHNNLSPNCostDCNet MAE: KITTI →nuScenes 0 8000 16000 24000 MSG-CHNNLSPNCostDCNet RMSE: KITTI →nuScenes 0 1000 2000 3000 MSG-CHNNLSPNCostDCNet RMSE: VOID →NYUv2 Image onlySparse depth onlyImage + sparse depth Figure 2. Model sensitivity to input modalities. Depth completion networks have a high reliance on sparse depth modality. Performing inference in a novel domain without the RGB image, i.e., using just sparse depth as input, can improve over using both data modalities. (a) During an intialization stage, we augment the network encoder with an adaptation layer and train it using source domain data. (b) In the preparation stage, we learn a mapping from sparse depth features to image and sparse depth (proxy) embeddings. (c) During test time, we do not need the source dataset; we freeze the mapping and use its proxy embeddings for updating the adaptation layer parameters in test domain. 3.1. Sensitivity Study on Data Modalities To motivate our approach, we begin with a sensitivity study of depth completion networks to input modalities, e.g. image, sparse depth, and the effect of domain shift on them. To this end, we alter the inputs by zeroing out either I or z to yield (I, z), (I0, z), and (I, z0), where I0 and z0 indicate the zero matrices with identical size to I and z, respectively. We eval- uate the pretrained models using (I, z), (I0, z), and (I, z0) to highlight their dependence on each input modality and to gauge their sensitivity when one modality gives no useful information at all. Fig. 1 and Fig. 2 show qualitative (error maps) and quantitative results (bar graphs), respectively, of pretrained depth completion models when fed the different inputs on the source dataset Ds and the target dataset Dt. In the source domain, inference using both image and sparse depth as inputs, i.e., ˆds(I, z), shows the best perfor- mance. Surprisingly, the inference using sparse depth alone (i.e., with null-image) ˆds(I0, z) is comparable to ˆds(I, z). This shows the first intuition behind our approach: (i) Even though depth inputs are sparse, they are sufficient to sup- port the reconstruction of the scene. Additionally, inference with image alone (i.e., null-depth) ˆds(I, z0) is worse than ˆds(I0, z) and ˆds(I, z), which suggests that a depth com- pletion network relies heavily on sparse depth modality for inference, and the image for guiding recovery of finer details. In the target test domain, expectedly, performance de- grades for inference using both image and sparse depth due to a covariate shift. Remarkably, we observe that predic- tions from sparse depth alone ˆdt(I0, z) remain consistent in performance to those using both inputs ˆdt(I, z). Moreover, we observe that in most cases ˆdt(I0, z), in fact, outperforms ˆdt(I, z) across several methods and datasets, i.e., inference without image information in the test domain is better than with it. Conversely, the performance gap between infer- ence with both inputs, ˆdt(I, z), and just the image, ˆdt(I, z0), becomes more evident under the domain shift. This observa- tion illustrates another intuition: (ii) The domain shift largely affects the image modality, and less so depth. The two intuitions above motivate our approach. As ob- ject shapes tend to persist across domains, and the measured sparse points being a coarse representation of them, we aim to leverage sparse depth modality to bridge the domain gap. To this end, we exploit the observation that depth comple- tion networks are able to recover (coarse) 3D scenes from sparse points alone and that the image serves to propagate and refine depth for regions lacking points. This is done by learning to map features encoding sparse depth inputs to features encoding both modalities in the source domain and, during test-time, recover the source domain features compatible with target domain sparse depth to guide model adaptation. Specifically, as observed, the covariate shift is largely photometric, so we propose to adapt the RGB image encoder branch by introducing a adaptation layer: a single convolutional layer designed to align target domain RGB embeddings to those of the source domain. As the rest of the network is frozen, adapting just the adaptation layer allows for low-cost model updates. Intuition for integrating adaptation layer.Guided by our observations, the adaptation layer should be (i) placed in the image encoder branch prior to the fusion of image and depth features, and (ii) located within later layers to modulate higher level representations (i.e., object shapes, as opposed to low-level edges). (iii) connected as a skip connection to decoder to more directly affect the output. 4(a) Initialize Meta Layer (b) Preparation –Learning proxy embedding Local SmoothnessPenalty Sparse DepthConsistency ProxyConsistency SupervisedLoss (c) AdaptationStopGrad Null Image Proxy loss Projection MLPs Frozen param.Updated param.Weight sharing Projection MLPs Output depth  Sparse depthImage Image StopGrad Null Image ImageSparse depthSparse depth Sparse depth Ground truth Source Dataset Target Dataset  Sparse depth Image PredictionEncoderDecoder Encoder Encoder Decoder AdaptationLayer Source Dataset  Source Dataset  EMA update 𝑔! ℎ\" 𝑔!# 𝑔! ℎ\" 𝑧! 𝐼! 𝑧\" 𝐼\" 𝐼# 𝑧! 𝑔!# 𝐼\" 𝐼# 𝑧\" 𝑧! 𝐼! 𝑑$! 𝑑$\" 𝑑\" Figure 3. Overview. (a) The pretraining stage integrates an adaptation layer into a pretrained encoder and pretrains the adaptation layer on the source dataset. (b) The preparation stage learns the proxy mapping of features encoding sparse depth to those encoding both inputs. (c) The adaptation stage deploys the model to the target domain and updates the adaptation layer by leveraging proxy embeddings as guidance. 3.2. Preparation Stage - Source Domain Initialize adaptation layer from source domain.Updating the entire network is largely infeasible in test-time adap- tation scenarios. For the sake of speed and efficiency, we implement an adaptation layermϕ, i.e., a convolutional layer, within the encoder of a pretrained network. Note that the entire network will be frozen during all stages of our method with the exception of the adaptation layer and proxy map- ping, where both will be initialized during preparation stage in the source domain; the proxy mapping will then be frozen and used to adapt mϕ in the target domain. To ease the adaptation process, we initialize the mϕ by minimizing a su- pervised loss over the source dataset (Fig. 3-(a)). We denote the pretrained encoder integrated with mϕ as eϕ. Learning proxy mapping from source domain.As ob- served in Fig. 1, the best results in the source domain are achieved by feeding in both the image and sparse depth modalities for inference. However, the image is susceptible to domain shift which degrades performance when the model is transferred to an unseen test domain. Conversely, sparse depth is more resilient to the domain shift than RGB images, i.e., the shape of a car (or another object) remains similar regardless of (synthetic or real) domain. Our method aims to leverage the sparse depth modality, which is less sensitive to the domain shift, in the downstream adaptation process. To this end, we employ a soft mapping [ 15] from just the encoded sparse depth features to sparse depth and image features to learn the photometric information that is captured from the same scene as the sparse point cloud. This strategy allows us to learn the mapping that projects the sparse depth features to “proxy” embeddings close to those that also en- code the image. In other words, it fills in what is missing in the image encoder branch by predicting the residual la- tent image encoding that is compatible with the input sparse depth, i.e., 3D scene. As this is trained in the source domain, the mapping naturally yields proxy embeddings that encode the source domain image (and sparse depth), which can be later used to guide the adaptation layer mϕ to transform test domain RGB features close to those of the source domain. This mapping by MLPs can be denoted as gψ(·), gψ′ (·) and hω(·); to learn them, we get two embeddings ppps and qqqs, ppps = hω(gψ(StopGrad(eϕ(I0, zs)))), qqqs = StopGrad(gψ′ (eϕ(Is, zs))) (1) where eϕ denotes the encoder augmented with the adap- tation layer trained on source dataset, Is, zs, the image and sparse depth from source domain, andI0 the null-image. The embedding modules gψ and hω are updated to maximize the similarity between ppps and qqqs. To learn them, we minimize: ℓprepare = 1 − ( ppps ∥ppps∥ · qqqs ∥qqqs∥), (2) where ∥ · ∥is L2-norm, and (aaa ·bbb) indicates the dot product of the vectors aaa and bbb. To this end, we first train the MLP heads gψ, hω by minimizing Eqn. 2. Note that the MLP head gψ′ is updated with EMA update following BYOL [15] to avoid collapse: gψ′ ← τ · gψ′ + (1 − τ) · gψ. Once the mapping is learned, we can freeze the embed- ding module and deploy it for test-time adaptation where we update the adaptation layer weights ϕ to maximize the similarity between the embeddings of a test domain image and sparse depth, and its proxy from the source domain. Nat- urally, due to the domain shift, the embeddings will yield 5low similarity scores; hence, maximizing the scores through our proxy embedding implicitly aligns the target RGB distri- bution to that of the source distribution, i.e., minimizing the cosine similarity between the source and target distributions. 3.3. Deploying Proxy Mapping to Target Domain Adaptation stageaims to update the adaptation layer param- eters by minimizing a test-time loss function over the target test domain data {It, zt} ∈ Dt. To do so, we deploy the learned proxy mapping module (MLP heads {g∗ ψ(·), g∗ ψ′ (·), and h∗ ω(·)}) along with the adaptation layer mϕ integrated into the frozen encoder as eϕ. Adaptation loss.For adaptation, our loss is composed of a linear combination of three loss terms: Ladapt = wzℓz + wsmℓsm + wproxyℓproxy, (3) where ℓz, ℓsm denote sparse depth consistency loss and lo- cal smoothness loss, respectively, ℓproxy is proxy mapping consistency loss, and w indicates a weight of each loss term. Sparse Depth Consistency.Sparse point clouds capture a coarse structure of the 3D scene.To obtain metric scale predictions consistent with the scene structure, we minimize L1 error between the sparse depth zt and the prediction ˆdt: ℓz = 1 |Ω(zt)| X x∈Ω(zt) |ˆdt(x) − zt(x)|, (4) where x ∈ Ω(zt) are the pixel locations where sparse points were projected onto the image plane. Local Smoothness. Based on the assumption of local smoothness and connectivity in a 3D scene, we impose the same in the predicted depth map ˆdt. Specifically, we apply an L1 penalty to its gradients in both the x- and y-directions (i.e., ∂X and ∂Y ). We balance the weight of each term with λX and λY , to allow discontinuities over object boundaries based on the image gradients, where λX(x) = e−|∂XIt(x)|, λY (x) = e−|∂Y It(x)|, and Ω denotes the image domain. ℓsm = 1 |Ω| X x∈Ω λX(x)|∂X ˆdt(x)| + λY (x)|∂Y ˆdt(x)|. (5) Proxy Consistency.In order to regularize the adaptation with the learned mapping from the previous stage, we freeze the weight parameters of MLP heads {g∗ ψ(·), h∗ ω(·)}, and update the parameters of the adaptation layer mϕ. First, we obtain the features pppt and qqqt using the null-image I0 in one and the given target test domain image It in the other: pppt = StopGrad(h∗ ω(g∗ ψ(eϕ(I0, zt)))), qqqt = g∗ ψ′ (eϕ(It, zt)). (6) We maximize the cosine similarity between the featureqqqt and pppt via a proxy loss ℓproxy to update adaptation layer mϕ: ℓproxy = 1 − ( pppt ∥pppt∥ · qqqt ∥qqqt∥). (7) 4. Experiments We demonstrate the effectiveness of our approach on a mix of both real and synthetic datasets including indoor SLAM/VIO scenarios (VOID [ 53], NYUv2 [ 30], SceneNet [ 29], and ScanNet [7]) and outdoor driving scenarios using lidar sen- sor (KITTI [45], Virtual KITTI (VKITTI) [12], nuScenes [1], and Waymo Open Dataset [42]). We chose three represen- tative architectures of current depth completion methods to test our method: MSG-CHN [ 23] (CNN-based), NLSPN [31] (SPN-based) and CostDCNet [21] (cost volume-based). All reported results are averaged over 5 independent trials. We describe implementation details, hyper-parameters used, hardware requirements, evaluation metrics as well as addi- tional experimental results in the Supp. Mat. Main Result. We use pretrained models (MSG-CHN, NLSPN, and CostDCNet) from the two source datasets, VOID for indoor, and KITTI for outdoor. For indoor, we adapt models pretrained on VOID to NYUv2, SceneNet, and ScanNet; for outdoors, we adapt from KITTI to VKITTI (with fog), nuScenes, and Waymo. BN Adapt denotes updat- ing the batch statistics (i.e., running mean and variance). BN Adapt, ℓz, ℓsm is a variation of TENT [49] which minimizes Eqn. 4, 5 instead of entropy by updating learnable scale factors. CoTTA denotes replacing proxy loss with L1 consis- tency loss w.r.t. the pretrained prediction [50]. ProxyTTA- fast denotes our method without batch norm update, which improves adaptation runtime by 25.32%. Our method consistently improves over baselines and vari- ants of BN Adapt (Table 1). Specifically, we improve over BN Adapt, ℓz, ℓsm by 11.60% on average across all meth- ods for indoor, 19.73% on outdoors, and 15.67% overall to achieve state-of-the-art performance. Qualitatively, Fig. 4 and Fig. 5 show that our method performs better in bound- ary regions and homogeneous regions, thus exhibiting less oversmoothing on curtains in Fig. 4-(a) and car in Fig. 5-(b), and undersmoothing on blackboard in Fig. 4-(d) and road in Fig. 5-(a), respectively, during adaptation. This trend is due to the proxy loss and the adaptation layer, which allows us to adapt with minimum weight adjustments while preserving high-level features (object shapes) learned from the source domain by mapping the target RGB modality to that of the source domain. Notably, ProxyTTA-fast still improves over BN Adapt even though we only adapt our adaptation layer, which demonstrates the effectiveness of our design choice as well as our proposed proxy embeddings. We visualize image and sparse depth features from the source and target domains along with proxy embeddings in the target domain using t-SNE [47] in Fig. 1 of Supp. Mat.; we observe that proxy embeddings are close to source domain features. Comparison to BN adaptation1 and CoTTA.To assess the impact of our adaptation layer, we compare to batch norm 1MSG-CHN lacks Batch Norm (BN) layer so we cannot use BN adapt. 6Method MAE RMSE MAE RMSE MAE RMSE KITTI → VKITTI-FOG KITTI → nuScenes KITTI → Waymo MSG-CHN Pretrained 2842.88 6557.38 3331.821 6449.094 1107.22 2962.45 CoTTA 730.6 ±11.67 3330.23 ±44.83 3157.69 6434.14 655.77 ±30.98 2213.27 ±98.80 ProxyTTA-fast (Ours) 728.24 ±3.73 3087.36 ±15.92 2834.08 ±17.64 6096.56 ±21.08 608.91 ±1.74 1921.83 ±2.54 NLSPN Pretrained 1309.99 7423.48 2656.609 6146.590 1175.83 3078.377 BN Adapt 1140.21 ±35.89 4592.86 ±198.21 11291.57 ±21.32 16670.87 ±52.56 7283.33 ±104.58 9670.36 ±250.22 BN Adapt, ℓz , ℓsm 775.20 ±5.65 3465.05 ±32.73 2928.51 ±75.89 8209.24 ±164.31 494.94 ±3.08 1921.17 ±338.06 CoTTA 767.93 ±5.47 3799.88 ±17.29 2650.45 ±15.04 6242.52 ±33.14 933.41 ±4.31 2763.88 ±143.48 ProxyTTA-fast 732.61 ±29.57 3002.19 ± 52.29 2733.96 ±34.32 6099.48 ±82.32 875.01 ±15.8 2400.17 ±21.44 ProxyTTA (Ours) 686.91 ±22.14 2666.70 ±56.64 2589.25 ±59.03 6006.18 ±90.66 477.28 ±3.32 1598.64 ±18.95 CostDCNet Pretrained 1042.98 6301.60 3064.724 6630.649 1093.79 2798.25 BN Adapt 1476.57 ±1.38 5428.20 ±8.15 2306.04 ±28.86 6391.98 ±48.97 596.08 ±5.55 1877.91 ±45.56 BN Adapt, ℓz , ℓsm 729.67 ±3.14 3413.76 ±14.59 2288.85 ±14.02 6338.38 ±31.31 469.97 ±2.47 1572.95 ±10.63 CoTTA 756.32 ±3.59 3686.69 ±14.75 2676.83 ±68.92 6099.49 ±66.79 689.94 ±1.95 2140.23 ±16.12 ProxyTTA-fast 756.98 ±31.07 3091.78 ±105.42 2595.81 ±12.13 6373.01 ±7.74 606.10 ±11.10 1817.79 ±19.14 ProxyTTA (Ours) 512.72 ±0.74 2735.01 ±3.53 2062.28 ±11.24 5509.96 ±23.41 466.44 ±1.63 1580.38 ±11.48 VOID → NYUv2 VOID → SceneNet VOID → ScanNet MSG-CHN Pretrained 1040.934 1528.983 281.28 645.01 687.988 1201.747 CoTTA 876.93 ±146.95 1148.62 ±173.53 223.19 ±14.77 498.46 ±28.21 619.37 ±4.14 1141.04 ±7.35 ProxyTTA-fast (Ours) 699.60 ±6.00 1120.37 ±9.76 192.74 ±1.72 424.49 ±4.58 302.21 ±4.10 480.08 ±8.03 NLSPN Pretrained 388.87 702.80 167.250 438.71 233.33 431.20 BN Adapt 250.13 ±5.23 447.18 ±10.32 143.61 ±6.34 385.56 ±9.84 207.00 ±0.57 401.41 ±2.84 BN Adapt, ℓz , ℓsm 147.55 ±1.36 271.10 ±2.17 120.48 ±1.94 345.91 ±7.14 82.76 ±0.47 181.97 ±1.21 CoTTA 390.50 ±8.29 704.72 ±16.74 205.02 ±1.79 540.01 ±4.08 234.77 ±1.52 496.18 ±2.75 ProxyTTA-fast 168.43 ±3.46 309.48 ±6.92 124.67 ±1.33 357.56 ±2.59 104.06 ±11.03 232.84 ±20.46 ProxyTTA (Ours) 124.41 ±2.27 240.73 ±5.72 113.93 ±1.49 333.41 ±4.32 74.77 ±0.31 166.61 ±0.45 CostDCNet Pretrained 189.10 446.71 173.37 443.22 144.31 458.69 BN Adapt 160.31 ±2.7 410.55 ±10.70 176.62 ±0.72 446.32 ±8.52 159.65 ±4.63 399.14 ±13.92 BN Adapt, ℓz , ℓsm 136.80 ±5.35 338.59 ±22.36 134.22 ±2.33 385.9 ±6.68 68.44 ±0.46 164.59 ±2.82 CoTTA 147.69 ±5.3 376.87 ±21.25 136.42 ±3.41 405.38 ±11.63 101.98 ±1.53 322.63 ±5.04 ProxyTTA-fast 131.93 ±2.58 269.02 ±5.61 129.99 ±3.88 353.86 ±7.91 128.12 ±3.41 244.62 ±7.53 ProxyTTA (Ours) 95.87 ±2.16 203.83 ±4.72 125.75 ±1.93 357.12 ±4.13 68.17 ±0.44 162.35 ±1.12 Table 1. Qualitative results. For indoors, we adapt from VOID to NYUv2, SceneNet, and ScanNet; for outdoors, from KITTI to VKITTI with fog, nuScenes, and Waymo. Bold denotes best and Italics second-best. ProxyTTA-fast denotes our method without updating BatchNorm. (BN) adaptation from TENT [49]. In BN adaptation, we only update the batch norm layer’s scale and shift factor based on the loss function. On average, BN Adapt with ℓz, ℓsm im- proves the pretrained model by 32.77%; whereas, updating just our adaptation layer (ProxyTTA-fast) improves it more by 34.07% (Table 1). The improvement of ProxyTTA-fast over BN adapt demonstrates the efficacy of updating adapta- tion layer, which directly adjusts the high-level features from the RGB branch guided by proxy loss, where BN adapt re- aligns the learned source features from both RGB and range sensors by updating feature statistics. Nonetheless, the best results are achieved when we in- clude batch norm update, which improves the pretrained model by 44.53%, but at the cost ≈33.2% of total extra time. The improvement of ProxyTTA over BN adapt implies that the large domain discrepancy may not be addressed by adapting only BN parameters (i.e., scaling and shifting); ProxyTTA explicitly adjusts RGB features by updating the adaptation layer with proxy embeddings as guidance. We also compared our approach to CoTTA [50], which adapts the whole model parametersusing the prediction from the teacher model updated by exponential moving average of pretrained weight and the model prediction. We combined additional loss ℓz, ℓsm on top of CoTTA loss, since we ob- served that the models cannot be adapted with CoTTA alone. Specifically, our method without proxy shows a 25.26% aver- age improvement on the CoTTA method. CoTTA updates the whole parameters including RGB and sparse depth branch, which causes a drift from the learned model parameters. On the other side, our method only updates additional layer at RGB branch, based on the study from the most domain dis- crepancy comes from RGB modality as studied in Sec. 3.1, and this prevents the model from a drift from learned domain. Also, CoTTA assumes the test-time augmentation can miti- gate the domain shift. However, the results shows test-time augmentation on RGB image, causing a small distributional shift, may not solve a large domain discrepancy. Also, our method with batch normalization layer update shows 26.52% average improvement, while using 25.05% less adaptation time. Note: CoTTA costs not only additional memory for teacher model but also inference time to get the teacher model prediction, even if CoTTA does not require any preparation process. Overall, our method shows 21.09% average improvement over BN adapt and CoTTA methods. 7BN AdaptOurs Sparse Depth Predicted Depth Image Error Map CoTTA (a) (b) (c) (d) BN AdaptOursCoTTA Figure 4. Qualitative results on NYUv2. For indoors scenarios, ProxyTTA performs better in boundary regions displaying the discontinuity in depth (e.g., curtains, (a)), as well as homogeneous regions (e.g., blackboard, (d)). Boxes highlight detailed comparisons. BN Adapt Ours(a) (b) Image Sparse Depth CoTTA Predicted Depth Error Map Figure 5. Qualitative results on NuScenes. For outdoor adaptation scenarios, ProxyTTA improves over BN Adapt and CoTTA, notably in both depth-discontinuous regions (e.g., car in (b)) and homogeneous regions (e.g., road in (a) and (b)). Boxes highlight detailed comparisons. 5. Discussion We have proposed a method for test-time adaptation for depth completion that leverages the strength of complemen- tary multi-sensor setup in the presence of domain shift. By studying model sensitivity to each input modality as well as the data under domain shift, we designed a way to exploit the modality (sparse depth) that is less sensitive to guide adaptation. We do so through a proxy embedding that learns the photometric information from the source domain that is compatible with the sparse depth depicting a 3D scene. Our proxy embedding works well as a regularizer for scenarios where there exists covariate shifts in photometry (i.e., KITTI to VKITTI) as well as scene layouts (i.e., VOID to NYUv2 and SceneNet). While one may surmise that the applica- tion of the embeddings are specific to scene distributions, we show otherwise. VOID (classrooms, laboratories, and gardens), NYUv2 (households and shopping centers), and SceneNet (randomly arranged synthetic rooms) all differ in layouts. The proxy embedding captures latent photomet- ric features of the object shapes populating them; the same proxy embedding can be transferred across domains even when scene differ, but share objects within them. This leads to possible limitations in the scenarios where the source dataset is sampled from scenes that do not share any objects with the target test dataset; in this case, the proxy embeddings should give little to no gain and one must rely on generic regularizers like local smoothness. Additionally, while we follow the conventions in TTA and assume access to the source dataset prior to deployment, in reality, many models are trained on private datasets, so adapting “off-the- shelf” models remains a challenge. In such cases, one must incorporate our preparation pipeline into their model train- ing and release the adaptation layer and proxy embedding module together with network weights. Nonetheless, this is the first test-time adaptation work in depth completion; in ad- dition to our findings, we release models, dataset, adaptation, and evaluation code, and hope to further motivate interest in TTA for multi-modal tasks like depth completion. Acknowledgements. This work was supported by NSF 2112562 Athena AI Institute. 8References [1] Holger Caesar, Varun Bankiti, Alex H Lang, Sourabh V ora, Venice Erin Liong, Qiang Xu, Anush Krishnan, Yu Pan, Gi- ancarlo Baldan, and Oscar Beijbom. nuscenes: A multi- modal dataset for autonomous driving. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 11621–11631, 2020. 6, 13 [2] Dian Chen, Dequan Wang, Trevor Darrell, and Sayna Ebrahimi. Contrastive test-time adaptation. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 295–305, 2022. 2 [3] Yun Chen, Bin Yang, Ming Liang, and Raquel Urtasun. Learn- ing joint 2d-3d representations for depth completion. In Pro- ceedings of the IEEE/CVF International Conference on Com- puter Vision, pages 10023–10032, 2019. 2 [4] Xinjing Cheng, Peng Wang, Chenye Guan, and Ruigang Yang. Cspn++: Learning context and resource aware convolutional spatial propagation networks for depth completion. In Pro- ceedings of the AAAI Conference on Artificial Intelligence , pages 10615–10622, 2020. 3, 12 [5] Sungha Choi, Seunghan Yang, Seokeon Choi, and Sungrack Yun. Improving test-time adaptation via shift-agnostic weight regularization and nearest source prototypes. In ECCV, pages 440–458. Springer, 2022. 2 [6] Safa Cicek and Stefano Soatto. Unsupervised domain adapta- tion via regularized conditional alignment. In Proceedings of the IEEE/CVF international conference on computer vision, pages 1416–1425, 2019. 3 [7] Angela Dai, Angel X Chang, Manolis Savva, Maciej Hal- ber, Thomas Funkhouser, and Matthias Nießner. Scannet: Richly-annotated 3d reconstructions of indoor scenes. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 5828–5839, 2017. 6, 13 [8] Yiming Dou, Fengyu Yang, Yi Liu, Antonio Loquercio, and Andrew Owens. Tactile-augmented radiance fields. In Pro- ceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, 2024. 2 [9] Abdelrahman Eldesokey, Michael Felsberg, and Fahad Shah- baz Khan. Propagating confidences through cnns for sparse data regression. arXiv preprint arXiv:1805.11913, 2018. 3 [10] Abdelrahman Eldesokey, Michael Felsberg, Karl Holmquist, and Michael Persson. Uncertainty-aware cnns for depth com- pletion: Uncertainty from beginning to end. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 12014–12023, 2020. 3 [11] Xiaohan Fei, Alex Wong, and Stefano Soatto. Geo-supervised visual depth prediction. IEEE Robotics and Automation Let- ters, 4(2):1661–1668, 2019. 12 [12] Adrien Gaidon, Qiao Wang, Yohann Cabon, and Eleonora Vig. Virtual worlds as proxy for multi-object tracking analysis. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 4340–4349, 2016. 2, 6, 13 [13] Yaroslav Ganin and Victor Lempitsky. Unsupervised domain adaptation by backpropagation. In International conference on machine learning, pages 1180–1189. PMLR, 2015. 2, 3 [14] Andreas Geiger, Philip Lenz, Christoph Stiller, and Raquel Urtasun. Vision meets robotics: The kitti dataset. The Inter- national Journal of Robotics Research, 32:1231 – 1237, 2013. 12 [15] Jean-Bastien Grill, Florian Strub, Florent Altché, Corentin Tallec, Pierre Richemond, Elena Buchatskaya, Carl Doer- sch, Bernardo Avila Pires, Zhaohan Guo, Mohammad Ghesh- laghi Azar, et al. Bootstrap your own latent-a new approach to self-supervised learning. Advances in neural information processing systems, 33:21271–21284, 2020. 5, 14 [16] Christopher G. Harris and M. J. Stephens. A combined corner and edge detector. In Alvey Vision Conference, 1988. 13 [17] Mu Hu, Shuling Wang, Bin Li, Shiyu Ning, Li Fan, and Xiao- jin Gong. Penet: Towards precise and efficient image guided depth completion. In 2021 IEEE International Conference on Robotics and Automation (ICRA), pages 13656–13662. IEEE, 2021. 3 [18] Zixuan Huang, Junming Fan, Shenggan Cheng, Shuai Yi, Xiaogang Wang, and Hongsheng Li. Hms-net: Hierarchi- cal multi-scale sparsity-invariant network for sparse depth completion. IEEE Transactions on Image Processing , 29: 3429–3441, 2019. 2 [19] Yusuke Iwasawa and Yutaka Matsuo. Test-time classifier adjustment module for model-agnostic domain generalization. In NIPS, pages 2427–2440, 2021. 2 [20] Maximilian Jaritz, Raoul De Charette, Emilie Wirbel, Xavier Perrotton, and Fawzi Nashashibi. Sparse and dense data with cnns: Depth completion and semantic segmentation. In 2018 International Conference on 3D Vision (3DV), pages 52–60. IEEE, 2018. 3 [21] Jaewon Kam, Jungeon Kim, Soongjin Kim, Jaesik Park, and Seungyong Lee. Costdcnet: Cost volume based depth com- pletion for a single rgb-d image. In European Conference on Computer Vision, pages 257–274. Springer, 2022. 3, 6 [22] Youngeun Kim, Donghyeon Cho, Kyeongtak Han, Priyadarshini Panda, and Sungeun Hong. Domain adaptation without source data. IEEE Transactions on Artificial Intelligence, 2(6):508–518, 2021. 2 [23] Ang Li, Zejian Yuan, Yonggen Ling, Wanchao Chi, Chong Zhang, et al. A multi-scale guided cascade hourglass network for depth completion. In Proceedings of the IEEE/CVF Winter Conference on Applications of Computer Vision, pages 32–40, 2020. 3, 6 [24] Yuankai Lin, Tao Cheng, Qi Zhong, Wending Zhou, and Hua Yang. Dynamic spatial propagation network for depth com- pletion. In Proceedings of the AAAI Conference on Artificial Intelligence, pages 1638–1646, 2022. 3 [25] Tian Yu Liu, Parth Agrawal, Allison Chen, Byung-Woo Hong, and Alex Wong. Monitored distillation for positive congruent depth completion. In Computer Vision–ECCV 2022: 17th European Conference, Tel Aviv, Israel, October 23–27, 2022, Proceedings, Part II, pages 35–53. Springer, 2022. 2 [26] Yuejiang Liu, Parth Kothari, Bastien Van Delft, Baptiste Bellot-Gurlet, Taylor Mordan, and Alexandre Alahi. TTT++: When does self-supervised test-time training fail or thrive? In NIPS, pages 21808–21820, 2021. 2, 3 [27] Adrian Lopez-Rodriguez, Benjamin Busam, and Krystian Mikolajczyk. Project to adapt: Domain adaptation for depth completion from noisy and sparse sensor data. InProceedings of the Asian Conference on Computer Vision, 2020. 3 9[28] Fangchang Ma, Guilherme Venturelli Cavalheiro, and Sertac Karaman. Self-supervised sparse-to-dense: Self-supervised depth completion from lidar and monocular camera. In 2019 International Conference on Robotics and Automation (ICRA), pages 3288–3295. IEEE, 2019. 2, 3 [29] John McCormac, Ankur Handa, Stefan Leutenegger, and An- drew J Davison. Scenenet rgb-d: 5m photorealistic images of synthetic indoor trajectories with ground truth. arXiv preprint arXiv:1612.05079, 2016. 2, 6, 13 [30] Pushmeet Kohli Nathan Silberman, Derek Hoiem and Rob Fergus. Indoor segmentation and support inference from rgbd images. In ECCV, 2012. 2, 6 [31] Jinsun Park, Kyungdon Joo, Zhe Hu, Chi-Kuei Liu, and In So Kweon. Non-local spatial propagation network for depth completion. In Computer Vision–ECCV 2020: 16th European Conference, Glasgow, UK, August 23–28, 2020, Proceedings, Part XIII 16, pages 120–136. Springer, 2020. 3, 6, 12 [32] Xingchao Peng, Qinxun Bai, Xide Xia, Zijun Huang, Kate Saenko, and Bo Wang. Moment matching for multi-source domain adaptation. In Proceedings of the IEEE/CVF inter- national conference on computer vision, pages 1406–1415, 2019. 2, 3 [33] Jiaxiong Qiu, Zhaopeng Cui, Yinda Zhang, Xingdi Zhang, Shuaicheng Liu, Bing Zeng, and Marc Pollefeys. Deepli- dar: Deep surface normal guided depth prediction for outdoor scene from sparse lidar data and single color image. In Pro- ceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 3313–3322, 2019. 3 [34] Chao Qu, Ty Nguyen, and Camillo Taylor. Depth completion via deep basis fitting. In Proceedings of the IEEE/CVF Winter Conference on Applications of Computer Vision, pages 71–80, 2020. 3 [35] Chao Qu, Wenxin Liu, and Camillo J Taylor. Bayesian deep basis fitting for depth completion with uncertainty. InProceed- ings of the IEEE/CVF international conference on computer vision, pages 16147–16157, 2021. 3 [36] Kyeongha Rho, Jinsung Ha, and Youngjung Kim. Guide- former: Transformers for image guided depth completion. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 6250–6259, 2022. 3 [37] Inkyu Shin, Yi-Hsuan Tsai, Bingbing Zhuang, Samuel Schul- ter, Buyu Liu, Sparsh Garg, In So Kweon, and Kuk-Jin Yoon. Mm-tta: multi-modal test-time adaptation for 3d semantic segmentation. In CVPR, pages 16928–16937, 2022. 2 [38] Shreyas S Shivakumar, Ty Nguyen, Ian D Miller, Steven W Chen, Vijay Kumar, and Camillo J Taylor. Dfusenet: Deep fusion of rgb and sparse depth information for image guided dense depth completion. In 2019 IEEE Intelligent Transporta- tion Systems Conference (ITSC), pages 13–20. IEEE, 2019. 2 [39] Nathan Silberman, Derek Hoiem, Pushmeet Kohli, and Rob Fergus. Indoor segmentation and support inference from rgbd images. In European Conference on Computer Vision, 2012. 13 [40] Akash Deep Singh, Yunhao Ba, Ankur Sarker, Howard Zhang, Achuta Kadambi, Stefano Soatto, Mani Srivastava, and Alex Wong. Depth estimation from camera image and mmwave radar point cloud. In Proceedings of the IEEE/CVF Con- ference on Computer Vision and Pattern Recognition, pages 9275–9285, 2023. 3 [41] Junha Song, Jungsoo Lee, In So Kweon, and Sungha Choi. Ecotta: Memory-efficient continual test-time adaptation via self-distilled regularization. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition , pages 11920–11929, 2023. 2 [42] Pei Sun, Henrik Kretzschmar, Xerxes Dotiwalla, Aurelien Chouard, Vijaysai Patnaik, Paul Tsui, James Guo, Yin Zhou, Yuning Chai, Benjamin Caine, et al. Scalability in perception for autonomous driving: Waymo open dataset. InProceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 2446–2454, 2020. 6, 13 [43] Yu Sun, Eric Tzeng, Trevor Darrell, and Alexei A Efros. Unsupervised domain adaptation through self-supervision. arXiv preprint arXiv:1909.11825, 2019. 3 [44] Yu Sun, Xiaolong Wang, Zhuang Liu, John Miller, Alexei Efros, and Moritz Hardt. Test-time training with self- supervision for generalization under distribution shifts. In ICML, pages 9229–9248. PMLR, 2020. 2, 3 [45] Jonas Uhrig, Nick Schneider, Lukas Schneider, Uwe Franke, Thomas Brox, and Andreas Geiger. Sparsity invariant cnns. In 2017 international conference on 3D Vision (3DV), pages 11–20. IEEE, 2017. 2, 6, 12, 13 [46] Rishi Upadhyay, Howard Zhang, Yunhao Ba, Ethan Yang, Blake Gella, Sicheng Jiang, Alex Wong, and Achuta Kadambi. Enhancing diffusion models with 3d perspective geometry constraints. ACM Transactions on Graphics (TOG), 42(6): 1–15, 2023. 2 [47] Laurens Van der Maaten and Geoffrey Hinton. Visualizing data using t-sne. Journal of machine learning research, 9(11), 2008. 6 [48] Wouter Van Gansbeke, Davy Neven, Bert De Brabandere, and Luc Van Gool. Sparse and noisy lidar completion with rgb guidance and uncertainty. In 2019 16th international conference on machine vision applications (MVA), pages 1–6. IEEE, 2019. 3 [49] Dequan Wang, Evan Shelhamer, Shaoteng Liu, Bruno Ol- shausen, and Trevor Darrell. Tent: Fully test-time adaptation by entropy minimization. arXiv preprint arXiv:2006.10726, 2020. 2, 6, 7 [50] Qin Wang, Olga Fink, Luc Van Gool, and Dengxin Dai. Continual test-time domain adaptation. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 7201–7211, 2022. 2, 6, 7 [51] Alex Wong and Stefano Soatto. Bilateral cyclic constraint and adaptive regularization for unsupervised monocular depth prediction. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 5644–5653, 2019. 2 [52] Alex Wong and Stefano Soatto. Unsupervised depth comple- tion with calibrated backprojection layers. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 12747–12756, 2021. 2 [53] Alex Wong, Xiaohan Fei, Stephanie Tsuei, and Stefano Soatto. Unsupervised depth completion from visual inertial odome- 10try. IEEE Robotics and Automation Letters, 5(2):1899–1906, 2020. 2, 6, 12 [54] Alex Wong, Safa Cicek, and Stefano Soatto. Learning topol- ogy from synthetic data for unsupervised depth completion. IEEE Robotics and Automation Letters , 6(2):1495–1502, 2021. [55] Alex Wong, Xiaohan Fei, Byung-Woo Hong, and Stefano Soatto. An adaptive framework for learning unsupervised depth completion. IEEE Robotics and Automation Letters, 6 (2):3120–3127, 2021. 2 [56] Yangchao Wu, Tian Yu Liu, Hyoungseob Park, Stefano Soatto, Dong Lao, and Alex Wong. Augundo: Scaling up augmen- tations for unsupervised depth completion. arXiv preprint arXiv:2310.09739, 2023. 2 [57] Yan Xu, Xinge Zhu, Jianping Shi, Guofeng Zhang, Hujun Bao, and Hongsheng Li. Depth completion from sparse lidar data with depth-normal constraints. In Proceedings of the IEEE/CVF International Conference on Computer Vision , pages 2811–2820, 2019. 3 [58] Fengyu Yang, Chenyang Ma, Jiacheng Zhang, Jing Zhu, Wen- zhen Yuan, and Andrew Owens. Touch and go: Learning from human-collected vision and touch. Neural Information Processing Systems (NeurIPS) - Datasets and Benchmarks Track, 2022. 2 [59] Fengyu Yang, Jiacheng Zhang, and Andrew Owens. Gener- ating visual scenes from touch. International Conference on Computer Vision (ICCV), 2023. [60] Fengyu Yang, Chao Feng, Ziyang Chen, Hyoungseob Park, Daniel Wang, Yiming Dou, Ziyao Zeng, Xien Chen, Rit Gan- gopadhyay, Andrew Owens, and Alex Wong. Binding touch to everything: Learning unified multimodal tactile represen- tations. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, 2024. 2 [61] Yanchao Yang, Alex Wong, and Stefano Soatto. Dense depth posterior (ddp) from single image and sparse range. In Pro- ceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 3353–3362, 2019. 3 [62] Zhang Youmin, Guo Xianda, Poggi Matteo, Zhu Zheng, Huang Guan, and Mattoccia Stefano. Completionformer: Depth completion with convolutions and vision transformers. arXiv preprint arXiv:2304.13030, 2023. 3 [63] Yinda Zhang and Thomas Funkhouser. Deep depth comple- tion of a single rgb-d image. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition , pages 175–185, 2018. 3 11Supplementary Materials Summary of contents • In Section A, we present the GPU time of each adaptation method to show the effectiveness of our method. • In Section B, we present the preliminary observations with image and range inputs of varying sparsity. • In Section C, we describe the datasets used. • In Section D, we present the comparison of qualitative results on Waymo dataset’s adverse weather sample from pretrained and adapted model to show the effectiveness of our adaptation under adverse weather conditions. • In Section E, we present the hyperparameter settings for result reproduction and we elucidate evaluation details. • In Section F, we provide a study on the learned proxy embedding with a visualization. • In Section G, we present an ablation study of the loss components in our method. • In Section H, we present the results on KITTI → VKITTI adaptation. • In Section I, we present the results on a different source dataset (Waymo → VKITTI). • In Section J, we show a qualitavive result of the prelimi- nary observation. A. Adaptation speed We compare the GPU time of our adaptation method with the baselines (BN Adapt, CoTTA) on VKITTI in Table 2. Compared to CoTTA, our adaptation method does not require multiple inferences to get the pseudo-prediction (de- rived from averaging teacher model predictions with differ- ent RGB augmentations) used to adapt the student model. Yet, our method requires an additional computation for the proxy embedding. Thus, the proxy layer’s size relative to the model size causes the adaptation time difference. For exam- ple, CoTTA reduced the total time by 38.9% over ProxyTTA- fast on MSGCHN, which is a light-weight depth completion model. In this case, the proxy layer is relatively larger than in other models, where multiple inferences require less com- putation than the proxy layer. As a result, the total time is increased in MSGCHN. However, for large models (NLSPN, CostDCNet), ProxyTTA reduced total time by 56.6% over CoTTA; our proxy layer size is relatively smaller than the large models, while still improving performance by 26.52%. Compared to BN Adapt, our method requires additional pa- rameters for the adaptation layer and the proxy layer. Hence, our method is 38.18% slower in adaptation time, 19.36% slower in evaluation time, and 33.16% in total. Yet, our method improves errors by 15.67% over BN Adapt. Model Method Adaptation time Evaluation time Total time MSGCHN CoTTA 88.9 (-38.9%) 8.66 (-1.0%) 81.2 (-41.3%) ProxyTTA-fast 136.6 8.8 145.4 NLSPN CoTTA 717.5 (+67.4%) 75.3 (-10.9%) 792.8 (+60.0%) BN Adapt 185.0 (-20.8%) 82.8 (-0.8%) 267.8 (-15.6%) ProxyTTA-fast 168.2 (-28.0%) 83.4 (-0.1%) 251.6 (-20.66%) ProxyTTA 233.6 83.5 317.1 CostDCNet CoTTA 329.1 (+78.2%) 33.6 (-51.0%) 369.1 (+43.2%) BN Adapt 82.1 (-55.5%) 42.5 (-37.9%) 125.6 (-50.8%) ProxyTTA-fast 141.9 (-23.2%) 68.7 (+0.3%) 210.6 (-16.8%) ProxyTTA 184.7 68.5 253.2 Table 2. GPU time for various methods and models, tested on Virtual KITTI. Time is in milliseconds (ms). ‘Adaptation time’ denotes the time required to adapt (or train) each method for a single test data point. ‘Evaluation time’ denotes the time taken to test each method for a test data instance.‘Total time’ is the sum of the Adaptation and Evaluation times. B. Further observations on image/range inputs We present additional preliminary observations of the image and range sensor inputs with varying sparsity. Since pre- vious works [4, 31] state that the depth completion model propagates the sparse depth to the dense depth guided by image features, one can raise a question on our preliminary results in the main paper without the lidar input, such as there’s no sparse point to propagate to the near pixels. We clarify that the results are intended to highlight the domain distrepancy. Therefore, we show additional results with 1%, 5%, and 10% of sparse points in the range input on indoor datasets, as shown in Table 3. As we increase the range points, the performance is improved yet still worse than the sparse-depth-only results in Tab. 8. C. Datasets KITTI [14] is composed of calibrated RGB images with synchronized point clouds from Velodyne lidar, inertial, and GPS information, and from more than 61 driving scenes. There are ≈80K raw image frames and associated sparse depth maps, both with ≈5% density, available for depth completion [45]. Semi-dense depth is available for the lower 30% of the image space, and 11 neighboring raw lidar scans comprise the ground-truth depth. We did not use a test or validation set, and the training set contains ≈86K single images. VOID [53] contains synchronized 640×480 RGB images and sparse depth maps from indoor scenes of laboratories and classrooms and from outdoor scenes of gardens. Sparse depth maps (of ≈0.5% density and containing≈1,500 sparse dense points) are obtained by the VIO system XIVO [ 11], and dense ground-truth depth maps are obtained by active stereo. VOID uses rolling shutter to capture challenging 6 DoF motion for 56 sequences - as opposed to KITTI’s typically planar motion. We use a training set of ≈46K 12Method MSG-CHN NLSPN CostDCNet MSG-CHN NLSPN CostDCNet Dataset VOID → NYUv2 VOID → ScanNet MAE RMSE MAE RMSE MAE RMSE MAE RMSE MAE RMSE MAE RMSE Image + sparse depth (1%) 1643.34 2177.71 602.17 858.19 809.36 1144.91 1597.41 2240.43 490.13 738.77 665.57 982.32 Image + sparse depth (5%) 996.54 1599.14 379.45 638.55 427.69 736.23 809.38 1455.69 240.55 441.70 337.39 620.53 Image + sparse depth (10%) 785.65 1376.93 327.41 591.99 339.31 622.75 581.93 1165.63 191.75 379.10 264.66 516.74 Sparse depth only 734.13 1046.28 237.47 402.47 147.76 354.57 211.86 444.62 162.29 276.29 88.25 205.46 Table 3. Model sensitivity to input modalities with varying sparsity. images to prepare the model. NYUv2 [39] contains 372K synchronized 640×480 RGB images and depth maps (via Microsoft Kinect) from 464 indoor scenes of household, office, and commercial types. To generate sparse depth maps in the style of SLAM/VIO, we used the Harris corner detector [ 16] to sample ≈1,500 points from the depth maps. We use a set of 654 test set images for adaptation. ScanNet [7] contains 2.5 million images and dense depth maps for 1,513 indoor scenes. To generate sparse depth maps in the style of SLAM/VIO, we used the Harris corner detector [16] to sample ≈1,500 points from the depth maps. We use a set of ≈21K test images for adaptation. Virtual KITTI (VKITTI) [12] contains ≈17K 1242×375 images from 35 synthetic videos created by ap- plying 7 variations in weather, lighting, or camera angle to each of 5 cloned KITTI [ 45] videos. There exists a large domain gap between RGB images from VKITTI and KITTI, even though the virtual worlds created in Unity by [12] are similar to KITTI scenes. Thus, we only use the dense depth maps of VKITTI to avoid the domain gap in photometric varations. The sparse depth maps are obtained by simulating KITTI’s lidar-generated sparse depth measurements such that the marginal distribution of VKITTI’s sparse points mimics that of KITTI’s. We use a set of ≈2,300 test images for the adaptation. nuScenes [1] consists of 1600×900 calibrated RGB im- ages and synchronized sparse point clouds, 27.4K images from 1000 outdoor driving scenes for training, and 5.8K im- ages from 150 scenes for testing. We set up the ground truth for the test images by merging projected sparse depth from forward-backward frames. The setup code will be released to clarify further details and reproducibility. SceneNet [29] contains 5 million 320×240 RGB images and depth maps from indoor trajectories of randomly ar- ranged rooms. We use a single split (out of 17 available) containing 1000 subsequences of 300 images each, gener- ated by recording the same scene over a trajectory. Because there are no sparse depth maps provided, we sampled from the depth map via Harris corner detector [16] to mimic the sparse depth produced by SLAM/VIO. The final 375 corners Dataset Learning Rate wsm wz wproxy Inner Iter. MSG-CHN VKITTI 2e-3 1.0 1.0 0.2 1 VKITTI-FOG 5e-3 3.0 1.0 0.1 1 nuScenes 3e-3 9.0 1.0 0.2 1 SceneNet 2e-3 8.0 1.0 0.1 3 NYUv2 2e-4 0.8 1.0 0.4 3 ScanNet 5e-3 8.0 1.0 0.3 3 NLSPN VKITTI 2e-3 0.8 1.0 0.4 1 VKITTI-FOG 1e-3 1.0 1.0 0.2 1 nuScenes 1e-3 1.0 1.0 0.1 1 SceneNet 2e-3 0.7 1.0 2.0 3 NYUv2 4e-3 5.0 1.0 1.0 3 ScanNet 1e-4 2.0 1.0 0.3 3 CostDCNet VKITTI 4e-3 4.5 1.0 0.1 1 VKITTI-FOG 5e-3 3.0 1.0 0.04 1 nuScenes 5e-3 3.0 1.0 0.1 1 SceneNet 7e-3 2.0 1.0 0.2 3 NYUv2 6e-3 4.0 1.0 0.1 3 ScanNet 3e-3 1.0 1.0 0.2 3 Table 4. Hyperparameters. For MSG-CHN, NLSPN, and CostDC- Net methods for initialization, preparation, and adaptation. are obtained by using k-means to subsample the resulting points, representing 0.49% of the total pixels. We use a set of ≈2,300 test images for adaptation. Waymo Open Dataset[42] contains 1920×1280 RGB images and lidar scans from autonomous vehicles. The training set contains ≈158K images from 798 scenes and the validation set ≈40K images from 202 scenes, collected at 10Hz. Objects are annotated across the full 360 ◦ field. We obtain our validation set by sampling from the whole validation dataset every 0.6 seconds. Range sensor inputs are obtained by projecting the top lidar’s point cloud scan to the camera frame. We obtained the ground truth by projecting 10 forward and backward frames from front lidar and top lidar to the image frame, which approximately counts for 1 second of capture. To assume that the reprojected scenes 13Figure 6. Qualitative results on Waymo. For outdoor adaptation scenarios, ProxyTTA can adapt under the adverse weather condition, such as raining condition (top row) and low-illumination (bottom row). are static, we removed the moving objects in the scenes using object annotations. Also, outlier removal is utilized for filtering out errorenous depth points. D. Qualitative results on adverse weather con- ditions Typically, in real-world scenarios, most systems will en- counter non-ideal sensing conditions, which will degrade performance. For example, existing pretrained depth (com- pletion) models will fail under adverse weather conditions, such as nighttime (low-illumination) or rain. To address such failure modes of existing models, we adapt CostDCNet using Proxy-TTA. We demonstrate this capability in Fig. 6, where we improve over the pretrained model significantly as shown in the range and error map visualizations. E. Implementation details Hyperparameter. We specifically note the hyperparameters of three methods for initialization, preparation, and adapta- tion on Table 4. Epochs and training detailsAdaptation occurs in a single epoch, with ‘the number of iterations per data point’ (inner- iter) specified in Tab. 4. During initialization and preparation stages, the adaptation and proxy layers are trained for 6 epochs. Batch sizes for all methods are: 48 for preparation stage, 16 for initialization and adaptation stages, with the exception of ScanNet [6], using a batch size of 36. To prevent collapse during preparation stage, we follow the protocol of [15]; we exploit the projection / prediction layers and divide online / target branch, and update target projection layer with exponential moving average of online branch. We used embedding dimension and hidden dimension of 512 for MSGCHN, and 1024 for CostDCNet and NLSPN. The learning rates for initialization and preparation stage will be released with the code release. Evaluation. We evaluate our adaptation models on bottom- cropped regions in the outdoor dataset, where the sparse depth exists. For outdoor dataset, models are evaluated on the bottom cropped region of the test split, 1242 × 240 for Virtual KITTI, and 1600 × 544 for nuScenes. For indoor dataset, we evaluated the models on the entire region. The definition of the error metrics in evaluation are described in Table 6. We evaluate our model on depth range from 0.0 to 80.0 meters for the ourdoor, and 0.2 to 5.0 meters for the indooor. F. Discussion on learned proxy embeddings Here, we provide the t-SNE visualization of image & sparse depth and proxy embedding from source and target. Fig. 7 shows the embeddings visualized by t-SNE, where the target domain proxy embeddings’ centroid is closer to that of source’s proxy and image & sparse depth embed- dings, than to the centroid of target’s image & sparse depth embeddings, highlighting effectiveness of proxy embedding for adaptation. 14KITTI → Waymo KITTI → VKITTI-FOG KITTI → nuScenes Method ℓz ℓsm ℓproxy MAE RMSE MAE RMSE MAE RMSE MSG-CHN ✓ 951.25 ±3.14 3512.07 ±6.40 978.84 ±3.36 3561.40 ±15.48 3164.46 ±11.32 6453.54 ±17.31 ✓ ✓ 613.01 ±1.99 1935.43 ±9.14 732.61 ±6.02 3113.11 ±21.78 2865.15 ±9.96 6144.48 ±24.14 ✓ ✓ ✓ 608.91 ±1.74 1921.83 ±2.54 728.24 ±3.73 3087.36 ±15.92 2834.08 ±17.64 6096.56 ±21.08 NLSPN ✓ 837.66 ± 8.73 3668.94 ± 25.90 715.86 ±26.36 3034.21 ± 57.65 5076.83 ±53.85 9710.88 ± 89.76 ✓ ✓ 489.46 ±5.45 1613.66 ±30.04 705.14 ±16.86 3059.64 ±97.85 2783.61 ±159.62 6313.4 ±276.09 ✓ ✓ ✓ 477.28 ±3.32 1598.64 ±18.95 686.91 ±22.14 2666.70 ±56.64 2589.25 ±59.03 6006.18 ±90.66 CostDCNet ✓ 816.33 ±32.01 3431.96 ±55.34 807.62 ±69.12 3254.83 ±179.90 3135.11 ±81.76 7596.49 ±159.16 ✓ ✓ 469.52 ±2.54 1594.38 ±6.10 516.93 ±1.62 2751.21 ±17.42 2067.42 ±10.23 5487.85 ±37.21 ✓ ✓ ✓ 466.44 ±1.63 1580.38 ±11.48 512.72 ±0.74 2735.01 ±3.53 2062.28 ±11.24 5509.96 ±23.41 VOID → NYUv2 VOID → SceneNet VOID → ScanNet MSG-CHN ✓ 971.64 ±66.86 1291.45 ±45.67 242.11 ±4.24 491.48 ±10.49 462.95 ±34.84 659.9 ±37.93 ✓ ✓ 1005.49 ±25.97 1329.76 ±25.01 194.60 ±3.64 425.16 ±10.58 330.20 ±48.46 503.73 ±57.14 ✓ ✓ ✓ 699.60 ±6.00 1120.37 ±9.76 192.74 ±1.72 424.49 ±4.58 302.21 ±4.10 480.08 ±8.03 NLSPN ✓ 145.72 ±6.55 271.78 ± 9.91 130.49 ±13.64 337.14 ±28.38 112.38 ±1.72 234.60 ±3.46 ✓ ✓ 128.17 ±4.13 240.97 ±3.86 118.65 ±2.24 337.63 ±2.58 77.84 ±0.28 169.81 ±0.50 ✓ ✓ ✓ 124.41 ±2.27 240.73 ±5.72 113.93 ±1.49 333.41 ±4.32 74.77 ±0.31 166.61 ±0.45 CostDCNet ✓ 152.43 ±13.07 432.20 ±54.51 213.4 ±19.52 597.22 ±49.78 91.13 ±1.40 286.17 ±9.07 ✓ ✓ 101.31 ±1.67 217.77 ±6.00 134.51 ±4.23 360.33 ±9.67 69.02 ±0.51 164.90 ±2.38 ✓ ✓ ✓ 95.87 ±2.16 203.83 ±4.72 125.75 ±1.93 357.12 ±4.13 68.17 ±0.44 162.35 ±1.12 Table 5. Ablation study of each loss term. Note that NLSPN and CostDCNet update the adaptation layer and batch normalization layers, yet MSGCHN only updates the adaptation layer. Metric Definition MAE 1 |Ω| P x∈Ω |ˆd(x) − dgt(x)| RMSE \u0000 1 |Ω| P x∈Ω |ˆd(x) − dgt(x)|2\u00011/2 Table 6. Error metrics. dgt denotes the ground-truth depth. Figure 7. t-SNE plot of learned embeddings on VOID and NYUv2. G. Ablation study Here, we ablate the effect of each loss term denoted with the checkmarks in Table 5. Using sparse depth consistency loss ℓz (Eqn. 4) alone can improve the pretrained model as it learns the shapes of the test domain. However, because of the sparsity, the supervision signal is weak, leading the model to exhibit artifacts and distortions in the depth map. Including a local smoothness loss ℓsm (Eqn. 5) mitigates this by propagating depth to nearby regions. However, without knowledge of 3D shapes compatible with the sparse points, KITTI → VKITTI Method MAE RMSE MSG-CHN Pretrained 2433.46 6675.16 CoTTA 839.19 ±12.78 3625.38 ±39.35 ProxyTTA-fast (Ours) 800.88 ±1.86 3268.26 ±4.12 NLSPN Pretrained 1469.19 8060.97 BN Adapt 1016.87 ±8.84 3453.00 ±3.21 BN Adapt, ℓz , ℓsm 855.12 ±14.56 3516.85 ±58.63 CoTTA 775.09 ±3.63 3585.37 ±13.31 ProxyTTA-fast 849.43 ±3.61 3540.44 ±3.57 ProxyTTA (Ours) 639.19 ±5.68 2934.36 ±33.80 CostDCNet Pretrained 845.35 3774.01 BN Adapt 1248.35 ±0.25 4267.64 ±0.62 BN Adapt, ℓz , ℓsm 1016.87 ±8.84 3453.00 ±3.21 CoTTA 698.42 ±9.93 3324.59 ±30.21 ProxyTTA-fast 822.49 ±13.55 3331.24 ±55.30 ProxyTTA (Ours) 639.91 ±8.92 2951.21 ±30.93 Table 7. Additional results for test-time adaptation for depth com- pletion on KITTI →VKITTI. the wrong predictions are sometimes propagated as in the left bounding box region from Row 1, Column 4 of Fig. 4. The best-performing method employs the proposed proxy embeddings as a regularizer to guide the adaptation layer update. As the proxy mapping produces test-time features that follow the distribution of the source domain, minimizing our proxy consistency loss (Eqn. 6) implicitly aligns the test domain features to those of the source domain that are com- patible with the 3D scene observed by the test-time sparse point cloud. Not only does this improve overall performance, 15Method MSG-CHN NLSPN CostDCNet MSG-CHN NLSPN CostDCNet Dataset VOID →NYUv2 VOID →ScanNet MAE RMSE MAE RMSE MAE RMSE MAE RMSE MAE RMSE MAE RMSE Image only 2072.78 2462.63 969.14 1228.44 1359.16 1619.40 2001.90 2451.681 899.41 1151.12 1216.17 1459.46 Sparse depth only 734.13 1046.28 237.47 402.47 147.76 354.57 211.86 444.62 162.29 276.29 88.25 205.46 Image + sparse depth 1040.93 1528.98 387.36 704.66 189.10 446.71 316.646 698.633 232.332 431.199 144.311 458.692 Dataset KITTI →Waymo KITTI →nuScenes Image only 12766.791 18324.83 18829.96 24495.73 13598.50 18376.15 11823.061 17244.44 15835.04 22613.78 12794.65 16744.15 Sparse depth only 861.13 2706.75 1290.28 3571.26 1210.93 3102.49 3943.97 7306.33 2540.58 6203.66 2996.28 6773.06 Image + sparse depth 1103.33 2969.39 1173.26 3092.02 1084.18 2819.42 3331.82 6449.09 2656.61 6146.59 3064.72 6630.65 Table 8. Model sensitivity to input modalities. Depth completion networks have a high reliance on sparse depth modality. Performing inference in a novel domain without the RGB image, i.e., using just sparse depth as input, can improve over using both data modalities. Waymo → VKITTI-FOG Method MAE RMSE MSG-CHN Pretrained 1473.14 4676.19 CoTTA 1348.02 ±38.03 4016.67 ±28.16 ProxyTTA-fast (Ours) 1052.78 ±5.74 3891.05 ±17.34 NLSPN Pretrained 2734.27 37621.10 BN Adapt, ℓz , ℓsm 1205.96 ±40.14 3857.88 ±101.15 CoTTA 2485.66 ±18.05 6307.96 ±48.64 ProxyTTA (Ours) 808.16 ±7.86 3536.58 ±91.15 CostDCNet Pretrained 1261.00 4360.37 BN Adapt, ℓz , ℓsm 742.99 ±2.17 3403.00 ±3.62 CoTTA 1150.16 ±5.69 4134.16 ±9.15 ProxyTTA (Ours) 724.77 ±5.18 3349.21 ±29.00 Table 9. Additional results for test-time adaptation for depth com- pletion on Waymo →VKITTI-FOG. but it also reduces standard deviation in error, which can be interpreted as an increase in the stability of the adapta- tion. We show qualitative comparisons against BN Adapt in Fig. 4, where boxes highlight improvements by fixing erroneous propagation by local smoothness (e.g., bleeding effect, which is not mitigated by using image gradients as guidance in Eqn. 5). Quantitatively, we improve over the baseline by an average of 21.09% across all methods and datasets, demonstrating the efficacy of our proxy embedding. H. KITTI→ VKITTI results Here, we present additional results on KITTI → VKITTI adaptation. Test-time adaptation results are shown in Table 7. Consistent with the trends observed in the main paper, our method outperforms over both BN Adapt and CoTTA, with a 21.82% improvement compared to BN Adapt and 12.6% improvement over CoTTA. I. Experiment with different source dataset In our main paper, the only source dataset for outdoor adap- tation scenario was KITTI which is the most popular outdoor depth completion dataset. To validate our method’s appli- cability to models trained on diverse source datasets, we include additional results from adaptation scenarios using a model trained on the Waymo dataset, as shown in Table 9. Our method shows an improvement over CoTTA and BN Adapt by 21.70%. A noteworthy observation from the Waymo adaptation results, when compared to the KITTI → VKITTI-fog results from the main paper, is that the adaptation result of KITTI outperforms that of Waymo. This difference is caused by from the domain discrepancies between KITTI and VKITTI- fog datasets versus the domain gap between Waymo and VKITTI-fog. For example, VKITTI’s object appearances and resolution (1226 ×370 for KITTI, and 1242 ×375 for VKITTI) are more akin to those in the KITTI dataset. Conversely, the Waymo dataset features higher resolution (1920×1280) and different object shapes compared to KITTI and VKITTI. Hence, the adaptation result is influenced by the extent of domain discrepancy between the source and target datasets. J. Quantitative preliminary results To provide a precise observation, we provide the quantitative results of model sensitive study in Tab. 8. 16",
      "meta_data": {
        "arxiv_id": "2402.03312v4",
        "authors": [
          "Hyoungseob Park",
          "Anjali Gupta",
          "Alex Wong"
        ],
        "published_date": "2024-02-05T18:59:52Z",
        "pdf_url": "https://arxiv.org/pdf/2402.03312v4.pdf"
      },
      "llm_extracted_info": {
        "main_contributions": "The paper introduces ProxyTTA, an online test-time adaptation (TTA) method for depth completion that effectively bridges the performance gap due to domain shift in a single pass. It presents a novel study on how domain shift affects different data modalities, observing that sparse depth is more resilient than image modality. Based on this, it proposes an embedding module trained in the source domain that maps sparse depth features to proxy embeddings, serving as guidance to adapt auxiliary parameters (an adaptation layer) at test time. This method is shown to improve over baselines by an average of 21.09% and is the first to introduce TTA for depth completion.",
        "methodology": "The proposed ProxyTTA method operates in three stages: initialization, preparation, and adaptation. In the initialization stage, an adaptation layer (a convolutional layer) is integrated into the pretrained encoder and trained on the source dataset, with the rest of the network frozen. The preparation stage learns a proxy mapping by minimizing a cosine similarity loss between features encoded from sparse depth (with a null-image) and features encoded from both image and sparse depth, all from the source domain. This mapping creates 'proxy embeddings' that capture source domain photometric information. During the test-time adaptation stage, the learned proxy mapping is frozen, and only the adaptation layer parameters are updated. The adaptation loss is a linear combination of sparse depth consistency loss (L1 error), local smoothness loss (L1 penalty on gradients weighted by image gradients), and a proxy consistency loss, which maximizes the cosine similarity between target domain image and sparse depth embeddings and the fixed proxy embeddings from the source domain. This implicitly aligns target RGB features to the source distribution.",
        "experimental_setup": "The method was evaluated on various indoor and outdoor scenarios using a mix of real and synthetic datasets. Indoor datasets include VOID, NYUv2, SceneNet, and ScanNet, primarily for VIO settings. Outdoor datasets include KITTI, Virtual KITTI (VKITTI), nuScenes, and Waymo Open Dataset, primarily for lidar sensor data. Three representative depth completion architectures were used: MSG-CHN (CNN-based), NLSPN (SPN-based), and CostDCNet (cost volume-based). Adaptation scenarios included VOID to NYUv2/SceneNet/ScanNet for indoor, and KITTI to VKITTI (with fog)/nuScenes/Waymo for outdoor. The method was compared against pretrained models, BN Adapt (updating batch statistics), BN Adapt with sparse depth consistency and local smoothness losses, and CoTTA (continual test-time domain adaptation). Performance was measured using Mean Absolute Error (MAE) and Root Mean Squared Error (RMSE) on specific cropped regions and depth ranges for outdoor and full images for indoor settings.",
        "limitations": "The effectiveness of the proxy embedding may be limited in scenarios where the source dataset is sampled from scenes that do not share any objects with the target test dataset. In such cases, the proxy embeddings would offer minimal gain, requiring reliance on generic regularizers like local smoothness. Additionally, the method assumes access to the source dataset prior to deployment for the preparation stage. This presents a challenge for 'off-the-shelf' models trained on private datasets, where the preparation pipeline (adaptation layer and proxy embedding module) would need to be released alongside the network weights during initial model training.",
        "future_research_directions": "The authors aim to motivate further interest in Test-Time Adaptation for multi-modal tasks like depth completion. Implicitly, future work could involve developing strategies to apply this TTA method to 'off-the-shelf' models trained on private datasets, perhaps by standardizing the release of adaptation layers and proxy embedding modules. Another direction could be to investigate robust proxy embeddings or adaptation strategies for scenarios with extreme domain discrepancies where source and target scenes share very few, if any, common objects or layouts."
      }
    },
    {
      "title": "Stationary Latent Weight Inference for Unreliable Observations from Online Test-Time Adaptation"
    },
    {
      "title": "What How and When Should Object Detectors Update in Continually Changing Test Domains?",
      "abstract": "It is a well-known fact that the performance of deep learning models\ndeteriorates when they encounter a distribution shift at test time. Test-time\nadaptation (TTA) algorithms have been proposed to adapt the model online while\ninferring test data. However, existing research predominantly focuses on\nclassification tasks through the optimization of batch normalization layers or\nclassification heads, but this approach limits its applicability to various\nmodel architectures like Transformers and makes it challenging to apply to\nother tasks, such as object detection. In this paper, we propose a novel online\nadaption approach for object detection in continually changing test domains,\nconsidering which part of the model to update, how to update it, and when to\nperform the update. By introducing architecture-agnostic and lightweight\nadaptor modules and only updating these while leaving the pre-trained backbone\nunchanged, we can rapidly adapt to new test domains in an efficient way and\nprevent catastrophic forgetting. Furthermore, we present a practical and\nstraightforward class-wise feature aligning method for object detection to\nresolve domain shifts. Additionally, we enhance efficiency by determining when\nthe model is sufficiently adapted or when additional adaptation is needed due\nto changes in the test distribution. Our approach surpasses baselines on widely\nused benchmarks, achieving improvements of up to 4.9\\%p and 7.9\\%p in mAP for\nCOCO $\\rightarrow$ COCO-corrupted and SHIFT, respectively, while maintaining\nabout 20 FPS or higher.",
      "full_text": "What, How, and When Should Object Detectors Update in Continually Changing Test Domains? Jayeon Yoo1 Dongkwan Lee1 Inseop Chung1 Donghyun Kim2∗ Nojun Kwak1∗ 1Seoul National University 2Korea University 1{jayeon.yoo, biancco, jis3613, nojunk}@snu.ac.kr 2d kim@korea.ac.kr Abstract It is a well-known fact that the performance of deep learning models deteriorates when they encounter a dis- tribution shift at test time. Test-time adaptation (TTA) al- gorithms have been proposed to adapt the model online while inferring test data. However, existing research pre- dominantly focuses on classification tasks through the op- timization of batch normalization layers or classification heads, but this approach limits its applicability to various model architectures like Transformers and makes it chal- lenging to apply to other tasks, such as object detection. In this paper, we propose a novel online adaption approach for object detection in continually changing test domains, considering which part of the model to update, how to up- date it, and when to perform the update. By introducing architecture-agnostic and lightweight adaptor modules and only updating these while leaving the pre-trained backbone unchanged, we can rapidly adapt to new test domains in an efficient way and prevent catastrophic forgetting. Fur- thermore, we present a practical and straightforward class- wise feature aligning method for object detection to resolve domain shifts. Additionally, we enhance efficiency by deter- mining when the model is sufficiently adapted or when ad- ditional adaptation is needed due to changes in the test dis- tribution. Our approach surpasses baselines on widely used benchmarks, achieving improvements of up to 4.9%p and 7.9%p in mAP for COCO → COCO-corrupted and SHIFT, respectively, while maintaining about 20 FPS or higher. 1. Introduction Although deep learning models have demonstrated remark- able success in numerous vision-related tasks, they remain susceptible to domain shifts where the test data distribu- tion differs from that of the training data [3, 25, 40]. In real-world applications, domain shifts frequently occur at test-time due to natural variations, corruptions, changes in weather conditions (e.g., fog, rain) , camera sensor differ- Figure 1. We propose an online adaptation method for object detection in continually changing test domains. Object detectors trained with clean images suffer from performance degradation due to various corruption, such as camera sensor degradation or environmental changes (Direct-Test). Updating full parameters for online adaptation require a large number of test samples and vul- nerable to drastic domain changes (Full-Finetuning), while using only our lightweight adaptor is robust and quickly adapts within a few time steps (Ours). We can further improve efficiency by skip- ping unnecessary adaptation steps (Ours-Skip). ences (e.g., pixelate, defocus blur) , and various other fac- tors. Test-Time Adaptation (TTA) [3, 25, 30, 40, 43, 47] has been proposed to solve the domain shifts in test-time by adapting models to a specific target (test) distribution in an online manner. Furthermore, it is essential to take into account continuously changing test distributions, as the test distribution has the potential to undergo changes and devel- opments as time progresses (i.e., Continual Test-time Adap- tation (CTA)). For instance, autonomous driving systems may experience transitions from clear and sunny conditions to rainy or from daytime to nighttime, which causes contin- ually changing domain shifts [39]. While it is an important research topic, continual test-time adaptation for object de- tection has not been well explored. Recently, several TTA methods [6, 29, 36] tailored for 1 arXiv:2312.08875v1  [cs.CV]  12 Dec 2023object detection have been proposed. ActMAD [29] aligns all the output feature maps ( RC×H×W ) after Batch Nor- malization (BN) layers [14] to adapt the test domain to be similar to that of the training domain. However, this ap- proach requires significant memory during adaptation and does not explicitly consider the objects present in the image. TeST [36] and STFAR [6] adapt to a test domain by utiliz- ing weak and strong augmented test samples with a teacher- student network [37], but they significantly increase infer- ence costs since they require additional forward passes and update steps. Also, these methods update all network pa- rameters, making them highly inefficient in online adapta- tion and vulnerable to losing task-specific knowledge when the test domain experiences continual or drastic changes. In this paper, we aim to develop an efficient continual test-time adaptation (CTA) method for object detection. We investigate the following three key aspects to improve ef- ficiency; what to update: while previous TTA methods for object detection [6, 29, 36] use full fine-tuning, updating all parameters at test time, they are inefficient and prone to losing task-specific knowledge in relatively complex object detection tasks. Updating BN layers, as done in many TTA methods for classification [17, 30, 43, 47], is not as effective for object detection, given its smaller batch size compared to classification and the limitation in applying various back- bones, such as Transformer [26, 41].how to update: several previous TTA methods for object detection [6, 36] adapt the model by using teacher-student networks, resulting in a significant decrease in inference speed, which is detri- mental during test time. While another existing method [29] aligns feature distributions for adaptation, it does not con- sider each object individually, focusing only on image fea- tures, making it less effective for object detection. when to update: most TTA or CTA methods update models using all incoming test samples. However, it is inefficient to update continuously the model if it is already sufficiently adapted when the change of the test domain is not significant. To this end, (1) we propose an efficient continual test- time adaptation method for object detectors to adapt to continually changing test domains through the use of lightweight adaptors which require only 0.54%∼0.89% ad- ditional parameters compared to the full model. It exhibits efficiency in parameters, memory usage, and adaptation time, along with robustness to continuous domain shifts without catastrophic forgetting. Additionally, it demon- strates wide applicability to various backbone types com- pared to BN-based TTA methods [17, 22, 30, 43, 47, 48]. (2) To enhance the adaptation effectiveness in the object detec- tion task, we align the feature distribution of the test domain with that of the training domain at both the image-level and object-level using only the mean and variance of features. For estimating the mean of the test domain features, we employ Exponentially Moving Average (EMA) as we can leverage only the current incoming test samples, not the en- tire test domain data. Due to the unavailability of training data access, we utilize only the mean and variance of the features from a few training samples. (3) We also introduce two novel criteria that do not require additional resources to determine when the model needs adaptation to enhance efficiency in a continually changing test domain environ- ment. As illustrated in Fig. 1, our approach Ours, employ- ing adaptors, tends to adapt much faster to domain changes compared to full parameter updates. This enables efficient TTA by using only a few test samples to update the adaptor and skipping the rest of the updates as shown in Ours-Skip. Our main contributions are summarized as follows: • We introduce an architecture-agnostic lightweight adap- tor, constituting only a maximum of 0.89% of the total model parameters, into the backbone of the object de- tector to adapt the model in a continually changing test domain. This approach ensures efficiency in parameters, memory usage, and adaptation speed, demonstrating the robust preservation of task-specific knowledge owing to its inherent structural characteristics. • We propose a straightforward and effective adaptation loss for CTA in object detection tasks. This is achieved by aligning the distribution of training and test domain fea- tures at both the image and object levels, utilizing only the mean and variance of a few training samples and EMA- updated mean features of the test domain. • We also propose two criteria to determine when the model requires adaptation, enabling dynamic skipping or resum- ing adaptation as needed. This enhancement significantly boosts inference speed by up to about 2 times while main- taining adaptation performance. • Our adaptation method proves effective for diverse types of domain shifts, including weather changes and sensor variations, regardless of whether the domain shift is dras- tic or continuous. In particular, our approach consistently improves the mAP by up to 7.9% in COCO →COCO-C and SHIFT-Discrete/Continuous with higher than 20 FPS. 2. Related Work Test-time adaptation. Recently, there has been a surge of interest in research that adapts models online using unla- beled test samples while simultaneously inferring the test sample to address the domain shift problem, where the test data distribution differs from that of the training data. There are two lines for online adaptation to the test do- main, Test-time Training (TTT) and Test-time Adaptation (TTA). TTT [1, 2, 25, 40] involves modifying the model architecture during training to train it with self-supervised loss, allowing adaptation to the test domain in the test time by applying this self-supervised loss to the unlabeled test samples. On the other hand, TTA aims to adapt the trained model directly to the test domain without specifically tai- 2lored model architectures or losses during training time. NORM [35] and DUA [28] address the domain shifts by adjusting the statistics of batch normalization (BN) layers using the current test samples, without updating other pa- rameters, inspired by [21]. Following this, [22, 30, 43, 48] and [17] update the affine parameters of BN layers using unsupervised loss, entropy minimization loss to enhance the confidence of test data predictions, and feature distribution alignments loss, respectively. Several studies [15, 16] up- date the classifier head using the pseudo-prototypes from the test domain. However, these methods limit their appli- cability to architectures without BN layers or to object de- tection tasks that involve multiple objects in a single im- age. Others [29, 38, 47] update full parameters for online adaptation to the test domain in an online manner, but this approach is inefficient and susceptible to the noisy signal from the unsupervised loss. While existing TTA methods are oriented towards classification tasks, we aim to propose an effective and efficient method for online adaptation in the object detection task. Continual test-time adaptation. Recent studies [31, 44] point out that existing TTA methods have primarily focused on adapting to test domains following an i.i.d assumption and may not perform well when the test data distribution deviates from this assumption. [44] introduces a Contin- ual TTA (CTA) method designed for scenarios where the test domain continuously changes over time. This poses challenges in preventing the model from over-adapting to a particular domain shift and preserving the knowledge of the pre-trained model to avoid catastrophic forgetting. In the field of CTA, the self-training strategy adopting an Exponentially Moving Average (EMA) teacher-student structure is attracting interest as an effective algorithm en- abling robust representation to be learned through self- knowledge distillation. In many studies, the EMA teacher- student structure and catastrophic restoration of source model weights have been proposed as a solution to achieve the goal of CTA [4, 44, 45]. Approaches using source re- play [32], and anti-forgetting regularization [30] have also achieved good performances in robust continuous adapta- tion. Furthermore, there is growing attention on methods that mitigate the computational and memory challenges as- sociated with CTA, such as [12], which relies on updates to batch normalization statistics. Test-time adaptive object detection. Research on TTA for Object Detection (TTAOD) is progressively emerging [6, 29, 36, 42]. Most existing TTAOD methods [6, 36, 42] exploit a teacher-student network to adapt to the test do- main, following the self-training approach commonly em- ployed in Unsupervised Domain Adaptation for object de- tection [7, 18, 19, 34]. However, it is inefficient for TTA due to data augmentation requirements and additional for- ward and backward steps, resulting in slower inference speeds and higher memory usage. Another approach, Act- MAD [29], aligns the distributions of output feature maps after all BN layers along the height, width, and channel axes to adapt to the test domain. However, this location-aware feature alignment is limited to datasets with fixed location priors, such as driving datasets, and is less effective for nat- ural images like COCO. Additionally, CTA for Object De- tection (CTAOD)have not been thoroughly explored. There- fore, there is a need for an effective CTAOD method con- sidering memory and time efficiency. 3. Method To enable the efficient and effective Continual Test-time Adaptation of Object Detectors (CTAOD), we introduce an approach that specifies which part of the model should be updated, describes how to update those using unlabeled test data, and determines whether we perform model updates or not to improve efficiency. 3.1. Preliminary Assume that we have an object detector h ◦ gΘ, here h and g are the RoI head and the backbone, respectively with their parameters being Θ. The training dataset is denoted as Dtrain = {(xi, yi)}N i=1, where xi ∼ Ptrain(x) and yi = ( bboxi, ci), containing information on the bounding box (bbox) and class label ci ∈ C. Consider deploying the detector to the test environments where the test data at pe- riod T is denoted as xT j ∼ PT test(x), PT test ̸= Ptrain and PT test deviates from the i.i.d. assumption. In addition, the domain of PT test continually changes according to T (i.e., PT test ̸= PT−1 test ). Our goal is to adapt the detector h ◦ g to PT test using only test data xT j while making predictions. 3.2. What to update: Adaptation via an adaptor Previous methods [6, 29, 36, 42] adapt the model to the test domain by updating all parameters Θ, leading to in- efficiency at test time and a high risk of losing task knowl- edge from the training data. In contrast, we adapt the model by introducing an adaptor with an extremely small set of parameters and updating only this module while freezing Θ. We introduce a shallow adaptor in parallel for each block, inspired by [5, 13], where transformer-based mod- els are fine-tuned for downstream tasks through parameter- efficient adaptors, as shown in Fig. 2. Each adaptor consists of down-projection layers Wdown ∈ Rd×d r , up-projection layers Wup ∈ R d r ×d and ReLUs, where d denotes the in- put channel dimension and r is the channel reduction ratio set to 32 for all adaptors. We use MLP layers for the Trans- former block (Fig. 2a) and 1×1 convolutional layers for the ResNet block (Fig. 2b) to introduce architecture-agnostic adaptors. The up-projection layer is initialized to 0 values so that the adaptor does not modify the output of the block, 3(a) A block of Transformer  (b) A block of ResNet Figure 2. We attach an adaptor, which is a shallow and low-rank MLP or CNN, to every N block in parallel. We update only these adaptors while other parameters are frozen. Our approach can be applied to diverse architectures including CNNs and Transformers. but as the adaptor is gradually updated, it adjusts the output of the block to adapt to the test domain. Even as the adaptor is updated in the test domain, the original backbone param- eter Θ remains frozen and fully preserved. This structural preservation, as evident in Ours in Fig. 1, enables robust and efficient adaptation to domain changes by maintaining relatively complex task knowledge in object detection and updating very few parameters. 3.3. How to update: EMA feature alignment To adapt the object detector to the test domain, we align the feature distribution of the test domain with that of the training data, inspired by [17, 29, 38]. In contrast to these methods that solely align image feature distribution, we ad- ditionally align object-level features in a class-wise manner, considering class frequency, to enhance its effectiveness for object detection. As the training data is not accessible dur- ing test time, we pre-compute the first and second-order statistics, denoted as µtr = E[Ftr] and Σtr = Var[Ftr], where the operators E and Var represent the mean and vari- ance respectively. The features Ftr = {gΘ(xtr)} are com- puted using only 2,000 training samples, a small subset of the training data. Since a sufficient amount of test domain data is not available at once, and only the current incoming test data, whose features are denoted as Ft te, is accessible at time step t, we estimate the mean of test data features using an exponentially moving average (EMA) as follows: µt te = (1 − α) · µt−1 te + α · E[Ft te], s.t. µ0 te = µtr. (1) Considering the typically small batch size in object detec- tion compared to classification, we approximate the vari- ance of the test features as Σte ≃ Σtr to reduce instability. Image-level feature alignment. We estimate the training and test feature distributions as normal distributions and minimize the KL divergence between them as follows: Limg = DKL(N(µtr, Σtr), N(µt te, Σtr)). (2) Region-level class-wise feature alignment. In object de- tection, we deal with multiple objects within a single image, making it challenging to apply the class-wise feature align- ment proposed in [38], a TTA method for classification. To handle region-level features that correspond to an object, we use ground truth bounding boxes for the training data and utilize the class predictions of RoI pooled features, ft te, for unlabeled test data. In object detection, domain shifts often result in lower recall rates, as a significant number of proposals are predicted as background [20]. To mitigate this issue, we filter out features with background scores exceed- ing a specific threshold. Subsequently, we assign them to the foreground class with the highest probability, as follows: Fk,t te = {ft te|argmax c pfg = k, pbg < 0.5}, where hcls(ft te) = [pfg , pbg] = [p0, ..., pC−1, pbg]. (3) We estimate the class-wise feature distribution of the test domain by exploiting Fk,t te and Eq.1. Furthermore, we in- troduce a weighting scheme for aligning features of less frequently appearing classes, taking into account the severe class imbalance where specific instance ( e.g., person) may appear multiple times within a single image, as follows: Nk,t = Nk,t−1 + ||Fk,t te ||, s.t. Nk,0 = 0 wk,t = log \u0012maxi Ni,t Nk,t \u0013 + 0.01 Lobj = X k wk,t · DKL(N(µk tr, Σk tr), N(µk,t te , Σk tr)). (4) Here, the class-wise mean µk and variance Σk of the train- ing and test data are obtained in the same way as the image- level features. We can effectively adapt the object detector by updating the model to align the feature distribution at both the image and object levels as L = Limg + Lobj. 3.4. When to update: Adaptation on demand As shown in Fig. 1, Ours, which only updates the adaptor proposed in Sec. 3.2, efficiently adapts to changes in the test domain, even with a small subset of early test samples. We leverage its rapid adaptation characteristics to reduce com- putational costs by skipping model updates ( i.e., skipping backward passes) when the model has already sufficiently adapted to the current test domain and resuming model up- dates when confronted with a new test domain. Therefore, we introduce two criteria to determine when to update the model or not as follows: (Criterion 1) When the distribution gap exceeds the in- domain distribution gap. Recall that Limg (Eq. 2) mea- sures the distribution gap between the test and train distri- butions. We assume a model is well-adapted to the current test domain when Limg is closer to the in-domain distri- bution gap. We measure the in-domain distribution gap by 4(a) The ratio of Limg to Din KL (b) The ratio of Limg to Lt ema Figure 3. The test domain undergoes a shift every 4,000 time steps, and each metric reaches its peak at the same intervals. sampling two disjoint subsets, xi and xj, of training fea- tures Ftr from Sec. 3.3 as follows: Din KL = DKL(N(µi tr, Σi tr), N(µj tr, Σj tr)), (5) where µi tr, Σi tr are obtained from xi ∼ Ptrain(x) and µj tr, Σj tr from xj ∼ Ptrain(x). In other words, if Limg is noticeably higher than the in-domain distribution gapDin KL, we consider a model encountering a test domain whose dis- tribution differs from Ptrain(x) and needs to be updated. Based on this, we introduce a new index Limg Din KL . Fig. 3a plots the trend of this index during the model adaptation to a con- tinually changing test domain. It shows that the index has a large value in the early stages of a domain change, decreases rapidly, and then maintains a value close to 1. This index exhibits a similar trend regardless of the backbone type and dataset, as included in the appendix. Therefore, we establish the criterion that model updates are necessary when this in- dex exceeds a certain threshold, τ1, as Limg Din KL > τ1. (Criterion 2 ) When the distribution gap suddenly in- creases. Additionally, we can determine when the test dis- tribution changes and model updates are necessary by ob- serving the trend of the distribution gap ( i.e., Limg). The convergence of Limg indicates that a model is well-adapted to the current test domain. To put it differently, Limg will exhibit a sudden increase when the model encounters a new test domain. We introduce an additional index, denoted as Limg Ltema , representing the ratio of the currentLimg to its expo- nentially moving averageLt ema at time t. We calculate it us- ing the following formula:Lt ema = 0.99·Lt−1 ema+0.01·Limg. Fig. 3b illustrates the trend of the ratio of Limg over the timesteps. It tends to reach a value of 1 as the loss stabilizes at a specific level. Nevertheless, when the model encounters shifts in the test distribution, the ratio experiences a sharp increase, indicating the necessity of a model update when it exceeds a specific threshold, τ2, as Limg Ltema > τ2. If at least one of the two criteria is satisfied, we conclude that the model requires adaptation and proceed to update it. 4. Experiments Sec. 4.1 presents the two object detection benchmark datasets with test distributions that change continuously, ei- ther in a drastic or gradual manner, and our implementation detail is in 4.2. Sec. 4.4 compares our method with other TTA baselines described in Secs. 4.3.. We present detailed ablation studies of our method analyzing the effectiveness and efficiency of our method in terms of what, how, and when to update the models for CTAOD in Sec. 4.5. 4.1. Datasets We experiment with the following three scenarios. COCO → COCO-C simulates continuous and drastic real- istic test domain changes over a long sequence. MS-COCO [23] collects 80 classes of common objects in their natural context with 118k training images and 5k validation images. COCO-C is created by employing 15 types of realistic cor- ruptions [27], such as image distortion and various weather conditions, to simulate test domain changes. In the experi- ments, the model is only trained on the COCO train set and sequentially evaluated on each corruption in the COCO-C validation set during test-time for reproducing continually changing test domains. Finally, the model is evaluated on the original COCO validation set to assess how well it pre- serves knowledge of the original domain (denoted as Org.). SHIFT-(Discrete / Continuous) [39] is a synthetic driving image dataset with 6 classes under different conditions us- ing five weather attributes (clear, cloudy, overcast, fog, rain) and three time-of-day attributes ( daytime, dawn, night ). In SHIFT-Discrete, there are image sets for each attribute, and the model is sequentially evaluated on these attributes, cloudy → overcast → foggy → rainy → dawn → night → clear which contains 2.4k, 1.6k, 2.7k, 3.2k, 1.2k, 1.4k, and 2.8k validation images, respectively. This simulates scenar- ios where the domain undergoes drastic changes. InSHIFT- Continuous, the model is evaluated on four sequences, each consisting of 4k frames, continuously transitioning from clear to foggy (or rainy) and back to clear. 4.2. Implementation Detail We experiment with Faster-RCNN [33] models using ResNet50 [10] and Swin-Tiny [26] as a backbone with FPN [24]. For the COCO → COCO-C adaptation, we em- ploy the publicity available models trained on COCO re- leased in [46] and [26] for ResNet5- and Swin-Tiny-based Faster-RCNN, respectively. For SHIFT experiments, mod- els are trained on the training domain using the detectron2 framework following [33] and [26]. For test-time adapta- tion, we always set the learning to 0.001 for the SGD opti- mizer, and α of Eq. 1 to 0.01, while τ1 and τ2 are set to 1.1 5Table 1. Comparison of mAP, the number of backward and forward passes, and FPS between baselines and our model on COCO→ COCO- C. Our model consistently outperforms baselines on the two different backbones. Furthermore, Ours-Skip with ResNet notably reduces backward passes by as much as 90.5%, leading to a significantly improved frames per second (FPS) rate by up to 109.9%. Noise Blur Weather Digital # step Backbone Method Gau Sht Imp Def Gls Mtn Zm Snw Frs Fog Brt Cnt Els Px Jpg Org. Avg. For. Back. FPS Swin-T [26] Direct-Test 9.7 11.4 10.0 13.4 7.5 12.1 5.2 20.7 24.8 36.1 36.0 12.9 19.1 4.9 15.8 43.0 17.7 80K 0 21.5 ActMAD 10.7 12.0 9.4 12.3 5.7 9.5 4.5 15.3 17.5 27.6 28.2 1.1 16.7 2.6 8.7 36.3 13.9 80K 80K 8.3 Mean-Teacher 10.0 12.1 11.2 12.8 8.1 12.1 4.9 19.6 23.7 34.9 34.0 8.0 18.9 6.1 17.6 41.0 17.2 160K 80K 6.9 Ours 13.6 16.6 16.1 14.0 13.6 14.2 8.3 23.7 27.2 37.4 36.4 27.2 27.2 22.2 22.3 42.3 22.6 80K 80K 9.5 Ours-Skip 13.3 15.3 15.1 14.0 12.8 13.9 6.5 22.0 25.4 35.5 34.9 26.5 25.9 23.4 20.2 41.2 21.6 80K 9.7K 17.7 ResNet50 [10] Direct-Test 9.1 11.0 9.8 12.6 4.5 8.8 4.6 19.1 23.1 38.4 38.0 21.4 15.6 5.3 11.9 44.2 17.3 80K 0 25.8 NORM 9.9 11.9 11.0 12.6 5.2 9.1 5.1 19.4 23.5 38.2 37.6 22.4 17.2 5.7 10.3 43.4 17.5 80K 0 25.8 DUA 9.8 11.7 10.8 12.8 5.2 8.9 5.1 19.3 23.7 38.4 37.8 22.3 17.2 5.4 10.1 44.1 17.1 80K 0 25.8 ActMAD 9.1 9.6 7.0 11.0 3.2 6.1 3.3 12.8 14.0 27.7 27.8 3.9 12.9 2.3 7.2 34.3 10.5 80K 80K 9.6 Mean-Teacher 9.6 12.5 12.0 4.0 2.9 4.8 3.1 16.2 23.5 35.1 34.0 21.8 16.6 8.2 12.7 40.3 14.5 160K 80K 8.1 Ours 12.7 17.8 17.5 12.4 11.5 11.3 6.6 22.8 26.9 38.6 38.5 28.0 25.1 21.2 22.2 41.8 22.2 80K 80K 10.1 Ours-Skip 14.4 17.1 16.0 13.9 11.7 12.2 6.3 22.1 25.5 37.7 37.1 25.5 24.1 23.1 21.1 42.8 21.9 80K 7.6K 21.2 and 1.05, respectively. We use the same hyper-parameters across all backbones and datasets. All experiments are con- ducted with a batch size of 4. 4.3. Baselines Direct-Test evaluates the model trained in the training do- main without adaptation to the test domain. ActMAD [29] is a TTA method aligning the distribution of output features across all BN layers. To apply ActMAD to the Swin Trans- former-based model, we align the output features of the LN layers. We implement Mean-Teacher using a teacher- student network framework to reproduce as close as possi- ble to TeST [36], as its implementation is not publicly avail- able. We follow the FixMatch [37] augmentation method and report results after tuning all hyper-parameters in our scenario. NORM [35] and DUA [28], TTA methods ini- tially designed for classification, are directly applicable to detection tasks by either mixing a certain amount of current batch statistics or updating batch statistics via EMA. How- ever, these are only compatible with architectures contain- ing BN layers. Additional details are provided in Appendix. 4.4. Main Results We compare the performance of each method using mAP and efficiency metrics, including the number of forward and backward passes, as well as FPS during test-time adapta- tion. Results of COCO and SHIFT are in Tab. 1 and 2, re- spectively. COCO → COCO-C. Tab. 1 demonstrates the effective adaptation performance of Ours in the challenging COCO benchmark with 80 classes due to object-level class-wise feature alignment. ActMAD also aligns feature distribution for TTA, but is not effective since it only aligns whole fea- ture maps without considering specific classes in the im- age. NORM and DUA, applicable only to ResNet [10], show minimal performance improvement by adaptation as they are not specifically tailored for object detection and only modify batch statistics across the entire feature map. Ad- ditionally, ActMAD and Mean-Teacher, updating full pa- rameters, gradually lose task knowledge in the continually changing test distributions, resulting in much lower perfor- mance on Org. , the domain identical to the training data, than that of Direct-Test. In contrast, Ours effectively pre- vents catastrophic forgetting by freezing the original param- eters of the models and updating only the adaptor, obtain- ing performance on par with Direct-Test on the Org. do- main and consistently high performance across corrupted domains, with an average mAP improvement of 4.9%p compared to that of Direct-Test. Furthermore, leveraging the rapid adaptation ability of the adaptor,Ours-Skip, which skips unnecessary adaptation, allows using only a maxi- mum of about 12% of the total samples for adaptation with- out significant performance loss. This leads to a substantial improvement in inference speed, more than doubling com- pared to other TTA methods, reaching over 17.7 FPS. SHIFT-Discrete. Ours is also effective in SHIFT, which simulates continuous changes in weather and time in driv- ing scenarios according to the left section of Tab. 2. Espe- cially, Ours shows significant improvements in mAP by 7- 9%p, particularly for the foggy and dawn attributes where Direct-Test obtains lower performance due to severe do- main shift. In contrast, with ActMAD, catastrophic forget- ting takes place when adapting to the cloudy and overcast weather. This is due to the updating of the full parame- ters, despite that Direct-Test already shows proficient per- formance in these conditions. As a result, the performance in the later domains is worse than that of the Direct-Test. DUA, which updates batch statistics using EMA, shows a gradual decrease in performance as the domain contin- uously changes, resulting in much lower performance in the original clear domain ( i.e., clear ). On the other hand, NORM, which utilizes the statistics of the current batch samples, exhibits no catastrophic forgetting and relatively good adaptation, as SHIFT is a relatively easier task com- pared to COCO due to having only 6 classes. Compared to NORM, Ours shows better adaptation performance, and is 6Table 2. Comparison of mAP, the number of backward and forward passes, and FPS between baselines and our model on SHIFT-Discrete and SHIFT-Continuous. Baselines perform effectively in a particular setting but lack generalizability across various settings. Our method consistently achieves results that are either better or on par with the best model in all settings, demonstrating its strong stability. Ours-Skip also effectively reduces the number of backward passes without compromising mAP performance, resulting in a higher FPS. SHIFT-Discrete SHIFT-Continuous mAP # step mAP # Avg. step Backbone Method cloudy overc. fog rain dawn night clear Avg. For. Back. FPS clear↔fog clear ↔rain For. Back. FPS Swin-T [26] Direct-Test 50.0 38.9 23.1 45.1 26.9 39.5 45.9 38.5 15.3K 0 27.5 18.1 21.1 4K 0 28.3 ActMAD 49.8 38.4 21.4 43.1 19.0 32.0 44.8 35.5 15.3K 15.3K 9.3 15.6 16.3 4K 4K 9.8 Mean-Teacher 50.0 39.2 25.7 45.4 26.0 37.5 42.2 38.0 15.3K 15.3K 7.8 20.4 24.3 8K 4K 6.5 Ours 50.3 39.2 32.2 46.7 30.4 39.9 44.3 40.4 15.3K 15.3K 11.2 23.9 22.6 4K 4K 11.6 Ours-Skip 50.3 39.7 29.1 47.1 30.2 41.5 45.9 40.6 15.3K 6.1K 20.0 25.1 23.8 4K 0.83K 19.2 ResNet50 [10] Direct-Test 49.4 37.9 19.7 43.1 20.1 35.3 45.6 35.9 15.3K 0 30.1 12.1 15.4 4K 0 30.0 NORM 49.7 38.6 22.9 44.7 25.1 37.4 45.5 37.7 15.3K 0 30.1 16.9 19.4 4K 0 30.0 DUA 45.2 31.5 27.7 31.9 15.2 18.6 21.1 27.3 15.3K 0 30.1 22.5 22.4 4K 0 30.0 ActMAD 49.2 37.7 18.0 40.6 16.0 32.9 44.3 34.1 15.3K 15.3K 11.3 12.7 16.3 4K 4K 11.2 Mean-Teacher 49.6 38.4 26.8 43.4 26.6 33.1 41.6 37.1 15.3K 15.3K 9.9 16.0 20.8 8K 4K 9.8 Ours 49.7 38.7 27.4 46.3 27.4 37.6 43.8 38.7 15.3K 15.3K 12.9 20.9 21.9 4K 4K 13.9 Ours-Skip 49.7 38.8 26.9 46.2 27.6 38.8 45.0 39.0 15.3K 8.9K 21.5 20.0 22.5 4K 0.75K 21.3 Table 3. Comparison of adaptation performance (mAP), the num- ber of trainable parameters (# Params), and memory usage (Cache) according to which part of the backbone is updated. SD / SC de- notes SHIFT-Discrete/Continuous, respectively. mAP # Params Cache Backbone Trainable Params SD SC Num Ratio Avg. Max Swin-T Full-params 38.4 20.6 27.7M 100% 0.86 11.0 LayerNorm 38.5 20.0 0.03M 0.1% 0.65 7.49 adaptor (Ours) 40.4 23.2 0.15M 0.5% 0.65 6.96 ResNet50 Full-params 37.6 20.4 23.7M 100% 1.65 9.29 BatchNorm 37.9 20.2 0.05M 0.2% 1.47 9.11 adaptor (Ours) 38.7 21.7 0.21M 0.9% 1.48 5.41 also applicable to BN-layer-free Swin Transformers. SHIFT-Continuous. In scenarios where the test domain gradually changes across the entire sequence, Ours also demonstrates effectiveness, improving mAP by up to 7%p, as shown in the right section of Tab. 2. WhileDUA performs well in the clear to foggy transition, it is prone to catas- trophic forgetting in situations where the sequence becomes longer, and the test domain changes more diversely, as seen in the left section. Our strategy for determining when model adaptation is necessary is particularly effective in SHIFT. It improves FPS by about 9, reaching about 20 FPS, while en- hancing mAP. This is likely due to avoiding overfitting that can occur when adapting to all repetitive frames in SHIFT, which consists of continuous frames, leading to improve- ments in both inference speed and adaptation performance. 4.5. Additional Analyses We aim to demonstrate the effectiveness and detailed anal- ysis of our proposed model in terms of 1) which parts of, 2) how, and 3) when the model should be updated. Which part to update? Tab. 3 shows how updating dif- ferent parts of the backbone model affects the performance and the memory usage during continual test-time adapta- Table 4. Ablation on each component of our loss. SHIFT-D / C denotes SHIFT-Discrete / Continuous, respectively. The left and right value in each cell corresponds to the mAP for the Swin-T and ResNet50 backbone, respectively. Limg Lobj COCO SHIFT-D. SHIFT-C. - - 17.7/ 17.3 38.5/ 35.9 19.6/ 13.8 ✔ - 16.7/ 18.1 36.6/ 37.0 19.1/ 16.0 ✔ no class weight 17.8/ 18.9 39.7/ 38.0 25.1/ 23.4 ✔ class weight wk,t 22.6/ 22.2 40.4/ 38.7 23.2/ 21.7 tion. We compare (1) updating full parameters, (2) affine parameters of the normalization layer, and (3) our proposed adaptor for each backbone on the SHIFT dataset. Although our adaptor has fewer parameters, about 0.9% or less of the full parameters, it demonstrates the best adaptation perfor- mance. Updating only the affine parameters of the normal- ization layer, while having fewer parameters, seems less ef- fective for adaptation in object detection compared to clas- sification [30, 43]. Additionally, our adaptor requires only about 60% of the memory compared to updating the full parameters, making it memory-efficient. Ablation study on each component in our loss. Tab. 4 presents the effects of image-level feature alignment,Limg, object-level feature class-wise alignment Lobj, and class frequency weighting wk,t proposed to address class im- balance. Aligning only the image-level feature distribu- tion with Limg (first row) leads to modest adaptation in the ResNet50 backbone, while performance in the Swin- T backbone is even lower than without adaptation. No- tably, aligning object-level features with Lobj leads to a substantial improvement, with the mAP increasing by approximately 10%p compared to the no-adaptation sce- nario. Introducing class-specific frequency-based weighting wk,t, despite a slight performance decrease in the SHIFT- Continuous setting, proves highly effective, particularly in scenarios with significant class imbalance, such as COCO 7(a) Swin Transformer backbone  (b) ResNet50 backbone Figure 4. Comparison of mAP and FPS fromOurs-Skip with vary- ing values of τ1 (♦) and τ2 (▲) against Evenly-Skip (×), adapting every N-th instances, on COCO→COCO-C using both (a) Swin- T and (b) ResNet50. The upward and rightward movement indi- cates a better strategy with higher mAP and faster inference speed, showing that Ours-Skip is consistently better than Evenly-Skip. (a) Accumulated number of backward steps (b) Number of backward steps and mAP of Direct-Test in each domain Figure 5. Analysis of the adaptation of Ours-Skip. with 80 classes, where it enhances the mAP by around 5%p. Trade-off between adaptation performance and effi- ciency according to different skipping strategies. Fig. 4 presents mAP and FPS depending on the values ofτ1 and τ2 in the Sec. 3.4 on COCO → COCO-C, which are used for two criteria to determine when the adaptation is needed. We also show the simple baselineEvenly-Skip, which adapts ev- ery N-th step and skips the rest. In Fig. 4, the blue lines (▲) show the results when τ1 is changing from 1.0 to infinity, where only criterion 2 is used, while τ2 is fixed at 1.05. As τ1 decreases, more adaptation is required, leading to slower FPS but higher mAP. The green lines (♦) show the results of changing τ2, where ‘τ2 = inf’ denotes using only criterion 1, without criterion 2. For all main experiments, we set τ1 and τ2 as 1.1 and 1.05, respectively, considering the balance between mAP and FPS. Additionally, our skipping strategy consistently outperforms Evenly-Skip, achieving higher val- ues in both mAP and FPS. This indicates that our criterion for deciding when to bypass model updates provides an ef- fective balance between accuracy and speed. When do models actually update? We analyze when the model actually skips adaptation and only performs infer- ence or actively utilizes test samples for model adaptation based on the two criteria we propose. This analysis is con- ducted in COCO to COCO-C with 15 corruption domains and 1 original domain. Fig. 5a plots the number of back- ward passes, i.e., the number of batches of test samples used for adaptation, with different values of τ1 for the two backbones. The horizontal and vertical axes represent se- quentially incoming test domains and the cumulative back- ward numbers, respectively. A steep slope in a region in- dicates frequent adaptation, while a gentle slope indicates skipping adaptation, performing only inference. Notably, even without explicit information about when the test do- main changes, the model actively performs adaptation, es- pecially right after the test domain changes. This trend is consistent regardless of changes in τ value or backbone type. Furthermore, it is evident that the number of backward passes is primarily determined by the value ofτ1 rather than the type of backbone, suggesting that a consistent τ1 value can be used irrespective of the backbone. Fig. 5b visually represents the adaptation tendencies by dividing backward steps for each domain in the case of Swin-T backbone with τ1 = 1.1. More clearly, it shows that adaptation occurs ac- tively around the points where each domain changes, and af- terward, adaptation happens intermittently or almost not at all. The light pink bars represent the performance ofDirect- Test, showing that domains with initially high model per- formance tend to have less adaptation, while domains with lower performance initially need more adaptation. In other words, the amount of skipping adaptation is proportional to the amount of the domain shift. Interestingly, the second do- main, ’Shot Noise’, shows almost no adaptation despite the lower performance of the Direct-Test. We conjecture that the preceding domain, ’Gaussian Noise’, shares a similar nature of noise, leading the model to decide that additional adaptation steps may not be necessary. As a result, our skip- ping strategy enables the model to efficiently adapt, consid- ering both the original domain the model is trained on and the previous domain the model has been adapted to. 5. Conclusion We introduce an efficient Continual Test-time Adaptation (CTA) method for object detection in the continually chang- ing domain. Our approach involves 1) lightweight adap- tors, 2) class-wise object-level feature alignment, and 3) skipping unnecessary adaptation. These contributions col- lectively yield a highly efficient and effective adaptation method, showcasing robustness to diverse domain shifts, and achieving notable improvements in mAP performance across various CTA scenarios without serious slowdown in the inference speed. 8What, How, and When Should Object Detectors Update in Continually Changing Test Domains? Supplementary Material 6. Additional Details for Baselines We provide additional implementation details for each base- line model. Our framework incorporates all baseline models using the official code except Mean-Teacher. The results of the experiments are reported based on the optimal hyperpa- rameters that yield the best results in our scenario. ActMAD [29] As ActMAD exclusively conducts experi- ments on the KITTI dataset, where all images have a con- stant height and width (e.g., 370 x 1224), ensuring consis- tent feature map sizes for all samples. ActMAD can easily align them along the spatial axis. However, in the general setting of object detection tasks, such as the COCO bench- mark set, where image sizes and width-to-height ratios vary, aligning feature maps along the spatial axis becomes chal- lenging due to different sizes. To adapt ActMAD to our COCO → COCO-C scenario, we perform center cropping on the feature maps to match the size of training domain fea- ture maps and the current test sample feature maps. We em- ploy a learning rate of 1e-5 for COCO and 1e-4 for SHIFT, respectively. Mean-Teacher As the official code of TeST [36] is not available, we implement the EMA-updated Teacher and Student models following TeST [36], to conduct experi- ments in our scenarios. TeST involves three forward steps for a batch: forwarding weakly augmented samples through the student network, strong augmented samples through the teacher network, and original samples through the teacher network for outputs. However, for a fair comparison, we perform two forward steps, forwarding the original sample through the teacher network and strong augmented sam- ples through the student network, to make predictions be- fore adaptation for every samples. We utilize a learning rate of 1e-5 and set the EMA update rate for the teacher network to 0.999. NORM [35] We set the hyperparameter N that controls the trade-off between training statistics and estimated tar- get statistics as 128. DUA [28] We set the momentum decay as 0.94, minimum momentum constant as 1e-4, and the initial momentum de- cay as 1e-3. 7. The effect of Bottleneck Reduction Ratio in the Adaptor Table 5 shows the results for COCO → COCO-C, SHIFT- Discrete, and SHIFT-Continuous based on the dimension reduction ratio ( r) discussed in Section 3.2, representing Table 5. Comparison of adaptation performance (mAP), the num- ber of trainable parameters (# Params), and memory usage (Cache) according to r of Sec. 3.2, the bottleneck reduction ratio in the adaptor. We set r as 32 for all our experiments in the main paper. SD / SC denotes SHIFT-Discrete / Continuous, respectively. mAP # Params Cache Backbone r COCO SD SC Num Ratio Avg. Max Swin-T 1 22.6 40.0 21.3 4.33M 15.7% 0.75 7.51 2 22.6 40.3 23.2 2.17M 7.85% 0.73 7.27 4 22.6 40.4 23.2 1.09M 3.95% 0.70 7.06 8 22.6 40.4 23.2 0.55M 2.00% 0.69 7.00 16 22.6 40.4 23.2 0.28M 1.02% 0.67 6.98 32 22.6 40.4 23.2 0.15M 0.54% 0.65 6.96 64 22.6 40.4 23.2 0.08M 0.29% 0.65 6.95 ResNet50 1 22.5 38.7 20.8 6.31M 26.7% 1.55 5.89 2 22.4 38.7 20.9 3.16M 13.4% 1.51 5.64 4 22.3 38.6 21.3 1.59M 6.71% 1.49 5.52 8 22.3 38.6 21.4 0.80M 3.39% 1.48 5.46 16 22.2 38.6 21.4 0.41M 1.73% 1.48 5.43 32 22.2 38.7 21.4 0.21M 0.89% 1.48 5.41 64 22.1 38.7 21.3 0.11M 0.48% 1.48 5.40 the ratio of bottleneck size compared to the input size in the adaptor. The adaptation performance remains consistent across different r values. However, in the case of r = 1 in SHIFT experiments, mAP decreases, potentially due to catastrophic forgetting resulting from a large number of adaptable parameters. Since increasing the value of r sig- nificantly reduces the number of learnable parameters and memory usage, we set r to 32 in all other experiments. 8. Results on the KITTI Dataset We conduct additional experiments on the KITTI [8] dataset, the commonly used object detection dataset consist- ing of driving scenes with 8 classes (car, van, truck, person, person sitting, cyclist, tram, misc). To simulate the continu- ally changing domains, we use the following scenario ( Fog → Rain → Snow → Clear) as done in [29]. We use the physics-based rendered dataset [9] forfog and rain and sim- ulate snow using the corruption library from [11]. We use the same split of [29], which divides the 7,441 training sam- ples into 3,740 training and 3,741 test samples. We train the Faster-RCNN using 3,741 training samples representing the Clear attribute with Swin-Transformer and ResNet50 back- bones, and evaluate it sequentially on Fog, Rain, Snow, and Clear test samples. We conduct all experiments with a batch size of 16 on 1 RTX A6000 GPU. Table 6 shows the mAP@50, the num- 1Table 6. Comparison of mAP, the number of backward and forward passes, FPS, and memory usage between baselines and our models on the continually changing KITTI datasets ( Fog → Rain → Snow → Clear). Our models improve mAP@50 by 15.1 and 11.3 for Swin-T and ResNet50 backbone, respectively, compared to Direct-Test while maintaining comparable FPS. All experiments are conducted with a batch size of 16. mAP@50 # For. Steps # Backward Steps FPS Cache Backbone Method Fog Rain Snow Clear Avg. All Fog Rain Snow Clear All Avg. Avg. Max Swin-T Direct-Test 46.9 69.5 28.7 89.6 58.7 936 0 0 0 0 0 24.7 0.4 5.5 ActMAD 53.3 78.1 41.2 90.7 65.8 936 234 234 234 234 936 16.8 0.8 21.9 Mean-Teacher 54.5 80.2 43.2 92.4 67.6 936 234 234 234 234 936 10.0 1.0 22.6 Ours 56.7 82.1 64.6 91.8 73.8 936 234 234 234 234 936 17.1 0.4 11.8 Ours-Skip 57.4 81.5 64.3 91.3 73.6 936 234 65 224 36 559 22.9 0.4 11.8 ResNet50 Direct-Test 33.4 63.5 29.8 88.6 53.8 936 0 0 0 0 0 27.7 0.8 4.3 NORM 38.4 66.4 35.9 87.3 57.0 936 0 0 0 0 0 27.7 0.8 4.3 DUA 34.8 67.7 30.9 89.0 55.6 936 0 0 0 0 0 27.7 0.8 4.3 ActMAD 40.4 66.5 42.7 84.5 58.5 936 234 234 234 234 936 18.5 1.6 22.6 Mean-Teacher 39.6 71.3 43.5 88.2 60.6 936 234 234 234 234 936 11.1 1.8 31.1 Ours 45.6 71.4 52.5 88.3 64.5 936 234 234 234 234 936 18.8 0.8 9.4 Ours-Skip 45.8 71.3 50.9 88.4 64.1 936 234 111 98 45 488 24.5 0.8 9.4 (a) GT bounding boxes. (b) Prediction results of Direct-Test. (c) Prediction results of Ours. Figure 6. Results of COCO images corrupted by Shot-Noise. In the analysis of Sec. 4.5, we conjecture that Ours largely skips adaptation in Shot-Noise domain, despite the low mAP of Direct-Test, because the model has already adapted to a similar domain, Gaussian-Noise. In (c), at the first step before adaptation to the Shot-Noise, our model already predicts ’Oven’ and ’Refrigerator’ which Direct-Test fails to detect. This results in a much faster adaptation, and Ours successfully detects various objects, including rare ones such as ’Fire Hydrants’, in the remaining images of the Shot-Noise domain. ber of forward and backward steps, FPS, and memory usage (Cache). Ours improves the mAP@50 by 15.1 and 10.7 for Swin-T and ResNet50 backbones, respectively, compared to Direct-Test. Compared to ActMAD and Mean-Teacher, our model not only improves the adaptation performance but also reduces memory usage, as we update only an ex- tremely small number of parameters of the adaptor. Further- more, using our skipping criteria of Sec. 3.4 with τ = 1.1 and β = 1.05, we can improve FPS by more than 5.8 with- out sacrificing mAP@50, resulting in much faster inference 2(a) GT bounding boxes. (b) Prediction results of Direct-Test. (c) Prediction results of Ours. Figure 7. Results for COCO images corrupted by Pixelate. In the Pixelate domain, where the model has already experienced various corruptions in a long sequence, Ours initially incorrectly detects objects. In (c), it misidentifies a bed as a couch in the first step. However, it rapidly adapts to the Pixelate domain and effectively detects various objects. Notably, even in cases whereDirect-Testcorrectly identifies objects but with low confidence, Ours detects them with much higher confidence. (a) GT bounding boxes. (b) Prediction results of Direct-Test. (c) Prediction results of Ours. Figure 8. Results for SHIFT-Discrete with continually changing attributes, foggy → rainy → dawn → night. speed compared to other TTA baselines. 39. Qualitative Results Fig. 6 and 7 and Fig. 8 show the qualitative results of Ours and Direct-Test which predict the samples without adapta- tion for COCO → COCO-C and SHIFT, respectively. 9.1. COCO → COCO-C Fig. 6 and 7 compare the prediction results for COCO im- ages corrupted. When the model encounters test images with various corruptions sequentially ( Gaussian-Noise → Shot-Noise → Impulse-Noise → Defocus-Blur → Glass- Blur → Motion-Blur → Zoom-Blur → Snow → Frost → Fog → Brightness → Contrast → Elastic-Transform → Pixelate → JPEG-Compression → Original), Fig. 6 and 7 shows the results when the test images are corrupted by Shot-Noise and Pixelate, respectively. Compared to Direct- Test, our model adapts to the current domain within a few steps, such as 100 iterations, and detects various objects very well in the remaining incoming images. 9.2. SHIFT-Discrete Fig. 8 shows the qualitative results for SHIFT-Discrete. In the SHIFT-Discrete scenario, the model encounters environ- ments sequentially, transitioning from cloudy → overcast → foggy → rainy → dawn → night → clear. Figure. 8 se- lectively shows the foggy → rainy → dawn → night se- quence, where the domain gap from the original clear envi- ronments is relatively large. Compared to Direct-Test, Ours detects various objects such as ’cars’ and ’pedestrians’ re- gardless of distribution changes. References [1] Alexander Bartler, Florian Bender, Felix Wiewel, and Bin Yang. Ttaps: Test-time adaption by aligning prototypes using self-supervision. In 2022 International Joint Conference on Neural Networks (IJCNN), pages 1–8. IEEE, 2022. 2 [2] Alexander Bartler, Andre B ¨uhler, Felix Wiewel, Mario D¨obler, and Bin Yang. Mt3: Meta test-time training for self- supervised test-time adaption. In International Conference on Artificial Intelligence and Statistics , pages 3080–3090. PMLR, 2022. 2 [3] Malik Boudiaf, Romain Mueller, Ismail Ben Ayed, and Luca Bertinetto. Parameter-free online test-time adaptation. In Proceedings of the IEEE/CVF Conference on Computer Vi- sion and Pattern Recognition, pages 8344–8353, 2022. 1 [4] Dhanajit Brahma and Piyush Rai. A probabilistic frame- work for lifelong test-time adaptation. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 3582–3591, 2023. 3 [5] Shoufa Chen, Chongjian Ge, Zhan Tong, Jiangliu Wang, Yib- ing Song, Jue Wang, and Ping Luo. Adaptformer: Adapting vision transformers for scalable visual recognition.Advances in Neural Information Processing Systems, 35:16664–16678, 2022. 3 [6] Yijin Chen, Xun Xu, Yongyi Su, and Kui Jia. Stfar: Im- proving object detection robustness at test-time by self- training with feature alignment regularization.arXiv preprint arXiv:2303.17937, 2023. 1, 2, 3 [7] Jinhong Deng, Wen Li, Yuhua Chen, and Lixin Duan. Un- biased mean teacher for cross-domain object detection. In Proceedings of the IEEE/CVF Conference on Computer Vi- sion and Pattern Recognition, pages 4091–4101, 2021. 3 [8] Andreas Geiger, Philip Lenz, Christoph Stiller, and Raquel Urtasun. Vision meets robotics: The kitti dataset. The Inter- national Journal of Robotics Research , 32(11):1231–1237, 2013. 1 [9] Shirsendu Sukanta Halder, Jean-Franc ¸ois Lalonde, and Raoul de Charette. Physics-based rendering for improving robustness to rain. In Proceedings of the IEEE/CVF Interna- tional Conference on Computer Vision, pages 10203–10212, 2019. 1 [10] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image recognition. In Proceed- ings of the IEEE conference on computer vision and pattern recognition, pages 770–778, 2016. 5, 6, 7 [11] Dan Hendrycks and Thomas Dietterich. Benchmarking neu- ral network robustness to common corruptions and perturba- tions. arXiv preprint arXiv:1903.12261, 2019. 1 [12] Junyuan Hong, Lingjuan Lyu, Jiayu Zhou, and Michael Spranger. Mecta: Memory-economic continual test-time model adaptation. In The Eleventh International Conference on Learning Representations, 2022. 3 [13] Edward J Hu, Yelong Shen, Phillip Wallis, Zeyuan Allen- Zhu, Yuanzhi Li, Shean Wang, Lu Wang, and Weizhu Chen. Lora: Low-rank adaptation of large language models. arXiv preprint arXiv:2106.09685, 2021. 3 [14] Sergey Ioffe and Christian Szegedy. Batch normalization: Accelerating deep network training by reducing internal co- variate shift. In International conference on machine learn- ing, pages 448–456. pmlr, 2015. 2 [15] Yusuke Iwasawa and Yutaka Matsuo. Test-time classifier adjustment module for modelagnostic domain generaliza- tion. In Advances in Neural Information Processing Systems (NeurIPS), 2021. 3 [16] Minguk Jang, Sae-Young Chung, and Hye Won Chung. Test- time adaptation via self-training with nearest neighbor infor- mation. In International Conference on Learning Represen- tations (ICLR), 2023. 3 [17] Sanghun Jung, Jungsoo Lee, Nanhee Kim, Amirreza Sha- ban, Byron Boots, and Jaegul Choo. Cafa: Class-aware fea- ture alignment for test-time adaptation. In Proceedings of the IEEE/CVF International Conference on Computer Vi- sion, 2023. 2, 3, 4 [18] Mehran Khodabandeh, Arash Vahdat, Mani Ranjbar, and William G Macready. A robust learning approach to domain adaptive object detection. In Proceedings of the IEEE/CVF International Conference on Computer Vision , pages 480– 490, 2019. 3 [19] Seunghyeon Kim, Jaehoon Choi, Taekyung Kim, and Chang- ick Kim. Self-training and adversarial background regular- ization for unsupervised domain adaptive one-stage object 4detection. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 6092–6101, 2019. 3 [20] Xianfeng Li, Weijie Chen, Di Xie, Shicai Yang, Peng Yuan, Shiliang Pu, and Yueting Zhuang. A free lunch for unsuper- vised domain adaptive object detection without source data. In Proceedings of the AAAI Conference on Artificial Intelli- gence, pages 8474–8481, 2021. 4 [21] Yanghao Li, Naiyan Wang, Jianping Shi, Jiaying Liu, and Xiaodi Hou. Revisiting batch normalization for practical do- main adaptation, 2017. 3 [22] Hyesu Lim, Byeonggeun Kim, Jaegul Choo, and Sungha Choi. Ttn: A domain-shift aware batch normalization in test- time adaptation, 2023. 2, 3 [23] Tsung-Yi Lin, Michael Maire, Serge Belongie, James Hays, Pietro Perona, Deva Ramanan, Piotr Doll´ar, and C Lawrence Zitnick. Microsoft coco: Common objects in context. In Computer Vision–ECCV 2014: 13th European Conference, Zurich, Switzerland, September 6-12, 2014, Proceedings, Part V 13, pages 740–755. Springer, 2014. 5 [24] Tsung-Yi Lin, Piotr Doll ´ar, Ross Girshick, Kaiming He, Bharath Hariharan, and Serge Belongie. Feature pyra- mid networks for object detection. In Proceedings of the IEEE conference on computer vision and pattern recogni- tion, pages 2117–2125, 2017. 5 [25] Yuejiang Liu, Parth Kothari, Bastien Van Delft, Baptiste Bellot-Gurlet, Taylor Mordan, and Alexandre Alahi. Ttt++: When does self-supervised test-time training fail or thrive? Advances in Neural Information Processing Systems , 34: 21808–21820, 2021. 1, 2 [26] Ze Liu, Yutong Lin, Yue Cao, Han Hu, Yixuan Wei, Zheng Zhang, Stephen Lin, and Baining Guo. Swin transformer: Hierarchical vision transformer using shifted windows. In Proceedings of the IEEE/CVF international conference on computer vision, pages 10012–10022, 2021. 2, 5, 6, 7 [27] Claudio Michaelis, Benjamin Mitzkus, Robert Geirhos, Evgenia Rusak, Oliver Bringmann, Alexander S Ecker, Matthias Bethge, and Wieland Brendel. Benchmarking ro- bustness in object detection: Autonomous driving when win- ter is coming. arXiv preprint arXiv:1907.07484, 2019. 5 [28] M Jehanzeb Mirza, Jakub Micorek, Horst Possegger, and Horst Bischof. The norm must go on: Dynamic unsuper- vised domain adaptation by normalization. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pat- tern Recognition, pages 14765–14775, 2022. 3, 6, 1 [29] Muhammad Jehanzeb Mirza, Pol Jan ´e Soneira, Wei Lin, Ma- teusz Kozinski, Horst Possegger, and Horst Bischof. Act- mad: Activation matching to align distributions for test-time- training. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition , pages 24152– 24161, 2023. 1, 2, 3, 4, 6 [30] Shuaicheng Niu, Jiaxiang Wu, Yifan Zhang, Yaofo Chen, Shijian Zheng, Peilin Zhao, and Mingkui Tan. Efficient test-time model adaptation without forgetting. In Interna- tional conference on machine learning, pages 16888–16905. PMLR, 2022. 1, 2, 3, 7 [31] Shuaicheng Niu, Jiaxiang Wu, Yifan Zhang, Zhiquan Wen, Yaofo Chen, Peilin Zhao, and Mingkui Tan. Towards stable test-time adaptation in dynamic wild world. arXiv preprint arXiv:2302.12400, 2023. 3 [32] Mario obler, Robert A Marsden, and Bin Yang. Robust mean teacher for continual and gradual test-time adaptation. In Proceedings of the IEEE/CVF Conference on Computer Vi- sion and Pattern Recognition, pages 7704–7714, 2023. 3 [33] Shaoqing Ren, Kaiming He, Ross Girshick, and Jian Sun. Faster r-cnn: Towards real-time object detection with region proposal networks. 2016. 5 [34] Aruni RoyChowdhury, Prithvijit Chakrabarty, Ashish Singh, SouYoung Jin, Huaizu Jiang, Liangliang Cao, and Erik Learned-Miller. Automatic adaptation of object detectors to new domains using self-training. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), 2019. 3 [35] Steffen Schneider, Evgenia Rusak, Luisa Eck, Oliver Bring- mann, Wieland Brendel, and Matthias Bethge. Improving robustness against common corruptions by covariate shift adaptation. Advances in neural information processing sys- tems, 33:11539–11551, 2020. 3, 6, 1 [36] Samarth Sinha, Peter Gehler, Francesco Locatello, and Bernt Schiele. Test: Test-time self-training under distribution shift. In Proceedings of the IEEE/CVF Winter Conference on Ap- plications of Computer Vision, pages 2759–2769, 2023. 1, 2, 3, 6 [37] Kihyuk Sohn, David Berthelot, Nicholas Carlini, Zizhao Zhang, Han Zhang, Colin A Raffel, Ekin Dogus Cubuk, Alexey Kurakin, and Chun-Liang Li. Fixmatch: Simplifying semi-supervised learning with consistency and confidence. Advances in neural information processing systems, 33:596– 608, 2020. 2, 6 [38] Yongyi Su, Xun Xu, and Kui Jia. Revisiting realistic test- time training: Sequential inference and adaptation by an- chored clustering. Advances in Neural Information Process- ing Systems, 35:17543–17555, 2022. 3, 4 [39] Tao Sun, Mattia Segu, Janis Postels, Yuxuan Wang, Luc Van Gool, Bernt Schiele, Federico Tombari, and Fisher Yu. Shift: a synthetic driving dataset for continuous multi-task domain adaptation. In Proceedings of the IEEE/CVF Con- ference on Computer Vision and Pattern Recognition, pages 21371–21382, 2022. 1, 5 [40] Yu Sun, Xiaolong Wang, Zhuang Liu, John Miller, Alexei Efros, and Moritz Hardt. Test-time training with self- supervision for generalization under distribution shifts. In International conference on machine learning, pages 9229– 9248. PMLR, 2020. 1, 2 [41] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszko- reit, Llion Jones, Aidan N Gomez, Łukasz Kaiser, and Illia Polosukhin. Attention is all you need. Advances in neural information processing systems, 30, 2017. 2 [42] Vibashan VS, Poojan Oza, and Vishal M Patel. Towards on- line domain adaptive object detection. In Proceedings of the IEEE/CVF Winter Conference on Applications of Computer Vision, pages 478–488, 2023. 3 [43] Dequan Wang, Evan Shelhamer, Shaoteng Liu, Bruno Ol- shausen, and Trevor Darrell. Tent: Fully test-time adaptation by entropy minimization. arXiv preprint arXiv:2006.10726, 2020. 1, 2, 3, 7 5[44] Qin Wang, Olga Fink, Luc Van Gool, and Dengxin Dai. Continual test-time domain adaptation. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 7201–7211, 2022. 3 [45] Zehao Xiao, Xiantong Zhen, Shengcai Liao, and Cees GM Snoek. Energy-based test sample adaptation for domain gen- eralization. arXiv preprint arXiv:2302.11215, 2023. 3 [46] Mengde Xu, Zheng Zhang, Han Hu, Jianfeng Wang, Lijuan Wang, Fangyun Wei, Xiang Bai, and Zicheng Liu. End- to-end semi-supervised object detection with soft teacher. Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV), 2021. 5 [47] Marvin Zhang, Sergey Levine, and Chelsea Finn. Memo: Test time robustness via adaptation and augmentation. Advances in Neural Information Processing Systems , 35: 38629–38642, 2022. 1, 2, 3 [48] Bowen Zhao, Chen Chen, and Shu-Tao Xia1. Delta: Degradation-free fully test-time adaptation. In International Conference on Learning Representations (ICLR), 2023. 2, 3 6",
      "meta_data": {
        "arxiv_id": "2312.08875v1",
        "authors": [
          "Jayeon Yoo",
          "Dongkwan Lee",
          "Inseop Chung",
          "Donghyun Kim",
          "Nojun Kwak"
        ],
        "published_date": "2023-12-12T07:13:08Z",
        "pdf_url": "https://arxiv.org/pdf/2312.08875v1.pdf"
      },
      "llm_extracted_info": {
        "main_contributions": "The paper addresses the problem of object detector performance degradation due to continually changing test-time distribution shifts, a challenge where existing Test-Time Adaptation (TTA) methods are often inefficient, prone to catastrophic forgetting, or limited to classification tasks. The main contributions include proposing an efficient online Continual Test-time Adaptation (CTA) method for object detection; introducing architecture-agnostic, lightweight adaptor modules (0.54% to 0.89% of total parameters) that enable rapid, robust adaptation and prevent catastrophic forgetting; developing a practical class-wise feature alignment method at both image and object levels to effectively resolve domain shifts; and enhancing adaptation efficiency by introducing two novel criteria that dynamically determine when to adapt, thereby skipping unnecessary updates. The approach achieves significant performance improvements (up to 4.9%p and 7.9%p in mAP for COCO->COCO-corrupted and SHIFT, respectively) while maintaining high inference speeds (around 20 FPS or higher).",
        "methodology": "The proposed method focuses on three key aspects: 'what, how, and when' to update the model. For 'what to update', it introduces shallow, lightweight adaptors (low-rank MLPs for Transformer backbones and 1x1 CNNs for ResNet backbones) in parallel to each block of the detector's backbone. Only these adaptors are updated during test time, while the original pre-trained backbone parameters are frozen to preserve task-specific knowledge and ensure efficiency. For 'how to update', the method employs Exponentially Moving Average (EMA) feature alignment to match the feature distributions of the test domain with those of the training data. This involves using pre-computed mean (µtr) and variance (Σtr) from a small subset of training data, estimating the test-time mean (µt te) via EMA, and approximating the test variance (Σte \r\n\r\n Σtr) due to small batch sizes. The adaptation loss (L = Limg + Lobj) comprises an image-level alignment (Limg) based on KL divergence between training and test image feature distributions, and a region-level class-wise alignment (Lobj) which applies weighted KL divergence to object features, filtered by background scores and adjusted by class frequency weighting (wk,t) to mitigate class imbalance. For 'when to update', two novel, resource-free criteria are introduced to dynamically skip or resume adaptation: 1) if the current image-level distribution gap (Limg) relative to an in-domain distribution gap (Din KL) exceeds a threshold (τ1=1.1); and 2) if the current Limg relative to its exponentially moving average (Ltema) suddenly increases beyond a threshold (τ2=1.05). Adaptation proceeds if at least one criterion is met.",
        "experimental_setup": "The research employs Faster-RCNN models with ResNet50 and Swin-Tiny backbones, both integrated with FPN. Experiments were conducted across three primary scenarios: 1) COCO \r\n\r\n COCO-C, where models are trained on MS-COCO and sequentially evaluated on COCO-C (15 realistic corruptions applied to COCO validation set) to simulate continuous and drastic test domain changes, followed by evaluation on the original COCO validation set ('Org.') to assess catastrophic forgetting. 2) SHIFT-(Discrete / Continuous), a synthetic driving image dataset with 6 classes. SHIFT-Discrete simulates drastic changes by sequential evaluation across attributes (e.g., cloudy \r\n\r\n overcast \r\n\r\n foggy), while SHIFT-Continuous evaluates on sequences with gradual transitions (e.g., clear to foggy/rainy and back). Additionally, experiments were performed on the KITTI dataset (supplementary) for driving scenes (Fog \r\n\r\n Rain \r\n\r\n Snow \r\n\r\n Clear sequence). Baselines included Direct-Test (no adaptation), ActMAD, Mean-Teacher (reproducing TeST), NORM, and DUA. Performance was evaluated using mAP (and mAP@50 for KITTI), along with efficiency metrics such as the number of forward and backward passes, Frames Per Second (FPS), and memory usage (Cache). Key hyperparameters were a learning rate of 0.001 for the SGD optimizer, EMA α of 0.01, and thresholds τ1=1.1 and τ2=1.05, with a batch size of 4 (16 for KITTI).",
        "limitations": "1. The method approximates test feature variance by assuming it is similar to the training variance (\r\n\r\n_te \r\n\r\n \r\n\r\n_tr) due to small batch sizes, which might introduce inaccuracies if the test domain's true feature variance significantly deviates from the training data's.2. The dynamic 'when to update' criteria rely on fixed thresholds (\r\n1=1.1, \r\n2=1.05). While these values demonstrated effectiveness across tested scenarios and backbones, their optimality may be sensitive and potentially require tuning for highly diverse or novel domain shift patterns.3. The approach is primarily validated on common image corruptions and driving scenarios. Its generalizability and performance may vary for other distinct types of domain shifts or more complex, less-studied real-world environments.",
        "future_research_directions": "Not mentioned"
      }
    },
    {
      "title": "AnyDA: Anytime Domain Adaptation"
    },
    {
      "title": "Extrapolative Continuous-time Bayesian Neural Network for Fast Training-free Test-time Adaptation"
    },
    {
      "title": "Sequential Domain Adaptation by Synthesizing Distributionally Robust Experts",
      "abstract": "Least squares estimators, when trained on a few target domain samples, may\npredict poorly. Supervised domain adaptation aims to improve the predictive\naccuracy by exploiting additional labeled training samples from a source\ndistribution that is close to the target distribution. Given available data, we\ninvestigate novel strategies to synthesize a family of least squares estimator\nexperts that are robust with regard to moment conditions. When these moment\nconditions are specified using Kullback-Leibler or Wasserstein-type\ndivergences, we can find the robust estimators efficiently using convex\noptimization. We use the Bernstein online aggregation algorithm on the proposed\nfamily of robust experts to generate predictions for the sequential stream of\ntarget test samples. Numerical experiments on real data show that the robust\nstrategies may outperform non-robust interpolations of the empirical least\nsquares estimators.",
      "full_text": "Sequential Domain Adaptation by Synthesizing Distributionally Robust Experts Bahar Taskesen1 Man-Chung Yue2 Jos´e Blanchet 3 Daniel Kuhn 1 Viet Anh Nguyen3 4 Abstract Least squares estimators, when trained on a few target domain samples, may predict poorly. Su- pervised domain adaptation aims to improve the predictive accuracy by exploiting additional la- beled training samples from a source distribu- tion that is close to the target distribution. Given available data, we investigate novel strategies to synthesize a family of least squares estimator ex- perts that are robust with regard to moment condi- tions. When these moment conditions are speci- ﬁed using Kullback-Leibler or Wasserstein-type divergences, we can ﬁnd the robust estimators efﬁciently using convex optimization. We use the Bernstein online aggregation algorithm on the proposed family of robust experts to gener- ate predictions for the sequential stream of target test samples. Numerical experiments on real data show that the robust strategies may outperform non-robust interpolations of the empirical least squares estimators. 1. Introduction A natural approach to improving predictive performance in data-scarce tasks involves translating informative sig- nals from a data-abundant source domain to the data-scarce target domain. This transfer of knowledge is commonly referred to as domain adaptation or transfer learning, and it is increasingly applied in a wide range of settings, see for example Wilson & Cook (2020); Chu & Wang (2018); Weiss et al. (2016) and Redko et al. (2019). We consider the supervised domain adaptation setting with scarce labeled target data. The key challenge here is the 1Risk Analytics and Optimization Chair, Ecole Polytech- nique F´ed´erale de Lausanne 2Department of Applied Mathemat- ics, The Hong Kong Polytechnic University 3Department of Man- agement Science and Engineering, Stanford University 4VinAI Research, Vietnam. Correspondence to: Bahar Taskesen <ba- har.taskesen@epﬂ.ch>. Proceedings of the 38 th International Conference on Machine Learning, PMLR 139, 2021. Copyright 2021 by the author(s). absence of meaningful data to tune any parameters. How- ever, in many practically relevant applications, new data will arrive sequentially to enrich the information on the target domain. In this case, many online algorithms can be utilized to adaptively learn the best predictor on the target domain, which also guarantee optimal asymptotic regrets (Lattimore & Szepesv´ari, 2020). In this paper, we take a pragmatic approach to resolve a speciﬁc setup of the domain adaptation problem. We as- sume access to a scarce labelled target data, and the future target data arrives sequentially. For example, consider un- derstanding the dynamics of ride-sharing platforms requires insights about the demand and supply from both sides of the market. These insights are signalled through the ride fares, which can be explained by characteristics such as the travel distances and the origin-destination pairs of the trips, the time of the day as well as the weather conditions. The capability to correctly predict ride fares directly translates into improved proﬁt forecasts, and thus it vitally supports the growth of new-coming platforms. In a competitive mar- ket, a follower (e.g., Lyft) needs to target a slightly different market segment than the leader (e.g., Uber) who had entered earlier. Thus, the demand and supply characteristics for the follower may differ from those of the leader. Nevertheless, as both platforms provide on-demand transportation, it is reasonable to assume that their supply and demand dynam- ics are similar. The follower, who possesses limited data, can query demand on the leader’s platform to collect data in order to leap forward in its predictive precision. Our approach to solve this problem is illustrated in Figure 1 and it consists of two components: 1. Expert Generation Module: This module generates a set of competitive experts Eby ﬁne-tuning the explana- tory power of the source domain data and harnessing the signal guidance from the scarce target domain data. 2. Expert Aggregation Module: Acting on the sequential arrival of the unseen target data, this module aggregates the predictive capability of the generated experts via an online aggregation mechanism. In this work we will use the Bernstein Online Aggregation mechanism. We will propose two ways to generate the experts. The ﬁrst arXiv:2106.00322v1  [cs.LG]  1 Jun 2021U 1tT2`i :2M2`\u001ciBQM { aQHp2 /Bbi`B#miBQM\u001cHHv `Q#mbi H2\u001cbi b[m\u001c`2b 1tT2`i \u001b;;`2;\u001ciBQM a2i Q7 2tT2`ib { \u0000 1 ,..., \u0000 |E| }  \"2`Mbi2BM PMHBM2 \u001b;;`2;\u001ciBQM *mKmH\u001ciBp2 HQbb aQm`+2 /\u001ci\u001c ( bx i , by i ) N S i =1 6`\u001cK2 \u001c /BbiBM+i /Bbi`B#miBQM\u001cH `2;BQM _2T2\u001ci |E| iBK2b h\u001c`;2i /\u001ci\u001c ( bx j , by j ) N T j =1 lMb22M b2[m2MiB\u001cH i\u001c`;2i /\u001ci\u001c ( x j ,y j ) J j =1 Figure 1.The architecture of our framework for supervised domain adaptation when the unseen target test samples arrive sequentially. approach generates experts corresponding to optimal deci- sions along a path, with the intention to interpolate between the source and the target distributions. We will consider two types of trajectories, guided by either the Kullback-Leibler or the Wasserstein divergence. The second approach gen- erates distribution regions around both the source and the target. The intersection of these regions is used to generate distributionally robust experts. The geometrical intuition is to ﬁnd the “direction” induced by the aforementioned diver- gences, in which the source data can explain the target data. Once the experts are deployed, the aggregation mechanism is executed without re-adapting the experts. Our ultimate goal is to ensure a competitive performance in the short term and not in the asymptotic regime when the number of test samples from the target domain tends to in- ﬁnity. Indeed, as soon as the target sample size is sufﬁcient, training the machine learning model on all available target data becomes more attractive. From a short term horizon benchmark, our approach offers an appealingwarm start for online training procedure, and it may also lead to a faster convergence rate depending on the underlying algorithm. Contributions. Our paper explores the expert generation problem in the context of supervised domain adaptation. • We introduce a novel framework to synthesize a family of robust least squares experts by altering various moment- based distribution sets. These sets gradually interpolate from the source information to the target information, capturing different belief levels on the explanatory power of the source domain onto the target domain. • We present two intuitive strategies to construct the sets of moment information, namely the “Interpolate, then Robus- tify” and the “Surround, then Intersect” strategies. Both strategies are simply characterized by two parameters rep- resenting the aforementioned explanatory power of belief of the source domain and the level of desired robustness. • We show that when the moment information is prescribed using a Kullback-Leibler or a Wasserstein-type divergence, the experts are efﬁciently formed by solving convex opti- mization problems, that can even be solved by a ﬁrst-order gradient descent algorithm or off-the-shelf solvers. This paper is structured as follows. Section 2 delineates the problem setup and describes in details two common strate- gies to generate experts: the convex combination and the reweighting strategies. Section 3 introduces our framework to generate experts, while Section 4 and 5 dive into details about our “Interpolate, then Robustify” and our “Surround, then Intersect” strategies, respectively. Section 6 demon- strates experimentally that the proposed robust strategies systematically outperform non-robust interpolations of the empirical least squares estimators. Literature Review. Domain adaptation arises in various ap- plications including natural language processing (Søgaard, 2013; Li, 2012; Jiang & Zhai, 2007; Blitzer et al., 2006), sur- vival analysis (Li et al., 2016) and computer vision (Wang & Deng, 2018; Csurka, 2017). Domain adaptation methods can be classiﬁed into three categories. Unsupervised do- main adaptation only requires unlabelled target data, but in large amounts (Ghifary et al., 2016; Baktashmotlagh et al., 2013; Ganin & Lempitsky, 2015; Wang et al., 2020; Long et al., 2016; Ben-David et al., 2007; Courty et al., 2017). Semi-supervised domain adaptation requires labelled target data (Yao et al., 2015; Kumar et al., 2010; Sindhwani et al., 2005; Lopez-Paz et al., 2012; Saha et al., 2011; de Mathelin et al., 2020; Sun et al., 2011). Finally, supervised domain adaptation only requires scarce labelled target data (Motiian et al., 2017b;a; Tzeng et al., 2015; Koniusz et al., 2017). If the target data is scarce and label information is available, supervised domain adaptation outperforms unsupervised domain adaptation (Motiian et al., 2017b). The domain adaptation literature further ramiﬁes by imposing different distributional assumptions into covariate shift (Shimodaira, 2000; Sugiyama et al., 2008) or label shift (Lipton et al., 2018; Azizzadenesheli et al., 2019). The domain adaptation literature for regression problems focuses primarily on instance-based reweighting strate- gies (Garcke & Vanck, 2014; Sugiyama et al., 2008; Garcke & Vanck, 2014; Huang et al., 2006; Cortes & Mohri, 2014; Chen et al., 2016), which aim to minimize some distance between the source and target distributions. Most of the instance-based methods solve an optimization problem to ﬁnd the weights of the instances (Garcke & Vanck, 2014; Cortes et al., 2019), which may be computationally expen-sive when data is abundant. Other approaches rely on deep learning models to minimize the discrepancy between the domain distributions (Zhao et al., 2018; Richard et al., 2020). The literature on regression for domain adaptation also ex- tends towards boosting-based methods (Pardoe & Stone, 2010), and deep learning methods (Salaken et al., 2019). Our paper also uses ideas and techniques from robust op- timization and adversarial training, which have attracted considerable attention in machine learning (Namkoong & Duchi, 2016; Gao et al., 2018; Blanchet et al., 2019; Nguyen et al., 2019a). Robust optimization for least squares problem with uncertain data was studied in Ghaoui & Lebret (1997). Distributionally robust optimization with moment ambigu- ity sets was proposed in Delage & Ye (2010) and extended in Goh & Sim (2010) and Kuhn et al. (2019). Ambigu- ity sets prescribed by divergences were previously used to robustify Bayes classiﬁcation (Nguyen et al., 2019b; 2020). Our work is also similar to Chen et al. (2016) that con- sider unsupervised domain adaptation regression, and Wang et al. (2020) that consider robust domain adaption for the classiﬁcation setting. Notation. We use Id to denotes the identity matrix in Rd. The set ofp-by-ppositive (semi-)deﬁnite matrices is denoted by Sp ++ (Sp +). All proofs are relegated to the Appendix. 2. Problem Statement and Background We consider a generic linear regression setting, in which X is a d-dimensional covariate and Y is a univariate response variable. In the context of supervised domain adaptation, we have access to the source domain data(ˆxi,ˆyi)NS i=1 consisting of NS labelled samples drawn from the source distribution. In addition, we are given a limited number of NT labelled samples (ˆxj,ˆyj)NT j=1 from the target distribution. Our goal is to predict the responses of the test samples (xj,yj)J j=1, which are drawn from the target distribution and arrive se- quentially. To this end, we will construct several experts. In the linear regression setting, each expert is character- ized by a vector β ∈Rd. Given a covariate-response pair (x,y) ∈Rd×R, we use the square loss function to measure the mismatch between the expert’s predictionβ⊤xand the actual response y. Using the target domain data (ˆxi,ˆyi)NT i=1, one approach is to solve the ridge regression problem min β∈Rd 1 NT NT∑ j=1 (β⊤ˆxj −ˆyj)2 + η∥β∥2 2 for some η≥0 to obtain the empirical target predictor ˆβT =   1 NT NT∑ j=1 ˆxjˆx⊤ j + ηId   −1   1 NT NT∑ j=1 ˆxjˆyj  . When NT is small, however, the empirical target predictor may perform poorly on the future target data (xj,yj)J j=1. If the source domain distribution is sufﬁciently close to the target domain distribution, it is expedient to exploit the avail- able information in the source domain data to construct bet- ter predictors for the target domain data. With this promise, one can synthesize several predictors to form an ensemble of experts, and one can apply an online aggregation scheme to predict on the unseen target data. We now ﬁrst describe several interpolation schemes to generate experts. Convex Combination Strategy. Denote by ˆβS the empiri- cal source predictor, which is obtained by solving the ridge regression problem on the source data. The convex com- bination strategy generates predictors by forming convex combinations between ˆβS and ˆβT. More precisely, for any λ∈[0,1] a new predictor is synthesized by setting ˆβλ = λβS + (1 −λ)βT. The parameter λrepresents our belief in the explanatory power of the source domain data: if λ = 0 , the source domain has no power to explain the target domain, and we recover ˆβ0 = βT, the empirical target predictor. If λ= 1, the source domain has an absolute predictive power on the target domain, and it is beneﬁcial to use ˆβ1 = ˆβS because the sample size NS is large. Discretizing λ in the range [0,1] forms a family of experts E. Reweighting Strategy. Reweighting samples is a common strategy in domain adaptation, transfer learning and adver- sarial training. Garcke & Vanck (2014) synthesize experts, for example, by solving min β∈Rd NS∑ i=1 wh,i(β⊤ˆxi −ˆyi)2 + NT∑ j=1 (β⊤ˆxj −ˆyj)2 + η∥β∥2 2 for some non-negative weights wh,i determined via a Gaus- sian kernel with bandwidth h> 0 of the form wh,i = NS∑ l=1 αlexp ( −∥ˆxi −ˆxl∥2 2 + (ˆyi −ˆyl)2 h2 ) for i = 1,...,N S. Here, the parameter vector α ∈RNS + solves the exponential cone optimization problem max NT∑ j=1 log (NS∑ l=1 αlexp ( −∥ˆxj −ˆxl∥2 2 + (ˆyj −ˆyl)2 h2 )) s.t. NS∑ i=1 NS∑ l=1 αlexp ( −∥ˆxi −ˆxl∥2 2 + (ˆyi −ˆyl)2 h2 ) =NS. The predictor βh, parametrized by the kernel weight h, that solves the reweighted ridge regression problem has the form (NT∑ j=1 ˆxjˆx⊤ j + NS∑ i=1 wiˆxiˆx⊤ i +ηId )−1(NT∑ j=1 ˆxjˆyj+ NS∑ i=1 wiˆxiˆyi ) .Discretizing the bandwidth hforms a family of experts E. Bernstein Online Aggregation (BOA). We now give a brief overview on the BOA algorithm, which is a recur- sive expert aggregation procedure for sequential predic- tion (Cesa-Bianchi & Lugosi, 2006). For a given set of ex- perts E= {β1,...,β |E|}and an incumbent weight πk,j−1 for expert k at time j −1, this algorithm aggregates the individual expert’s predictions linearly based on the arrival of the input data (xj,yj) as ∑|E| k=1 πk,jβ⊤ k xj. The weights of the experts are updated using the exponential rule πk,j = exp(−υ(1 + υLk,j)Lk,j)πk,j−1 ∑|E| k=1 exp(−υ(1 + υLkj)Lk,j)πk,j−1 , where υ >0 is the learning rate and Lk,j=(β⊤ k xj−yj)2−∑|E| k=1(β⊤ k xj − yj)2πk,j−1. This algorithm is initialized with weights πk,0 ≥0 satisfying ∑|E| k=1 πk,0 = 1 . The cumulative loss for the stream of test data (xj,yj)J j=1 is J∑ j=1   |E|∑ k=1 πk,jβ⊤ k xj −yj   2 . (1) For the square loss, the BOA procedure is optimal for the model selection aggregation problem, that is, the excess risk of its batch version achieves the fast rate of convergence log(|E|)/J in deviation; see Wintenberger (2017). 3. Predictor Generation via Distributionally Robust Linear Regression We now specify our framework to generate the set of com- petitive experts Efor future prediction. Our construction is based on the premises that the source domain carries the ex- planatory power on the target domain to a certain extent and that the scarce target data can provide directional guidance to pull information from the source data. Moreover, we also leverage ideas from distributionally robust optimization and adversarial training, which have been shown to signiﬁcantly improve the out-of-sample predictive performance (Duchi & Namkoong, 2018; Mohajerin Esfahani & Kuhn, 2018; Blanchet et al., 2019; Gao, 2020; Lam, 2019). With this in mind, our expert generation scheme blends two elements: a distributional probing strategy and a robust estimation procedure. The distributional probing strategy frames the distribution set B, and then each expert is con- structed by solving a distributionally robust least squares estimation problem of the form inf β∈Rd sup Q∈B EQ[(β⊤X−Y)2], (2) where Q is a joint distribution over (X,Y ). Generating a collection of distribution sets B in a systematic manner and solving (2) for each such set will form a family of experts E. In a purely data-driven setting with no additional in- formation, it is attractive to probe into the distribu- tional regions in between the empirical source distribution ˆPS = N−1 S ∑NS i=1 δ(ˆxi,ˆyi) and the empirical target distribu- tion ˆPT = N−1 T ∑NT j=1 δ(ˆxj,ˆyj). Because probability distri- butions reside in inﬁnite-dimensional spaces, framing B in between ˆPS and ˆPT is a non-trivial task. Fortunately, be- cause the expected square loss only depends on the ﬁrst two moments of the joint distribution of (X,Y ), it sufﬁces to prescribe B using a ﬁnite parametrization of distributional moments. To this end, let p= d+1 represent the dimension of the joint vector (X,Y ). For a given set U on the space of mean vectors and covariance matricesRp×Sp +, we consider B as the lifted distribution set that contains all distributions whose moments belong to U, that is, B = {Q ∈M(Rp) : Q ∼(µ,Σ), (µ,Σ) ∈U}, where M(Rp) denotes the set of all distributions on Rp, and the notation Q ∼(µ,Σ) expresses that Q has mean µ and covariance matrix Σ. It is convenient to construct the moment information set U using a divergence on Rp ×Sp +. Deﬁnition 3.1 (Divergence). A divergence ψon Rp ×Sp + satisﬁes the following properties: • non-negativity: for any (µ,Σ), (ˆµ,ˆΣ) ∈Rp ×Sp +, we have ψ((µ,Σ) ∥(ˆµ,ˆΣ)) ≥0, • indiscernability: ψ((µ,Σ)∥(ˆµ,ˆΣ))=0 implies (µ,Σ)= (ˆµ,ˆΣ). In this paper, we will explore two divergences in the space of mean vectors and covariance matrices that are motivated by popular measures of dissimilarity between distributions. The divergenceD is motivated by the Kullback-Leibler (KL) divergence. Deﬁnition 3.2 (Kullback-Leibler-type divergence). The divergence D from tuple (µ,Σ) ∈ Rp ×Sp ++ to tuple (ˆµ,ˆΣ) ∈Rp ×Sp ++ amounts to D ( (µ,Σ) ∥(ˆµ,ˆΣ) ) ≜ (ˆµ−µ)⊤ˆΣ−1(ˆµ−µ)+Tr [ ΣˆΣ−1] −log det(ΣˆΣ−1)−p. In fact D is equivalent to the KL divergence between two non-degenerate Gaussian distributions N(µ,Σ) and N(ˆµ,ˆΣ) (up to a factor of 2). As a consequence, D is non-negative, and it collapses to 0 if and only if Σ = ˆΣ and µ = ˆµ. We can also show that D is afﬁne-invariant. However, we emphasize that D is not symmetric and D ( (µ,Σ) ∥(ˆµ,ˆΣ) ) ̸= D ( (ˆµ,ˆΣ) ∥(µ,Σ) ) in general. We also study the divergenceW which is motivated by the Wasserstein distance.Deﬁnition 3.3 (Wasserstein-type divergence). The diver- gence W between two tuples (µ,Σ) ∈ Rp ×Sp + and (ˆµ,ˆΣ) ∈Rp ×Sp + amounts to W ( (µ,Σ)∥(ˆµ,ˆΣ) ) ≜∥µ−ˆµ∥2 2+Tr [ Σ+ˆΣ−2 (ˆΣ 1 2 ΣˆΣ 1 2 )1 2 ] . The divergence W coincides with the squared type-2 Wasserstein distance between two Gaussian distributions N(µ,Σ) and N(ˆµ,ˆΣ) (Givens & Shortt, 1984). One can readily show that W is non-negative, and it vanishes if and only if (µ,Σ)=(ˆµ,ˆΣ). Thus, W is a symmetric divergence. In Sections 4 and 5 we examine in detail two strategies to frame U and its corresponding distribution set B in a principled manner, and we devise optimization techniques to solve the resulting robust estimation problems. 4. “Interpolate, then Robustify” Strategy “Interpolate, then Robustify” (IR) is an intuitive strategy to systematically probe into distributional regions between ˆPS and ˆPT. Let (ˆµS,ˆΣS) be the empirical mean vector and covariance matrix of ˆPS, that is, ˆµS = 1 NS NS∑ i=1 (ˆxi ˆyi ) , ˆΣS = 1 NS NS∑ i=1 (ˆxi ˆyi )(ˆxi ˆyi )⊤ −ˆµSˆµ⊤ S , and let (ˆµT,ˆΣT) be deﬁned analogously for ˆPT. The IR strategy applies repeatedly the following two steps to gen- erate distribution sets. First, interpolate between (ˆµS,ˆΣS) and (ˆµT,ˆΣT) to obtain a new pair (ˆµλ,ˆΣλ) parametrized by λ∈[0,1]. Second, construct a moment set Uλ,ρ as a ball of radius ρcircumscribing the pair (ˆµλ,ˆΣλ), then lift the moment set Uλ,ρ to the corresponding distribution set Bλ,ρ. More speciﬁcally, (ˆµλ,ˆΣλ) is the ψ-barycenter between (ˆµS,ˆΣS) and (ˆµT,ˆΣT), which is obtained by solving min µ∈Rp,Σ∈Sp + λψ((µ,Σ)∥(ˆµS,ˆΣS))+ (1−λ)ψ((µ,Σ)∥(ˆµT,ˆΣT)). (3) ⇢ ( bµ S , b⌃ S ) ( bµ \u0000 , b⌃ \u0000 ) ( bµ T , b⌃ T ) Figure 2.The dashed curve shows the barycenter interpolations parametrized by λ∈[0,1]. Ellipses represent Uλ,ρ at different λ. Then, we employ the divergence ψ to construct an uncer- tainty set Uλ,ρ in the mean-covariance matrix space as Uλ,ρ ≜ { (µ,Σ) ∈Rp ×Sp + : ψ((µ,Σ)∥(ˆµλ,ˆΣλ)) ≤ρ } . The outlined procedure is illustrated in Figure 2. An expert is now obtained by solving the distributionally robust least squares problem (2) with respect to the distribution set Bλ,ρ = {Q ∈M(Rp) : Q ∼(µ,Σ),(µ,Σ) ∈Uλ,ρ}. Notice that in this strategy the parameter λ ∈[0,1] char- acterizes the explanatory power of the source domain to the target domain: if λ = 0 , then (ˆµλ,ˆΣλ) = ( ˆµT,ˆΣT), and if λ = 1, then (ˆµλ,ˆΣλ) = ( ˆµS,ˆΣS). Thus, as λde- creases, (ˆµλ,ˆΣλ) is moving farther away from the source information (ˆµS,ˆΣS), and (ˆµλ,ˆΣλ) is pulled towards the target information (ˆµT,ˆΣT). The choice of the divergence ψinﬂuences both the barycen- ter problem (3) and the formation of the set Uλ,ρ. Next, we study the special case of the IR strategy with the KL-type divergence and the Wasserstein-type divergence. 4.1. Kullback-Leibler-type Divergence The KL-type divergence D in Deﬁnition 3.2 is not sym- metric. Hence, it is worthwhile to note that the barycenter problem (3) optimizes over (µ,Σ) being placed in the ﬁrst argument of D, and that the set Uλ,ρ is also deﬁned with the pair (µ,Σ) being placed in the ﬁrst argument. Under the divergence D, the barycenter (ˆµλ,ˆΣλ) admits a closed form expression. This fact is well-known in the ﬁeld of KL fusion of Gaussian distributions (Battistelli et al., 2013). Proposition 4.1 (KL barycenter). Suppose that ψ is the KL-type divergence. If ˆΣS,ˆΣT ≻0, then (ˆµλ,ˆΣλ) is the minimizer of the barycenter problem (3) with ˆΣλ = (λˆΣ−1 S + (1 −λ)ˆΣ−1 T )−1 ≻0, ˆµλ = ˆΣλ ( λˆΣ−1 S ˆµS + (1 −λ)ˆΣ−1 T ˆµT ) . For a given λ∈[0,1] and ρ≥0, the corresponding IR-KL expert is obtained by solving min β∈Rd { fλ,ρ(β) ≜ sup Q∈Bλ,ρ EQ[(β⊤X−Y)2] } . (4) Problem (4) can be efﬁciently solved using a gradient- descent algorithm. To do this, the next proposition estab- lishes the relevant properties of fλ,ρ. Proposition 4.2 (Properties of fλ,ρ). The function fλ,ρ is convex and continuously differentiable with ∇fλ,ρ(β)= 2κ⋆ ( ω2 ˆΣλw+(κ⋆−ω1)(ˆΣλ+ˆµλˆµ⊤ λ)w ) 1:d (κ⋆ −ω1)2 , where w = [β⊤,−1]⊤, ω1 = w⊤ˆΣλw, ω2 = (w⊤ˆµ)2 and κ⋆ ∈(ω1,ω1 ( 1 + 2ρ+ √1 + 4ρω2 ) /(2ρ)] is the unique solution of the equation ρ= (κ−ω1)−2ω1ω2 + (κ−ω1)−1ω1 + log(1−κ−1ω1).Furthermore, fλ,ρ is locally smooth at any β ∈Rd, i.e., there exist constants Cβ,ϵβ >0 such that for any β′∈Rd with ∥β′−β∥2 ≤ϵβ, we have ∥∇fλ,ρ(β′)−∇fλ,ρ(β)∥2 ≤ Cβ∥β′−β∥2. Thanks to Proposition 4.2, we can apply the adaptive gradi- ent method to solve problem(4) to global optimality, and the algorithm enjoys a sublinear rate |fλ,ρ( ¯βk) −fλ,ρ(β⋆ λ,ρ)|≤ O(k−1), where ¯βk is a certain average of the iterates, and β⋆ λ,ρ is an optimal solution of (4). The algorithm and its guarantees are detailed in Malitsky & Mishchenko (2019). 4.2. Wasserstein-type Divergence Under the divergence W in Deﬁnition 3.3, problem (3) re- sembles the Wasserstein barycenter in the space of Gaussian distributions. The result from Agueh & Carlier (2011, §6.2) implies that the barycenter (ˆµλ,ˆΣλ) admits a closed form expression following the McCann’s interpolant (McCann, 1997, Example 1.7). Proposition 4.3 (Wasserstein interpolation). Suppose that ψ is the Wasserstein-type divergence. If ˆΣS ≻ 0, then (ˆµλ,ˆΣλ) is the minimizer of problem (3) with ˆµλ = λˆµS + (1 −λ)ˆµT, ˆΣλ = (λIp + (1 −λ)L)ˆΣS(λIp + (1 −λ)L), where L= ˆΣ 1 2 T(ˆΣ 1 2 T ˆΣS ˆΣ 1 2 T)−1 2 ˆΣ 1 2 T. For a given λ∈[0,1] and ρ≥0, we obtain the correspond- ing IR-Wasserstein expert by solving a conic program using off-the-shelf solvers such as MOSEK ApS (2019). Proposition 4.4 (IR-Wasserstein expert). Suppose that ψ is the Wasserstein-type divergence. Problem (2) with B ≡ Bλ,ρ is equivalent to the second order cone program min β∈Rd (ˆΣλ + ˆµλˆµ⊤ λ) 1 2 [β −1 ] 2 + √ρ  [β −1 ] 2 . 5. “Surround, then Intersect” Strategy “Surround, then Intersect” (SI) probes naturally into the distributional space by intersecting two balls centered at the empirical moments. More speciﬁcally, this strategy cir- cumscribes (ˆµS,ˆΣS) (respectively, (ˆµT,ˆΣT)) with a ball of radius ρS (respectively, ρT) using the ψ-divergence. Conse- quentially, the moment information set UρS,ρT in the mean vector-covariance matrix space is deﬁned as UρS,ρT ≜    (µ,Σ) ∈Rp ×Sp + such that: ψ((µ,Σ)∥(ˆµS,ˆΣS)) ≤ρS ψ((µ,Σ)∥(ˆµT,ˆΣT)) ≤ρT Σ + µµ⊤⪰εIp    , where the small constant ε> 0 improves numerical stability. This construction is graphically illustrated in Figure 3. An expert is now obtained by solving the distributionally robust least squares problem (2) subject to the distributional set BρS,ρT = {Q ∈M(Rp) : Q ∼(µ,Σ), (µ,Σ) ∈UρS,ρT}. Note that BρS,ρT is well-deﬁned only if the radii (ρS,ρT) are sufﬁciently large so that the intersection of the two balls becomes non-empty. A sensible approach to set these pa- rameters is to ﬁx ρS and to ﬁnd a sufﬁciently large ρT so that UρS,ρT is non-empty. In this way, the SI strategy char- acterizes the explanatory power of the source domain to the target domain by the radius ρS: if ρS = 0 then UρS,ρT becomes a singleton {(ˆµS,ˆΣS)}, representing the belief that the source domain possess absolute explanatory power onto the target domain. As ρS increases, UρS,ρT is gradu- ally pulled towards the empirical target moments (ˆµT,ˆΣT). Next, we study the special case of the SI strategy with the KL-type divergence and the Wasserstein-type divergence. 5.1. Kullback-Leibler-type Divergence Recall that D is asymmetric and (µ,Σ) is the ﬁrst argument of D in the deﬁnition of UρS,ρT. We ﬁrst study conditions on ρT under which the ambiguity set BρS,ρT is non-empty. Proposition 5.1 (Minimum radius). Suppose that ψis the KL-type divergence. For any ρS > 0 the sets UρS,ρT and BρS,ρT are non-empty if ρT ≥D((ˆµγ⋆,ˆΣγ⋆) ∥(ˆµT,ˆΣT)), where γ⋆ is a maximizer of sup D((ˆµγ,ˆΣγ)∥(ˆµS,ˆΣS))+D((ˆµγ,ˆΣγ)∥(ˆµT,ˆΣT))−γρS s.t. γ ∈R+,ˆΣγ = (1 + γ)(γˆΣ−1 S + ˆΣ−1 T )−1 ∈Sp +, ˆµγ = ˆΣγ(γˆΣ−1 S ˆµS + ˆΣ−1 T ˆµT)/(1 + γ) ∈Rp The above optimization problem is effectively one- dimensional and can therefore be solved by bisection on γ. The next theorem asserts that the SI-KL experts are formed by solving a semideﬁnite program. Theorem 5.2 (SI-KL Expert). Suppose that ψ is the KL- type divergence and B ≡BρS,ρT is non-empty. Then β⋆ = (M⋆ XX)−1M⋆ XY solves problem (2), where (M⋆ XX,M⋆ XY ) is a solution of the convex semideﬁnite program sup τ s.t. M XX ∈Rd×d, MXY ∈Rd×1, MYY ∈R τ ∈R+, µ∈Rp, M∈Sp ++,t ∈R+ ˆµ⊤ k ˆΣ−1 k ˆµk −2ˆµ⊤ k ˆΣ−1 k µ+ Tr [ MˆΣ−1 k ] − log det(MˆΣ−1 k )−log(1−t) −p≤ρk ∀k∈{S,T} [M µ µ⊤ t ] ⪰0, [MXX MXY M⊤ XY MYY −τ ] ⪰0 M = [MXX MXY M⊤ XY MYY ] ⪰εIp. 5.2. Wasserstein-type Divergence The space Rp ×Sp + can be endowed with a distance in- herited from the Wasserstein distance between Gaussian⇢ T ⇢ S ( bµ S , b⌃ S ) ( bµ T , b⌃ T ) ( bµ S , b⌃ S ) ( bµ T , b⌃ T ) ( bµ S , b⌃ S ) ( bµ T , b⌃ T ) ( bµ S , b⌃ S ) ( bµ T , b⌃ T ) Figure 3.Varying (ρS,ρT) frames different moment sets UρS,ρT (hatched regions). The radius ρS increases from left to right. distribution. For any ρS > 0, the minimum radius for ρT that makes BρS,ρT non-empty is known in closed form. Proposition 5.3 (Minimum radius). Suppose that ψis the Wasserstein-type divergence. For anyρS >0 the sets UρS,ρT and BρS,ρT are non-empty if ρT ≥ (√ W((ˆµS,ˆΣS) ∥(ˆµT,ˆΣT)) −√ρS )2 . The next theorem asserts that the SI-Wasserstein experts are constructed by solving a semideﬁnite program. Theorem 5.4 (SI-Wasserstein expert). Suppose that ψ is the Wasserstein-type divergence and B ≡BρS,ρT is non- empty. Then β⋆ = (M⋆ XX)−1M⋆ XY solves problem (2), where (M⋆ XX,M⋆ XY ) is a solution of the linear semidef- inite program sup τ s.t. M XX ∈Rd×d,MXY ∈Rd×1,MYY ∈R τ ∈R+,µ ∈Rp,M,H ∈Sp +,CS,CT ∈Rp×p ∥ˆµk∥2 2−2ˆµ⊤ kµ+Tr [ M+ ˆΣk−2Ck ] ≤ρk[H C k C⊤ k ˆΣk ] ⪰0   k∈{S,T} [M −H µ µ⊤ ] ⪰0 [MXX MXY M⊤ XY MYY −τ ] ⪰0, M= [MXX MXY M⊤ XY MYY ] ⪰εIp. 6. Numerical Experiments The second-order cone and semideﬁnite programs are mod- elled in MATLAB via Y ALMIP (L¨ofberg, 2004) and solved with MOSEK ApS (2019). All experiments are run on an Intel i7-8700 CPU (3.2 GHz) computer with 16GB RAM. The corresponding codes are available at https: //github.com/RAO-EPFL/DR-DA.git. We now aim to assess the performance of experts and demon- strate the effects of robustness. In all experiments we gener- ate the set E= {β1,...,β |E|}of experts with |E|= 10. We consider four family of robust experts generated by: • IR-KL: with ρ=D((ˆµT,ˆΣT)∥(ˆµS,ˆΣS))/(3|E|) and λis spaced from 1 to 0 in exponentially increasing steps.1 1We say that λ is spaced from a to b in K exponen- tially increasing steps if λ1 = a and λk+1 = λk −(a − b) exp(k)/∑K−1 i=1 exp(i) for all k∈{2,...,K −1}. • IR-W ASS: withρ=W((ˆµT,ˆΣT)∥(ˆµS,ˆΣS))/(3|E|) and λ is spaced from 1 to 0 in exponentially increasing steps. • SI-KL: with ρS spaced from 10−3 to D((ˆµT,ˆΣT) ∥ (ˆµS,ˆΣS))−1 in exponentially increasing steps. For a given ρS, ρT is set to the sum of the minimum target radius satisfying the condition of Proposition 5.1 and ρS/2.2 • SI-WASS: with ρS spaced from 10−4 to W((ˆµT,ˆΣT) ∥ (ˆµS,ˆΣS)) in increasing exponential steps. For a given ρS, ρT is set to the sum of the minimum radius that satisﬁes the condition in Proposition 5.3 and ρS/2. We benchmark against the Convex Combination (CC) and Reweighting (RW) experts in Section 2 generated by • CC-L: with λequally spaced in [0,1], thus provides uni- formly spaced distributional regions in between domains. • CC-TL: with λequally spaced in [0,0.5], thus distribu- tional regions are formed around the target domain. • CC-SL: with λequally spaced in [0.5,1], thus distribu- tional regions are formed around the source domain. • CC-TE: with λspaced from 0 to 1 in exponentially in- creasing steps, thus the constructed distributional regions are concentrated towards the target domain. • CC-SE: with λspaced from 1 to 0 in exponentially in- creasing steps, thus the constructed distributional regions are concentrated towards the source domain. • RWS: with hequally spaced in [0.5,10]. We consider a family of sequential empirical ridge regres- sion estimators generated by training for each J over • LSE-T, the union of the target dataset (ˆxj,ˆyj)NT j=1, and the sequentially arriving target test data (xj,yj)J−1 j=1 , • LSE-T&S, the union of the source data (ˆxi,ˆyi)NS i=1, the target data (ˆxj,ˆyj)NT j=1 and the sequentially arriving tar- get test data (xj,yj)J−1 j=1 . Note that both LSE-T and LSE-T&S predictors dynamically incorporate the new data to adapt the prediction. Thereby, they have an unfair advantage in the long run over the other experts that are trained only once at the beginning with NT samples from the test domain. 2If d≥15, then the minimum value ofρS is set to 5 to improve numerical stability.Data Set Time IR-KL IR-W ASS SI-KL SI-W ASS CC-L CC-TL CC-SL CC-TE CC-SE RWS LSE-T LSE-T&S Uber&Lyft 5 17.65 1.00 199.28 1.01 34.04 98.43 12.03 155.71 1.74 1.45 119.65 11.08 10 13.67 1.00 111.52 1.01 30.85 99.22 11.40 161.72 1.58 1.34 137.15 6.32 50 13.39 1.00 60.29 1.01 25.87 85.06 9.72 147.45 1.42 1.16 57.85 2.12 100 15.24 1.00 59.06 1.01 26.01 85.77 9.91 148.49 1.41 1.12 31.25 1.57 US Births (2018) 5 79.83 1.02 44.71 1.00 64.99 257.60 25.13 432.09 2.07 4.50 727.88 39.17 10 115.47 1.02 39.35 1.00 45.59 195.14 18.33 339.11 1.60 3.29 524.39 19.28 50 107.40 1.01 40.04 1.00 42.74 192.46 13.12 361.51 1.31 2.00 191.27 5.20 100 117.03 1.01 53.13 1.00 45.35 208.65 12.94 397.33 1.22 1.75 104.75 3.19 Life Expectancy 5 33.18 1.00 6.24 1.03 17.24 77.06 7.38 125.71 1.46 1.15 255.08 20.72 10 25.59 1.00 5.45 1.02 12.49 60.19 5.50 104.00 1.40 1.15 167.15 10.73 50 19.81 1.00 8.70 1.01 7.57 44.00 3.10 84.98 1.38 1.10 39.83 3.15 100 19.02 1.00 8.25 1.005 6.82 41.40 2.68 83.60 1.38 1.08 20.42 2.10 House Prices in KC 5 1.58 1.00 1.21 1.01 3.98 8.87 2.12 13.31 1.29 1.23 11.75 3.70 10 1.52 1.00 1.20 1.01 3.58 7.77 2.02 11.70 1.27 1.23 6.93 2.25 50 1.34 1.00 1.31 1.01 2.79 6.52 1.86 10.37 1.27 1.20 3.91 1.30 100 1.34 1.00 1.30 1.01 2.65 6.54 1.91 10.74 1.27 1.18 2.72 1.12 California Housing 5 63.33 1.05 3.31 1.00 27.63 102.82 9.60 181.52 1.35 1.17 96.43 54.34 10 68.08 1.04 2.42 1.00 20.57 91.86 6.23 169.87 1.19 1.17 45.64 24.76 50 70.08 1.01 1.97 1.00 11.79 81.72 2.49 170.18 1.05 1.13 10.17 5.63 100 72.80 1.003 1.90 1.00 9.71 79.19 1.83 173.96 1.04 1.14 5.81 3.39 Table 1.Normalized cumulative loss values averaged over 100 independent runs. The main reason behind using exponential step sizes origi- nates from the asymmetric nature of D. For simplicity, we also use it for experts with W. To ensure fairness in the competition between experts, we vary the parameters of the non-robust experts also in exponential steps. We compare the performance of our model against the above non-robust benchmarks on 5 Kaggle datasets:3 • Uber&Lyft contains d=38 features of Uber and Lyft cab rides in Boston including the distances, date and time of the hailing, a weather summary for that day. The predic- tion target is the price of the ride. We divide the dataset based on the company, Uber (source) and Lyft (target). • US Births (2018) has d= 36 predictive features of child births in the United States in the year of 2018 including the gender of the infant, mother’s weight gain, and mother’s per-pregnancy body mass index. The task is to predict the weight of the infants. We divide the dataset based on gender: male (source) and female (target). • Life Expectancy contains d = 19 predictive features, and the target variable is the life expectancy at birth. The dataset is divided into two subgroups: developing (source) and developed (target) countries. • House Prices in King Country contains d= 14 predic- tive variables, the target variable is the transaction price of the houses. We split the dataset into two domains: houses built in [1950,2000) (source) and [2000,2010] (target). • California Housing Prices has d= 9 predictive features, the target variable is the price of houses. We divide this dataset into houses with less than an hour drive to the ocean shore (source) and houses in inland (target). We use all samples from the source domain for training, and we form the target training set by drawing NT =dsamples 3Descriptions and download links are provided in the appendix. from the target dataset. Later, we randomly sample J = 1000 data points from the remaining target samples to form the sequentially arriving target test samples. Note that the performance of the experts is sensitive to the data, and thus we replicate this procedure 100 times. We set the regularization parameter of the ridge regression problem to η = 10−6 and the learning rate of the BOA algorithm to υ= 0.5. We measure the performance of the experts by the cumulative loss (1) calculated for every J. Table 1 shows the average cumulative loss of each aggre- gated expert obtained by the BOA algorithm for all datasets and for J= {5,10,50,100}across 100 independent runs. In each row, the minimum loss is normalized to 1, and the remaining entries are presented by the multiplicative factor of the minimum value. This result suggests that the IR- WASS and SI-WASS experts perform favorably over the competitors in that their cumulative loss at each time step is substantially lower than that of most other competitors. Figure 4.Cumulative loss averaged over 100 runs, Uber&Lyft. Figure 4 demonstrates how the average cumulative lossin (1) grows over time for the Uber&Lyft dataset. Figure 4 shows that the loss of LSE-T &S is initially constant at a high level, which highlights the discrepancy between the two domain distributions. The growth rate of LSE-T decays faster than that of other experts, and the time when LSE-T saturates indicates when the combined target domain data alone is sufﬁcient to construct a single, competitive predictor without using any source domain data. Concluding Remarks. The theoretical and experimental results in this paper suggest that IR-WASS and SI-WASS are attractive schemes to generate a family of robust least squares experts. Moreover, the IR-WASS and SI-WASS experts are extremely easy to compute because it requires solving only a second-order cone or a linear semideﬁnite program. We observe that KL-type divergence schemes are less numerically stable due to the computation of the log- determinant and the inverse of a nearly singular covariance matrix ˆΣT. Setting the parameters for KL-type divergence schemes is also harder due to the asymmetry of the diver- gence D. While this paper focuses solely on interpolating schemes, it would also be interesting to explore extrapolat- ing schemes in future research. Acknowledgments Material in this paper is based upon work supported by the Air Force Ofﬁce of Scientiﬁc Research under award number FA9550-20-1-0397. Additional support is gratefully ac- knowledged from NSF grants 1915967, 1820942, 1838676, and also from the China Merchant Bank. Man-Chung Yue gratefully acknowledges the support by HKRGC under the Early Career Scheme Funding 25302420. References Agueh, M. and Carlier, G. Barycenters in the Wasserstein space. SIAM Journal on Mathematical Analysis, 43(2): 904–924, 2011. Azizzadenesheli, K., Liu, A., Yang, F., and Anandkumar, A. Regularized learning for domain adaptation under label shifts. In International Conference on Learning Representations, 2019. Baktashmotlagh, M., Harandi, M. T., Lovell, B. C., and Salz- mann, M. Unsupervised domain adaptation by domain invariant projection. In IEEE International Conference on Computer Vision, pp. 769–776, 2013. Battistelli, G., Chisci, L., Fantacci, C., Farina, A., and Graziano, A. Consensus CPHD ﬁlter for distributed multi- target tracking. IEEE Journal of Selected Topics in Signal Processing, 7(3):508–520, 2013. Ben-David, S., Blitzer, J., Crammer, K., Pereira, F., et al. Analysis of representations for domain adaptation. Ad- vances in Neural Information Processing Systems , 19: 137, 2007. Bernstein, D. S. Matrix Mathematics: Theory, Facts, and Formulas. Princeton University Press, 2009. Bertsekas, D. Convex Optimization Theory. Athena Scien- tiﬁc, 2009. Blanchet, J., Kang, Y ., and Murthy, K. Robust Wasserstein proﬁle inference and applications to machine learning. Journal of Applied Probability, 56(3):830–857, 2019. Blitzer, J., McDonald, R., and Pereira, F. Domain adaptation with structural correspondence learning. In Conference on Empirical Methods in Natural Language Processing, pp. 120–128, 2006. Cesa-Bianchi, N. and Lugosi, G. Prediction, Learning, and Games. Cambridge University Press, 2006. Chen, X., Monfort, M., Liu, A., and Ziebart, B. D. Robust covariate shift regression. In Artiﬁcial Intelligence and Statistics, pp. 1270–1279, 2016. Chu, C. and Wang, R. A survey of domain adaptation for neural machine translation. In International Conference on Computational Linguistics, pp. 1304–1319. Associa- tion for Computational Linguistics, 2018. Cortes, C. and Mohri, M. Domain adaptation and sam- ple bias correction theory and algorithm for regression. Theoretical Computer Science, 519:103 – 126, 2014. Cortes, C., Mohri, M., and Medina, A. M. Adaptation based on generalized discrepancy. Journal of Machine Learning Research, 20(1):1–30, 2019. Courty, N., Flamary, R., Tuia, D., and Rakotomamonjy, A. Optimal transport for domain adaptation. IEEE Transac- tions on Pattern Analysis and Machine Intelligence, 39 (9):1853–1865, 2017. Csurka, G. A Comprehensive Survey on Domain Adaptation for Visual Applications, pp. 1–35. Springer International Publishing, 2017. de Mathelin, A., Richard, G., Mougeot, M., and Vayatis, N. Adversarial weighting for domain adaptation in regres- sion. arXiv preprint arXiv:2006.08251, 2020. Delage, E. and Ye, Y . Distributionally robust optimization under moment uncertainty with application to data-driven problems. Operations Research, 58(3):595–612, 2010. Duchi, J. and Namkoong, H. Learning models with uni- form performance via distributionally robust optimization. arXiv preprint arXiv:1810.08750, 2018.Ganin, Y . and Lempitsky, V . Unsupervised domain adapta- tion by backpropagation. In International Conference on Machine Learning, pp. 1180–1189, 2015. Gao, R. Finite-sample guarantees for Wasserstein distri- butionally robust optimization: Breaking the curse of dimensionality. arXiv preprint arXiv:2009.04382, 2020. Gao, R., Xie, L., Xie, Y ., and Xu, H. Robust hypothesis testing using Wasserstein uncertainty sets. InAdvances in Neural Information Processing Systems, pp. 7913–7923, 2018. Garcke, J. and Vanck, T. Importance weighted inductive transfer learning for regression. In Joint European con- ference on machine learning and knowledge discovery in databases, pp. 466–481, 2014. Ghaoui, L. E. and Lebret, H. Robust solutions to least- squares problems with uncertain data. SIAM Journal on Matrix Analysis and Applications, 18(4):1035–1064, 1997. Ghifary, M., Kleijn, W. B., Zhang, M., Balduzzi, D., and Li, W. Deep reconstruction-classiﬁcation networks for unsu- pervised domain adaptation. In European Conference on Computer Vision, pp. 597–613, 2016. Givens, C. and Shortt, R. A class of Wasserstein metrics for probability distributions. The Michigan Mathematical Journal, 31(2):231–240, 1984. Goh, J. and Sim, M. Distributionally robust optimization and its tractable approximations. Operations Research, 58(4):902–917, 2010. Huang, J., Gretton, A., Borgwardt, K., Sch¨olkopf, B., and Smola, A. Correcting sample selection bias by unlabeled data. Advances in Neural Information Processing Sys- tems, 19:601–608, 2006. Jiang, J. and Zhai, C. Instance weighting for domain adapta- tion in NLP. In Association of Computational Linguistics, pp. 264–271, 2007. Koniusz, P., Tas, Y ., and Porikli, F. Domain adaptation by mixture of alignments of second-or higher-order scatter tensors. In IEEE Conference on Computer Vision and Pattern Recognition, pp. 4478–4487, 2017. Kuhn, D., Mohajerin Esfahani, P., Nguyen, V . A., and Shaﬁeezadeh-Abadeh, S. Wasserstein distributionally robust optimization: Theory and applications in machine learning. In Operations Research & Management Science in the Age of Analytics, pp. 130–166. 2019. Kumar, A., Saha, A., and Daume, H. Co-regularization based semi-supervised domain adaptation. Advances in Neural Information Processing Systems , pp. 478–486, 2010. Lam, H. Recovering best statistical guarantees via the empir- ical divergence-based distributionally robust optimization. Operations Research, 67(4):1090–1105, 2019. Lattimore, T. and Szepesv´ari, C. Bandit Algorithms. Cam- bridge University Press, 2020. Li, Q. Literature survey: Domain adaptation algorithms for natural language processing. Department of Computer Science The Graduate Center, The City University of New York, pp. 8–10, 2012. Li, Y ., Wang, L., Wang, J., Ye, J., and Reddy, C. K. Transfer learning for survival analysis via efﬁcient L2,1-norm regu- larized Cox regression. In IEEE International Conference on Data Mining, pp. 231–240, 2016. Lipton, Z., Wang, Y .-X., and Smola, A. Detecting and correcting for label shift with black box predictors. In International Conference on Machine Learning, pp. 3122– 3130, 2018. L¨ofberg, J. Y ALMIP: A toolbox for modeling and optimiza- tion in MATLAB. In IEEE International Conference on Robotics and Automation, pp. 284–289, 2004. Long, M., Zhu, H., Wang, J., and Jordan, M. I. Unsuper- vised domain adaptation with residual transfer networks. In International Conference on Neural Information Pro- cessing Systems, pp. 136–144, 2016. Lopez-Paz, D., Hern´andez-Lobato, J. M., and Sch¨olkopf, B. Semi-supervised domain adaptation with non-parametric copulas. In International Conference on Neural Informa- tion Processing Systems, pp. 665–673, 2012. Malitsky, Y . and Mishchenko, K. Adaptive gradient descent without descent. arXiv preprint arXiv:1910.09529, 2019. McCann, R. J. A convexity principle for interacting gases. Advances in Mathematics, 128(1):153–179, 1997. Mohajerin Esfahani, P. and Kuhn, D. Data-driven distribu- tionally robust optimization using the Wasserstein met- ric: Performance guarantees and tractable reformulations. Mathematical Programming, 171(1):115–166, 2018. MOSEK ApS. The MOSEK optimization toolbox. Version 9.2., 2019. Motiian, S., Jones, Q., Iranmanesh, S., and Doretto, G. Few-shot adversarial domain adaptation. In Advances in Neural Information Processing Systems, volume 30, pp. 6670–6680, 2017a.Motiian, S., Piccirilli, M., Adjeroh, D. A., and Doretto, G. Uniﬁed deep supervised domain adaptation and general- ization. In IEEE International Conference on Computer Vision, pp. 5715–5725, 2017b. Namkoong, H. and Duchi, J. C. Stochastic gradient methods for distributionally robust optimization with f- divergences. In Advances in Neural Information Process- ing Systems, volume 29, pp. 2208–2216, 2016. Nguyen, V . A., Shaﬁeezadeh-Abadeh, S., Yue, M.-C., Kuhn, D., and Wiesemann, W. Calculating optimistic likeli- hoods using (geodesically) convex optimization. In Ad- vances in Neural Information Processing Systems, 2019a. Nguyen, V . A., Shaﬁeezadeh-Abadeh, S., Yue, M.-C., Kuhn, D., and Wiesemann, W. Optimistic distributionally robust optimization for nonparametric likelihood approximation. In Advances in Neural Information Processing Systems 32, 2019b. Nguyen, V . A., Si, N., and Blanchet, J. Robust Bayesian classiﬁcation using an optimistic score ratio. In Interna- tional Conference on Machine Learning, 2020. Pardoe, D. and Stone, P. Boosting for regression transfer. In International Conference on Machine Learning, 2010. Redko, I., Morvant, E., Habrard, A., Sebban, M., and Ben- nani, Y .Advances in Domain Adaptation Theory. Else- vier, 2019. Richard, G., de Mathelin, A., H´ebrail, G., Mougeot, M., and Vayatis, N. Unsupervised multi-source domain adaptation for regression. 2020. Saha, A., Rai, P., Daum ´e, H., Venkatasubramanian, S., and DuVall, S. L. Active supervised domain adapta- tion. In Machine Learning and Knowledge Discovery in Databases, pp. 97–112, 2011. Salaken, S. M., Khosravi, A., Nguyen, T., and Nahavandi, S. Seeded transfer learning for regression problems with deep learning. Expert Systems with Applications , 115: 565 – 577, 2019. Shaﬁeezadeh-Abadeh, S., Nguyen, V . A., Kuhn, D., and Mohajerin Esfahani, P. Wasserstein distributionally robust Kalman ﬁltering. In Advances in Neural Information Processing Systems, volume 31, pp. 8474–8483, 2018. Shimodaira, H. Improving predictive inference under covari- ate shift by weighting the log-likelihood function. Jour- nal of Statistical Planning and Inference, 90(2):227–244, 2000. Sindhwani, V ., Niyogi, P., and Belkin, M. A co- regularization approach to semi-supervised learning with multiple views. In ICML workshop on learning with multiple views, pp. 74–79, 2005. Sion, M. On general minimax theorems. Paciﬁc Journal of Mathematics, 8(1):171–176, 1958. Søgaard, A. Semi-supervised learning and domain adapta- tion in natural language processing. Synthesis Lectures on Human Language Technologies, 6(2):1–103, 2013. Still, G. Lectures on Parametric Optimization: An Introduc- tion. 2018. Sugiyama, M., Suzuki, T., Nakajima, S., Kashima, H., von B¨unau, P., and Kawanabe, M. Direct importance estima- tion for covariate shift adaptation. Annals of the Institute of Statistical Mathematics, 60(4):699–746, 2008. Sun, Q., Chattopadhyay, R., Panchanathan, S., and Ye, J. A two-stage weighting framework for multi-source domain adaptation. In Advances in Neural Information Process- ing Systems, volume 24, pp. 505–513, 2011. Tzeng, E., Hoffman, J., Darrell, T., and Saenko, K. Si- multaneous deep transfer across domains and tasks. In IEEE International Conference on Computer Vision, pp. 4068–4076, 2015. Villani, C. Optimal Transport: Old and New . Springer Science & Business Media, 2008. Wang, H., Liu, A., Yu, Z., Yue, Y ., and Anandkumar, A. Distributionally robust learning for unsupervised domain adaptation. arXiv preprint arXiv:2010.05784, 2020. Wang, M. and Deng, W. Deep visual domain adaptation: A survey. Neurocomputing, 312:135 – 153, 2018. Weiss, K., Khoshgoftaar, T. M., and Wang, D. A survey of transfer learning. Journal of Big Data, 3(1):1–40, 2016. Wilson, G. and Cook, D. J. A survey of unsupervised deep domain adaptation. ACM Transactions on Intelligent Systems and Technology, 11(5):1–46, 2020. Wintenberger, O. Optimal learning with Bernstein online aggregation. Machine Learning, 106(1):119–141, 2017. Yao, T., Pan, Y ., Ngo, C.-W., Li, H., and Mei, T. Semi- supervised domain adaptation with subspace learning for visual recognition. In IEEE conference on Computer Vision and Pattern Recognition, pp. 2142–2150, 2015. Zhao, H., Zhang, S., Wu, G., Moura, J. M. F., Costeira, J. P., and Gordon, G. J. Adversarial multiple source domain adaptation. In Advances in Neural Information Processing Systems, volume 31, 2018.A. Appendix A.1. Proof of Section 4 Proof of Proposition 4.1. Note that optimization problem (3) constitutes an unbounded convex optimization problem when ψis the Kullback-Leibler-type divergence of Deﬁnition 3.1. Let g(µ,Σ) ≜ λD((µ,Σ) ∥(ˆµS,ˆΣS)) + (1−λ)D((µ,Σ) ∥ (ˆµT,ˆΣT)), then, the ﬁrst order optimality condition reads ∇µg(µ,Σ) = 2λˆΣ−1 S (µ−ˆµS) + 2(1−λ)ˆΣ−1 T (µ−ˆµT) = 0, ∇Σg(µ,Σ) = λˆΣ−1 S −λΣ−1 + (1 −λ)ˆΣ−1 T −(1 −λ)Σ−1 = 0. One can then show (ˆµλ,ˆΣλ) provided in statement of Proposition 4.1 solves the system of equalities above. Below we prove Proposition 4.2. In the proof of Proposition 4.2 and its auxiliary lemmas, Lemma A.1 and Lemma A.2, we omit the subscripts λand ρto avoid clutter. Lemma A.1 (Dual problem). Fix (ˆµ,ˆΣ) ∈Rp ×Sp ++ and ρ ≥0. For any symmetric matrix H ∈Sp, the optimization problem    sup µ,Σ Tr [ H(Σ + µµ⊤) ] s.t. Tr [ ΣˆΣ−1] −log det(ΣˆΣ−1) −p+ (µ−ˆµ)⊤ˆΣ−1(µ−ˆµ) ≤ρ, Σ ≻0 (A.5a) admits the dual formulation { inf κ(ρ−ˆµ⊤ˆΣ−1ˆµ) + κ2ˆµ⊤ˆΣ−1[κˆΣ−1 −H]−1 ˆΣ−1ˆµ−κlog det(I−ˆΣ 1 2 HˆΣ 1 2 /κ) s.t. κ ≥0, κˆΣ−1 ≻H. (A.5b) Proof of Lemma A.1. For any µ∈Rp such that (µ−ˆµ)⊤ˆΣ−1(µ−ˆµ) ≤ρ, denote the set Sµ as Sµ ≜ { Σ ∈Sp ++ : Tr [ ΣˆΣ−1] −log det Σ≤ρµ } , where ρµ ∈R is deﬁned as ρµ ≜ ρ+p−log det ˆΣ−(µ−ˆµ)⊤ˆΣ−1(µ−ˆµ). Using these auxiliary notations, problem (A.5a) can be re-expressed as a nested program of the form sup µ µ⊤Hµ+ sup Σ∈Sµ Tr [ HΣ ] s.t. (µ−ˆµ)⊤ˆΣ−1(µ−ˆµ) ≤ρ, where we emphasize that the constraint on µis redundant, but it is added to ensure the feasibility of the inner supremum over Σ for every feasible value of µof the outer problem. We now proceed to reformulate the supremum subproblem over Σ. Assume momentarily that H ̸= 0 and that µsatisﬁes (µ−ˆµ)⊤ˆΣ−1(µ−ˆµ) < ρ. In this case, one can verify that ˆΣ is a Slater point of the convex set Sµ. Using a duality argument, we ﬁnd sup Σ∈Sµ Tr [ HΣ ] = sup Σ≻0 inf φ≥0 Tr [ HΣ ] + φ ( ρµ −Tr [ˆΣ−1Σ ] + log det Σ ) = inf φ≥0 { φρµ + sup Σ≻0 { Tr [ (H−φˆΣ−1)Σ ] + φlog det Σ }} , where the last equality follows from strong duality (Bertsekas, 2009, Proposition 5.3.1). If H−φˆΣ−1 ̸≺0, then the inner supremum problem becomes unbounded. To see this, let σ ∈R+ be the maximum eigenvalue of H −φˆΣ−1 with the corresponding eigenvector v, then the sequence (Σk)k∈N with Σk = I+ kvv⊤attains the asymptotic maximum objective value of +∞. If H−φˆΣ−1 ≺0 then the inner supremum problem admits the unique optimal solution Σ⋆(φ) = φ(φˆΣ−1 −H)−1, (A.6)which is obtained by solving the ﬁrst-order optimality condition. By placing this optimal solution into the objective function and arranging terms, we have sup Σ∈Sµ Tr [ HΣ ] = inf φ≥0 φˆΣ−1≻H φ ( ρ−(µ−ˆµ)⊤ˆΣ−1(µ−ˆµ) ) −φlog det(I−ˆΣ 1 2 HˆΣ 1 2 /φ). (A.7) We now argue that the above equality also holds whenµis chosen such that (µ−ˆµ)⊤ˆΣ−1(µ−ˆµ) = ρ. In this case, Sµ collapses into a singleton {ˆΣ}, and the left-hand side supremum problem attains the value Tr [ HˆΣ ] . The right-hand side inﬁmum problem becomes inf φ≥0 φˆΣ−1≻H −φlog det(I−ˆΣ 1 2 HˆΣ 1 2 /φ). One can show using the l’Hopital rule that lim φ↑+∞ −φlog det(I−ˆΣ 1 2 HˆΣ 1 2 /φ) = Tr [ HˆΣ ] , which implies that the equality holds. Furthermore, when H = 0, the left-hand side of (A.7) evaluates to 0, while the inﬁmum problem on the right-hand side of (A.7) also attains the optimal value of 0 asymptotically as φdecreases to 0. This implies that (A.7) holds for all H ∈Sp and for any µsatisfying (µ−ˆµ)⊤ˆΣ−1(µ−ˆµ) ≤ρ. The above line of argument shows that problem (A.5a) can now be expressed as the following maximin problem sup µ:(µ−ˆµ)⊤ˆΣ−1(µ−ˆµ)≤ρ inf φ≥0 φˆΣ−1≻H µ⊤Hµ+ φ ( ρ−(µ−ˆµ)⊤ˆΣ−1(µ−ˆµ) ) −φlog det(I−ˆΣ 1 2 HˆΣ 1 2 /φ). For any φ≥0 such that φˆΣ−1 ≻H, the objective function is concave in µ. For any µ, the objective function is convex in φ. Furthermore, the feasible set of µis convex and compact, and the feasible set of φis convex. As a consequence, we can apply Sion’s minimax theorem (Sion, 1958) to interchange the supremum and the inﬁmum operators, and problem (A.5a) is equivalent to inf φ≥0 φˆΣ−1≻H    φρ−φlog det(I−ˆΣ 1 2 HˆΣ 1 2 /φ) + sup µ:(µ−ˆµ)⊤ˆΣ−1(µ−ˆµ)≤ρ µ⊤Hµ−φ(µ−ˆµ)⊤ˆΣ−1(µ−ˆµ)   . For any φwhich is feasible for the outer problem, the inner supremum problem is a convex quadratic optimization problem because φˆΣ−1 ≻H. Using a strong duality argument, the value of the inner supremum equals to the value of inf ν≥0 { νρ−(ν+ φ)ˆµ⊤ˆΣ−1ˆµ+ sup µ µ⊤(H−(φ+ ν)ˆΣ−1)µ+ 2(ν+ φ)(ˆΣ−1ˆµ)⊤µ } = inf ν≥0 νρ−(ν+ φ)ˆµ⊤ˆΣ−1ˆµ+ (ν+ φ)2(ˆΣ−1ˆµ)⊤[(φ+ ν)ˆΣ−1 −H]−1(ˆΣ−1ˆµ), where the equality follows from the fact that the unique optimal solution in the variable µis given by (φ+ ν)[(φ+ ν)ˆΣ−1 −H]−1 ˆΣ−1ˆµ. (A.8) By combining two layers of inﬁmum problem and using a change of variables κ ←φ+ ν, problem (A.5a) can now be written as { inf κ(ρ−ˆµ⊤ˆΣ−1ˆµ) + κ2ˆµ⊤ˆΣ−1[κˆΣ−1 −H]−1 ˆΣ−1ˆµ−φlog det(I−ˆΣ 1 2 HˆΣ 1 2 /φ) s.t. φ ≥0, φˆΣ−1 ≻H, κ−φ≥0. (A.9) We now proceed to eliminate the multiplier φfrom the above problem. To this end, rewrite the above optimization problem as inf κ(ρ−ˆµ⊤ˆΣ−1ˆµ) + κ2ˆµ⊤ˆΣ−1[κˆΣ−1 −H]−1 ˆΣ−1ˆµ+ g(κ) s.t. κ ≥0, κˆΣ−1 ≻H,where g(κ) is deﬁned for every feasible value of κas g(κ) ≜ { inf −φlog det(I−ˆΣ 1 2 HˆΣ 1 2 /φ) s.t. φ ≥0, φˆΣ−1 ≻H, φ≤κ. (A.10) Let g0(φ) denote the objective function of the above optimization, which is independent of κ. Let σ1,...,σ p be the eigenvalues of ˆΣ 1 2 HˆΣ 1 2 , we can write the function gdirectly using the eigenvalues σ1,...,σ p as g0(φ) = −φ p∑ i=1 log(1 −σi/φ). It is easy to verify by basic algebra manipulation that the gradient of g0 satisﬁes ∇g0(φ) = p∑ i=1 [ log ( φ φ−σi ) − φ φ−σi ] + p≤0, which implies that the value of φthat solves (A.10) is κ, and thus g(κ) = −κlog det(I−ˆΣ 1 2 HˆΣ 1 2 /κ). Substituting φby κ in problem (A.9) leads to the desired claim. Lemma A.2 (Optimal solution attaining f(β)). For any (ˆµ,ˆΣ) ∈Rp ×Sp ++, ρ∈R++ and w ∈Rp, f(β) equals to the optimal value of the optimization problem { sup µ,Σ≻0 w⊤(Σ + µµ⊤)w s.t. Tr [ ΣˆΣ−1] −log det(ΣˆΣ−1) −p+ (µ−ˆµ)⊤ˆΣ−1(µ−ˆµ) ≤ρ, (A.11a) which admits the unique optimal solution Σ⋆ = κ⋆(κ⋆ˆΣ−1 −ww⊤)−1, µ ⋆ = Σ⋆ˆΣ−1ˆµ, (A.11b) with κ⋆ >w⊤ˆΣwbeing the unique solution of the nonlinear equation ρ= (w⊤ˆµ)2w⊤ˆΣw (κ−w⊤ˆΣw)2 + w⊤ˆΣw κ−w⊤ˆΣw + log ( 1 −w⊤ˆΣw κ ) . (A.11c) Moreover, we haveκ⋆ ≤w⊤ˆΣw ( 1 + 2ρ+ √ 1 + 4ρ(w⊤ˆµ)2) /(2ρ). Proof of Lemma A.2. First, note that f(β) = sup Q∈B EQ [ (β⊤X−Y)2] = sup Q∈B EQ [ w⊤ξξ⊤w ] = sup (µ,Σ)∈U w⊤( Σ + µµ⊤) w, which, by the deﬁnition of U and deﬁnition (3.2), equals to the optimal value of problem (A.11a). From the duality result in Lemma A.1, problem (A.11a) is equivalent to inf κ(ρ−ˆµ⊤ˆΣ−1ˆµ) + (κˆΣ−1ˆµ)⊤[κˆΣ−1 −ww⊤]−1(κˆΣ−1ˆµ) −κlog det(I−ˆΣ 1 2 ww⊤ˆΣ 1 2 /κ) s.t. κ ≥0, κˆΣ−1 ≻ww⊤. Applying Bernstein (2009, Fact 2.16.3), we have the equalities det(I−ˆΣ 1 2 ww⊤ˆΣ 1 2 /κ) = 1 −w⊤ˆΣw/κ (κˆΣ−1 −ww⊤)−1 = κ−1 ˆΣ + κ−2( 1 −w⊤ˆΣw/κ )−1 ˆΣww⊤ˆΣ, and thus by some algebraic manipulations we can rewrite f(β) = { inf κρ+ κ(w⊤ˆµ)2 κ−w⊤ˆΣw −κlog ( 1 −w⊤ˆΣw/κ ) s.t. κ>w ⊤ˆΣw. (A.12)Let f0 be the objective function of the above optimization problem. The gradient of f0 satisﬁes ∇f0(κ) = ρ−(w⊤ˆµ)2w⊤ˆΣw (κ−w⊤ˆΣw)2 − w⊤ˆΣw κ−w⊤ˆΣw −log ( 1 −w⊤ˆΣw κ ) . By the above expression of ∇f0(κ) and the strict convexity of f0(κ), the value κ⋆ that solves (A.11c) is also the unique minimizer of (A.12). In other words, f0(κ) = f(β). We now proceed to show that (µ⋆,Σ⋆) deﬁned as in (A.11b) is feasible and optimal. First, we prove feasibility of (µ⋆,Σ⋆). By direct computation, (µ⋆ −ˆµ)⊤ˆΣ−1(µ⋆ −ˆµ) = ˆµ⊤(ˆΣ−1Σ⋆ −I)ˆΣ−1(Σ⋆ˆΣ−1 −I)ˆµ= (ˆµ⊤w)2w⊤ˆΣw (κ⋆ −w⊤ˆΣw)2 . (A.13a) Moreover, because Σ⋆ˆΣ−1 = I+ (κ⋆ −w⊤ˆΣw)−1 ˆΣww⊤, we have Tr [ Σ⋆ˆΣ−1] −log det(Σ⋆ˆΣ−1) −p= (κ⋆ −w⊤ˆΣw)−1w⊤ˆΣw+ log ( 1 −w⊤ˆΣw κ⋆ ) . (A.13b) Combining (A.13a) and (A.13b), we have Tr [ Σ⋆ˆΣ−1] −log det(Σ⋆ˆΣ−1) −p+ (µ⋆ −ˆµ)⊤ˆΣ−1(µ⋆ −ˆµ) = ρ, where the ﬁrst equality follows from the deﬁnition of D, and the second equality follows from the fact thatκ⋆ solves (A.11c). This shows the feasibility of (µ⋆,Σ⋆). Next, we prove the optimality of (µ⋆,Σ⋆). Through a tedious computation, one can show that w⊤(Σ⋆ + (µ⋆)(µ⋆)⊤)w= w⊤(Σ⋆ + Σ⋆ˆΣ−1ˆµˆµ⊤ˆΣ−1Σ⋆)w =w⊤ˆΣw ( 1 + w⊤ˆΣw κ⋆ −w⊤ˆΣw ) + (ˆµ⊤w)2 ( 1 + 2w⊤ˆΣw κ⋆ −w⊤ˆΣw ) + (w⊤ˆµ)2(w⊤ˆΣw)2 (κ⋆ −w⊤ˆΣw)2 = κ⋆w⊤ˆΣw κ⋆ −w⊤ˆΣw + (κ⋆)2(ˆµ⊤w)2 (κ⋆ −w⊤ˆΣw)2 = κ⋆w⊤ˆΣw κ⋆ −w⊤ˆΣw + κ⋆(ˆµ⊤w)2w⊤ˆΣw (κ⋆ −w⊤ˆΣw)2 + κ⋆(ˆµ⊤w)2 κ⋆ −w⊤ˆΣw =κ⋆ρ−κ⋆log ( 1 −w⊤ˆΣw κ⋆ ) + κ⋆(ˆµ⊤w)2 κ⋆ −w⊤ˆΣw = f0(κ⋆) = f(β), where the antepenultimate equality follows from the fact that κ⋆ solves (A.11c), and the last equality holds because κ⋆ is the minimizer of (A.12). Therefore, (µ⋆,Σ⋆) is optimal to problem (A.11a). The uniqueness of (µ⋆,Σ⋆) now follows from the unique solution of Σ and µwith respect to the dual variables from (A.6) and (A.8), respectively. It now remains to show the upper bound on κ⋆. Towards that end, we note that for any κ>w ⊤ˆΣw, 0 = ρ−(w⊤ˆµ)2w⊤ˆΣw (κ⋆ −w⊤ˆΣw)2 − w⊤ˆΣw κ⋆ −w⊤ˆΣw −log ( 1 −w⊤ˆΣw κ⋆ ) >ρ −(w⊤ˆµ)2w⊤ˆΣw (κ⋆ −w⊤ˆΣw)2 − w⊤ˆΣw κ⋆ −w⊤ˆΣw . Solving the above quadratic inequality in the variable κ⋆ −w⊤ˆΣwyields the desired bound. This completes the proof. We are now ready to prove Proposition 4.2. Proof of Proposition 4.2. The convexity of f follows immediately by noting that it is the pointwise supremum of the family of convex functions EQ[(β⊤X−Y)2] parametrized by Q.To prove the continuously differentiability and the formula for the gradient, recall the expression(A.12) for the function f(β): f(β) = { inf κρ+ κ(w⊤ˆµ)2 κ−w⊤ˆΣw −κlog ( 1 −w⊤ˆΣw/κ ) s.t. κ>w ⊤ˆΣw. (A.14) Problem (A.14) has only one constraint. Therefore, LICQ (hence MFCQ) always holds, which implies that the Lagrange multiplier ζβ of problem (A.14) is unique for any β. Also, it is easy to see that the constraint of problem (A.14) is never binding. So, ζβ = 0 for any β. The Lagrangian function Lβ : R ×R →R is given by Lβ(κ,ζ) = ρκ+ ω2κ κ−ω1 −κlog ( 1 −ω1 κ ) + ζ(ω1 −κ), where ω1 = w⊤ˆΣwand ω2 = (w⊤ˆµ)2. The ﬁrst derivative with respect to κis dLβ dκ (κ,ζ) = ρ− ω1ω2 (κ−ω1)2 −log ( 1 −ω1 κ ) − ω1 κ−ω1 −ζ. The second derivative with respect to κis d2Lβ dκ2 (κ,ζ) = ω1 (κ−ω1)3 ( 2ω2 + ω1 κ (κ−ω1) ) . From the proof of Lemma A.2, we have that the minimizer κβ of problem (A.14) is precisely the κ⋆ deﬁned by equa- tion (A.11c) (below we write κβ instead of κ⋆ to emphasize and keep track of the dependence on β). Therefore, for any β, the minimizer κβ exists and is unique. So, there exists some constant ηβ >0 such that d2Lβ dκ2 (κβ,ζβ) ≥ηβ >0. Therefore, for any β, the strong second order condition atκβ holds (see Still (2018, Deﬁnition 6.2)). By Still (2018, Theorem 6.7), ∇f(β) = ∇βLβ(κβ,ζβ) = ∇βLβ(κβ,0) ∀β ∈Rd. (A.15) Then we compute ∇wLβ(κ,ζ) = ∇w [ κ(w⊤ˆµ)2 κ−w⊤ˆΣw −κlog ( 1 −w⊤ˆΣw κ ) + ζ(w⊤ˆΣw−κ) ] = 2κω2 (κ−ω1)2 ˆΣw+ 2κ (κ−ω1) ˆµˆµ⊤w+ 2κ (κ−ω1) ˆΣw+ 2ζˆΣw. Hence, ∇βLβ(κ,ζ) = dw dβ ⊤ · ∇wLβ(κ,ζ) = [Id 0d] · ∇wLβ(κ,ζ), which, when combined with (A.15), yields the desired gradient formula ∇f(β) = 2κβ ( ω2 ˆΣw+(κβ−ω1)(ˆΣ+ ˆµˆµ⊤)w ) 1:d (κβ −ω1)2 . By Still (2018, Theorem 6.5), the function β ↦→κβ is locally Lipschitz continuous, i.e., for any β ∈Rd, there exists cβ,ϵβ >0 such that if ∥β′−β∥2 ≤ϵβ, then |κβ′−κβ|≤ cβ∥β′−β∥2 . Note that ω1 and ω2 are both locally Lipschitz continuous in β. Also, it is easy to see that κβ >ω1 for any β. Thus, ∇f(β) is locally Lipschitz continuous in β.Proof of 4.3. Noting that problem (3) is the barycenter problem between two Gaussian distributions with respect to the Wasserstein distance, the proof then directly follows from Agueh & Carlier (2011, §6.2) and McCann (1997, Example 1.7). Proof of Proposition 4.4. Again we omit the subscripts λand ρ. Reminding that ξ= (X,Y ), we ﬁnd sup Q∈B EQ[(β⊤X−Y)2] = sup Q∈B EQ[(w⊤ξ)2] =    inf κ ( ρ−∥ˆµ∥2 2 −Tr [ˆΣ ]) + z+ Tr [ Z ] s.t. κ ∈R+, z∈R+, Z∈Sp +[ κI−ww⊤ κˆΣ 1 2 κˆΣ 1 2 Z ] ⪰0, [ κI−ww⊤ κˆµ κˆµ⊤ z ] ⪰0 = { inf κ ( ρ−∥ˆµ∥2 2 −Tr [ˆΣ ]) + κ2ˆµ⊤(κI−ww⊤)−1ˆµ+ κ2 Tr [ˆΣ(κI−ww⊤)−1] s.t. κ ≥∥w∥2 2, (A.16) where the second equality follows from Kuhn et al. (2019, Lemma 2). By applying Bernstein (2009, Fact 2.16.3), we ﬁnd (κI−ww⊤)−1 = κ−1I+ κ−2( 1 −∥w∥2 2/κ )−1 ww⊤. (A.17) Combining (A.16) and (A.17), we get sup Q∈B EQ[(β⊤X−Y)2] = { inf κρ+ κw⊤(ˆΣ + ˆµˆµ⊤)w/(κ−∥w∥2 2) s.t. κ ≥∥w∥2 2. One can verify through the ﬁrst-order optimality condition that the optimal solution κ⋆ is κ⋆ = ∥w∥2  ∥w∥2 + √ w⊤(ˆΣ + ˆµˆµ⊤)w ρ  , and by replacing this value κ⋆ into the objective function, we ﬁnd sup Q∈B EQ[(β⊤X−Y)2] = (√ w⊤(ˆΣ + ˆµˆµ⊤)w+ √ρ∥w∥2 )2 , which then completes the proof.A.2. Proof of Section 5 Lemma A.3 (Compactness). For k∈{S,T}, the set Vk = {(µ,M) ∈Rp ×Sp ++ : M −µµ⊤∈Sp ++,D((µ,M −µµ⊤) ∥(ˆµk,ˆΣk)) ≤ρk} is convex and compact. Furthermore, the set V ≜ {(µ,M) ∈Rp ×Sp ++ : (µ,M −µµ⊤) ∈UρS,ρT} is also convex and compact. Proof of Lemma A.3. For any (µ,M) ∈Rp ×Sp ++ such that M −µµ⊤∈Sp ++, we ﬁnd D ( (µ,M −µµ⊤) ∥(ˆµk,ˆΣk) ) =(µ−ˆµk)⊤ˆΣ−1 k (µ−ˆµk) + Tr [ (M −µµ⊤)ˆΣ−1] −log det((M −µµ⊤)ˆΣ−1 k ) −p =ˆµ⊤ k ˆΣ−1 k ˆµk −2ˆµ⊤ k ˆΣ−1 k µ+ Tr [ MˆΣ−1 k ] −log det(MˆΣ−1 k ) −log(1 −µ⊤M−1µ) −p, (A.18) where in the last expression, we have used the determinant formula (Bernstein, 2009, Fact 2.16.3) to rewrite det(M −µµ⊤) = (1 −µ⊤M−1µ) detM. Because M −µµ⊤∈Sp ++, one can show that 1 −µ⊤M−1µ >0 by invoking the Schur complement, and as such, the logarithm term in the last expression is well-deﬁned. Moreover, we can write Vk =    (µ,M) : (µ,M) ∈Rp ×Sp ++, M−µµ⊤∈Sp ++, ∃t∈R+ : ˆµ⊤ k ˆΣ−1 k ˆµk −2ˆµ⊤ k ˆΣ−1 k µ+ Tr [ MˆΣ−1 k ] −log det(MˆΣ−1 k ) −log(1 −t) −p≤ρ[M µ µ⊤ t ] ⪰0    , (A.19) which is a convex set. Notice that by Schur complement, the semideﬁnite constraint is equivalent to t≥µ⊤M−1µ. Next, we show that Vk is compact. Denote by Uk = {(µ,Σ) ∈Rp ×Sp + : D((µ,Σ)∥(ˆµk,ˆΣk)) ≤ρk}. Then, it is easy to see that Vk is the image of Uk under the continuous mapping (µ,Σ) ↦→(µ,Σ + µµ⊤). Therefore, it sufﬁces to prove the compactness of Uk. Towards that end, we note that D ( (µ,Σ) ∥(ˆµk,ˆΣk) ) = (ˆµk −µ)⊤ˆΣ−1 k (ˆµk −µ) + Tr [ ΣˆΣ−1 k ] −log det(ΣˆΣ−1 k ) −p is a continuous and coercive function in (µ,Σ). Thus, as a level set of D ( (µ,Σ) ∥(ˆµk,ˆΣk) ) , Uk is closed and bounded, and hence compact. To prove the last claim, by the deﬁnitions of V and UρS,ρT we write V = {(µ,M) ∈Rp ×Sp ++ : (µ,M −µµ⊤) ∈UρS,ρT} ={(µ,M) ∈Rp ×Sp ++ : (µ,M) ∈VS}∩{(µ,M) ∈Rp ×Sp ++ : (µ,M) ∈VT}∩{(µ,M) ∈Rp ×Sp ++ : M ⪰εI}. (A.20) The convexity of {(µ,M) ∈Rp ×Sp ++ : (µ,M −µµ⊤) ∈UρS,ρT}then follows from the convexity of the three sets in (A.20). Furthermore, from the ﬁrst part of the proof, we know that both {(µ,M) ∈Rp ×Sp ++ : (µ,M) ∈VS}and {(µ,M) ∈Rp ×Sp ++ : (µ,M) ∈VT}are compact sets, so is their intersection. Also, the last set {(µ,M) ∈Rp ×Sp ++ : M ⪰εI}in (A.20) is closed. Since any closed subset of a compact set is again compact, we conclude that V is compact. This completes the proof.Proof of Theorem 5.2. As ξ= (X,Y ), we can rewrite min β∈Rd sup Q∈BρS,ρT EQ[(β⊤X−Y)2] (A.21a) = min β∈Rd sup Q∈BρS,ρT [β −1 ]⊤ EQ[ξξ⊤] [β −1 ] (A.21b) = min β∈Rd sup (µ,M−µµ⊤)∈UρS,ρT [ β −1 ]⊤ M [ β −1 ] = min β∈Rd sup (µ,M)∈V [β −1 ]⊤ M [β −1 ] = sup (µ,M)∈V min β∈Rd [ β −1 ]⊤ M [ β −1 ] (A.21c) = sup (µ,M)∈V MYY −M⊤ XY M−1 XXMXY (A.21d) where (A.21c) follows from the Sion’s minimax theorem, which holds because the objective function is convex inβ, concave in M, and Lemma A.3. Equation (A.21d) exploits the unique optimal solution in β as β⋆ = M−1 XXMXY , in which the matrix inverse is well deﬁned because M ≻0 for any feasible M. Finally, after an application of the Schur complement reformulation to (A.21d), the nonlinear semideﬁnite program in the theorem statement follows from representations (A.19) and (A.20). This completes the proof. Proof of Proposition 5.3. It is well-known that the space of probability measures equipped with the Wasserstein distance W2 is a geodesic metric space (see Villani (2008, Section 7) for example), meaning that for any two probability distributions N0 and N1, there exists a constant-speed geodesic curve [0,1] ∋a↦→Na satisfying W2(Na,Na′) = |a−a′|W2(N0,N1) ∀a,a′∈[0,1]. The claim follows trivially if W2(NS,NT) ≤√ρS. Therefore, we assume W2(NS,NT) >√ρS. Consider the the geodesic Nt from N0 = NS to N1 = NT. Also, denote by Uk = {(µ,Σ) ∈Rp ×Sp + : D((µ,Σ) ∥ (ˆµk,ˆΣk)) ≤ρk}for k∈{S,T}. Then, US and UT has empty intersection if and only if W2(Na,NS) ≤√ρS =⇒W2(Na,NT) >√ρT ∀a∈[0,1], which is in turn equivalent to aW2(NT,NS) ≤√ρS =⇒(1 −a)W2(NT,NS) ≤√ρT ∀a∈[0,1]. Picking a= √ρS W2(NT,NS) ∈(0,1), then we have ( 1 − √ρS W2(NT,NS) ) W2(NT,NS) ≤√ρT. The above inequality can be rewritten as W2(NT,NS) ≤√ρS + √ρT, which contradicts with our supposition ρT ≥ (√ W((ˆµS,ˆΣS) ∥(ˆµT,ˆΣT)) −√ρS )2 . Thus, US and UT has non-empty intersection.Proof of Theorem 5.4. As ξ= (X,Y ), we can rewrite min β∈Rd sup Q∈BρS,ρT(ˆP) EQ[(β⊤X−Y)2] (A.22a) = min β∈Rd sup (µ,M−µµ⊤)∈UρS,ρT [ β −1 ]⊤ M [ β −1 ] = sup (µ,M−µµ⊤)∈UρS,ρT min β∈Rd [β −1 ]⊤ M [β −1 ] (A.22b) = sup (µ,M−µµ⊤)∈UρS,ρT MYY −M⊤ XY M−1 XXMXY (A.22c) where (A.22b) follows from the Sion’s minimax theorem, which holds because the objective function is convex inβ, concave in M, and the set UρS,ρT is compact (Shaﬁeezadeh-Abadeh et al., 2018, Lemma A.6). Equation (A.22c) exploits the unique optimal solution in βas β⋆ = M−1 XXMXY , in which the matrix inverse is well deﬁned because M −µµ⊤⪰εI for any feasible M. B. Additional Numerical Results In the following the details of the datasets used in Section 6 are presented. • Uber&Lyft4 has NS = 5000 instances in the source domain and 5000 available samples in the target domain. • US Births (2018)5 has NS = 5172 samples in the source domain and 4828 available samples in the target domain. • Life Expectancy6 has NS = 1407 instances in the source domain and 242 available samples in the target domain. • House Prices in King County7 has NS = 543 instances in the source domain and 334 available samples in the target domain. • California Housing Prices8 has NS = 9034 instances in the source domain, and 6496 available instances in the target domain. Figure A.5 demonstrates how the average cumulative loss in (1) grows over time for the US Births (2018), Life Expectancy, House Prices in KC and California Housing datasets. The results suggest that the IR-W ASS and SI-W ASS experts perform favorably over the competitors in that their cumulative loss at each time step is lower than that of most other competitors. 4Available publicly at https://www.kaggle.com/brllrb/uber-and-lyft-dataset-boston-ma 5Available publicly at https://www.kaggle.com/des137/us-births-2018 6Available publicly at https://www.kaggle.com/kumarajarshi/life-expectancy-who 7Available publicly at https://www.kaggle.com/c/house-prices-advanced-regression-techniques/data 8The modiﬁed version that we use is available publicly at https://www.kaggle.com/camnugent/ california-housing-prices and the original dataset is available publicly at https://www.dcc.fc.up.pt/˜ltorgo/ Regression/cal_housing.html(a) US Births (2018)  (b) Life Expectancy (c) House Prices in KC  (d) California Housing Figure A.5.Cumulative loss averaged over 100 runs on logarithmic scale",
      "meta_data": {
        "arxiv_id": "2106.00322v1",
        "authors": [
          "Bahar Taskesen",
          "Man-Chung Yue",
          "Jose Blanchet",
          "Daniel Kuhn",
          "Viet Anh Nguyen"
        ],
        "published_date": "2021-06-01T08:51:55Z",
        "pdf_url": "https://arxiv.org/pdf/2106.00322v1.pdf"
      },
      "llm_extracted_info": {
        "main_contributions": "The paper addresses the challenge of poor predictive accuracy of least squares estimators when trained on scarce target domain samples in supervised domain adaptation. It introduces a novel framework to synthesize a family of robust least squares experts by altering various moment-based distribution sets. Two intuitive strategies, \"Interpolate, then Robustify\" (IR) and \"Surround, then Intersect\" (SI), are proposed to construct these sets, capturing different belief levels on the source domain's explanatory power onto the target domain. When moment information is prescribed using Kullback-Leibler or Wasserstein-type divergences, the robust estimators are shown to be efficiently formed by solving convex optimization problems. These experts are then aggregated using the Bernstein Online Aggregation algorithm to generate predictions for sequential target test samples. Numerical experiments on real data demonstrate that the proposed robust strategies can systematically outperform non-robust interpolations of empirical least squares estimators.",
        "methodology": "The proposed framework integrates two main components: an Expert Generation Module and an Expert Aggregation Module. The Expert Generation Module constructs a set of competitive experts by solving distributionally robust least squares estimation problems. This is achieved by defining moment ambiguity sets (B) that contain distributions whose moments belong to a specified set (U). The two strategies for constructing U are: (1) \"Interpolate, then Robustify\" (IR): It interpolates between empirical source and target moments to obtain a barycenter, then forms a robustness ball of a certain radius around this barycenter. (2) \"Surround, then Intersect\" (SI): It defines two robustness balls around the empirical source and target moments, and then takes their intersection. Both strategies leverage Kullback-Leibler (KL) type or Wasserstein (WASS) type divergences to quantify distributional dissimilarity. Experts derived using KL-type divergence are solved efficiently via a gradient-descent algorithm, while those derived using Wasserstein-type divergence are formulated as second-order cone programs (for IR-WASS) or linear semidefinite programs (for SI-WASS) and solved with off-the-shelf solvers like MOSEK. The Expert Aggregation Module utilizes the Bernstein Online Aggregation (BOA) algorithm to combine the predictions of these generated experts for sequentially arriving target test samples, updating expert weights based on their performance.",
        "experimental_setup": "The performance of the proposed robust experts (IR-KL, IR-WASS, SI-KL, SI-WASS) was evaluated on five Kaggle regression datasets: Uber&Lyft, US Births (2018), Life Expectancy, House Prices in King Country, and California Housing Prices. For each dataset, a source and target domain were defined. All available source data was used for training, while an initial target training set consisted of NT = d samples (where d is the number of features). Subsequently, J = 1000 data points from the remaining target samples were used as a sequentially arriving test stream. The experimental procedure was replicated 100 times to ensure robustness. Ridge regression had a regularization parameter η = 10^-6, and the Bernstein Online Aggregation learning rate was υ = 0.5. The robust experts generated 10 experts each, with parameters (λ for IR, ρS for SI) spaced exponentially. These were benchmarked against various convex combination strategies (CC-L, CC-TL, CC-SL, CC-TE, CC-SE), a reweighting strategy (RWS), and two dynamically adapting sequential empirical ridge regression estimators (LSE-T and LSE-T&S) which incorporated new data over time. Performance was measured by the normalized cumulative loss, averaged over 100 independent runs, at different time steps J = {5, 10, 50, 100}.",
        "limitations": "The paper primarily focuses on ensuring competitive performance in the short term, noting that for asymptotic regimes with sufficient target sample size, directly training machine learning models on all available target data becomes more attractive. A key limitation identified is the numerical instability of KL-type divergence schemes, which stems from the computation of the log-determinant and the inverse of potentially nearly singular covariance matrices. Furthermore, setting parameters for KL-type divergence schemes is challenging due to the inherent asymmetry of the D divergence. The benchmark sequential empirical ridge regression estimators (LSE-T and LSE-T&S) were acknowledged to possess an \"unfair advantage\" in the long run because they dynamically incorporated new data, unlike the proposed experts which were trained only once at the beginning with initial target samples.",
        "future_research_directions": "Future research could extend the current work by exploring extrapolating schemes, in addition to the interpolating schemes that were the sole focus of this paper."
      }
    },
    {
      "title": "Test-Time Training with Self-Supervision for Generalization under Distribution Shifts",
      "abstract": "In this paper, we propose Test-Time Training, a general approach for\nimproving the performance of predictive models when training and test data come\nfrom different distributions. We turn a single unlabeled test sample into a\nself-supervised learning problem, on which we update the model parameters\nbefore making a prediction. This also extends naturally to data in an online\nstream. Our simple approach leads to improvements on diverse image\nclassification benchmarks aimed at evaluating robustness to distribution\nshifts.",
      "full_text": "Test-Time Training with Self-Supervision for Generalization under Distribution Shifts Yu Sun1 Xiaolong Wang1 2 Zhuang Liu1 John Miller1 Alexei A. Efros1 Moritz Hardt1 Abstract In this paper, we propose Test-Time Training, a general approach for improving the performance of predictive models when training and test data come from different distributions. We turn a sin- gle unlabeled test sample into a self-supervised learning problem, on which we update the model parameters before making a prediction. This also extends naturally to data in an online stream. Our simple approach leads to improvements on di- verse image classiﬁcation benchmarks aimed at evaluating robustness to distribution shifts. 1. Introduction Supervised learning remains notoriously weak at generaliza- tion under distribution shifts. Unless training and test data are drawn from the same distribution, even seemingly minor differences turn out to defeat state-of-the-art models (Recht et al., 2018). Adversarial robustness and domain adapta- tion are but a few existing paradigms that try to anticipate differences between the training and test distribution with either topological structure or data from the test distribution available during training. We explore a new take on gener- alization that does not anticipate the distribution shifts, but instead learns from them at test time. We start from a simple observation. The unlabeled test sample xpresented at test time gives us a hint about the distribution from which it was drawn. We propose to take advantage of this hint on the test distribution by allowing the model parameters θto depend on the test sample x, but not its unknown label y. The concept of a variable decision boundary θ(x) is powerful in theory since it breaks away from the limitation of ﬁxed model capacity (see additional discussion in Section A1), but the design of a feedback mechanism from xto θ(x) raises new challenges in practice that we only begin to address here. 1University of California, Berkeley 2University of California, San Diego. Correspondence to: Yu Sun <yusun@berkeley.edu>. Proceedings of the 37 th International Conference on Machine Learning, Vienna, Austria, PMLR 119, 2020. Copyright 2020 by the author(s). Our proposed test-time training method creates a self- supervised learning problem based on this single test sample x, updating θat test time before making a prediction. Self- supervised learning uses an auxiliary task that automatically creates labels from unlabeled inputs. In our experiments, we use the task of rotating each input image by a multiple of 90 degrees and predicting its angle (Gidaris et al., 2018). This approach can also be easily modiﬁed to work outside the standard supervised learning setting. If several test samples arrive in a batch, we can use the entire batch for test-time training. If samples arrive in an online stream, we obtain further improvements by keeping the state of the parameters. After all, prediction is rarely a single event. The online version can be the natural mode of deployment under the additional assumption that test samples are produced by the same or smoothly changing distribution shifts. We experimentally validate our method in the context of object recognition on several standard benchmarks. These include images with diverse types of corruption at various levels (Hendrycks & Dietterich, 2019), video frames of moving objects (Shankar et al., 2019), and a new test set of unknown shifts collected by (Recht et al., 2018). Our algorithm makes substantial improvements under distribu- tion shifts, while maintaining the same performance on the original distribution. In our experiments, we compare with a strong baseline (labeled joint training) that uses both supervised and self- supervised learning at training-time, but keeps the model ﬁxed at test time. Recent work shows that training-time self- supervision improves robustness (Hendrycks et al., 2019a); our joint training baseline corresponds to an improved imple- mentation of this work. A comprehensive review of related work follows in Section 5. We complement the empirical results with theoretical inves- tigations in Section 4, and establish an intuitive sufﬁcient condition on a convex model of when Test-Time Training helps; this condition, roughly speaking, is to have correlated gradients between the loss functions of the two tasks. Project website: https://test-time-training.github.io/. arXiv:1909.13231v3  [cs.LG]  1 Jul 2020Test-Time Training with Self-Supervision for Generalization under Distribution Shifts 2. Method This section describes the algorithmic details of our method. To set up notation, consider a standard K-layer neural net- work with parameters θk for layer k. The stacked parameter vector θ = ( θ1,...,θ K) speciﬁes the entire model for a classiﬁcation task with loss function lm(x,y; θ) on the test sample (x,y). We call this the main task, as indicated by the subscript of the loss function. We assume to have training data (x1,y1),..., (xn,yn) drawn i.i.d. from a distribution P. Standard empirical risk minimization solves the optimization problem: min θ 1 n n∑ i=1 lm(xi,yi; θ). (1) Our method requires a self-supervised auxiliary task with loss function ls(x). In this paper, we choose the rotation prediction task (Gidaris et al., 2018), which has been demon- strated to be simple and effective at feature learning for convolutional neural networks. The task simply rotates x in the image plane by one of 0, 90, 180 and 270 degrees and have the model predict the angle of rotation as a four- way classiﬁcation problem. Other self-supervised tasks in Section 5 might also be used for our method. The auxiliary task shares some of the model parameters θe = ( θ1,...,θ κ) up to a certain κ ∈ {1,...,K }. We designate those κlayers as a shared feature extractor. The auxiliary task uses its own task-speciﬁc parameters θs = (θ′ κ+1,...,θ ′ K). We call the unshared parameters θs the self-supervised task branch, and θm = (θκ+1,...,θ K) the main task branch . Pictorially, the joint architecture is a Y-structure with a shared bottom and two branches. For our experiments, the self-supervised task branch has the same architecture as the main branch, except for the output dimensionality of the last layer due to the different number of classes in the two tasks. Training is done in the fashion of multi-task learning (Caru- ana, 1997); the model is trained on both tasks on the same data drawn fromP. Losses for both tasks are added together, and gradients are taken for the collection of all parameters. The joint training problem is therefore min θe,θm,θs 1 n n∑ i=1 lm(xi,yi; θm,θe) + ls(xi; θs,θe). (2) Now we describe the standard version of Test-Time Training on a single test sample x. Simply put, Test-Time Training ﬁne-tunes the shared feature extractor θe by minimizing the auxiliary task loss on x. This can be formulated as min θe ls(x; θs,θe). (3) Denote θ∗ e the (approximate) minimizer of Equation 3. The model then makes a prediction using the updated parameters θ(x) = (θ∗ e,θm). Empirically, the difference is negligible between minimizing Equation 3 over θe versus over both θe and θs. Theoretically, the difference exists only when optimization is done with more than one gradient step. Test-Time Training naturally beneﬁts from standard data augmentation techniques. On each test sample x, we per- form the exact same set of random transformations as for data augmentation during training, to form a batch only con- taining these augmented copies of xfor Test-Time Training. Online Test-Time Training. In the standard version of our method, the optimization problem in Equation 3 is al- ways initialized with parameters θ= (θe,θs) obtained by minimizing Equation 2. After making a prediction on x, θ∗ e is discarded. Outside of the standard supervised learning setting, when the test samples arrive online sequentially, the online version solves the same optimization problem as in Equation 3 to update the shared feature extractor θe. How- ever, on test sample xt, θis instead initialized with θ(xt−1) updated on the previous sample xt−1. This allows θ(xt) to take advantage of the distributional information available in x1,...,x t−1 as well as xt. 3. Empirical Results We experiment with both versions of our method (standard and online) on three kinds of benchmarks for distribution shifts, presented here in the order of visually low to high- level. Our code is available at the project website. Network details. Our architecture and hyper-parameters are consistent across all experiments. We use ResNets (He et al., 2016b), which are constructed differently for CIFAR-10 (Krizhevsky & Hinton, 2009) (26-layer) and Ima- geNet (Russakovsky et al., 2015) (18-layer). The CIFAR-10 dataset contains 50K images for training, and 10K images for testing. The ImageNet contains 1.2M images for train- ing and the 50K validation images are used as the test set. ResNets on CIFAR-10 have three groups, each containing convolutional layers with the same number of channels and size of feature maps; our splitting point is the end of the second group. ResNets on ImageNet have four groups; our splitting point is the end of the third group. We use Group Normalization (GN) instead of Batch Nor- malization (BN) in our architecture, since BN has been shown to be ineffective when training with small batches, for which the estimated batch statistics are not accurate (Ioffe & Szegedy, 2015). This technicality hurts Test-Time Training since each batch only contains (augmented) copies of a single image. Different from BN, GN is not dependent on batch size and achieves similar results on our baselines.Test-Time Training with Self-Supervision for Generalization under Distribution Shifts originalgauss shot impulsedefocus glass motion zoom snow frost fog bright contrastelasticpixelate jpeg 0 10 20 30 40 50Error (%) Object recognition task only Joint training (Hendrycks et al. 2019) TTT TTT-Online Figure 1.Test error (%) on CIFAR-10-C with level 5 corruptions.We compare our approaches, Test-Time Training (TTT) and its online version (TTT-Online), with two baselines: object recognition without self-supervision, and joint training with self-supervision but keeping the model ﬁxed at test time. TTT improves over the baselines and TTT-Online improves even further. We report results with BN in Section A4 of the appendix for completeness. We directly compare our architecture to that of Hendrycks et al. (2018) in subsection A4.5. Optimization details. For joint training (Equation 2), we use stochastic gradient descent with standard hyper- parameters as (Huang et al., 2016; He et al., 2016a). For Test-Time Training (Equation 3), we use stochastic gradient descent with the learning rate set to that of the last epoch during training, which is 0.001 in all our experiments. We set weight decay and momentum to zero during Test-Time Training, inspired by practice in (He et al., 2018; Liu et al., 2018). For the standard version of Test-Time Training, we take ten gradient steps, using batches independently gener- ated by the same image. For online version of Test-Time Training, we take only one gradient step given each new im- age. We use random crop and random horizontal ﬂip for data augmentation. See Section A2 of the appendix for computa- tional aspects of our method. In all the tables and ﬁgures, object recognition task onlyrefers to the plain ResNet model (using GN, unless otherwise speciﬁed); joint training refers to the model jointly trained on both the main task and the self-supervised task, ﬁxed at test time; this has been pro- posed as the method in Hendrycks et al. (2019a); Test-Time Training (TTT) refers to the standard version described sec- tion 2; and online Test-Time Training (TTT-Online)refers to the online version that does not discardθ(xt) for xt arriving sequentially from the same distribution. Performance for TTT-Online is calculated as the average over the entire test set; we always shufﬂe the test set before TTT-Online to avoid ordering artifacts. 3.1. Object Recognition on Corrupted Images Hendrycks & Dietterich (2019) propose to benchmark ro- bustness of object recognition with 15 types of corruptions from four broad categories: noise, blur, weather and digital. Each corruption type comes in ﬁve levels of severity, with level 5 the most severe (details and sample images in the ap- pendix). The corruptions are simulated to mimic real-world corruptions as much as possible on copies of the test set for both CIFAR-10 and ImageNet. The new test sets are named as CIFAR-10-C and ImageNet-C, respectively. In the pro- posed benchmark, training should be done on the original training set, and the diversity of corruption types should make it difﬁcult for any methods to work well across the board if it relies too much on corruption speciﬁc knowledge. For online Test-Time Training, we take the entire test set as a stream of incoming images, and update and test on each image in an online manner as it arrives. CIFAR-10-C. Our results on the level 5 corruptions (most severe) are shown in Figure 1. The results on levels 1-4 are shown in Section A4 in appendix. Across all ﬁve levels and 15 corruption types, both standard and online versions of Test-Time Training improve over the object recognition task only baseline by a large margin. The standard version always improves over joint training, and the online version often improves signiﬁcantly (>10%) over joint training and never hurts by more than 0.2%. Speciﬁcally, TTT-Online contributes >24% on the three noise types and 38% on pix- elation. For a learning problem with the seemingly unstable setup that abuses a single image, this kind of consistency is rather surprising. The baseline ResNet-26 with object recognition task only has error 8.9% on the original test set of CIFAR-10. The joint training baseline actually improves performance on the original to 8.1%. More surprisingly, unlike many other methods that trade off original performance for robustness, Test-Time Training further improves on the original test set by 0.2% consistently over multiple independent trials. This suggests that our method does not choose between speciﬁcity and generality.Test-Time Training with Self-Supervision for Generalization under Distribution Shifts originalgauss shot impulsedefocus glass motion zoom snow frost fog bright contrastelasticpixelate jpeg 0 20 40 60Accuracy (%) Object recognition task only Joint training (Hendrycks et al. 2019) TTT TTT-Online 0 20000 40000 Number of samples 60 62 64 66 68 70 72 74 76Accuracy (%) Original Sliding window average 0 20000 40000 Number of samples 12 15 18 21 24 27 30 33Accuracy (%) Gaussian Noise Sliding window average 0 20000 40000 Number of samples 16 18 20 22 24 26 28 30 32Accuracy (%) Defocus Blur Sliding window average 0 20000 40000 Number of samples 28 30 32 34 36 38Accuracy (%) Zoom Blur Sliding window average 0 20000 40000 Number of samples 33 36 39 42 45 48 51 54Accuracy (%) Fog Sliding window average 0 20000 40000 Number of samples 30 33 36 39 42 45 48 51Accuracy (%) Elastic Transform Sliding window average Figure 2.Test accuracy (%) on ImageNet-C with level 5 corruptions.Upper panel: Our approaches, TTT and TTT-Online, show signiﬁcant improvements in all corruption types over the two baselines. Lower panel: We show the accuracy of TTT-Online as the average over a sliding window of 100 samples; TTT-Online generalizes better as more samples are evaluated (x-axis), without hurting on the original distribution. We use accuracy instead of error here because the baseline performance is very low for most corruptions. Separate from our method, it is interesting to note that joint training consistently improves over the single-task baseline, as discovered by Hendrycks et al. (2019a). Hendrycks & Dietterich (2019) have also experimented with various other training methods on this benchmark, and point to Adversar- ial Logit Pairing (ALP) (Kannan et al., 2018) as the most effective approach. Results of this additional baseline on all levels of CIFAR-10-C are shown in the appendix, along with its implementation details. While surprisingly robust under some of the most severe corruptions (especially the three noise types), ALP incurs a much larger error (by a factor of two) on the original distribution and some corruptions (e.g. all levels of contrast and fog), and hurts performance signiﬁcantly when the corruptions are not as severe (espe- cially on levels 1-3); this kind of tradeoff is to be expected for methods based on adversarial training. ImageNet-C. Our results on the level 5 corruptions (most severe) are shown in Figure 2. We use accuracy instead of error for this dataset because the baseline performance is very low for most corruptions. The general trend is roughly the same as on CIFAR-10-C. The standard version of TTT always improves over the baseline and joint training, while the online version only hurts on the original by 0.1% over the baseline, but signiﬁcantly improves (by a factor of more than three) on many of the corruption types. In the lower panel of Figure 2, we visualize how the accu- racy (averaged over a sliding window) of the online version changes as more images are tested. Due to space constraints, we show this plot on the original test set, as well as every third corruption type, following the same order as in the original paper. On the original test set, there is no visible trend in performance change after updating on the 50,000 samples. With corruptions, accuracy has already risen sig- niﬁcantly after 10,000 samples, but is still rising towards the end of the 50,000 samples, indicating room for additional improvements if more samples were available. Without seeing a single label, TTT-Online behaves as if we were training on the test set from the appearance of the plots.Test-Time Training with Self-Supervision for Generalization under Distribution Shifts orig gauss shot impul defoc glass motn zoom snow frost fog brit contr elast pixel jpeg TTT-Online 8.2 25.8 22.6 30.6 14.6 34.4 18.3 17.1 20.0 18.0 16.9 11.2 15.6 21.6 18.1 21.2 UDA-SS 9.0 28.2 26.5 20.8 15.6 43.7 24.5 23.8 25.0 24.9 17.2 12.7 11.6 22.1 20.3 22.6 Table 1.Test error (%) on CIFAR-10-C with level 5 corruption.Comparison between online Test-Time Training (TTT-Online) and unsupervised domain adaptation by self-supervision (UDA-SS) (Sun et al., 2019) with access to the entire (unlabeled) test set during training. We highlight the lower error in bold. We have abbreviated the names of the corruptions, in order: original test set, Gaussian noise, shot noise, impulse noise, defocus blur, glass blue, motion blur, zoom blur, snow, frost, fog, brightness, contrast, elastic transformation, pixelation, and JPEG compression. The reported numbers for TTT-Online are the same as in Figure 1. See complete table in Table A2. 0 2000 4000 6000 8000 Number of samples 12 16 20 24 28 32 36 40 44 48Error (%) Gaussian Noise Joint training TTT TTT-Online UDA-SS 0 2000 4000 6000 8000 Number of samples 9 12 15 18 21 24 27 30 33 36Error (%) Shot Noise Joint training TTT TTT-Online UDA-SS 0 2000 4000 6000 8000 Number of samples 15 20 25 30 35 40 45 50Error (%) Impulse Noise Joint training TTT TTT-Online UDA-SS Figure 3.Test error (%) on CIFAR-10-C, for the three noise types, with gradually changing distribution.The distribution shifts are created by increasing the standard deviation of each noise type from small to large, the further we go on the x-axis. As the samples get noisier, all methods suffer greater errors the more we evaluate into the test set, but online Test-Time Training (TTT-Online) achieves gentler slopes than joint training. For the ﬁrst two noise types, TTT-Online also achieves better results over unsupervised domain adaptation by self-supervision (UDA-SS) (Sun et al., 2019). Comparison with unsupervised domain adaptation. Table 1 empirically compares online Test-Time Training (TTT-Online) with unsupervised domain adaptation through self-supervision (UDA-SS) (Sun et al., 2019), which is sim- ilar to our method in spirit but is designed for the setting of unsupervised domain adaptation (Section 5 provides a sur- vey of other related work in this setting). Given labeled data from the training distribution and unlabeled data from the test distribution, UDA-SS hopes to ﬁnd an invariant repre- sentation that extracts useful features for both distributions by learning to perform a self-supervised task, speciﬁcally rotation prediction, simultaneously on data from both. It then learns a labeling function on top of the invariant rep- resentation using the labeled data. In our experiments, the unlabeled data given to UDA-SS is the entire test set itself without the labels. Because TTT-Online can only learn from the unlabeled test samples that have already been evaluated on, it is given less information than UDA-SS at all times. In this sense, UDA- SS should be regarded as an oracle rather than a baseline. Surprisingly, TTT-Online outperforms UDA-SS on 13 out of the 15 corruptions as well as the original distribution. Our explanation is that UDA-SS has to ﬁnd an invariant representation for both distributions, while TTT-Online only adapts the representation to be good for the current test distribution. That is, TTT-Online has the ﬂexibility to forget the training distribution representation, which is no longer relevant. This suggests that in our setting, forgetting is not harmful and perhaps should even be taken advantage of. Gradually changing distribution shifts.In our previous experiments, we have been evaluating the online version under the assumption that the test inputs xt for t= 1...nare all sampled from the same test distribution Q, which can be different from the training distribution P. This assumption is indeed satisﬁed for i.i.d. samples from a shufﬂed test set. But here we show that this assumption can in fact be relaxed to allow xt ∼Qt, where Qt is close to Qt+1 (in the sense of distributional distance). We call this the assumption of gradually changing distribution shifts. We perform experiments by simulating such distribution shifts on the three noise types of CIFAR-10-C. For each noise type, xt is corrupted with standard deviation σt, and σ1,...,σ n interpolate between the standard deviation of level 1 and level 5. So xt is more severely corrupted as we evaluate further into the test set and t grows larger. As shown in Figure 3, TTT-Online still improves upon joint training (and our standard version) with this relaxed assumption, and even upon UDA-SS for the ﬁrst two noise types.Test-Time Training with Self-Supervision for Generalization under Distribution Shifts Accuracy (%) Airplane Bird Car Dog Cat Horse Ship Average Object recognition task only 67.9 35.8 42.6 14.7 52.0 42.0 66.7 41.4 Joint training (Hendrycks et al., 2019a) 70.2 36.7 42.6 15.5 52.0 44.0 66.7 42.4 TTT (standard version) 70.2 39.2 42.6 21.6 54.7 46.0 77.8 45.2 TTT-Online 70.2 39.2 42.6 22.4 54.7 46.0 77.8 45.4 Table 2.Class-wise and average classiﬁcation accuracy (%) on CIFAR classes in VID-Robust, adapted from (Shankar et al., 2019). Test-Time Training (TTT) and online Test-Time Training (TTT-Online) improve over the two baselines on average, and by a large margin on “ship” and “dog” classes where the rotation task is more meaningful than in classes like “airplane” (sample images in Figure A7). 3.2. Object Recognition on Video Frames The Robust ImageNet Video Classiﬁcation (VID-Robust) dataset was developed by Shankar et al. (2019) from the Ima- geNet Video detection dataset (Russakovsky et al., 2015), to demonstrate how deep models for object recognition trained on ImageNet (still images) fail to adapt well to video frames. The VID-Robust dataset contains 1109 sets of video frames in 30 classes; each set is a short video clip of frames that are similar to an anchor frame. Our results are reported on the anchor frames. To map the 1000 ImageNet classes to the 30 VID-Robust classes, we use the max-conversion function in Shankar et al. (2019). Without any modiﬁcations for videos, we apply our method to VID-Robust on top of the same ImageNet model as in the previous subsection. Our classiﬁcation accuracy is reported in Table 3. In addition, we take the seven classes in VID-Robust that overlap with CIFAR-10, and re-scale those video frames to the size of CIFAR-10 images, as a new test set for the model trained on CIFAR-10 in the previous subsection. Again, we apply our method to this dataset without any modiﬁcations. Our results are shown in Table 2, with a breakdown for each class. Noticing that Test-Time Training does not improve on the airplane class, we inspect some airplane samples (Figure A7), and observe black margins on two sides of most images, which provide a trivial hint for rotation prediction. In addition, given an image of airplanes in the sky, it is often impossible even for humans to tell if it is rotated. This shows that our method requires the self-supervised task to be both well deﬁned and non-trivial. 3.3. CIFAR-10.1: Unknown Distribution Shifts CIFAR-10.1 (Recht et al., 2018) is a new test set of size 2000 modeled after CIFAR-10, with the exact same classes and image dimensionality, following the dataset creation process documented by the original CIFAR-10 paper as closely as possible. The purpose is to investigate the distribution shifts present between the two test sets, and the effect on object recognition. All models tested by the authors suffer a large performance drop on CIFAR-10.1 comparing to CIFAR-10, even though there is no human noticeable difference, and Method Accuracy (%) Object recognition task only 62.7 Joint training (Hendrycks et al., 2019a) 63.5 TTT (standard version) 63.8 TTT-Online 64.3 Table 3.Test accuracy (%) on VID-Robust dataset (Shankar et al., 2019). TTT and TTT-Online improve over the baselines. Method Error (%) Object recognition task only 17.4 Joint training (Hendrycks et al., 2019a) 16.7 TTT (standard version) 15.9 Table 4.Test error (%) on CIFAR-10.1 (Recht et al., 2018). TTT is the ﬁrst method to improve the performance of an existing model on this new test set. both have the same human accuracy. This demonstrates how insidious and ubiquitous distribution shifts are, even when researchers strive to minimize them. The distribution shifts from CIFAR-10 to CIFAR-10.1 pose an extremely difﬁcult problem, and no prior work has been able to improve the performance of an existing model on this new test set, probably because: 1) researchers cannot even identify the distribution shifts, let alone describe them mathematically; 2) the samples in CIFAR-10.1 are only revealed at test time; and even if they were revealed during training, the distribution shifts are too subtle, and the sample size is too small, for domain adaptation (Recht et al., 2018). On the original CIFAR-10 test set, the baseline with only object recognition has error 8.9%, and with joint training has 8.1%; comparing to the ﬁrst two rows of Table 4, both suffer the typical performance drop (by a factor of two). TTT yields an improvement of 0.8% (relative improvement of 4.8%) over joint training. We recognize that this improve- ment is small relative to the performance drop, but see it as an encouraging ﬁrst step for this very difﬁcult problem.Test-Time Training with Self-Supervision for Generalization under Distribution Shifts 0 10 20 30 40 50 60 Gradient inner product 0 1 2 3 4 5Improvement (%) Level 5 Level 4 Level 3 Level 2 Level 1 0 10 20 30 40 50 60 Gradient inner product 0 5 10 15 20 25 30 35Improvement (%) Level 5 Level 4 Level 3 Level 2 Level 1 Figure 4.Scatter plot of the inner product between the gradients (on the shared feature extractor θe) of the main task lm and the self- supervised task le, and the improvement in test error (%) from Test-Time Training, for the standard (left) and online (right) version. Each point is the average over a test set, and each scatter plot has 75 test sets, from all 15 types of corruptions over ﬁve levels as described in subsection 3.1. The blue lines and bands are the best linear ﬁts and the 99% conﬁdence intervals. The linear correlation coefﬁcients are 0.93 and 0.89 respectively, indicating strong positive correlation between the two quantities, as suggested by Theorem 1. 4. Theoretical Results This section contains our preliminary study of when and why Test-Time Training is expected to work. For convex models, we prove that positive gradient correlation between the loss functions leads to better performance on the main task after Test-Time Training. Equipped with this insight, we then empirically demonstrate that gradient correlation governs the success of Test-Time Training on the deep learning model discussed in Section 3. Before stating our main theoretical result, we ﬁrst illustrate the general intuition with a toy model. Consider a regression problem where x∈Rd denotes the input, y1 ∈R denotes the label, and the objective is the square loss (ˆy−y1)2/2 for a prediction ˆy. Consider a two layer linear network parametrized by A∈Rh×d and v ∈Rh (where hstands for the hidden dimension). The prediction according to this model is ˆy= v⊤Ax, and the main task loss is lm(x,y1; A,v) = 1 2 ( y1 −v⊤Ax )2 . (4) In addition, consider a self-supervised regression task that also uses the square loss and automatically generates a label ys for x. Let the self-supervised head be parametrized by w∈Rh. Then the self-supervised task loss is ls(x,y2; A,w) = 1 2 ( y2 −w⊤Ax )2 . (5) Now we apply Test-Time Training to update the shared feature extractor Aby one step of gradient descent on ls, which we can compute with y2 known. This gives us A′←A−η ( y2 −w⊤Ax )( −wx⊤) , (6) where A′is the updated matrix and ηis the learning rate. If we set η= η∗where η∗= y1 −v⊤Ax (y2 −w⊤Ax) v⊤wx⊤x, (7) then with some simple algebra, it is easy to see that the main task loss lm(x,y1; A′,v) = 0. Concretely, Test-Time Training drives the main task loss down to zero with a single gradient step for a carefully chosen learning rate. In prac- tice, this learning rate is unknown since it depends on the unknown y1. However, since our model is convex, as long as η∗is positive, it sufﬁces to set η to be a small positive constant (see details in the appendix). If x̸= 0, one sufﬁ- cient condition for η∗to be positive (when neither loss is zero) is to have sign ( y1 −v⊤Ax ) = sign ( y2 −w⊤Ax ) (8) and v⊤w>0 . (9) For our toy model, both parts of the condition above have an intuition interpretation. The ﬁrst part says that the mistakes should be correlated, in the sense that predictions from both tasks are mistaken in the same direction. The second part, v⊤w>0, says that the decision boundaries on the feature space should be correlated. In fact, these two parts hold iff. ⟨∇lm(A),∇ls(A)⟩>0 (see a simple proof of this fact in the appendix). To summarize, if the gradients have positive correlation, Test-Time Training is guaranteed to reduce the main task loss. Our main theoretical result extends this to general smooth and convex loss functions.Test-Time Training with Self-Supervision for Generalization under Distribution Shifts Theorem 1. Let lm(x,y; θ) denote the main task loss on test instance x,y with parameters θ, and ls(x; θ) the self- supervised task loss that only depends onx. Assume that for all x,y, lm(x,y; θ) is differentiable, convex andβ-smooth in θ, and both ∥∇lm(x,y; θ)∥,∥∇ls(x,θ)∥≤ Gfor all θ. With a ﬁxed learning rate η= ϵ βG2 , for every x,y such that ⟨∇lm(x,y; θ),∇ls(x; θ)⟩>ϵ, (10) we have lm(x,y; θ) >lm(x,y; θ(x)), (11) where θ(x) = θ−η∇ls(x; θ) i.e. Test-Time Training with one step of gradient descent. The proof uses standard techniques in optimization, and is left for the appendix. Theorem 1 reveals gradient correlation as a determining factor of the success of Test-Time Training in the smooth and convex case. In Figure 4, we empirically show that our insight also holds for non-convex loss func- tions, on the deep learning model and across the diverse set of corruptions considered in Section 3; stronger gradient cor- relation clearly indicates more performance improvement over the baseline. 5. Related Work Learning on test instances. Shocher et al. (2018) pro- vide a key inspiration for our work by showing that image super-resolution could be learned at test time simply by try- ing to upsample a downsampled version of the input image. More recently, Bau et al. (2019) improve photo manipula- tion by adapting a pre-trained GAN to the statistics of the input image. One of the earlier examples of this idea comes from Jain & Learned-Miller (2011), who improve Viola- Jones face detection (Viola et al., 2001) by bootstrapping the more difﬁcult faces in an image from the more easily detected faces in that same image. The online version of our algorithm is inspired by the work of Mullapudi et al. (2018), which makes video segmentation more efﬁcient by using a student model that learns online from a teacher model. The idea of online updates has also been used in Kalal et al. (2011) for tracking and detection. A recent work in echocardiography (Zhu et al., 2019) improves the deep learning model that tracks myocardial motion and cardiac blood ﬂow with sequential updates. Lastly, we share the philosophy of transductive learning (Vapnik, 2013; Gam- merman et al., 1998), but have little in common with their classical algorithms; recent work by Tripuraneni & Mackey (2019) theoretically explores this for linear prediction, in the context of debiasing the LASSO estimator. Self-supervised learning studies how to create labels from the data, by designing various pretext tasks that can learn semantic information without human annotations, such as context prediction (Doersch et al., 2015), solving jig- saw puzzles (Noroozi & Favaro, 2016), colorization (Lars- son et al., 2017; Zhang et al., 2016), noise prediction (Bo- janowski & Joulin, 2017), feature clustering (Caron et al., 2018). Our paper uses rotation prediction (Gidaris et al., 2018). Asano et al. (2019) show that self-supervised learn- ing on only a single image, surprisingly, can produce low- level features that generalize well. Closely related to our work, Hendrycks et al. (2019a) propose that jointly training a main task and a self-supervised task (our joint training baseline in Section 3) can improve robustness on the main task. The same idea is used in few-shot learning (Su et al., 2019), domain generalization (Carlucci et al., 2019), and unsupervised domain adaptation (Sun et al., 2019). Adversarial robustness studies the robust risk RP,∆(θ) = Ex,y∼P maxδ∈∆ l(x + δ,y; θ), where l is some loss function, and ∆ is the set of perturbations; ∆ is often chosen as the Lp ball, for p ∈{1,2,∞}. Many popular algorithms formulate and solve this as a robust optimization problem (Goodfellow et al., 2014; Madry et al., 2017; Sinha et al., 2017; Raghunathan et al., 2018; Wong & Kolter, 2017; Croce et al., 2018), and the most well known technique is adversarial training. Another line of work is based on randomized smoothing (Cohen et al., 2019; Salman et al., 2019), while some other approaches, such as input transformations (Guo et al., 2017; Song et al., 2017), are shown to be less effective (Athalye et al., 2018). There are two main problems with the approaches above. First, all of them can be seen as smoothing the decision boundary. This establishes a theoretical tradeoff between accuracy and robustness (Tsipras et al., 2018; Zhang et al., 2019), which we also observe empirically with our adversarial training baseline in Section 3. Intuitively, the more diverse ∆ is, the less effective this one-boundary-ﬁts-all approach can be for a particular element of ∆. Second, adversarial methods rely heavily on the mathematical structure of ∆, which might not accurately model perturbations in the real world. Therefore, generalization remains hard outside of the ∆ we know in advance or can mathematically model, especially for non-adversarial distribution shifts. Empirically, Kang et al. (2019) shows that robustness for one ∆ might not transfer to another, and training on the L∞ball actually hurts robustness on the L1 ball. Non-adversarial robustness studies the effect of corrup- tions, perturbations, out-of-distribution examples, and real- world distribution shifts (Hendrycks et al., 2019b;a; 2018; Hendrycks & Gimpel, 2016). Geirhos et al. (2018) show that training on images corrupted by Gaussian noise makes deep learning models robust to this particular noise type, but does not improve performance on images corrupted by another noise type e.g. salt-and-pepper noise.Test-Time Training with Self-Supervision for Generalization under Distribution Shifts Unsupervised domain adaptation (a.k.a. transfer learn- ing) studies the problem of distribution shifts, when an unlabeled dataset from the test distribution (target domain) is available at training time, in addition to a labeled dataset from the training distribution (source domain) (Chen et al., 2011; Gong et al., 2012; Long et al., 2015; Ganin et al., 2016; Long et al., 2016; Tzeng et al., 2017; Hoffman et al., 2017; Csurka, 2017; Chen et al., 2018). The limitation of the problem setting, however, is that generalization might only be improved for this speciﬁc test distribution, which can be difﬁcult to anticipate in advance. Prior work try to anticipate broader distributions by using multiple and evolv- ing domains (Hoffman et al., 2018; 2012; 2014). Test-Time Training does not anticipate any test distribution, by chang- ing the setting of unsupervised domain adaptation, while taking inspiration from its algorithms. Our paper is a follow- up to Sun et al. (2019), which we explain and empirically compare with in Section 3. Our update rule can be viewed as performing one-sample unsupervised domain adaptation on the ﬂy, with the caveat that standard domain adaptation techniques might become ill-deﬁned when there is only one sample from the target domain. Domain generalization studies the setting where a meta distribution generates multiple environment distributions, some of which are available during training (source), while others are used for testing (target) (Li et al., 2018; Shankar et al., 2018; Muandet et al., 2013; Balaji et al., 2018; Ghifary et al., 2015; Motiian et al., 2017; Li et al., 2017a; Gan et al., 2016). With only a few environments, information on the meta distribution is often too scarce to be helpful, and with many environments, we are back to the i.i.d. setting where each environment can be seen as a sample, and a strong baseline is to simply train on all the environments (Li et al., 2019). The setting of domain generalization is limited by the inherent tradeoff between speciﬁcity and generality of a ﬁxed decision boundary, and the fact that generalization is again elusive outside of the meta distribution i.e. the actual P learned by the algorithm. One (few)-shot learning studies how to learn a new task or a new classiﬁcation category using only one (or a few) sample(s), on top of a general representation that has been learned on diverse samples (Snell et al., 2017; Vinyals et al., 2016; Fei-Fei et al., 2006; Ravi & Larochelle, 2016; Li et al., 2017b; Finn et al., 2017; Gidaris & Komodakis, 2018). Our update rule can be viewed as performing one-shot self- supervised learning and can potentially be improved by progress in one-shot learning. Continual learning (a.k.a. learning without forgetting) studies the setting where a model is made to learn a sequence of tasks, and not forget about the earlier ones while training for the later (Li & Hoiem, 2017; Lopez-Paz & Ranzato, 2017; Kirkpatrick et al., 2017; Santoro et al., 2016). In contrast, with Test-Time Training, we are not concerned about forgetting the past test samples since they have already been evaluated on; and if a past sample comes up by any chance, it would go through Test-Time Training again. In addition, the impact of forgetting the training set is minimal, because both tasks have already been jointly trained. Online learning (a.k.a. online optimization) is a well- studied area of learning theory (Shalev-Shwartz et al., 2012; Hazan et al., 2016). The basic setting repeats the following: receive xt, predict ˆyt, receive yt from a worst-case oracle, and learn. Final performance is evaluated using the regret, which colloquially translates to how much worse the online learning algorithm performs in comparison to the best ﬁxed model in hindsight. In contrast, our setting never reveals any yt during testing even for the online version, so we do not need to invoke the concept of the worst-case oracle or the regret. Also, due to the lack of feedback from the envi- ronment after predicting, our algorithm is motivated to learn (with self-supervision) before predicting ˆyt instead of after. Note that some of the previously covered papers (Hoffman et al., 2014; Jain & Learned-Miller, 2011; Mullapudi et al., 2018) use the term “online learning” outside of the learning theory setting, so the term can be overloaded. 6. Discussion The idea of test-time training also makes sense for other tasks, such as segmentation and detection, and in other ﬁelds, such as speech recognition and natural language process- ing. For machine learning practitioners with prior domain knowledge in their respective ﬁelds, their expertise can be leveraged to design better special-purpose self-supervised tasks for test-time training. Researchers for general-purpose self-supervised tasks can also use test-time training as an evaluation benchmark, in addition to the currently prevalent benchmark of pre-training and ﬁne-tuning. More generally, we hope this paper can encourage re- searchers to abandon the self-imposed constraint of a ﬁxed decision boundary for testing, or even the artiﬁcial division between training and testing altogether. Our work is but a small step toward a new paradigm where much of the learning happens after a model is deployed. Acknowledgements. This work is supported by NSF grant 1764033, DARPA and Berkeley DeepDrive. This paper took a long time to develop, and beneﬁted from con- versations with many of our colleagues, including Ben Recht and his students Ludwig Schmidt, Vaishaal Shanker and Becca Roelofs; Ravi Teja Mullapudi, Achal Dave and Deva Ramanan; and Armin Askari, Allan Jabri, Ashish Kumar, Angjoo Kanazawa and Jitendra Malik.Test-Time Training with Self-Supervision for Generalization under Distribution Shifts References Asano, Y . M., Rupprecht, C., and Vedaldi, A. Surprising effectiveness of few-image unsupervised feature learning. arXiv preprint arXiv:1904.13132, 2019. Athalye, A., Carlini, N., and Wagner, D. Obfuscated gradients give a false sense of security: Circumvent- ing defenses to adversarial examples. arXiv preprint arXiv:1802.00420, 2018. Balaji, Y ., Sankaranarayanan, S., and Chellappa, R. Metareg: Towards domain generalization using meta-regularization. In Advances in Neural Information Processing Systems, pp. 998–1008, 2018. Bau, D., Strobelt, H., Peebles, W., Wulff, J., Zhou, B., Zhu, J.-Y ., and Torralba, A. Semantic photo manipulation with a generative image prior. ACM Transactions on Graphics (TOG), 38(4):59, 2019. Bojanowski, P. and Joulin, A. Unsupervised learning by predicting noise. In Proceedings of the 34th International Conference on Machine Learning-Volume 70, pp. 517– 526. JMLR. org, 2017. Carlucci, F. M., D’Innocente, A., Bucci, S., Caputo, B., and Tommasi, T. Domain generalization by solving jigsaw puzzles. In Proceedings of the IEEE Conference on Com- puter Vision and Pattern Recognition , pp. 2229–2238, 2019. Caron, M., Bojanowski, P., Joulin, A., and Douze, M. Deep clustering for unsupervised learning of visual features. In Proceedings of the European Conference on Computer Vision (ECCV), pp. 132–149, 2018. Caruana, R. Multitask learning. Machine learning, 28(1): 41–75, 1997. Chen, M., Weinberger, K. Q., and Blitzer, J. Co-training for domain adaptation. In Advances in neural information processing systems, pp. 2456–2464, 2011. Chen, X., Sun, Y ., Athiwaratkun, B., Cardie, C., and Wein- berger, K. Adversarial deep averaging networks for cross- lingual sentiment classiﬁcation. Transactions of the Asso- ciation for Computational Linguistics, 6:557–570, 2018. Cohen, J. M., Rosenfeld, E., and Kolter, J. Z. Certiﬁed adversarial robustness via randomized smoothing. arXiv preprint arXiv:1902.02918, 2019. Croce, F., Andriushchenko, M., and Hein, M. Provable robustness of relu networks via maximization of linear regions. arXiv preprint arXiv:1810.07481, 2018. Csurka, G. Domain adaptation for visual applications: A comprehensive survey. arXiv preprint arXiv:1702.05374, 2017. Ding, G. W., Wang, L., and Jin, X. AdverTorch v0.1: An adversarial robustness toolbox based on pytorch. arXiv preprint arXiv:1902.07623, 2019. Doersch, C., Gupta, A., and Efros, A. A. Unsupervised visual representation learning by context prediction. In Proceedings of the IEEE International Conference on Computer Vision, pp. 1422–1430, 2015. Fei-Fei, L., Fergus, R., and Perona, P. One-shot learning of object categories. IEEE transactions on pattern analysis and machine intelligence, 28(4):594–611, 2006. Finn, C., Abbeel, P., and Levine, S. Model-agnostic meta- learning for fast adaptation of deep networks. In Proceed- ings of the 34th International Conference on Machine Learning-Volume 70, pp. 1126–1135. JMLR. org, 2017. Gammerman, A., V ovk, V ., and Vapnik, V . Learning by transduction. In Proceedings of the Fourteenth conference on Uncertainty in artiﬁcial intelligence , pp. 148–155. Morgan Kaufmann Publishers Inc., 1998. Gan, C., Yang, T., and Gong, B. Learning attributes equals multi-source domain generalization. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pp. 87–97, 2016. Ganin, Y ., Ustinova, E., Ajakan, H., Germain, P., Larochelle, H., Laviolette, F., Marchand, M., and Lempitsky, V . Domain-adversarial training of neural networks. The Journal of Machine Learning Research, 17(1):2096–2030, 2016. Geirhos, R., Temme, C. R., Rauber, J., Sch¨utt, H. H., Bethge, M., and Wichmann, F. A. Generalisation in humans and deep neural networks. In Advances in Neural Information Processing Systems, pp. 7538–7550, 2018. Ghifary, M., Bastiaan Kleijn, W., Zhang, M., and Balduzzi, D. Domain generalization for object recognition with multi-task autoencoders. In Proceedings of the IEEE international conference on computer vision, pp. 2551– 2559, 2015. Gidaris, S. and Komodakis, N. Dynamic few-shot visual learning without forgetting. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pp. 4367–4375, 2018. Gidaris, S., Singh, P., and Komodakis, N. Unsupervised rep- resentation learning by predicting image rotations. arXiv preprint arXiv:1803.07728, 2018. Gong, B., Shi, Y ., Sha, F., and Grauman, K. Geodesic ﬂow kernel for unsupervised domain adaptation. In2012 IEEE Conference on Computer Vision and Pattern Recognition, pp. 2066–2073. IEEE, 2012.Test-Time Training with Self-Supervision for Generalization under Distribution Shifts Goodfellow, I. J., Shlens, J., and Szegedy, C. Explain- ing and harnessing adversarial examples. arXiv preprint arXiv:1412.6572, 2014. Guo, C., Rana, M., Cisse, M., and van der Maaten, L. Coun- tering adversarial images using input transformations. arXiv preprint arXiv:1711.00117, 2017. Hazan, E. et al. Introduction to online convex optimization. Foundations and Trends® in Optimization, 2(3-4):157– 325, 2016. He, K., Zhang, X., Ren, S., and Sun, J. Deep residual learn- ing for image recognition. In Proceedings of the IEEE conference on computer vision and pattern recognition, pp. 770–778, 2016a. He, K., Zhang, X., Ren, S., and Sun, J. Identity mappings in deep residual networks. In European conference on computer vision, pp. 630–645. Springer, 2016b. He, K., Girshick, R., and Doll ´ar, P. Rethinking imagenet pre-training. arXiv preprint arXiv:1811.08883, 2018. Hendrycks, D. and Dietterich, T. Benchmarking neural network robustness to common corruptions and perturba- tions. arXiv preprint arXiv:1903.12261, 2019. Hendrycks, D. and Gimpel, K. A baseline for detecting misclassiﬁed and out-of-distribution examples in neural networks. arXiv preprint arXiv:1610.02136, 2016. Hendrycks, D., Mazeika, M., Wilson, D., and Gimpel, K. Using trusted data to train deep networks on labels cor- rupted by severe noise. InAdvances in neural information processing systems, pp. 10456–10465, 2018. Hendrycks, D., Lee, K., and Mazeika, M. Using pre-training can improve model robustness and uncertainty. arXiv preprint arXiv:1901.09960, 2019a. Hendrycks, D., Mazeika, M., Kadavath, S., and Song, D. Improving model robustness and uncertainty estimates with self-supervised learning. arXiv preprint, 2019b. Hoffman, J., Kulis, B., Darrell, T., and Saenko, K. Discover- ing latent domains for multisource domain adaptation. In European Conference on Computer Vision, pp. 702–715. Springer, 2012. Hoffman, J., Darrell, T., and Saenko, K. Continuous man- ifold based adaptation for evolving visual domains. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pp. 867–874, 2014. Hoffman, J., Tzeng, E., Park, T., Zhu, J.-Y ., Isola, P., Saenko, K., Efros, A. A., and Darrell, T. Cycada: Cycle- consistent adversarial domain adaptation. arXiv preprint arXiv:1711.03213, 2017. Hoffman, J., Mohri, M., and Zhang, N. Algorithms and theory for multiple-source adaptation. In Advances in Neural Information Processing Systems, pp. 8246–8256, 2018. Huang, G., Sun, Y ., Liu, Z., Sedra, D., and Weinberger, K. Q. Deep networks with stochastic depth. In European conference on computer vision, pp. 646–661. Springer, 2016. Ioffe, S. and Szegedy, C. Batch normalization: Accelerating deep network training by reducing internal covariate shift. arXiv preprint arXiv:1502.03167, 2015. Jain, V . and Learned-Miller, E. Online domain adaptation of a pre-trained cascade of classiﬁers. In CVPR 2011, pp. 577–584. IEEE, 2011. Kalal, Z., Mikolajczyk, K., and Matas, J. Tracking-learning- detection. IEEE transactions on pattern analysis and machine intelligence, 34(7):1409–1422, 2011. Kang, D., Sun, Y ., Brown, T., Hendrycks, D., and Steinhardt, J. Transfer of adversarial robustness between perturbation types. arXiv preprint arXiv:1905.01034, 2019. Kannan, H., Kurakin, A., and Goodfellow, I. Adversarial logit pairing. arXiv preprint arXiv:1803.06373, 2018. Kirkpatrick, J., Pascanu, R., Rabinowitz, N., Veness, J., Des- jardins, G., Rusu, A. A., Milan, K., Quan, J., Ramalho, T., Grabska-Barwinska, A., et al. Overcoming catastrophic forgetting in neural networks. Proceedings of the national academy of sciences, 114(13):3521–3526, 2017. Krizhevsky, A. and Hinton, G. Learning multiple layers of features from tiny images. Technical report, Citeseer, 2009. Larsson, G., Maire, M., and Shakhnarovich, G. Colorization as a proxy task for visual understanding. In CVPR, 2017. Li, D., Yang, Y ., Song, Y .-Z., and Hospedales, T. M. Deeper, broader and artier domain generalization. In Proceed- ings of the IEEE International Conference on Computer Vision, pp. 5542–5550, 2017a. Li, D., Zhang, J., Yang, Y ., Liu, C., Song, Y .-Z., and Hospedales, T. M. Episodic training for domain gen- eralization. arXiv preprint arXiv:1902.00113, 2019. Li, Y ., Tian, X., Gong, M., Liu, Y ., Liu, T., Zhang, K., and Tao, D. Deep domain generalization via conditional invariant adversarial networks. In Proceedings of the European Conference on Computer Vision (ECCV), pp. 624–639, 2018.Test-Time Training with Self-Supervision for Generalization under Distribution Shifts Li, Z. and Hoiem, D. Learning without forgetting. IEEE transactions on pattern analysis and machine intelligence, 40(12):2935–2947, 2017. Li, Z., Zhou, F., Chen, F., and Li, H. Meta-sgd: Learning to learn quickly for few-shot learning. arXiv preprint arXiv:1707.09835, 2017b. Liu, Z., Sun, M., Zhou, T., Huang, G., and Darrell, T. Re- thinking the value of network pruning. arXiv preprint arXiv:1810.05270, 2018. Long, M., Cao, Y ., Wang, J., and Jordan, M. I. Learn- ing transferable features with deep adaptation networks. arXiv preprint arXiv:1502.02791, 2015. Long, M., Zhu, H., Wang, J., and Jordan, M. I. Unsupervised domain adaptation with residual transfer networks. In Advances in Neural Information Processing Systems, pp. 136–144, 2016. Lopez-Paz, D. and Ranzato, M. Gradient episodic memory for continual learning. In Advances in Neural Information Processing Systems, pp. 6467–6476, 2017. Madry, A., Makelov, A., Schmidt, L., Tsipras, D., and Vladu, A. Towards deep learning models resistant to adversarial attacks. arXiv preprint arXiv:1706.06083 , 2017. Motiian, S., Piccirilli, M., Adjeroh, D. A., and Doretto, G. Uniﬁed deep supervised domain adaptation and gen- eralization. In Proceedings of the IEEE International Conference on Computer Vision, pp. 5715–5725, 2017. Muandet, K., Balduzzi, D., and Sch ¨olkopf, B. Domain generalization via invariant feature representation. In International Conference on Machine Learning, pp. 10– 18, 2013. Mullapudi, R. T., Chen, S., Zhang, K., Ramanan, D., and Fatahalian, K. Online model distillation for efﬁcient video inference. arXiv preprint arXiv:1812.02699, 2018. Noroozi, M. and Favaro, P. Unsupervised learning of visual representations by solving jigsaw puzzles. In European Conference on Computer Vision , pp. 69–84. Springer, 2016. Raghunathan, A., Steinhardt, J., and Liang, P. Certiﬁed defenses against adversarial examples. arXiv preprint arXiv:1801.09344, 2018. Ravi, S. and Larochelle, H. Optimization as a model for few-shot learning. IEEE transactions on pattern analysis and machine intelligence, 2016. Recht, B., Roelofs, R., Schmidt, L., and Shankar, V . Do cifar-10 classiﬁers generalize to cifar-10? arXiv preprint arXiv:1806.00451, 2018. Russakovsky, O., Deng, J., Su, H., Krause, J., Satheesh, S., Ma, S., Huang, Z., Karpathy, A., Khosla, A., Bernstein, M., Berg, A. C., and Fei-Fei, L. ImageNet Large Scale Visual Recognition Challenge. International Journal of Computer Vision (IJCV) , 115(3):211–252, 2015. doi: 10.1007/s11263-015-0816-y. Salman, H., Yang, G., Li, J., Zhang, P., Zhang, H., Razen- shteyn, I., and Bubeck, S. Provably robust deep learn- ing via adversarially trained smoothed classiﬁers. arXiv preprint arXiv:1906.04584, 2019. Santoro, A., Bartunov, S., Botvinick, M., Wierstra, D., and Lillicrap, T. Meta-learning with memory-augmented neu- ral networks. In International conference on machine learning, pp. 1842–1850, 2016. Shalev-Shwartz, S. et al. Online learning and online con- vex optimization. Foundations and Trends® in Machine Learning, 4(2):107–194, 2012. Shankar, S., Piratla, V ., Chakrabarti, S., Chaudhuri, S., Jyothi, P., and Sarawagi, S. Generalizing across domains via cross-gradient training. arXiv preprint arXiv:1804.10745, 2018. Shankar, V ., Dave, A., Roelofs, R., Ramanan, D., Recht, B., and Schmidt, L. Do image classiﬁers generalize across time? arXiv, 2019. Shocher, A., Cohen, N., and Irani, M. zero-shot super- resolution using deep internal learning. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pp. 3118–3126, 2018. Sinha, A., Namkoong, H., and Duchi, J. Certifying some dis- tributional robustness with principled adversarial training. arXiv preprint arXiv:1710.10571, 2017. Snell, J., Swersky, K., and Zemel, R. Prototypical networks for few-shot learning. In Advances in Neural Information Processing Systems, pp. 4077–4087, 2017. Song, Y ., Kim, T., Nowozin, S., Ermon, S., and Kushman, N. Pixeldefend: Leveraging generative models to understand and defend against adversarial examples. arXiv preprint arXiv:1710.10766, 2017. Su, J.-C., Maji, S., and Hariharan, B. Boosting supervi- sion with self-supervision for few-shot learning. arXiv preprint arXiv:1906.07079, 2019. Sun, Y ., Tzeng, E., Darrell, T., and Efros, A. A. Unsuper- vised domain adaptation through self-supervision. arXiv preprint, 2019.Test-Time Training with Self-Supervision for Generalization under Distribution Shifts Tripuraneni, N. and Mackey, L. Debiasing linear prediction. arXiv preprint arXiv:1908.02341, 2019. Tsipras, D., Santurkar, S., Engstrom, L., Turner, A., and Madry, A. Robustness may be at odds with accuracy. arXiv preprint arXiv:1805.12152, 2018. Tzeng, E., Hoffman, J., Saenko, K., and Darrell, T. Adver- sarial discriminative domain adaptation. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pp. 7167–7176, 2017. Vapnik, V .The nature of statistical learning theory. Springer science & business media, 2013. Vinyals, O., Blundell, C., Lillicrap, T., Wierstra, D., et al. Matching networks for one shot learning. In Advances in neural information processing systems, pp. 3630–3638, 2016. Viola, P., Jones, M., et al. Rapid object detection using a boosted cascade of simple features. CVPR (1), 1(511- 518):3, 2001. Wong, E. and Kolter, J. Z. Provable defenses against adver- sarial examples via the convex outer adversarial polytope. arXiv preprint arXiv:1711.00851, 2017. Zhang, H., Yu, Y ., Jiao, J., Xing, E. P., Ghaoui, L. E., and Jor- dan, M. I. Theoretically principled trade-off between ro- bustness and accuracy. arXiv preprint arXiv:1901.08573, 2019. Zhang, R., Isola, P., and Efros, A. A. Colorful image col- orization. In European conference on computer vision, pp. 649–666. Springer, 2016. Zhu, W., Huang, Y ., Vannan, M. A., Liu, S., Xu, D., Fan, W., Qian, Z., and Xie, X. Neural multi-scale self-supervised registration for echocardiogram dense tracking. arXiv preprint arXiv:1906.07357, 2019.Appendix: Test-Time Training with Self-Supervision for Generalization under Distribution Shifts A1. Informal Discussion on Our Variable Decision Boundary In the introduction, we claim that in traditional supervised learning θgives a ﬁxed decision boundary, while ourθgives a variable decision boundary. Here we informally discuss this claim. Denote the input space Xand output space Y. A decision boundary is simply a mapping f : X →Y. Let Θ be a model class e.g Rd. Now consider a family of parametrized functions gθ : X→Y , where θ∈Θ. In the context of deep learning, gis the neural network architecture and θcontains the parameters. We say that f is a ﬁxed decision boundary w.r.t. g and Θ if there exists θ ∈Θ s.t. f(x) = gθ(x) for every x ∈X , and a variable decision boundary if for every x∈X, there exists θ∈Θ s.t. f(x) = gθ(x). Note how selection of θcan depend on xfor a variable decision boundary, and cannot for a ﬁxed one. It is then trivial to verify that our claim is true under those deﬁnitions. A critical reader might say that with an arbitrarily large model class, can’t every decision boundary be ﬁxed? Yes, but this is not the end of the story. Let d = dim( X) × dim(Y), and consider the enormous model class Θ′= Rd which is capable of representing all possible mappings be- tween Xand Y. Let g′ θ′ simply be the mapping represented by θ′ ∈Θ′. A variable decision boundary w.r.t. g and Θ then indeed must be a ﬁxed decision boundary w.r.t. g′and Θ′, but we would like to note two things. First, without any prior knowledge, generalization in Θ′is impossible with any ﬁnite amount of training data; reasoning about g′and Θ′is most likely not productive from an algorithmic point of view, and the concept of a variable decision boundary is to avoid such reasoning. Second, selecting θbased on xfor a variable decision boundary can be thought of as “training” on all points x ∈Rd; however, “training” only happens when necessary, for the xthat it actually encounters. Altogether, the concept of a variable decision boundary is different from what can be described by traditional learning theory. A formal discussion is beyond the scope of this paper and might be of interest to future work. A2. Computational Aspects of Our Method At test time, our method is 2 × batch size × number of iterations times slower than regular test- ing, which only performs a single forward pass for each sample. As the ﬁrst work on Test-Time Training, this paper is not as concerned about computational efﬁciency as improving robustness, but here we provide two poten- tial solutions that might be useful, but have not been thor- oughly veriﬁed. The ﬁrst is to use the thresholding trick on ls, introduced as a solution for the small batches prob- lem in the method section. For the models considered in our experiments, roughly 80% of the test instances fall below the threshold, so Test-Time Training can only be performed on the other 20% without much effect on per- formance, because those 20% contain most of the sam- ples with wrong predictions. The second is to reduce the number of iterations of test-time updates. For the online version, the number of iterations is al- ready 1, so there is nothing to do. For the standard ver- sion, we have done some preliminary experiments setting number of iterations to 1 (instead of 10) and learn- ing rate to 0.01 (instead of 0.001), and observing results almost as good as the standard hyper-parameter setting. A more in depth discussion on efﬁciency is left for future works, which might, during training, explicitly make the model amenable to fast updates. A3. Proofs Here we prove the theoretical results in the main paper. A3.1. The Toy Problem The following setting applies to the two lemmas; this is simply the setting of our toy problem, reproduced here for ease of reference.Test-Time Training with Self-Supervision for Generalization under Distribution Shifts Consider a two layer linear network parametrized by A∈ Rh×d (shared) and v,w ∈Rh (ﬁxed) for the two heads, respectively. Denote x∈Rd the input and y1,y2 ∈R the labels for the two tasks, respectively. For the main task loss lm(A; v) = 1 2 ( y1 −v⊤Ax )2 , (12) and the self-supervised task loss ls(A; w) = 1 2 ( y2 −w⊤Ax )2 , (13) Test-Time Training yields an updated matrix A′←A−η ( y2 −w⊤Ax )( −wx⊤) , (14) where ηis the learning rate. Lemma 1. Following the exposition of the main paper, let η∗= (y1 −v⊤Ax) (y2 −w⊤Ax)v⊤wx⊤x. (15) Assume η∗∈[ϵ,∞) for some ϵ> 0. Then for any η∈(0,ϵ], we are guaranteed an improvement on the main loss i.e. lm(A′) <lm(A). Proof. From the exposition of the main paper, we know that lm(A−η∗∇lsA)) = 0, which can also be derived from simple algebra. Then by convexity, we have lm(A−η∇ls(A)) (16) = lm (( 1 − η η∗ ) A+ η η∗(A−η∗∇ls(A)) ) (17) ≤ ( 1 − η η∗ ) lm(A) + 0 (18) ≤ ( 1 −η ϵ ) lm(A) (19) <lm(A), (20) where the last inequality uses the assumption that lm(A) > 0, which holds because η∗>0. Lemma 2. Deﬁne ⟨U,V⟩= vec (U)⊤vec (V) i.e. the Frobenious inner product, then sign (η∗) = sign (⟨∇lm(A),∇ls(A)⟩) . (21) Proof. By simple algebra, ⟨∇lm(A),∇ls(A)⟩ = ⟨ ( y1 −v⊤Ax )( −vx⊤) , ( y2 −w⊤Ax )( −wx⊤) ⟩ = ( y1 −v⊤Ax )( y2 −w⊤Ax ) Tr ( xv⊤wx⊤) = ( y1 −v⊤Ax )( y2 −w⊤Ax ) v⊤wx⊤x, which has the same sign as η∗. A3.2. Proof of Theorem 1 For any η, by smoothness and convexity, lm(x,y; θ(x)) = lm(x,y; θ−η∇ls(x; θ)) ≤lm(x,y; θ) + η⟨∇lm(x,y; θ),∇ls(x,θ)⟩ + η2β 2 ∥∇ls(x; θ)∥2 . Denote η∗= ⟨∇lm(x,y; θ),∇ls(x,θ)⟩ β∥∇ls(x; θ)∥2 . Then Equation 22 becomes lm(x,y; θ−η∗∇ls(x; θ)) (22) ≤lm(x,y; θ) −⟨∇lm(x,y; θ),∇ls(x,θ)⟩2 2β∥∇ls(x; θ)∥2 . (23) And by our assumptions on the gradient norm and gradient inner product, lm(x,y; θ) −lm(x,y; θ−η∗∇ls(x; θ)) ≥ ϵ2 2βG2 . (24) Because we cannot observe η∗in practice, we instead use a ﬁxed learning rate η = ϵ βG2 , as stated in Theorem 1. Now we argue that this ﬁxed learning rate still improves performance on the main task. By our assumptions, η∗ ≥ ϵ βG2 , so η ∈(0,η∗]. Denote g= ∇ls(x; θ), then by convexity of lm, lm(x,y; θ(x)) = lm(x,y; θ−ηg) (25) = lm ( x,y; ( 1 − η η∗ ) θ+ η η∗(θ−η∗g) ) (26) ≤ ( 1 − η η∗ ) lm(x,y; θ) + η η∗lm(x,y; θ−η∗g) (27) Combining with Equation 24, we have lm(x,y; θ(x)) ≤ ( 1 − η η∗ ) lm(x,y; θ) + η η∗ ( lm(x,y; θ) − ϵ2 2βG2 ) = lm(x,y; θ) − η η∗ ϵ2 2βG2 Since η/η∗>0, we have shown that lm(x,y; θ) −lm(x,y; θ(x)) >0. (28)Test-Time Training with Self-Supervision for Generalization under Distribution Shifts A4. Additional Results on the Common Corruptions Dataset For table aethetics, we use the following abbreviations: B for baseline, JT for joint training, TTT for Test-Time Train- ing standard version, and TTT-Online for online Test-Time Training i.e. the online version. We have abbreviated the names of the corruptions, in order: original test set, Gaussian noise, shot noise, impulse noise, defocus blur, glass blue, motion blur, zoom blur, snow, frost, fog, brightness, contrast, elastic transformation, pixelation, and JPEG compression. A4.1. Results Using Batch Normalization As discussed in the results section, Batch Normalization (BN) is ineffective for small batches, which are the inputs for Test-Time Training (both standard and online version) since there is only one sample available when forming each batch; therefore, our main results are based on a ResNet using Group Normalization (GN). Figure A2 and Table A1 show results of our method on CIFAR-10-C level 5, with a ResNet using Batch Normalization (BN). These results are only meant to be a point of reference for the curious readers. In the early stage of this project, we have experimented with two potential solutions to the small batches problem with BN. The naive solution is to ﬁx the BN layers during Test-Time Training. but this diminishes the performance gains since there are fewer shared parameters. The better solution, adopted for the results below, is hard example mining: instead of updating on all inputs, we only update on inputs that incur large self-supervised task loss ls, where the large improvements might counter the negative effects of inaccurate statistics. Test-Time Training (standard version) is still very effective with BN. In fact, some of the improvements are quite dra- matic, such as on contrast (34%), defocus blue (18%) and Gaussian noise (22% comparing to joint-training, and 16% comparing to the baseline). Performance on the original distribution is still almost the same, and the original error with BN is in fact slightly lower than with GN, and takes half as many epochs to converge. We did not further experiment with BN because of two rea- sons: 1) The online version does not work with BN, because the problem with inaccurate batch statistics is exacerbated when training online for many (e.g. 10000) steps. 2) The baseline error for almost every corruption type is signiﬁ- cantly higher with BN than with GN. Although unrelated to the main idea of our paper, we make the interesting note that GN signiﬁcantly improves model robustness. A4.2. Additional Baseline: Adversarial Logit Pairing As discussed in the results section, Hendrycks & Dietterich (2019) point to Adversarial Logit Pairing (ALP) (Kannan et al., 2018) as an effective method for improving model robustness to corruptions and perturbations, even though it was designed to defend against adversarial attacks. We take ALP as an additional baseline on all benchmarks based on CIFAR-10 (using GN), following the training proce- dure in Kannan et al. (2018) and their recommended hyper- parameters. The implementation of the adversarial attack comes from the codebase of Ding et al. (2019). We did not run ALP on ImageNet because the two papers we reference for this method, Kannan et al. (2018) and Hendrycks & Di- etterich (2019), did not run on ImageNet or make any claim or recommendation. A4.3. Results on CIFAR-10-C and ImageNet-C, Level 5 Table A2 and Table A3 correspond to the bar plots in the results section. Two rows of Table A2 have been presented as Table 1 in the main text. A4.4. Results on CIFAR-10-C, Levels 1-4 The following bar plots and tables are on levels 1-4 of CIFAR-10-C. The original distribution is the same for all levels, so are our results on the original distribution. A4.5. Direct Comparison with Hendrycks et al. (2019a) The following comparison has been requested by an anony- mous reviewer for our ﬁnal version. Our joint training baseline is based on Hendrycks et al. (2019a), but also incor- porates some architectural changes (see below). We found these changes improved the robustness of our method, and felt that it was important to give the baseline the same ben- eﬁt. Note that our joint training baseline overall performs better than Hendrycks: Compare Table S2 to Figure 3 of Hendrycks et al. (2019a) (provided by the authors), our baseline has average error of 22.8% across all corruptions and levels, while their average error is 28.6%. Summary of architectural changes: 1) Group Normalization (GN) instead of Batch Normalization (BN). For complete- ness, the results with BN are provided in Table S1; c.f. GN results in Table S2 which signiﬁcantly improves robustness, with or without self-supervision. 2) We split after the sec- ond residual group, while they split after the third residual group right before the linear layer. This consistently gives about 0.5% - 1% improvement. 3) We use a ResNet-26, while they use a 40-2 Wide ResNet. But our baseline still performs better than their method even though our network is 4x smaller, due to the two tricks above.Test-Time Training with Self-Supervision for Generalization under Distribution Shifts Gaussian Noise  Shot Noise  Impulse Noise  Defocus Blur  Frosted Glass Blur Motion Blur  Zoom Blur  Snow  Frost  Fog Brightness  Contrast  Elastic  Pixelate  JPEG Figure A1.Sample images from the Common Corruptions Benchmark, taken from the original paper by Hendrycks & Dietterich (2019). originalgauss shot impulsedefocus glass motion zoom snow frost fog bright contrastelasticpixelate jpeg 0 20 40 60Error (%) Object recognition task only Joint training (Hendrycks et al. 2019) TTT Figure A2.Test error (%) on CIFAR-10-C, level 5, ResNet-26 with Batch Normalization. orig gauss shot impul defoc glass motn zoom snow frost fog brit contr elast pixel jpeg B 7.9 63.9 58.8 64.3 46.3 54.6 41.6 45.9 31.9 44.0 37.5 13.0 69.2 33.8 61.4 31.7 JT 7.5 70.7 65.6 67.2 43.1 55.4 40.9 42.7 30.3 44.5 42.5 12.7 58.6 30.7 62.6 31.9 TTT 7.9 47.9 45.2 54.8 27.6 50.4 31.5 30.9 28.7 34.3 26.9 12.6 35.2 30.6 51.2 31.3 Table A1.Test error (%) on CIFAR-10-C, level 5, ResNet-26 with Batch Normalization. orig gauss shot impul defoc glass motn zoom snow frost fog brit contr elast pixel jpeg B 8.9 50.5 47.2 56.1 23.7 51.7 24.3 26.3 25.6 34.4 28.1 13.5 25.0 27.4 55.8 29.8 JT 8.1 49.4 45.3 53.4 24.2 48.5 24.8 26.4 25.0 32.5 27.5 12.6 25.3 24.0 51.6 28.7 TTT 7.9 45.6 41.8 50.0 21.8 46.1 23.0 23.9 23.9 30.0 25.1 12.2 23.9 22.6 47.2 27.2 TTT-Online 8.2 25.8 22.6 30.6 14.6 34.4 18.3 17.1 20.0 18.0 16.9 11.2 15.6 21.6 18.1 21.2 UDA-SS 9.0 28.2 26.5 20.8 15.6 43.7 24.5 23.8 25.0 24.9 17.2 12.7 11.6 22.1 20.3 22.6 ALP 16.5 22.7 22.9 28.3 25.0 25.6 27.4 23.1 25.2 27.2 64.8 21.7 73.6 23.0 20.2 18.9 Table A2.Test error (%) on CIFAR-10-C, level 5, ResNet-26. orig gauss shot impul defoc glass motn zoom snow frost fog brit contr elast pixel jpeg B 68.9 1.3 2.0 1.3 7.5 6.6 11.8 16.2 15.7 14.9 15.3 43.9 9.7 16.5 15.3 23.4 JT 69.1 2.1 3.1 2.1 8.7 6.7 12.3 16.0 15.3 15.8 17.0 45.3 11.0 18.4 19.7 22.9 TTT 69.0 3.1 4.5 3.5 10.1 6.8 13.5 18.5 17.1 17.9 20.0 47.0 14.4 20.9 22.8 25.3 TTT-Online 68.8 26.3 28.6 26.9 23.7 6.6 28.7 33.4 35.6 18.7 47.6 58.3 35.3 44.3 47.8 44.3 Table A3.Test accuracy (%) on ImageNet-C, level 5, ResNet-18.Test-Time Training with Self-Supervision for Generalization under Distribution Shifts originalgauss shot impulsedefocus glass motion zoom snow frost fog bright contrastelasticpixelate jpeg 0 10 20 30 40 50Error (%) Object recognition task only Joint training (Hendrycks et al. 2019) TTT TTT-Online Figure A3.Test error (%) on CIFAR-10-C, level 4. See the results section for details. orig gauss shot impul defoc glass motn zoom snow frost fog brit contr elast pixel jpeg B 8.9 46.4 39.2 44.8 15.3 52.5 19.1 20.5 21.3 26.9 13.3 10.5 13.7 20.8 35.3 26.9 JT 8.1 45.0 38.3 42.2 16.4 50.2 20.7 20.5 21.1 25.4 14.1 10.0 14.7 19.0 33.2 25.1 TTT 7.9 41.5 35.4 39.8 15.0 47.8 19.1 18.4 20.1 24.0 13.5 10.0 14.1 17.7 29.4 24.5 TTT-Online 8.2 22.9 20.0 23.9 11.2 35.1 15.6 13.8 18.6 15.9 12.3 9.7 11.9 16.7 13.6 19.8 ALP 16.5 21.3 20.5 24.5 20.7 25.9 23.7 21.4 24.2 23.9 42.2 17.5 53.7 22.1 19.1 18.5 Table A4.Test error (%) on CIFAR-10-C, level 4, ResNet-26. originalgauss shot impulsedefocus glass motion zoom snow frost fog bright contrastelasticpixelate jpeg 0 10 20 30 40Error (%) Object recognition task only Joint training (Hendrycks et al. 2019) TTT TTT-Online Figure A4.Test error (%) on CIFAR-10-C, level 3. See the results section for details. orig gauss shot impul defoc glass motn zoom snow frost fog brit contr elast pixel jpeg B 8.9 42.2 35.1 30.7 12.2 41.7 18.6 17.5 19.0 25.3 10.8 9.7 11.6 15.3 21.7 24.6 JT 8.1 40.2 34.4 29.9 12.2 37.9 20.8 17.3 18.4 25.0 11.4 9.2 12.0 15.2 20.8 22.8 TTT 7.9 37.2 31.6 28.6 11.5 35.8 19.1 15.8 17.8 23.3 11.0 9.1 11.6 14.3 18.9 22.3 TTT-Online 8.2 21.3 17.7 17.9 9.0 23.4 15.3 12.5 16.4 15.8 10.9 9.0 10.7 12.8 12.2 18.7 ALP 16.5 20.0 19.3 20.5 19.2 21.2 24.0 20.5 20.9 24.2 30.1 16.6 39.6 20.9 17.8 18.0 Table A5.Test error (%) on CIFAR-10-C, level 3, ResNet-26.Test-Time Training with Self-Supervision for Generalization under Distribution Shifts originalgauss shot impulsedefocus glass motion zoom snow frost fog bright contrastelasticpixelate jpeg 0 10 20 30 40Error (%) Object recognition task only Joint training (Hendrycks et al. 2019) TTT TTT-Online Figure A5.Test error (%) on CIFAR-10-C, level 2. See the results section for details. orig gauss shot impul defoc glass motn zoom snow frost fog brit contr elast pixel jpeg B 8.9 31.7 22.6 24.3 9.9 42.6 14.9 14.7 21.7 18.4 9.8 9.1 10.0 13.1 17.1 22.4 JT 8.1 31.0 22.6 23.4 9.1 39.2 16.4 14.2 21.2 17.5 9.4 8.3 10.6 12.8 15.9 20.5 TTT 7.9 28.8 20.7 23.0 9.0 36.6 15.4 13.1 20.2 16.9 9.2 8.3 10.2 12.5 14.8 19.7 TTT-Online 8.2 16.8 13.8 15.5 8.5 23.4 13.3 11.5 16.8 12.7 9.4 8.4 9.7 12.4 11.5 17.0 ALP 16.5 18.0 17.2 19.0 17.8 20.7 21.2 19.3 19.0 20.1 22.4 16.3 29.2 20.3 17.4 17.8 Table A6.Test error (%) on CIFAR-10-C, level 2, ResNet-26. originalgauss shot impulsedefocus glass motion zoom snow frost fog bright contrastelasticpixelate jpeg 0 10 20 30 40Error (%) Object recognition task only Joint training (Hendrycks et al. 2019) TTT TTT-Online Figure A6.Test error (%) on CIFAR-10-C, level 1. See the results section for details. orig gauss shot impul defoc glass motn zoom snow frost fog brit contr elast pixel jpeg B 8.9 21.7 17.1 17.0 9.0 44.0 12.1 13.9 14.3 13.4 9.2 8.9 9.0 13.2 12.0 17.3 JT 8.1 20.4 16.6 16.9 8.2 40.5 12.2 13.0 13.1 12.3 8.4 8.1 8.5 12.9 11.3 15.9 TTT 7.9 19.1 15.8 16.5 8.0 37.9 11.7 12.2 12.8 11.9 8.2 8.0 8.3 12.6 11.1 15.5 TTT-Online 8.2 13.8 11.9 12.2 8.5 24.4 10.5 11.5 12.4 10.7 8.5 8.3 8.6 12.4 10.7 14.4 ALP 17.0 16.8 17.6 16.8 20.9 18.7 19.0 17.3 17.5 17.4 16.1 18.4 20.4 17.0 17.2 17.5 Table A7.Test error (%) on CIFAR-10-C, level 1, ResNet-26.Test-Time Training with Self-Supervision for Generalization under Distribution Shifts Figure A7.Sample Images from the VID-Robust dataset (Shankar et al., 2019) in the results section adapted to CIFAR-10. Each row shows eight sample images from one class. The seven classes shown are, in order: airplane, bird, car, dog, cat, horse, ship.",
      "meta_data": {
        "arxiv_id": "1909.13231v3",
        "authors": [
          "Yu Sun",
          "Xiaolong Wang",
          "Zhuang Liu",
          "John Miller",
          "Alexei A. Efros",
          "Moritz Hardt"
        ],
        "published_date": "2019-09-29T08:09:15Z",
        "pdf_url": "https://arxiv.org/pdf/1909.13231v3.pdf"
      },
      "llm_extracted_info": {
        "main_contributions": "The paper proposes Test-Time Training (TTT), a novel approach to improve the generalization of predictive models under distribution shifts between training and test data. TTT converts a single unlabeled test sample into a self-supervised learning problem, on which model parameters are updated before making a prediction. An online version of TTT is introduced, which further enhances performance by preserving parameter states across sequential test samples. The method achieves substantial improvements on diverse image classification benchmarks, including corrupted images (CIFAR-10-C, ImageNet-C), video frames (VID-Robust), and datasets with unknown distribution shifts (CIFAR-10.1), while maintaining or slightly improving performance on the original distribution. The authors also provide theoretical insights, showing that a positive gradient correlation between the main and self-supervised task losses is a sufficient condition for TTT to reduce the main task loss in convex models, which is empirically validated.",
        "methodology": "The core methodology involves a neural network with a Y-structure, consisting of a shared feature extractor and two task-specific branches for the main task (e.g., classification) and a self-supervised auxiliary task. The auxiliary task used is rotation prediction, where the model predicts the angle (0, 90, 180, or 270 degrees) of a rotated input image. Models are initially trained jointly on both tasks using multi-task learning on the training data. At test time, for each unlabeled test sample, the shared feature extractor's parameters are fine-tuned by minimizing only the self-supervised auxiliary task loss on that specific sample. The standard TTT takes ten gradient steps per test sample. The online version of TTT initializes parameters with those adapted from the previous test sample in a sequence and takes one gradient step per new image. Group Normalization (GN) is used in the architecture instead of Batch Normalization (BN) to mitigate issues with small batch sizes during test-time updates.",
        "experimental_setup": "The method was evaluated on three categories of benchmarks for distribution shifts: 1. Object recognition on corrupted images using CIFAR-10-C and ImageNet-C datasets (15 corruption types, 5 severity levels). 2. Object recognition on video frames using the VID-Robust dataset. 3. Object recognition on the CIFAR-10.1 dataset, which has subtle, unknown distribution shifts. The architectures used were ResNets (26-layer for CIFAR-10, 18-layer for ImageNet). Baselines included a plain ResNet ('object recognition task only'), a model jointly trained on both tasks but fixed at test time ('joint training'), Unsupervised Domain Adaptation by Self-Supervision (UDA-SS) as an oracle comparison, and Adversarial Logit Pairing (ALP) for robustness comparison. Optimization for both joint training and TTT used Stochastic Gradient Descent (SGD) with specific learning rates (0.001 for TTT), zero weight decay, and zero momentum for TTT. Data augmentation (random crop, horizontal flip) was applied. Performance was measured using test error (%) or accuracy (%).",
        "limitations": "Test-Time Training (TTT) incurs a higher computational cost compared to standard inference, being approximately '2 × batch size × number of iterations' times slower. The effectiveness of TTT is dependent on the self-supervised task being 'well-defined and non-trivial'; for instance, it showed limited improvement on classes where rotational cues were ambiguous or trivial (e.g., 'airplane' class with black margins). Batch Normalization (BN) is not effective with the single-image batch sizes typically used in TTT, necessitating the use of Group Normalization (GN). While empirical evidence extends the theoretical findings, the formal theoretical guarantees are primarily for smooth and convex loss functions.",
        "future_research_directions": "Future work includes extending Test-Time Training to other machine learning tasks like segmentation, detection, speech recognition, and natural language processing. There is potential to leverage domain-specific expertise to design more effective special-purpose self-supervised tasks. The authors suggest using TTT as a new evaluation benchmark for general-purpose self-supervised tasks. Further research into improving the computational efficiency of TTT, potentially through strategies like thresholding updates based on self-supervised loss or designing models more amenable to fast updates during training, is encouraged. More broadly, the paper advocates for abandoning the traditional fixed decision boundary constraint at test time and the artificial separation of training and testing, moving towards a paradigm where learning is continuous and occurs after model deployment. A formal theoretical discussion on the concept of a variable decision boundary is also suggested."
      }
    },
    {
      "title": "Towards Stable Test-time Adaptation in Dynamic Wild World",
      "abstract": "Test-time adaptation (TTA) has shown to be effective at tackling distribution\nshifts between training and testing data by adapting a given model on test\nsamples. However, the online model updating of TTA may be unstable and this is\noften a key obstacle preventing existing TTA methods from being deployed in the\nreal world. Specifically, TTA may fail to improve or even harm the model\nperformance when test data have: 1) mixed distribution shifts, 2) small batch\nsizes, and 3) online imbalanced label distribution shifts, which are quite\ncommon in practice. In this paper, we investigate the unstable reasons and find\nthat the batch norm layer is a crucial factor hindering TTA stability.\nConversely, TTA can perform more stably with batch-agnostic norm layers, \\ie,\ngroup or layer norm. However, we observe that TTA with group and layer norms\ndoes not always succeed and still suffers many failure cases. By digging into\nthe failure cases, we find that certain noisy test samples with large gradients\nmay disturb the model adaption and result in collapsed trivial solutions, \\ie,\nassigning the same class label for all samples. To address the above collapse\nissue, we propose a sharpness-aware and reliable entropy minimization method,\ncalled SAR, for further stabilizing TTA from two aspects: 1) remove partial\nnoisy samples with large gradients, 2) encourage model weights to go to a flat\nminimum so that the model is robust to the remaining noisy samples. Promising\nresults demonstrate that SAR performs more stably over prior methods and is\ncomputationally efficient under the above wild test scenarios.",
      "meta_data": {
        "arxiv_id": "2302.12400v1",
        "authors": [
          "Shuaicheng Niu",
          "Jiaxiang Wu",
          "Yifan Zhang",
          "Zhiquan Wen",
          "Yaofo Chen",
          "Peilin Zhao",
          "Mingkui Tan"
        ],
        "published_date": "2023-02-24T02:03:41Z",
        "pdf_url": "https://arxiv.org/pdf/2302.12400v1.pdf"
      }
    },
    {
      "title": "Towards Stable Test-time Adaptation in Dynamic Wild World",
      "abstract": "Test-time adaptation (TTA) has shown to be effective at tackling distribution\nshifts between training and testing data by adapting a given model on test\nsamples. However, the online model updating of TTA may be unstable and this is\noften a key obstacle preventing existing TTA methods from being deployed in the\nreal world. Specifically, TTA may fail to improve or even harm the model\nperformance when test data have: 1) mixed distribution shifts, 2) small batch\nsizes, and 3) online imbalanced label distribution shifts, which are quite\ncommon in practice. In this paper, we investigate the unstable reasons and find\nthat the batch norm layer is a crucial factor hindering TTA stability.\nConversely, TTA can perform more stably with batch-agnostic norm layers, \\ie,\ngroup or layer norm. However, we observe that TTA with group and layer norms\ndoes not always succeed and still suffers many failure cases. By digging into\nthe failure cases, we find that certain noisy test samples with large gradients\nmay disturb the model adaption and result in collapsed trivial solutions, \\ie,\nassigning the same class label for all samples. To address the above collapse\nissue, we propose a sharpness-aware and reliable entropy minimization method,\ncalled SAR, for further stabilizing TTA from two aspects: 1) remove partial\nnoisy samples with large gradients, 2) encourage model weights to go to a flat\nminimum so that the model is robust to the remaining noisy samples. Promising\nresults demonstrate that SAR performs more stably over prior methods and is\ncomputationally efficient under the above wild test scenarios.",
      "meta_data": {
        "arxiv_id": "2302.12400v1",
        "authors": [
          "Shuaicheng Niu",
          "Jiaxiang Wu",
          "Yifan Zhang",
          "Zhiquan Wen",
          "Yaofo Chen",
          "Peilin Zhao",
          "Mingkui Tan"
        ],
        "published_date": "2023-02-24T02:03:41Z",
        "pdf_url": "https://arxiv.org/pdf/2302.12400v1.pdf"
      }
    },
    {
      "title": "Improved Test-Time Adaptation for Domain Generalization",
      "abstract": "The main challenge in domain generalization (DG) is to handle the\ndistribution shift problem that lies between the training and test data. Recent\nstudies suggest that test-time training (TTT), which adapts the learned model\nwith test data, might be a promising solution to the problem. Generally, a TTT\nstrategy hinges its performance on two main factors: selecting an appropriate\nauxiliary TTT task for updating and identifying reliable parameters to update\nduring the test phase. Both previous arts and our experiments indicate that TTT\nmay not improve but be detrimental to the learned model if those two factors\nare not properly considered. This work addresses those two factors by proposing\nan Improved Test-Time Adaptation (ITTA) method. First, instead of heuristically\ndefining an auxiliary objective, we propose a learnable consistency loss for\nthe TTT task, which contains learnable parameters that can be adjusted toward\nbetter alignment between our TTT task and the main prediction task. Second, we\nintroduce additional adaptive parameters for the trained model, and we suggest\nonly updating the adaptive parameters during the test phase. Through extensive\nexperiments, we show that the proposed two strategies are beneficial for the\nlearned model (see Figure 1), and ITTA could achieve superior performance to\nthe current state-of-the-art methods on several DG benchmarks. Code is\navailable at https://github.com/liangchen527/ITTA.",
      "full_text": "Improved Test-Time Adaptation for Domain Generalization Liang Chen1 Yong Zhang2* Yibing Song3 Ying Shan2 Lingqiao Liu1∗ 1 The University of Adelaide 2 Tencent AI Lab 3 AI3 Institute, Fudan University {liangchen527, zhangyong201303, yibingsong.cv}@gmail.com yingsshan@tencent.com lingqiao.liu@adelaide.edu.au Abstract The main challenge in domain generalization (DG) is to handle the distribution shift problem that lies between the training and test data. Recent studies suggest that test-time training (TTT), which adapts the learned model with test data, might be a promising solution to the problem. Gen- erally, a TTT strategy hinges its performance on two main factors: selecting an appropriate auxiliary TTT task for up- dating and identifying reliable parameters to update during the test phase. Both previous arts and our experiments in- dicate that TTT may not improve but be detrimental to the learned model if those two factors are not properly consid- ered. This work addresses those two factors by proposing an Improved Test-Time Adaptation (ITTA) method. First, in- stead of heuristically defining an auxiliary objective, we pro- pose a learnable consistency loss for the TTT task, which con- tains learnable parameters that can be adjusted toward bet- ter alignment between our TTT task and the main prediction task. Second, we introduce additional adaptive parameters for the trained model, and we suggest only updating the adap- tive parameters during the test phase. Through extensive ex- periments, we show that the proposed two strategies are ben- eficial for the learned model (see Figure 1), and ITTA could achieve superior performance to the current state-of-the-art methods on several DG benchmarks. Code is available at https://github.com/liangchen527/ITTA. 1. Introduction Recent years have witnessed the rapid development of deep learning models, which often assume the training and test data are from the same domain and follow the same distribution. However, this assumption does not always hold in real-world scenarios. Distribution shift among the source and target domains is ubiquitous in related areas [35], such as autonomous driving or object recognition tasks, resulting *Corresponding authors. This work is done when L. Chen is an intern in Tencent AI Lab. 0.5 1.1 0.5 1.2 0.5 0.5 0.5 1.4 0.4 0.4 0.4 0.3 art cartoon photo sketch 79.9 75.4 94.4 75.8 83.3 76.0 94.4 76.7 84.7 78.0 94.5 78.2 Figure 1. Performance improvements from the proposed two strate- gies (i.e. introducing a learnable consistency loss and including additional adaptive parameters to improve TTT) for the baseline model (i.e. ResNet18 [30] with existing augmentation strategy [75]). Experiments are conducted on the PACS dataset [37] with the leave- one-out setting. Following [27], we use 60 sets of random seeds and hyper-parameters for each target domain. The reported average accuracy and error bars verify the effectiveness of our method. in poor performances for delicately designed models and hindering the further application of deep learning techniques. Domain generalization (DG) [2,8,16,23,24,31,38 –40,40, 44, 47, 51, 52, 69], designed to generalize a learned model to unseen target domains, has attracted a great deal of attention in the research community. The problem can be traced back to a decade ago [7], and various approaches have been pro- posed to push the DG boundary ever since. Those efforts in- clude invariant representation learning [28,47,49,58], adver- sarial learning [23,40,44,69], augmentation [9,41,42,66,75], or meta-learning [2, 16, 38, 39]. Despite successes on certain occasions, a recent study [27] shows that, under a rigorous evaluation protocol, most of these arts are inferior to the baseline empirical risk minimization (ERM) method [61]. This finding is not surprising, as most current arts strive to decrease the distribution shift only through the training data while overlooking the contributions from test samples. Recently, the test-time training (TTT) technique [60] has been gaining momentum for easing the distribution shift problem. TTT lies its success in enabling dynamic tuning of the pretrained model with the test samples via an auxil- iary TTT task, which seems to be a promising effort when arXiv:2304.04494v2  [cs.CV]  16 Apr 2023confronting data from different domains. However, TTT is not guaranteed to improve the performance. Previous arts [46, 63] indicate that selecting an appropriate auxiliary TTT task is crucial, and an inappropriate one that does not align with the main loss may deteriorate instead of improv- ing the performance. Meanwhile, it is pointed out in [63] that identifying reliable parameters to update is also essential for generalization, which is in line with our experimental findings in Sec. 5.3. Both of these two tasks are non-trivial, and there are limited efforts made to address them. This paper aims to improve the TTT strategy for better DG. First, different from previous works that empirically define auxiliary objectives and assume they are aligned with the main task, our work does not make such assumptions. Instead, we suggest learning an appropriate auxiliary loss for test-time updating. Specifically, encouraged by recent successes in multi-view consistency learning [13,26,29], we propose to augment the consistency loss by adding learn- able parameters based on the original implementation, where the parameters can be adjusted to assure our TTT task can be more aligned with the main task and are updated by en- forcing the two tasks share the same optimization direction. Second, considering that identifying reliable parameters to update is an everlasting job given the growing size of current deep models, we suggest introducing new adaptive param- eters after each block during the test phase, and we only tune the new parameters by the learned consistency loss while leaving the original parameters unchanged. Through extensive evaluations on the current benchmark [27], we illustrate that the learnable consistency loss performs more effectively than the self-supervised TTT tasks adopted in previous arts [60, 63], and by tuning only the new adaptive parameters, our method is superior to existing strategies that update all the parameters or part of them. This work aims to ease the distribution shift problem by improving TTT, and the main contributions are three-fold: • We introduce a learnable consistency loss for test-time adaptation, which can be enforced to be more aligned with the main loss by tuning its learnable parameters. • We introduce new adaptive parameters for the trained model and only update them during the test phase. • We conduct experiments on various DG benchmarks and illustrate that our ITTA performs competitively against current arts under the rigorous setting [27] for both the multi-source and single-source DG tasks. 2. Related Works 2.1. Domain Generalization. Being able to generalize to new environments while de- ploying is a challenging and practical requirement for cur- rent deep models. Existing DG approaches can be roughly categorized into three types. (1) Invariant representation learning: The pioneering work [5] theoretically proves that if the features remain invariant across different domains, then they are general and transferable to different domains. Guided by this finding, [47] uses maximum mean discrep- ancy (MMD) to align the learned features, and [25] proposes to use a multi-domain reconstruction auto-encoder to obtain invariant features. More recently, [58] suggests maximiz- ing the inner product of gradients from different domains to enforce invariance, and a similar idea is proposed in [52] where these gradients are expected to be similar to their mean values. (2) Optimization algorithms: Among the different optimization techniques adopted in DG, prevail- ing approaches resort to adversarial learning [23, 40, 44, 69] and meta-learning [2, 16, 38, 39]. Adversarial training is often used to enforce the learned features to be agnostic about the domain information. In [23], a domain-adversarial neural network (DANN) is implemented by asking the main- stream feature to maximize the domain classification loss. This idea is also adopted in [44], where adversarial training and an MMD constraint are employed to update an auto- encoder. Meanwhile, the meta-learning technique is used to simulate the distribution shifts between seen and unseen environments [2, 16, 38, 39], and most of these works are developed based on the MAML framework [20]. (3) Aug- mentation: Most augmentation skills applied in the general- ization tasks are operated in the feature level [34, 41, 48, 75] except for [11,66,68] which mix images [68] or its phase [66] to synthesize new data. To enable contrastive learning, we incorporate an existing augmentation strategy [75] in our framework. This method originated from AdaIN [32], which synthesizes new domain information by mixing the statistics of the features. Similar ideas can be found in [42, 48]. 2.2. Test-Time Training and Adaptation Test-Time Training (TTT) is first introduced in [60]. The basic paradigm is to employ a test-time task besides the main task during the training phase and update the pre- trained model using the test data with only the test-time objective before the final prediction step. The idea is empir- ically proved effective [60] and further developed in other related areas [3, 10, 12, 14, 21, 22, 43, 56, 63, 65, 73, 74]. Most current works focus on finding auxiliary tasks for updat- ing during the test phase, and the efforts derive from self- supervion [3, 10, 21, 22, 43, 60], meta-learning [65, 73, 74], information entropy [63], pseudo-labeling [12, 14], to name a few. However, not all empirically selected test-time tasks are effective. A recent study [46] indicates that only when the auxiliary loss aligns with the main loss can TTT improve the trained model. Inspired by that, we propose a learnable consistency loss and enforce alignment between the two ob- jectives. Results show that our strategy can be beneficial for the trained model (see Figure 1).subtract Figure 2. Training process of ITTA. We use x from the source domain as input for the feature extractor fθ(·) to obtain the repre- sentation z and its augmented version z′, where the augmentation skill from [75] is applied. The classifier fϕ(·) and weight subnet- work fw(·) are used to compute the main loss Lmain and learnable consistency loss Lwcont. Please refer to our text for details. Meanwhile, [63] suggests that auxiliary loss is not the only factor that affects the performance. Selecting reliable parameters to update is also crucial within the TTT frame- work. Given the large size of current models, correctly iden- tifying these parameters may require tremendous amounts of effort. To this end, instead of heuristically selecting candi- dates, we propose to include new adaptive parameters for up- dating during the test phase. Experimental results show that the proposed method can obtain comparable performances against existing skills. 3. Methodology In the task of DG, we are often given access to data from S (S ≥ 1) source domains Ds = {D1, D2, ..., DS} and expect a model to make good prediction on unseen target domains Dt = {D1, D2, ..., DT } (T ≥ 1). Our method aims to improve the test-time training (TTT) strategy for better DG. The improvements are two-fold. First, we pro- pose a learnable consistency loss for the TTT task, which could be enforced to align with the main objective by tuning its learnable weights. Second, we suggest including addi- tional adaptive parameters and only updating these adaptive parameters during the test phase. 3.1. A Learnable Consistency Loss for TTT The TTT strategies have shown promising performances when dealing with distribution shift problems [43, 63]. How- ever, their successes are depended on the empirically selected auxiliary TTT tasks, which may deteriorate the performances if chosen improperly. Motivated by the recent successes in multi-view consistency learning [13, 26, 29], we suggest adopting a consistency loss in our TTT task. Note that the naive consistency loss is still not guaranteed to be effective as prior art [46] indicates that only when the auxiliary loss aligns with the main loss, can TTT improves the perfor- mance. To this end, we propose to augment the auxiliary loss with learnable parameters that could be adjusted toward a better alignment between the TTT and main tasks. In our case, we make the adopted consistency loss learnable by introducing a weight subnetwork that allows flexible ways Algorithm 1 Pseudo code of the training phase of ITTA in a PyTorch-like style. # fθ, fϕ, fw: feature extractor, classifier, weight subnetwork # α, 0: weight paramter, all zero tensor # training process for x, yin training loader: # load a minibatch with N samples def forward process(x, y): z, z′ = fθ.forward(x) # computing losses Lmain = CrossEntropyLoss(fϕ.forward(z), y) Lmain+ =CrossEntropyLoss(fϕ.forward(z′), y) Lwcont = MSELoss(fw.forward(z − z′), 0) return Lmain, Lwcont # SGD update: feature extractor and classifier Lmain, Lwcont = forward process(x, y) ([fθ.params, fϕ.params]).zero grad() (Lmain + αLwcont).backward() update( \u0002 fθ.params, fϕ.params \u0003 ) # compute objectives for updating weight subnetwork Lmain, Lwcont = forward process(x, y) Lmain.backward() ˆgmain = fθ.params.grad.clone().normalize() fθ.params.zero grad() Lwcont.backward() ˆgwcont = fθ.params.grad.clone().normalize() # SGD update: weight subnetwork MSELoss(ˆgmain, ˆgwcont).backward() fw.params.zero grad() update(fw.params) to measure the consistency between two views of the same instance. We first introduce the pipeline of our training framework. Given the D dimensional representation z ∈ RD1 and its corresponding augmented version z′ that are obtained from a feature extractor (i.e. {z, z′} = fθ(x), where x is an input image from Ds, and fθ(·) is the feature extractor parame- terized by θ. In our implementation, we use the existing augmentation method [75] to obtain z′ by modifying the intermediate activation in fθ(x). We show in our supplemen- tary material that our framework can also thrive with other augmentation strategies), our learnable consistency loss is given by, Lwcont = ∥fw(z − z′)∥, (1) where ∥ · ∥denotes the L2 norm; fw(·) is the weight sub- network parameterized by w. To make the training process more stable and potentially achieve better performance, we apply a dimension-wise nonlinear function to map each di- mension of z − z′ before calculating the L2 norm. That is, ∀h ∈ RD, fw(h) is implemented by stacking layers of a nonlinear function: ReLU(a ∗ h + b), where a ∈ RD and b ∈ RD are the weight and bias from the nonlinear function, 1We omit the batch dimensions of the variables for simplicity.… … subtract Figure 3. Test adaptation process of ITTA. Different from that in the training stage, we include additional adaptive parameters fΘ after each block of the feature extractor fθ. For each test sample x, the intermediate representations zi and z′i obtained from fi θ are passed to fi Θ before going to the next block fi+1 θ . We use the learnable consistency loss Lwcont as the objective to update fΘ. Please refer to our text for details. and different layers of a, bform the parameter w in fw. In effect, this creates a piecewise-linear mapping function for h: depending on the value of h, the output could be 0, a constant, or a scaling-and-shifted version of h. More studies about the design of fw are provided in our supplementary material. Compared to the naive consistency learning with- out fw, our Lwcont can be more flexible with an adjustable fw, which we show in the following is the key for learning an appropriate loss in the improved TTT framework. Combining Lwcont with the main loss Lmain which applies the cross-entropy loss (CE) for both the origi- nal and augmented inputs ( i.e. Lmain = CE(fϕ(z), y) + CE(fϕ(z′), y), where fϕ is the classifier parameterized by ϕ, and y is the corresponding label), the objective for the feature extractor and classifier can be formulated into, min{θ,ϕ} Lmain + αLwcont, (2) where α is the weight parameter that balances the contri- butions from the two terms. A simple illustration of the workflow is shown in Figure 2. From Eq. (2), the expected gradients for the feature ex- tractor from Lmain and Lwcont can be represented as, \u001a gmain = ∇θ(CE(fϕ(z), y) + CE(fϕ(z′), y)), (3) gwcont = ∇θ∥fw(z − z′)∥. (4) We observe that the direction of gwcont is also determined by the weight subnetwork fw(·), which should be close with gmain to ensure alignment between Lmain and Lwcont [46, 60]. To this end, we propose a straightforward solution by enforcing equality between the normalized versions of gmain and gwcont, and we use this term as the objective for updating fw(·), which gives, min w Lalign, s.t. Lalign = ∥ˆgmain − ˆgwcont∥, (5) where ˆgmain = gmain−Egmain σgmain , and similar for ˆgwcont. In our implementation, we update {θ, ϕ} and w in an alternative manner. Pseudo code of the training process are shown in Algorithm 1. Algorithm 2 Pseudo code of the test phase of ITTA in a PyTorch-like style. # fθ, fϕ: feature extractor, classifier # fw, fΘ: weight subnetwork, additional adaptive blocks # m, 0: total number of blocks in fθ, all zero tensor # test process for x in test loader: # load a test batch def forward process(x): z1, z′1 = f1 Θ.forward((f1 θ .forward(x))) # first blocks for i in range(2, m + 1): # the following m − 1 blocks zi, z′i = fi θ.forward(zi−1), fi θ.forward(z′i−1) zi, z′i = fi Θ.forward(zi), fi Θ.forward(z′i) return zi, z′i # test adaptation phase: SGD update additional adaptive parameters z, z′ = forward process(x) Lwcont = MSELoss(fw.forward(z − z′), 0) fΘ.params.zero grad() Lwcont.backward() update(fΘ.params) # final prediction z, = forward process(x) result = fϕ.forward(z) 3.2. Including Additional Adaptive Parameters Selecting expressive and reliable parameters to update during the test phase is also essential in the TTT frame- work [63]. Some strategies decide to update all the parame- ters from the feature extractor [3, 43], while others use only the parameters from the specific layers for updating [63, 71]. Given the fact that the sizes of current deep models are often very large and still growing, exhaustively trying different combinations among the millions of candidates seems to be an everlasting job. As there are no consensuses on which parameter should be updated, we suggest another easy alter- native in this work. Specifically, assuming there are a total of m blocks in the pretrained feature extractor fθ(·), and the i-th block can be denoted as fi θ(·). Then the intermediate representation zi from fi θ(·) can be formulated as, zi = fi θ(zi−1), s.t. z1 = f1 θ (x). (6) We propose to include additional adaptive blockfΘ that is parameterized by Θ after each block of fθ during the test- time adaptation phase, which reformulates Eq. (6) into, zi = fi Θ(fi θ(zi−1)), s.t. z1 = f1 Θ(f1 θ (x)), (7) where fΘ(·) does not change the dimension and sizes of the intermediate representations. In our work, we use a structure similar to fw to implement fΘ. Note zm is simplified as z in this phase, and the same process is applied for obtaining z′. Then, in the test-time adaptation phase, we suggest only updating the new adaptive parameters via the learned con- sistency loss. The optimization process can be written as,Table 1. Multi sources domain generalization. Experiments are conducted on the DomainBed benchmark [27]. All methods are examined for 60 trials in each unseen domain. Top5 accumulates the number of datasets where a method achieves the top 5 performances. The score here accumulates the numbers of the dataset where a specific art obtains larger accuracy than ERM on account of the variance. Best results are colored as red. Among the 22 methods compared, less than a quarter outperforms ERM in most datasets (Score ≥ 3). PACS VLCS OfficeHome TerraInc DomainNet Avg. Top5↑ Score↑ MMD [40] 81.3 ± 0.8 74.9 ± 0.5 59.9 ± 0.4 42.0 ± 1.0 7.9 ± 6.2 53.2 1 2 RSC [33] 80.5 ± 0.2 75.4 ± 0.3 58.4 ± 0.6 39.4 ± 1.3 27.9 ± 2.0 56.3 0 1 IRM [1] 80.9 ± 0.5 75.1 ± 0.1 58.0 ± 0.1 38.4 ± 0.9 30.4 ± 1.0 56.6 0 1 ARM [72] 80.6 ± 0.5 75.9 ± 0.3 59.6 ± 0.3 37.4 ± 1.9 29.9 ± 0.1 56.7 0 0 DANN [23] 79.2 ± 0.3 76.3 ± 0.2 59.5 ± 0.5 37.9 ± 0.9 31.5 ± 0.1 56.9 1 1 GroupGRO [55] 80.7 ± 0.4 75.4 ± 1.0 60.6 ± 0.3 41.5 ± 2.0 27.5 ± 0.1 57.1 0 1 CDANN [44] 80.3 ± 0.5 76.0 ± 0.5 59.3 ± 0.4 38.6 ± 2.3 31.8 ± 0.2 57.2 0 0 VREx [36] 80.2 ± 0.5 75.3 ± 0.6 59.5 ± 0.1 43.2 ± 0.3 28.1 ± 1.0 57.3 1 1 CAD [53] 81.9 ± 0.3 75.2 ± 0.6 60.5 ± 0.3 40.5 ± 0.4 31.0 ± 0.8 57.8 1 2 CondCAD [53] 80.8 ± 0.5 76.1 ± 0.3 61.0 ± 0.4 39.7 ± 0.4 31.9 ± 0.7 57.9 0 1 MTL [6] 80.1 ± 0.8 75.2 ± 0.3 59.9 ± 0.5 40.4 ± 1.0 35.0 ± 0.0 58.1 0 0 ERM [61] 79.8 ± 0.4 75.8 ± 0.2 60.6 ± 0.2 38.8 ± 1.0 35.3 ± 0.1 58.1 1 - MixStyle [75] 82.6 ± 0.4 75.2 ± 0.7 59.6 ± 0.8 40.9 ± 1.1 33.9 ± 0.1 58.4 1 1 MLDG [38] 81.3 ± 0.2 75.2 ± 0.3 60.9 ± 0.2 40.1 ± 0.9 35.4 ± 0.0 58.6 1 1 Mixup [68] 79.2 ± 0.9 76.2 ± 0.3 61.7 ± 0.5 42.1 ± 0.7 34.0 ± 0.0 58.6 2 2 Fishr [52] 81.3 ± 0.3 76.2 ± 0.3 60.9 ± 0.3 42.6 ± 1.0 34.2 ± 0.3 59.0 2 2 SagNet [48] 81.7 ± 0.6 75.4 ± 0.8 62.5 ± 0.3 40.6 ± 1.5 35.3 ± 0.1 59.1 1 2 SelfReg [34] 81.8 ± 0.3 76.4 ± 0.7 62.4 ± 0.1 41.3 ± 0.3 34.7 ± 0.2 59.3 2 3 Fish [58] 82.0 ± 0.3 76.9 ± 0.2 62.0 ± 0.6 40.2 ± 0.6 35.5 ± 0.0 59.3 3 4 CORAL [59] 81.7 ± 0.0 75.5 ± 0.4 62.4 ± 0.4 41.4 ± 1.8 36.1 ± 0.2 59.4 2 3 SD [51] 81.9 ± 0.3 75.5 ± 0.4 62.9 ± 0.2 42.0 ± 1.0 36.3 ± 0.2 59.7 4 4 Ours 83.8 ± 0.3 76.9 ± 0.6 62.0 ± 0.2 43.2 ± 0.5 34.9 ± 0.1 60.2 4 4 min Θ ∥fw(z − z′)∥, s.t. {z, z′} = fΘ(fθ(x)). (8) Note that different from the training phase, x in this stage is from the target domain Dt, and we use the online setting in [60] for updating. A simple illustration of the test adaptation pipeline is shown in Figure 3. For the final step, we use the original representation ob- tained from the pretrained feature extractor and the adapted adaptive parameters for prediction. Pseudo code of the test stage are shown in Algorithm 2. 4. Experiments 4.1. Settings Datasets. We evalute ITTA on five benchmark datasets: PACS [37] which consists of 9,991 images from 7 cate- gories. This dataset is probably the most widely-used DG benchmark owing to its large distributional shift across 4 do- mains including art painting, cartoon, photo, and sketch; VLCS [18] contains 10,729 images of 5 classes from 4 different datasets (i.e. domains) including PASCAL VOC 2007 [17], LabelMe [54], Caltech [19], and Sun [64] where each dataset is considered a domain in DG;OfficeHome [62] is composed of 15,588 images from 65 classes in office and home environments, and those images can be categorized into 4 domains (i.e. artistic, clipart, product, and real world); TerraInc [4] has 24,788 images from 10 classes. Those images are wild animals taken from 4 different locations (i.e. domains) including L100, L38, L43, and L46; Domain- Net [50] which contains 586,575 images from 345 classes, and the images in it can be depicted in 6 styles (i.e. clipart, infograph, painting, quickdraw, real, and sketch). Implementation details. For all the experiments, we use the ImageNet [15] pretrained ResNet18 [30] backbone that with 4 blocks as the feature extractor fθ, which could en- large the gaps in DG compared to larger models [70]. Corre- spondingly, we also include 4 blocks of additional adaptive parameters (i.e. fΘ), and each block is implemented with 5 layers of learnable parameters with weight initialized as all ones and bias initialized as all zeros. For the weight subnet- work fw, we use 10 layers of learnable parameters with the initialization skill similar to that of fΘ. The classifier fϕ is an MLP layer provided by the Domainbed benchmark [27]. For the weight parameter α in Eq. (2), we set it to be 1 for all experiments (please refer to our supplementary material for analysis). The random seeds, learning rates, batch size, and augmentation skills are all dynamically set for all the compared arts according to [27].Table 2. Single source domain generalization. Experiments are conducted on the PACS dataset [37]. Here A, C, P, and S are the art, cartoon, photo, and sketch domains in PACS. A→C represents models trained on the art domain and tested on the cartoon domain, and similar for others. All methods are examined for 60 trials in each unseen domain. Best results are colored as red. A→C A →P A →S C →A C →P C →S P →A P →C P →S S →A S →C S →P Avg. RSC 66.3 ±1.3 88.2±0.6 57.2±3.1 65.8±1.5 82.4±0.6 68.7±2.5 60.5±2.0 41.3±6.0 53.1±2.8 53.8±1.6 65.9±0.7 48.4±1.9 62.6 Fish 67.1 ±0.5 89.2±1.8 57.0±0.2 66.7±1.0 85.6±0.4 64.5±3.6 55.1±2.1 33.9±2.3 51.2±4.2 59.1±3.2 67.1±0.9 58.4±1.2 62.9 CDANN 66.5±1.7 92.2±0.6 65.0±0.9 70.6±0.1 82.9±1.4 67.7±3.0 60.6±0.3 42.2±6.4 46.9±9.9 51.4±2.3 60.7±1.2 51.9±0.4 63.2 SelfReg 63.9±1.9 90.1±1.0 56.8±2.2 70.2±2.3 85.4±0.3 70.2±2.2 60.9±2.6 38.8±4.0 50.5±3.2 54.5±4.7 66.2±1.2 51.7±4.1 63.3 DANN 67.5 ±1.6 91.2±1.3 67.5±1.3 70.6±1.0 81.4±0.4 66.6±1.1 54.1±2.3 33.5±2.7 52.8±2.3 53.8±1.7 64.4±0.7 58.9±0.8 63.5 CAD 67.1 ±1.5 89.6±0.4 60.2±0.2 67.7±3.1 83.7±1.4 70.2±2.6 60.6±2.6 38.3±3.7 53.8±3.2 50.7±1.6 65.8±1.3 54.4±1.7 63.5 GroupGRO66.5±1.2 90.5±1.5 58.9±2.5 70.8±0.9 85.7±1.2 69.7±1.8 62.3±2.1 41.1±2.7 48.2±4.1 54.8±0.5 65.2±1.6 53.9±1.4 64.0 MTL 67.3 ±1.0 90.1±1.0 58.9±0.7 70.2±1.8 84.2±2.2 71.9±0.7 58.3±2.7 38.5±2.7 52.8±1.5 55.4±3.1 66.1±1.3 55.2±2.6 64.1 IRM 67.5 ±1.8 93.0±0.5 62.9±4.7 67.6±1.3 83.8±0.4 68.9±0.8 63.7±1.8 39.9±3.7 49.0±5.4 54.9±1.4 63.1±2.1 54.9±1.4 64.1 ARM 66.0 ±2.4 91.2±0.7 58.7±6.9 70.6±0.8 84.2±1.0 69.1±0.9 59.2±1.8 42.1±5.6 52.1±3.0 60.0±0.6 62.9±3.3 53.8±2.0 64.2 Mixup 65.5 ±0.8 87.8±0.3 57.2±1.0 71.4±1.1 83.1±1.8 68.0±3.0 59.6±1.7 37.2±2.7 56.5±3.8 55.0±2.2 66.2±1.5 62.7±4.2 64.2 CORAL 66.8±0.5 90.3±0.7 61.5±1.9 67.9±2.1 85.4±0.3 70.4±1.3 55.9±2.9 40.4±4.9 49.8±8.5 55.8±2.1 67.6±0.9 58.9±3.8 64.2 SD 67.1 ±1.3 91.7±1.2 63.7±4.1 70.3±0.9 84.4±0.7 69.4±2.3 57.5±2.5 42.6±0.8 47.7±1.7 55.9±2.4 65.7±0.8 55.8±2.1 64.3 MMD 67.1 ±1.4 88.0±0.8 63.6±1.6 70.0±1.1 83.6±0.2 70.2±1.0 58.8±2.6 40.3±1.0 52.3±2.4 57.4±1.9 68.7±0.9 52.7±3.7 64.4 MLDG 67.3±2.0 90.8±0.5 64.4±0.9 70.8±1.0 84.2±0.3 69.7±1.8 61.6±1.0 41.3±5.1 50.4±0.2 49.9±2.5 66.8±0.4 58.7±3.4 64.7 CondCAD66.9±1.4 92.3±0.7 60.8±4.5 71.0±0.6 84.7±1.1 72.6±0.5 61.2±1.5 40.7±3.6 55.7±1.6 52.3±1.7 64.2±0.4 55.3±1.2 64.8 ERM 67.3 ±0.7 91.7±0.9 60.1±4.7 70.4±0.6 82.3±2.7 68.1±0.9 59.6±1.8 44.7±2.8 56.5±2.7 52.8±2.3 68.1±0.7 58.4±0.9 65.0 VREx 67.1 ±1.5 91.0±1.0 62.6±3.5 71.1±2.4 84.1±0.9 71.7±1.3 62.4±3.1 37.7±3.3 53.6±2.3 60.6±1.6 66.7±0.8 57.5±1.4 65.5 Fishr 67.9 ±1.9 92.7±0.3 62.4±4.7 71.2±0.5 83.4±0.6 70.2±1.1 60.0±2.3 42.7±3.2 57.1±3.9 55.7±3.7 68.4±1.0 62.0±3.1 66.1 SagNet 67.6±1.4 92.3±0.5 59.5±1.7 71.8±0.3 82.8±0.6 69.9±1.8 62.5±2.5 45.2±2.5 64.1±2.0 55.8±1.1 65.7±1.4 55.9±3.5 66.1 MixStyle 68.5±2.0 91.2±1.6 65.1±0.7 73.2±1.3 85.0±0.8 71.7±1.5 63.6±1.7 46.3±1.1 51.6±3.7 54.2±1.5 67.0±3.4 58.3±1.4 66.3 Ours 68.9 ±0.6 92.4±0.1 62.5±0.6 75.3±0.4 85.9±0.3 70.2±1.4 66.5±1.1 52.2±2.7 63.8±1.1 57.6±3.7 68.0±1.3 57.9±2.0 68.4 Training and evaluation details. For all the compared methods, we conduct 60 trials on each source domain, and each with 5,000 iteration steps. During the training stage, we split the examples from training domains to 8:2 (train:val) where the training and validation samples are dynamically selected among different training trials. During test, we select the model that performs the best in the validation samples and test it on the target domains. The strategy is referred to as the “training-domain validate set” model selec- tion method in [27]. For each domain in different datasets, the final performance is the average accuracy from the 60 trials. 4.2. Multi-Source Generalization In these experiments, all five benchmark datasets afore- mentioned are used for evaluation, and the leave-one-out strategy is adopted for training (i.e. with S = |Ds ∪Dt|2 −1, and T = 1). Results are shown in Table 1. We note that ERM method obtains favorable performance against existing arts. In fact, as a strong baseline, ERM is superior to half of the methods in the term of average accuracy, and only 5 arts (i.e. SelfReg [34], Fish [58], CORAL [59], SD [51], and ours) among the compared 22 methods outperforms ERM in most datasets (i.e. with Score ≥ 3). In comparison, the proposed ITTA is more effective than all other models on average. In particular, ITTA achieves the best performances in 3 out of the 5 benchmarks (i.e. PACS, VLCS, and TerraInc datasets) and 4 in the top 5. Note that although our method does not obtain the best performances in the OfficeHome and DomainNet benchmarks, it still outperforms more than half 2We use | · |to denote the number of domains in the environment. of the existing models. The results validate the effectiveness of our method when tested in the multi-source setting. We present results of average accuracy in each domain from different datasets in the supplementary material. Please refer to it for details. 4.3. Single-Source Generalization In these experiments, we adopt the widely-used PACS [37] benchmark for evaluation, and the models are trained on one domain while tested on the remaining three (i.e. with S = 1, and T = 3). Although some approaches, such as MLDG [38] and Fishr [52], may require more than one domain information for their trainings, we can simu- late multi-domain information using only the source domain, and thus the experimental settings are still feasible for them. Compared to the multi-source generalization task, the single- source generalization is considered more difficult due to the limited domain information during the training phase. Evalu- ation results are presented in Table 2. We note that the ERM method outperforms most state-of-the-art models, and only 5 models, including VREx [36], Fishr [52], SagNet [48], MixStyle [75], and the proposed ITTA, can obtain better re- sults than ERM in the term of average accuracy. Meanwhile, our method achieves the best performances when trained in 5 out of the 12 source domain, and it obtains the best perfor- mance on average, leading more than 2% than the second best (i.e. MixStyle [75]) and 3% the ERM method. In line with the findings in [27], we notice that the naive ERM method [61] can indeed perform favorably against most existing models under rigorous evaluation protocol. As a matter of fact, the proposed method is the only one that consistently outperforms ERM in both the multi-sourceTable 3. Evaluations of different TTT-based models in the unseen domain from PACS [37]. The reported accuracies (%) and standard deviations are computed from 60 trials in each target domain. Model Target domain Avg.Art Cartoon Photo Sketch Baseline 79.9 ±0.5 75.4±1.1 94.4±0.5 75.8±1.2 81.4±0.5 TTT [60] 81.5±0.8 77.6±0.6 94.3±0.2 78.4±0.7 83.0±0.2 MT3 [3] 82.0 ±1.0 76.5±1.0 94.1±0.2 77.7±1.3 82.6±0.6 TENT [63] 80.2±0.9 77.2±0.8 94.4±0.2 77.4±0.1 82.3±0.5 Ours 84.7 ±0.4 78.0±0.4 94.5±0.4 78.2±0.3 83.8±0.3 and single-source settings. These results indicate that DG remains challenging for current efforts that aim to ease the distribution shift only through training data, and using the proposed improved TTT strategy may be a promising direc- tion for solving DG. 5. Analysis All experiments in this section are conducted on the widely-used PACS benchmark [37] with the leave-one-out strategy. The experimental settings are the same as that illus- trated in Sec. 4.1. Please refer to our supplementary material for more analysis. 5.1. Compared with Other TTT-Based Models Using test-time adaptation to ease the distribution shift problem has been explored in previous works, such as the original TTT method [60] and MT3 [3]. Their differences lie in that TTT uses a rotation estimation task for the test-time objective, and MT3 adopts a contrastive loss for the task and implements the overall framework using MAML [20]. There is also a recently proposed TENT [63] that aims to minimize the entropy of the final results by tuning the parameters from the batch normalization (BN) layers. To analyze the overall effectiveness of our method, we compare ITTA with these arts using the same baseline (i.e. ResNet18 [30] backbone with the existing augmentation skill [75]). Results are shown in Table 3. We observe that all the com- pared TTT-based methods can improve the baseline model in almost all target domains except for the “Photo” domain, which might be due to the ImageNet pretraining [67]. This phenomenon demonstrates that the TTT strategy may be a promising effort for easing the distribution shift problem. Meanwhile, we observe that the proposed ITTA is superior to all other approaches in most target domains and leads in the term of average accuracy. The main reason is that compared to the empirically designed TTT tasks adopted in previous works, the proposed learnable consistency loss is enforced to be more aligned with the main loss, thus more suitable for the test-time adaptation task [46]. Meanwhile, compared to the strategies that update the original param- eters from the trained model, the adaptation of the newly included parameters is also more effective for the overall (a) Input (b) Ours w/o fw (c) Ours (d) Main Figure 4. Grad-CAM [57] visualizations from different loss terms. We use images with varying class labels from the four target do- mains of PACS [37] as inputs (i.e. art, cartoon, photo, and sketch domains from top to bottom). Ours w/o fw is the naive consis- tency loss with fw disabled in Eq. (1). The proposed learnable consistency loss can align well with the main classification task. TTT framework. In the following, we provide more analysis to support these claims. 5.2. Effectiveness of the Learnable Consistency Loss To examine the effectiveness of our learnable consistency loss, we conduct ablation studies by comparing our method with the following variants. (1) Ours w/o fw: we disable fw when computing the learnable consistency loss in Eq. (1), which uses the naive consistency loss for the auxiliary TTT task. (2) Ours w/ Ent.: after training the model using the baseline settings (i.e. ResNet18 with the augmentation strat- egy [75]), we use the entropy minimization task in [63] for the TTT task. (3) Ours w/ Rot.: we use the rotation estimation task in [60] for the TTT task. To ensure fair com- parisons, we use the same baseline settings and include the same additional adaptive parameters for all the variants. Results are shown in the 4th to 6th rows Table 4. We find that the results from the naive consistency loss ( i.e. Ours w/o fw) are slightly better than that from the other two specially-designed objectives (i.e. Ours w/ Ent. and Ours w/ Rot.) on average. Besides the possibility of deteriorating the performance [46], our results indicate that empirically select- ing a TTT task may also be far from optimal. Meanwhile, we observe that when enabling fw, the proposed learnable consistency loss is superior to that withoutfw in all target do-Table 4. Comparison between different TTT tasks and parameter selecting strategies in the unseen domain from the PACS benchmark [37]. Here the “Ent.”, “Rot.”, and “Lwcont” denotes the entropy minimization task in [63], the rotation estimation task in [60], and the proposed learnable consistency objective, the “All”, “BN”, and “Ada.” are the strategies that update all the parameters, parameters from the batch normalization layer, and the proposed strategy that updates only the new additional adaptive parameters. The reported accuracies (%) and standard deviations are computed from 60 trials in each target domain. Model TTT tasks Param selectings Target domain Avg.Ent. Rot. Lwcont All BN Ada. Art Cartoon Photo Sketch Ours − − ✓ − − ✓ 84.7±0.4 78.0 ±0.4 94.5 ±0.4 78.2 ±0.3 83.8 ±0.3 Ours w/ofw − − − − − ✓ 83.1±0.4 74.6 ±0.6 94.0 ±0.5 78.0 ±0.8 82.5 ±0.1 Ours w/ Ent. ✓ − − − − ✓ 79.9±2.4 77.3 ±0.3 94.8 ±0.8 77.6 ±0.4 82.4 ±0.8 Ours w/ Rot. − ✓ − − − ✓ 81.1±1.0 75.2 ±0.5 94.9 ±0.3 77.3 ±0.6 82.1 ±0.3 Ours w/o TTT − − ✓ − − − 83.3±0.5 76.0 ±0.5 94.4 ±0.5 76.7 ±1.4 82.8 ±0.3 Ours w/ All − − ✓ ✓ − − 83.0±0.7 77.0 ±1.4 94.5 ±0.7 77.4 ±0.9 83.0 ±0.2 Ours w/ BN − − ✓ − ✓ − 81.8±0.5 75.6 ±0.3 94.4 ±0.3 77.9 ±1.1 82.4 ±0.5 mains, and it leads in the term of average accuracy among the variants compared, illustrating its advantage against other adopted TTT tasks. These results are not surprising. By comparing the Grad-CAM [57] visualizations from the main classification task with the learnable and naive consistency losses in Figure 4, we find that the proposed learnable objec- tive can well align with the main loss when fw is enabled as the hot zones activated by these two tasks are similar, which guarantees the improvement for the test-time adapta- tion [46, 60]. Please refer to our supplementary material for more visualizations. 5.3. Effectiveness of the Adaptive Parameters We compare ITTA with three variants to demonstrate the effectiveness of the proposed additional adaptive parameters. (1) Ours w/o TTT: we do not update any parameters during the test phase. This variant is used to verify whether TTT can improve the pretrained model. (2) Ours w/ ALL: similar to the updating strategy in the original TTT method [60], we update all the parameters from the feature extractor during the test phase. (3) Ours w/ BN: following the suggestion from TENT [63], only parameters from the BN layers of the feature extractor are updated. Note the same pretrained model is shared for all variants in these experiments, and the objectives during the test adaptation phase are to minimize the same learned consistency loss. We list the results in the last three rows in Table 4. We observe that when only updating parameters from the BN layers, the performance is inferior to the strategy without test-time adaptation, and updating all the parameters does not ensure improvements in all target domains. The observations are in line with the findings in [63] that selecting reliable parameters to update is essential in the TTT system and may also interact with the choice of the TTT task. In comparison, when including additional adaptive parameters for updating, the pretrained model can be boosted in all environments. The results validate that our adaptive parameters are more effective than that selected with existing strategies [60, 63] when applied with the proposed learnable test-time objective. 5.4. Limitation Although the proposed learned loss can bring satisfaction improvements, we are aware that the lunch is not free. When the weight subnetwork fw is disabled, updating the joint loss in Eq. (2) only costs 1 forward and 1 backward. However, in order to update fw, we have to compute the second-order derivative in Eq. (5), which will require 1 more forward and 3 more backward processes, bringing extra burden to the system. Our future efforts aim to simplify the overall optimization process and reduce the cost for ITTA. 6. Conclusion In this paper, we aim to improve the current TTT strategy for alleviating the distribution shift problem in DG. First, given that the auxiliary TTT task plays a vital role in the over- all framework, and an empirically selecting one that does not align with the main task may potentially deteriorate instead of improving the performance, we propose a learnable con- sistency loss that can be enforced to be more aligned with the main loss by adjusting its learnable parameters. This strategy is ensured to improve the model and shows favorable perfor- mance against some specially-designed objectives. Second, considering that selecting reliable and effective parameters to update during the test phase is also essential while exhaus- tively trying different combinations may require tremendous effort, we propose a new alternative by including new ad- ditional adaptive parameters for adaptation during the test phase. This alternative is shown to outperform some pre- vious parameter selecting strategies via our experimental findings. By conducting extensive experiments under a rig- orous evaluation protocol, we show that our method can achieve superior performance against existing arts in both the multi-source and single-source DG tasks. Acknowledgements. Liang Chen is supported by the ChinaScholarship Council (CSC Student ID 202008440331). References [1] Martin Arjovsky, L´eon Bottou, Ishaan Gulrajani, and David Lopez-Paz. Invariant risk minimization. arXiv preprint arXiv:1907.02893, 2019. 5, 15, 16, 17 [2] Yogesh Balaji, Swami Sankaranarayanan, and Rama Chel- lappa. Metareg: Towards domain generalization using meta- regularization. In NeurIPS, 2018. 1, 2, 14, 15 [3] Alexander Bartler, Andre B¨uhler, Felix Wiewel, Mario D¨obler, and Bin Yang. Mt3: Meta test-time training for self- supervised test-time adaption. In AISTATS, 2022. 2, 4, 7 [4] Sara Beery, Grant Van Horn, and Pietro Perona. Recognition in terra incognita. In ECCV, 2018. 5, 17 [5] Shai Ben-David, John Blitzer, Koby Crammer, and Fernando Pereira. Analysis of representations for domain adaptation. In NeurIPS, 2006. 2 [6] Gilles Blanchard, Aniket Anand Deshmukh, Urun Dogan, Gyemin Lee, and Clayton Scott. Domain generalization by marginal transfer learning. arXiv preprint arXiv:1711.07910, 2017. 5, 15, 16, 17 [7] Gilles Blanchard, Gyemin Lee, and Clayton Scott. Generaliz- ing from several related classification tasks to a new unlabeled sample. In NeurIPS, 2011. 1 [8] Chaoqi Chen, Jiongcheng Li, Xiaoguang Han, Xiaoqing Liu, and Yizhou Yu. Compound domain generalization via meta- knowledge encoding. In CVPR, 2022. 1 [9] Chaoqi Chen, Luyao Tang, Feng Liu, Gangming Zhao, Yue Huang, and Yizhou Yu. Mix and reason: Reasoning over se- mantic topology with data mixing for domain generalization. In NeurIPS, 2022. 1 [10] Dian Chen, Dequan Wang, Trevor Darrell, and Sayna Ebrahimi. Contrastive test-time adaptation. In CVPR, 2022. 2 [11] Liang Chen, Yong Zhang, Yibing Song, Lingqiao Liu, and Jue Wang. Self-supervised learning of adversarial example: Towards good generalizations for deepfake detection. In CVPR, 2022. 2 [12] Liang Chen, Yong Zhang, Yibing Song, Jue Wang, and Lingqiao Liu. Ost: Improving generalization of deepfake detection via one-shot test-time training. In NeurIPS, 2022. 2, 12 [13] Ting Chen, Simon Kornblith, Mohammad Norouzi, and Geof- frey Hinton. A simple framework for contrastive learning of visual representations. In ICML, 2020. 2, 3 [14] Sungha Choi, Seunghan Yang, Seokeon Choi, and Sungrack Yun. Improving test-time adaptation via shift-agnostic weight regularization and nearest source prototypes. In ECCV, 2022. 2 [15] Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li, and Li Fei-Fei. Imagenet: A large-scale hierarchical image database. In CVPR, 2009. 5 [16] Qi Dou, Daniel Coelho de Castro, Konstantinos Kamnitsas, and Ben Glocker. Domain generalization via model-agnostic learning of semantic features. In NeurIPS, 2019. 1, 2 [17] Mark Everingham, Luc Van Gool, Christopher KI Williams, John Winn, and Andrew Zisserman. The pascal visual object classes (voc) challenge. IJCV, 88(2):303–338, 2010. 5 [18] Chen Fang, Ye Xu, and Daniel N Rockmore. Unbiased metric learning: On the utilization of multiple datasets and web images for softening bias. In ICCV, 2013. 5, 16 [19] Li Fei-Fei, Rob Fergus, and Pietro Perona. Learning gener- ative visual models from few training examples: An incre- mental bayesian approach tested on 101 object categories. In CVPR worksho, 2004. 5 [20] Chelsea Finn, Pieter Abbeel, and Sergey Levine. Model- agnostic meta-learning for fast adaptation of deep networks. In ICML, 2017. 2, 7 [21] Francois Fleuret et al. Uncertainty reduction for model adap- tation in semantic segmentation. In CVPR, 2021. 2 [22] Yossi Gandelsman, Yu Sun, Xinlei Chen, and Alexei A Efros. Test-time training with masked autoencoders. In NeurIPS, 2022. 2 [23] Yaroslav Ganin, Evgeniya Ustinova, Hana Ajakan, Pascal Germain, Hugo Larochelle, Franc ¸ois Laviolette, Mario Marc- hand, and Victor Lempitsky. Domain-adversarial training of neural networks. JMLR, 17(1):2096–2030, 2016. 1, 2, 5, 15, 16, 17 [24] Muhammad Ghifary, David Balduzzi, W Bastiaan Kleijn, and Mengjie Zhang. Scatter component analysis: A unified framework for domain adaptation and domain generalization. IEEE TPAMI, 39(7):1414–1430, 2016. 1 [25] Muhammad Ghifary, W Bastiaan Kleijn, Mengjie Zhang, and David Balduzzi. Domain generalization for object recognition with multi-task autoencoders. In ICCV, 2015. 2 [26] Jean-Bastien Grill, Florian Strub, Florent Altch ´e, Corentin Tallec, Pierre Richemond, Elena Buchatskaya, Carl Doer- sch, Bernardo Avila Pires, Zhaohan Guo, Mohammad Ghesh- laghi Azar, et al. Bootstrap your own latent-a new approach to self-supervised learning. In NeurIPS, 2020. 2, 3 [27] Ishaan Gulrajani and David Lopez-Paz. In search of lost domain generalization. In ICLR, 2021. 1, 2, 5, 6, 14, 15, 16, 17 [28] Sivan Harary, Eli Schwartz, Assaf Arbelle, Peter Staar, Shady Abu-Hussein, Elad Amrani, Roei Herzig, Amit Alfassy, Raja Giryes, Hilde Kuehne, et al. Unsupervised domain general- ization by learning a bridge across domains. In CVPR, 2022. 1 [29] Kaiming He, Haoqi Fan, Yuxin Wu, Saining Xie, and Ross Girshick. Momentum contrast for unsupervised visual repre- sentation learning. In CVPR, 2020. 2, 3 [30] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image recognition. In CVPR, 2016. 1, 5, 7, 14 [31] Shoubo Hu, Kun Zhang, Zhitang Chen, and Laiwan Chan. Domain generalization via multidomain discriminant analysis. In UAI, 2020. 1 [32] Xun Huang and Serge Belongie. Arbitrary style transfer in real-time with adaptive instance normalization. In ICCV, 2017. 2 [33] Zeyi Huang, Haohan Wang, Eric P Xing, and Dong Huang. Self-challenging improves cross-domain generalization. In ECCV, 2020. 5, 15, 16, 17[34] Daehee Kim, Youngjun Yoo, Seunghyun Park, Jinkyu Kim, and Jaekoo Lee. Selfreg: Self-supervised contrastive regular- ization for domain generalization. In ICCV, 2021. 2, 5, 6, 15, 16, 17 [35] Pang Wei Koh, Shiori Sagawa, Henrik Marklund, Sang Michael Xie, Marvin Zhang, Akshay Balsubramani, Weihua Hu, Michihiro Yasunaga, Richard Lanas Phillips, Irena Gao, et al. Wilds: A benchmark of in-the-wild distribu- tion shifts. In ICML, 2021. 1 [36] David Krueger, Ethan Caballero, Joern-Henrik Jacobsen, Amy Zhang, Jonathan Binas, Dinghuai Zhang, Remi Le Priol, and Aaron Courville. Out-of-distribution generalization via risk extrapolation (rex). In ICML, 2021. 5, 6, 15, 16, 17 [37] Da Li, Yongxin Yang, Yi-Zhe Song, and Timothy M Hospedales. Deeper, broader and artier domain generalization. In ICCV, 2017. 1, 5, 6, 7, 8, 12, 13, 14, 15 [38] Da Li, Yongxin Yang, Yi-Zhe Song, and Timothy M Hospedales. Learning to generalize: Meta-learning for do- main generalization. In AAAI, 2018. 1, 2, 5, 6, 15, 16, 17 [39] Da Li, Jianshu Zhang, Yongxin Yang, Cong Liu, Yi-Zhe Song, and Timothy M Hospedales. Episodic training for domain generalization. In ICCV, 2019. 1, 2 [40] Haoliang Li, Sinno Jialin Pan, Shiqi Wang, and Alex C Kot. Domain generalization with adversarial feature learning. In CVPR, 2018. 1, 2, 5, 15, 16, 17 [41] Pan Li, Da Li, Wei Li, Shaogang Gong, Yanwei Fu, and Timothy M Hospedales. A simple feature augmentation for domain generalization. In ICCV, 2021. 1, 2, 12, 14 [42] Xiaotong Li, Yongxing Dai, Yixiao Ge, Jun Liu, Ying Shan, and Ling-Yu Duan. Uncertainty modeling for out- of-distribution generalization. In ICLR, 2022. 1, 2 [43] Yizhuo Li, Miao Hao, Zonglin Di, Nitesh Bharadwaj Gun- davarapu, and Xiaolong Wang. Test-time personalization with a transformer for human pose estimation. In NeurIPS, 2021. 2, 3, 4 [44] Ya Li, Xinmei Tian, Mingming Gong, Yajing Liu, Tongliang Liu, Kun Zhang, and Dacheng Tao. Deep domain generaliza- tion via conditional invariant adversarial networks. In ECCV, 2018. 1, 2, 5, 15, 16, 17 [45] Yiying Li, Yongxin Yang, Wei Zhou, and Timothy Hospedales. Feature-critic networks for heterogeneous do- main generalization. In ICML, 2019. 14, 15 [46] Yuejiang Liu, Parth Kothari, Bastien van Delft, Baptiste Bellot-Gurlet, Taylor Mordan, and Alexandre Alahi. Ttt++: When does self-supervised test-time training fail or thrive? In NeurIPS, 2021. 2, 3, 4, 7, 8, 12, 14, 15 [47] Krikamol Muandet, David Balduzzi, and Bernhard Sch¨olkopf. Domain generalization via invariant feature representation. In ICML, 2013. 1, 2 [48] Hyeonseob Nam, HyunJae Lee, Jongchan Park, Wonjun Yoon, and Donggeun Yoo. Reducing domain gap by reducing style bias. In CVPR, 2021. 2, 5, 6, 15, 16, 17 [49] Prashant Pandey, Mrigank Raman, Sumanth Varambally, and Prathosh Ap. Generalization on unseen domains via inference- time label-preserving target projections. In CVPR, 2021. 1 [50] Xingchao Peng, Qinxun Bai, Xide Xia, Zijun Huang, Kate Saenko, and Bo Wang. Moment matching for multi-source domain adaptation. In ICCV, 2019. 5, 17 [51] Mohammad Pezeshki, Oumar Kaba, Yoshua Bengio, Aaron C Courville, Doina Precup, and Guillaume Lajoie. Gradient star- vation: A learning proclivity in neural networks. In NeurIPS, 2021. 1, 5, 6, 15, 16, 17 [52] Alexandre Rame, Corentin Dancette, and Matthieu Cord. Fishr: Invariant gradient variances for out-of-distribution gen- eralization. In ICML, 2022. 1, 2, 5, 6, 15, 16, 17 [53] Yangjun Ruan, Yann Dubois, and Chris J Maddison. Optimal representations for covariate shift. In ICLR, 2022. 5, 15, 16, 17 [54] Bryan C Russell, Antonio Torralba, Kevin P Murphy, and William T Freeman. Labelme: a database and web-based tool for image annotation. IJCV, 77(1):157–173, 2008. 5 [55] Shiori Sagawa, Pang Wei Koh, Tatsunori B Hashimoto, and Percy Liang. Distributionally robust neural networks for group shifts: On the importance of regularization for worst- case generalization. In ICLR, 2020. 5, 15, 16, 17 [56] Steffen Schneider, Evgenia Rusak, Luisa Eck, Oliver Bring- mann, Wieland Brendel, and Matthias Bethge. Improving robustness against common corruptions by covariate shift adaptation. In NeurIPS, 2020. 2 [57] Ramprasaath R Selvaraju, Michael Cogswell, Abhishek Das, Ramakrishna Vedantam, Devi Parikh, and Dhruv Batra. Grad- cam: Visual explanations from deep networks via gradient- based localization. In ICCV, 2017. 7, 8, 11, 13 [58] Yuge Shi, Jeffrey Seely, Philip HS Torr, N Siddharth, Awni Hannun, Nicolas Usunier, and Gabriel Synnaeve. Gradient matching for domain generalization. In ICLR, 2021. 1, 2, 5, 6, 15, 16, 17 [59] Baochen Sun and Kate Saenko. Deep coral: Correlation alignment for deep domain adaptation. In ECCV, 2016. 5, 6, 15, 16, 17 [60] Yu Sun, Xiaolong Wang, Zhuang Liu, John Miller, Alexei Efros, and Moritz Hardt. Test-time training with self- supervision for generalization under distribution shifts. In ICML, 2020. 1, 2, 4, 5, 7, 8, 11, 12, 13 [61] Vladimir Vapnik. The nature of statistical learning theory . Springer science & business media, 1999. 1, 5, 6, 15, 16, 17 [62] Hemanth Venkateswara, Jose Eusebio, Shayok Chakraborty, and Sethuraman Panchanathan. Deep hashing network for unsupervised domain adaptation. In CVPR, 2017. 5, 16 [63] Dequan Wang, Evan Shelhamer, Shaoteng Liu, Bruno Ol- shausen, and Trevor Darrell. Tent: Fully test-time adaptation by entropy minimization. In ICLR, 2021. 2, 3, 4, 7, 8, 11, 12, 13 [64] Jianxiong Xiao, James Hays, Krista A Ehinger, Aude Oliva, and Antonio Torralba. Sun database: Large-scale scene recog- nition from abbey to zoo. In CVPR, 2010. 5 [65] Zehao Xiao, Xiantong Zhen, Ling Shao, and Cees GM Snoek. Learning to generalize across domains on single test samples. In ICLR, 2022. 2 [66] Qinwei Xu, Ruipeng Zhang, Ya Zhang, Yanfeng Wang, and Qi Tian. A fourier-based framework for domain generaliza- tion. In CVPR, 2021. 1, 2 [67] Zhenlin Xu, Deyi Liu, Junlin Yang, Colin Raffel, and Marc Niethammer. Robust and generalizable visual representation learning via random convolutions. In ICLR, 2021. 7[68] Shen Yan, Huan Song, Nanxiang Li, Lincan Zou, and Liu Ren. Improve unsupervised domain adaptation with mixup training. arXiv preprint arXiv:2001.00677, 2020. 2, 5, 15, 16, 17 [69] Fu-En Yang, Yuan-Chia Cheng, Zu-Yun Shiau, and Yu- Chiang Frank Wang. Adversarial teacher-student representa- tion learning for domain generalization. In NeurIPS, 2021. 1, 2 [70] Nanyang Ye, Kaican Li, Haoyue Bai, Runpeng Yu, Lanqing Hong, Fengwei Zhou, Zhenguo Li, and Jun Zhu. Ood-bench: Quantifying and understanding two dimensions of out-of- distribution generalization. In CVPR, 2022. 5 [71] Fuming You, Jingjing Li, and Zhou Zhao. Test-time batch statistics calibration for covariate shift. arXiv preprint arXiv:2110.04065, 2021. 4 [72] Marvin Zhang, Henrik Marklund, Nikita Dhawan, Abhishek Gupta, Sergey Levine, and Chelsea Finn. Adaptive risk mini- mization: A meta-learning approach for tackling group distri- bution shift. arXiv preprint arXiv:2007.02931, 2020. 5, 15, 16, 17 [73] Marvin Zhang, Henrik Marklund, Nikita Dhawan, Abhishek Gupta, Sergey Levine, and Chelsea Finn. Adaptive risk mini- mization: Learning to adapt to domain shift. NeurIPS, 2021. 2 [74] Tao Zhong, Zhixiang Chi, Li Gu, Yang Wang, Yuanhao Yu, and Jin Tang. Meta-dmoe: Adapting to domain shift by meta- distillation from mixture-of-experts. In NeurIPS, 2022. 2 [75] Kaiyang Zhou, Yongxin Yang, Yu Qiao, and Tao Xiang. Do- main generalization with mixstyle. In ICLR, 2021. 1, 2, 3, 5, 6, 7, 12, 15, 16, 17 Appendix In this supplementary material, we provide, 1. Resource usage for ITTA in Section 7. 2. Grad-CAM visualizations of different loss terms in Section 8. 3. Parameter analysis of ITTA in Section 9; 4. Using a different augmentation skill for ITTA in Sec- tion 10. 5. Using different updating steps or a strategy for ITTA during the test phase in Section 11. 6. Using different network structures for the learnable consistency loss and adaptive parameters in Section 12. 7. Comparisons with other related methods in Section 13. 8. Detailed experimental results in the DomainBed bench- mark in Section 14. 7. Resource Usage Comparisons Between ITTA and the Baseline Model Requiring extra resources for our ITTA is a common lim- itation for existing test-time-based arts. To further evaluate our method, in this section, we compare FLOPS, model size, and inference time in Table 5. We compare only with ERM as most existing methods utilize the same network during in- ferences. We note that compare to the baseline model, ITTA requires extra Flops and processing time, this is because the adaptation process uses extra forward and backward steps during the test phase. While the parameters between the two models are similar because the newly included adaptive blocks are much smaller in size compared to the original model. Table 5. Resource comparisons during testing. Here inc. and exc. columns in ITTA indicate to include and exclude the TTA phase. Model Flops (G) Params (M) Time (s) Baseline 1.82 11.18 0.004 ITTA (inc.| exc.) 6.12 | 1.83 14.95 | 14.94 0.021 | 0.005 8. Grad-CAM Visualizations of Different Self- Supervised Objectives In Section 5 of the manuscript, we provide Grad-CAM [57] visualizations of our learnable consistency and the main losses to illustrate their alignment. To further show the differences between several TTT tasks [60, 63], we present more visual examples in this section. Results are shown in Figure 5. We observe that the entropy minimization [63] and rotation estimation [60] objectives do not activate the same regions as the main loss. As shown in the first row, for the class label of giraffe, both the main loss and our learned loss can correctly locate the two giraffes in the image, while the rotation estimation task can only locate one target, the same observation can be found when the learned weightsare disabled in our loss term. Meanwhile, although the two objects can be found for the entropy minimization task, the corresponding hot region does not align with that of the main loss. Similar phenomena can be observed in other samples. These visual examples demonstrate that our learned objective can better align with the main task than the TTT tasks adopted in previous works [60, 63], explaining why using the proposed learnable consistency loss can better improve TTT. 9. Parameter Analysis In this section, we analyze the hyper-parameter used in ITTA. We use the weight parameterα to balance the contri- butions from the main loss and weighted consistency loss (i.e. Lmain + αLwcont in Eq. (2) of our manuscript). To analyze the sensitivity of ITTA regarding different values of α, we conduct ablation studies in the PACS benchmark [37]. Results are listed in Table 6. We observe that the proposed ITTA can obtain favorable performances when α is in the range of 0.1 to 10, and it performs the best on average when setting as 1. We thus fix the parameter as 1 in all experi- ments. 10. A Different Augmentation Skill for ITTA In our manuscript, we use the existing augmentation strat- egy from [75] to obtain the augmented feature. In this sec- tion, we replace this implementation with that from [41] to further verify if our ITTA can still thrive with another aug- mentation skill. Different from [75] that mixes the statics of the feature to synthesize new information, [41] uses an affine transformation to create new features, where the weight for the transformation is sampled from a normal distribution with the mean value of one and standard value of zero, and the bias for the transformation is sampled from a normal distribution with the mean and standard values both zero. Experiments are conducted on the PACS benchmark [37] with the leave-one-out strategy. We compare ITTA with several different variants. (1) Ours w/o fw & TTT: this variant is the baseline model which uses the naive consistency loss for training and does not include TTT during the test phase. (2) Ours w/o fw: we disable the fw in our consistency loss, which uses the naive consistency loss for the test-time updating. (3) Ours w/o TTT: we do not update any parameters during the test phase. This variant is used to verify whether TTT can improve the pretrained model when replacing the augmentation strategy. We also compare these variants with the ERM method to show their effectivenesses. Results are listed in Table 7. We observe that ERM per- forms favorably against the baseline model, indicating that this augmentation strategy may not be beneficial for the training process. Meanwhile, we observe that when fw is disabled, the performances seem to decrease in 3 out of 4 target domains, and the average accuracy is also inferior to the baseline (i.e. Ours w/o fw & TTT). This result is in line with the finding in [46] that an inappropriate TTT task may deteriorate the performance. In comparison, we note that the performances are both improved when fw is enabled (i.e. Ours w/o TTT and Ours), which once again demonstrates that the proposed learnable consistency loss can improve the trained model. Moreover, we can also observe that when combining fw and TTT, our model is superior to other vari- ants and the ERM method. These results demonstrate that the proposed two strategies can improve the current TTT framework despite a less effective augmentation strategy. 11. Different Updating Steps or Strategies for ITTA In the manuscript, we use one TTT step for ITTA before during the testing step. In this section, we conduct experi- ments to evaluate the performances of ITTA with different TTT steps. Experiments are conducted on the PACS bench- mark [37] with the leave-one-out strategy, and each target domain is examined with 60 sets of random seeds and hyper- parameter settings. Results are listed in Table 8. We observe that the average accuracies of using more TTT steps are not improved greatly while the computational times are propor- tional to the TTT steps. To this end, we use one TTT step for ITTA as a compromise between accuracy and efficiency. We use the online setting from TTT [60] for all arts, which assumes test samples arrive sequentially and updates the adaptive blocks based on the states optimized from a previous sample. In this section, we also test ITTA in an episodic manner (i.e. Epi) [12]. Results in Table 8 suggest that while the episodic updating strategy performs slightly worse than the current scheme, and it still outperforms the baseline. 12. Different Network Structures for the Learnable Consistency Loss and Adaptive Parameters In our implementation, we use 10 layers of learnable pa- rameters for fw, and we use 5 layers of learnable parameters for fΘ after each block. In this section, we evaluate our ITTA with different network structures for these two mod- ules. Specifically, we compare the original implementation with the variants that use 1, 5, and 15 layers for fw and 1, 10, and 15 layers for fΘ to evaluate the performances of dif- ferent structures. Similarly, we conduct experiments on the PACS benchmark [37] with the leave-one-out strategy, and each target domain is examined with 60 sets of random seeds and hyper-parameter settings. Evaluation results are listed in Table 9. We observe that their differences in the average accuracy are rather subtle on account of the variances. To(a) Input (b) Entropy (c) Rotation (d) Ours w/o fw (e) Ours (f) Main Figure 5. Grad-CAM [57] visualizations from different loss terms. We use images with varying class labels (i.e. giraffe, elephant, house, and horse from top to bottom) from the four target domains of PACS [37] as inputs (i.e. art, cartoon, photo, and sketch domains from top to bottom). “Entropy” and “Rotation” here denote the entropy minimization and rotation estimation tasks in [63] and [60]. Ours w/o fw is the learnable consistency loss in Eq. (1) in the manuscript (i.e. ∥fw(z − z′)∥) when fw is disabled. The proposed learnable consistency loss can align well with the main classification task. Table 6. Sensitivity analysis of ITTA regarding different values ofα in the unseen domain from PACS [37]. The reported accuracies (%) and standard deviations are computed from 60 trials in each target domain. Values Target domain Avg.Art Cartoon Photo Sketch α = 0.1 83.9 ± 0.7 76.2 ± 1.1 94.8 ± 0.2 78.8 ± 0.8 83.4 ± 0.2 α = 1 (Ours) 84.7 ± 0.4 78.0 ± 0.4 94.5 ± 0.4 78.2 ± 0.3 83.8 ± 0.3 α = 10 83.9 ± 0.5 77.4 ± 0.6 94.2 ± 0.7 77.3 ± 0.8 83.2 ± 0.3 α = 100 81.5 ± 1.2 77.0 ± 0.6 92.6 ± 0.7 78.9 ± 2.1 82.5 ± 0.9 this end, we use the original implementation with 10 layers of learnable parameters for fw and 5 layers of learnable pa- rameters for fΘ, which performs relatively better than other variants. Since the adaptive blocks fΘ are attached after each layer of the network, one may wonder how the varying locations of the adaptive blocks affect the performance of ITTA. To answer this question, we further conduct experiments by adding the adaptive blocks after different layers of the orig- inal network. Denoting as Loc = lan given the n layers in the original network, we note that the model performs less effectively when the adaptive block is placed after the 1st layer of the network, and using all four adaptive blocks (i.e. ours) is more effective than other alternatives. 13. Comparisons with Other Related Methods Apart from the proposed ITTA, some other works also propose to include learnable parameters in their auxiliaryTable 7. Performances of our method with another augmentation strategy from [41] in the unseen domain from PACS [37]. The reported accuracies (%) and standard deviations are computed from 60 trials in each target domain. Model Target domain Avg.Art Cartoon Photo Sketch ERM 78.0 ± 1.3 73.4 ± 0.8 94.1 ± 0.4 73.6 ± 2.2 79.8 ± 0.4 Ours w/o fw & TTT 74.9 ± 0.4 74.1 ± 0.8 90.6 ± 0.3 79.7 ± 0.7 79.8 ± 0.4 Ours w/o fw 77.1 ± 1.0 73.6 ± 1.1 89.9 ± 0.4 78.4 ± 0.8 79.7 ± 0.2 Ours w/o TTT 77.5 ± 0.3 73.2 ± 0.6 92.4 ± 0.4 78.0 ± 1.0 80.3 ± 0.3 Ours (w/ fw & TTT) 79.2 ± 0.8 74.9 ± 1.1 92.2 ± 0.3 76.9 ± 0.7 80.8 ± 0.4 Table 8. Evaluations of ITTA in the unseen domain from PACS [37] with different TTT steps and updating strategies during the testing phase. The reported accuracies (%) and standard deviations are computed from 60 trials in each target domain. The time consumption (TC) is computed using one image with the size of 224 × 224. Epi. denotes updating ITTA in an episodic manner. Steps Target domain Avg. TCArt Cartoon Photo Sketch 1 step (Ours) 84.7 ± 0.4 78.0 ± 0.4 94.5 ± 0.4 78.2 ± 0.3 83.8 ± 0.3 2.4 ms 2 step 84.2 ± 0.9 77.5 ± 0.6 94.4 ± 0.4 79.1 ± 1.0 83.8 ± 0.1 4.2 ms 3 step 84.5 ± 1.2 77.6 ± 0.6 94.0 ± 0.6 79.3 ± 0.1 83.9 ± 0.3 6.1 ms Epi. 83.6 ± 0.7 77.9 ± 0.5 95.2 ± 0.1 76.6 ± 0.5 83.3 ± 0.4 losses. Examples include MetaReg [2] and Feature-Critic [45] which both suggest using meta-learning to produce more general models. The main difference between these arts and ITTA is that parameters in the auxiliary loss from [2,45] are gradually refined by episode training, and they are updated via a gradient alignment step in ITTA (see Sec. 3.1 in the manuscript), which is much simpler. In this sec- tion, we compare ITTA with these two arts in the PACS dataset [37] using the same settings aforementioned. Be- cause MetaReg [2] does not release codes, we thus directly cite the data from their paper in the comparison. Different from others, the results in [2] are averaged by 5 trials accord- ing to their paper, which is much less than our experimental settings. Meanwhile, we also compare with TTT++ [46] which suggests storing the momentum of the features from the source domain and enforcing the similarity between mo- mentums of features from the source and target domains. We use the same setting in Section 5.1 from the manuscript to evaluate TTT++. Results are listed in Table 10. We observe that our method consistently outperforms that from [2,45,46] for both the cases with and without TTT, indicating that the proposed learnable consistency loss and updating method is not only simpler but also more effective than the losses in [2, 45]. 14. Detailed Results in the DomainBed Bench- mark [27] this section presents the average accuracy in each domain from different datasets. As shown in Table 11, 12, 13, 14, and 15, these results are detailed illustrations of the results in Table 2 in our manuscript. For all the experiments, we use the “training-domain validate set” as the model selection method. A total of 22 methods are examined for 60 trials in each unseen domain, and all methods are trained with the leave-one-out strategy using the ResNet18 [30] backbones.Table 9. Performances of our method with different network structures for the consistency loss (i.e. fw) and adaptive parameters (i.e. fΘ) in the unseen domain from PACS [37]. Here ‘Loc=lan’ locates the adaptive block after the n-th layer of the model (‘la4’ is the last layer). The reported accuracies (%) and standard deviations are computed from 60 trials in each target domain. Structures Target domain Avg.Art Cartoon Photo Sketch Structures offw 1 layer 83.5 ±1.2 76.0 ±1.0 95.3 ±0.2 78.7 ±1.5 83.4 ±0.4 5 layers 83.7 ±0.6 76.8 ±0.9 94.6 ±0.3 78.8 ±0.3 83.5 ±0.3 10 layers (Ours) 84.7 ±0.4 78.0 ±0.4 94.5 ±0.4 78.2 ±0.3 83.8 ±0.3 15 layers 84.1 ±0.4 75.8 ±0.2 94.3 ±0.3 79.5 ±0.4 83.4 ±0.2 Structures offΘ 1 layer 84.0 ±0.6 77.4 ±0.5 94.4 ±0.5 78.3 ±0.4 83.5 ±0.3 5 layers (Ours) 84.7 ±0.4 78.0 ±0.4 94.5 ±0.4 78.2 ±0.3 83.8 ±0.3 10 layers 84.8 ±0.3 76.0 ±0.6 94.1 ±0.5 78.3 ±0.1 83.3 ±0.3 15 layers 83.9 ±0.8 76.0 ±0.5 93.8 ±0.4 78.7 ±1.4 83.1 ±0.6 Locations offΘ Loc=la1 83.4±0.7 76.8 ±0.3 94.4 ±0.3 77.8 ±0.3 83.1 ±0.3 Loc=la2 83.4±0.6 77.7 ±0.6 94.2 ±0.5 78.0 ±0.5 83.3 ±0.3 Loc=la3 84.0±0.4 77.5 ±0.3 94.4 ±0.1 77.8 ±0.1 83.4 ±0.2 Loc=la4 84.1±0.7 77.8 ±0.5 94.8 ±0.2 76.9 ±1.5 83.4 ±0.4 Table 10. Compare with learnable losses in [2, 45] in the unseen domain from PACS [37]. The reported accuracies ( %) and standard deviations are computed from 60 trials in each target domain except for [2] where the numbers are directly cited from their paper. Model Target domain Avg.Art Cartoon Photo Sketch MetaReg [2] 83.7 ± 0.2 77.2 ± 0.3 95.5 ± 0.2 70.3 ± 0.3 81.7 Feture-Critic [45] 78.4 ± 1.6 75.4 ± 1.2 92.6 ± 0.5 73.3 ± 1.4 80.0 ± 0.3 TTT++ [46] 84.3 ± 0.1 78.4 ± 0.5 93.8 ± 1.3 73.2 ± 3.2 82.4 ± 1.1 Ours w/o TTT 83.3 ± 0.5 76.0 ± 0.5 94.4 ± 0.5 76.7 ± 1.4 82.8 ± 0.3 Ours 84.7 ± 0.4 78.0 ± 0.4 94.5 ± 0.4 78.2 ± 0.3 83.8 ± 0.3 Table 11. Average accuracies on the PACS [37] datasets using the default hyper-parameter settings in DomainBed [27]. art cartoon photo sketch Average ERM [61] 78.0 ± 1.3 73.4 ± 0.8 94.1 ± 0.4 73.6 ± 2.2 79.8 ± 0.4 IRM [1] 76.9 ± 2.6 75.1 ± 0.7 94.3 ± 0.4 77.4 ± 0.4 80.9 ± 0.5 GroupGRO [55] 77.7 ± 2.6 76.4 ± 0.3 94.0 ± 0.3 74.8 ± 1.3 80.7 ± 0.4 Mixup [68] 79.3 ± 1.1 74.2 ± 0.3 94.9 ± 0.3 68.3 ± 2.7 79.2 ± 0.9 MLDG [38] 78.4 ± 0.7 75.1 ± 0.5 94.8 ± 0.4 76.7 ± 0.8 81.3 ± 0.2 CORAL [59] 81.5 ± 0.5 75.4 ± 0.7 95.2 ± 0.5 74.8 ± 0.4 81.7 ± 0.0 MMD [40] 81.3 ± 0.6 75.5 ± 1.0 94.0 ± 0.5 74.3 ± 1.5 81.3 ± 0.8 DANN [23] 79.0 ± 0.6 72.5 ± 0.7 94.4 ± 0.5 70.8 ± 3.0 79.2 ± 0.3 CDANN [44] 80.4 ± 0.8 73.7 ± 0.3 93.1 ± 0.6 74.2 ± 1.7 80.3 ± 0.5 MTL [6] 78.7 ± 0.6 73.4 ± 1.0 94.1 ± 0.6 74.4 ± 3.0 80.1 ± 0.8 SagNet [48] 82.9 ± 0.4 73.2 ± 1.1 94.6 ± 0.5 76.1 ± 1.8 81.7 ± 0.6 ARM [72] 79.4 ± 0.6 75.0 ± 0.7 94.3 ± 0.6 73.8 ± 0.6 80.6 ± 0.5 VREx [36] 74.4 ± 0.7 75.0 ± 0.4 93.3 ± 0.3 78.1 ± 0.9 80.2 ± 0.5 RSC [33] 78.5 ± 1.1 73.3 ± 0.9 93.6 ± 0.6 76.5 ± 1.4 80.5 ± 0.2 SelfReg [34] 82.5 ± 0.8 74.4 ± 1.5 95.4 ± 0.5 74.9 ± 1.3 81.8 ± 0.3 MixStyle [75] 82.6 ± 1.2 76.3 ± 0.4 94.2 ± 0.3 77.5 ± 1.3 82.6 ± 0.4 Fish [58] 80.9 ± 1.0 75.9 ± 0.4 95.0 ± 0.4 76.2 ± 1.0 82.0 ± 0.3 SD [51] 83.2 ± 0.6 74.6 ± 0.3 94.6 ± 0.1 75.1 ± 1.6 81.9 ± 0.3 CAD [53] 83.9 ± 0.8 74.2 ± 0.4 94.6 ± 0.4 75.0 ± 1.2 81.9 ± 0.3 CondCAD [53] 79.7 ± 1.0 74.2 ± 0.9 94.6 ± 0.4 74.8 ± 1.4 80.8 ± 0.5 Fishr [52] 81.2 ± 0.4 75.8 ± 0.8 94.3 ± 0.3 73.8 ± 0.6 81.3 ± 0.3 Ours 84.7 ± 0.4 78.0 ± 0.4 94.5 ± 0.4 78.2 ± 0.3 83.8 ± 0.3Table 12. Average accuracies on the VLCS [18] datasets using the default hyper-parameter settings in DomainBed [27]. Caltech LabelMe Sun VOC Average ERM [61] 97.7 ± 0.3 62.1 ± 0.9 70.3 ± 0.9 73.2 ± 0.7 75.8 ± 0.2 IRM [1] 96.1 ± 0.8 62.5 ± 0.3 69.9 ± 0.7 72.0 ± 1.4 75.1 ± 0.1 GroupGRO [55] 96.7 ± 0.6 61.7 ± 1.5 70.2 ± 1.8 72.9 ± 0.6 75.4 ± 1.0 Mixup [68] 95.6 ± 1.5 62.7 ± 0.4 71.3 ± 0.3 75.4 ± 0.2 76.2 ± 0.3 MLDG [38] 95.8 ± 0.5 63.3 ± 0.8 68.5 ± 0.5 73.1 ± 0.8 75.2 ± 0.3 CORAL [59] 96.5 ± 0.3 62.8 ± 0.1 69.1 ± 0.6 73.8 ± 1.0 75.5 ± 0.4 MMD [40] 96.0 ± 0.8 64.3 ± 0.6 68.5 ± 0.6 70.8 ± 0.1 74.9 ± 0.5 DANN [23] 97.2 ± 0.1 63.3 ± 0.6 70.2 ± 0.9 74.4 ± 0.2 76.3 ± 0.2 CDANN [44] 95.4 ± 1.2 62.6 ± 0.6 69.9 ± 1.3 76.2 ± 0.5 76.0 ± 0.5 MTL [6] 94.4 ± 2.3 65.0 ± 0.6 69.6 ± 0.6 71.7 ± 1.3 75.2 ± 0.3 SagNet [48] 94.9 ± 0.7 61.9 ± 0.7 69.6 ± 1.3 75.2 ± 0.6 75.4 ± 0.8 ARM [72] 96.9 ± 0.5 61.9 ± 0.4 71.6 ± 0.1 73.3 ± 0.4 75.9 ± 0.3 VREx [36] 96.2 ± 0.0 62.5 ± 1.3 69.3 ± 0.9 73.1 ± 1.2 75.3 ± 0.6 RSC [33] 96.2 ± 0.0 63.6 ± 1.3 69.8 ± 1.0 72.0 ± 0.4 75.4 ± 0.3 SelfReg [34] 95.8 ± 0.6 63.4 ± 1.1 71.1 ± 0.6 75.3 ± 0.6 76.4 ± 0.7 MixStyle [75] 97.3 ± 0.3 61.6 ± 0.1 70.4 ± 0.7 71.3 ± 1.9 75.2 ± 0.7 Fish [58] 97.4 ± 0.2 63.4 ± 0.1 71.5 ± 0.4 75.2 ± 0.7 76.9 ± 0.2 SD [51] 96.5 ± 0.4 62.2 ± 0.0 69.7 ± 0.9 73.6 ± 0.4 75.5 ± 0.4 CAD [53] 94.5 ± 0.9 63.5 ± 0.6 70.4 ± 1.2 72.4 ± 1.3 75.2 ± 0.6 CondCAD [53] 96.5 ± 0.8 62.6 ± 0.4 69.1 ± 0.2 76.0 ± 0.2 76.1 ± 0.3 Fishr [52] 97.2 ± 0.6 63.3 ± 0.7 70.4 ± 0.6 74.0 ± 0.8 76.2 ± 0.3 Ours 96.9 ± 1.2 63.7 ± 1.1 72.0 ± 0.3 74.9 ± 0.8 76.9 ± 0.6 Table 13. Average accuracies on the OfficeHome [62] datasets using the default hyper-parameter settings in DomainBed [27]. art clipart product real Average ERM [61] 52.2 ± 0.2 48.7 ± 0.5 69.9 ± 0.5 71.7 ± 0.5 60.6 ± 0.2 IRM [1] 49.7 ± 0.2 46.8 ± 0.5 67.5 ± 0.4 68.1 ± 0.6 58.0 ± 0.1 GroupGRO [55] 52.6 ± 1.1 48.2 ± 0.9 69.9 ± 0.4 71.5 ± 0.8 60.6 ± 0.3 Mixup [68] 54.0 ± 0.7 49.3 ± 0.7 70.7 ± 0.7 72.6 ± 0.3 61.7 ± 0.5 MLDG [38] 53.1 ± 0.3 48.4 ± 0.3 70.5 ± 0.7 71.7 ± 0.4 60.9 ± 0.2 CORAL [59] 55.1 ± 0.7 49.7 ± 0.9 71.8 ± 0.2 73.1 ± 0.5 62.4 ± 0.4 MMD [40] 50.9 ± 1.0 48.7 ± 0.3 69.3 ± 0.7 70.7 ± 1.3 59.9 ± 0.4 DANN [23] 51.8 ± 0.5 47.1 ± 0.1 69.1 ± 0.7 70.2 ± 0.7 59.5 ± 0.5 CDANN [44] 51.4 ± 0.5 46.9 ± 0.6 68.4 ± 0.5 70.4 ± 0.4 59.3 ± 0.4 MTL [6] 51.6 ± 1.5 47.7 ± 0.5 69.1 ± 0.3 71.0 ± 0.6 59.9 ± 0.5 SagNet [48] 55.3 ± 0.4 49.6 ± 0.2 72.1 ± 0.4 73.2 ± 0.4 62.5 ± 0.3 ARM [72] 51.3 ± 0.9 48.5 ± 0.4 68.0 ± 0.3 70.6 ± 0.1 59.6 ± 0.3 VREx [36] 51.1 ± 0.3 47.4 ± 0.6 69.0 ± 0.4 70.5 ± 0.4 59.5 ± 0.1 RSC [33] 49.0 ± 0.1 46.2 ± 1.5 67.8 ± 0.7 70.6 ± 0.3 58.4 ± 0.6 SelfReg [34] 55.1 ± 0.8 49.2 ± 0.6 72.2 ± 0.3 73.0 ± 0.3 62.4 ± 0.1 MixStyle [75] 50.8 ± 0.6 51.4 ± 1.1 67.6 ± 1.3 68.8 ± 0.5 59.6 ± 0.8 Fish [58] 54.6 ± 1.0 49.6 ± 1.0 71.3 ± 0.6 72.4 ± 0.2 62.0 ± 0.6 SD [51] 55.0 ± 0.4 51.3 ± 0.5 72.5 ± 0.2 72.7 ± 0.3 62.9 ± 0.2 CAD [53] 52.1 ± 0.6 48.3 ± 0.5 69.7 ± 0.3 71.9 ± 0.4 60.5 ± 0.3 CondCAD [53] 53.3 ± 0.6 48.4 ± 0.2 69.8 ± 0.9 72.6 ± 0.1 61.0 ± 0.4 Fishr [52] 52.6 ± 0.9 48.6 ± 0.3 69.9 ± 0.6 72.4 ± 0.4 60.9 ± 0.3 Ours 54.4 ± 0.2 52.3 ± 0.8 69.5 ± 0.3 71.7 ± 0.2 62.0 ± 0.2Table 14. Average accuracies on the TerraInc [4] datasets using the default hyper-parameter settings in DomainBed [27]. L100 L38 L43 L46 Average ERM [61] 42.1 ± 2.5 30.1 ± 1.2 48.9 ± 0.6 34.0 ± 1.1 38.8 ± 1.0 IRM [1] 41.8 ± 1.8 29.0 ± 3.6 49.6 ± 2.1 33.1 ± 1.5 38.4 ± 0.9 GroupGRO [55] 45.3 ± 4.6 36.1 ± 4.4 51.0 ± 0.8 33.7 ± 0.9 41.5 ± 2.0 Mixup [68] 49.4 ± 2.0 35.9 ± 1.8 53.0 ± 0.7 30.0 ± 0.9 42.1 ± 0.7 MLDG [38] 39.6 ± 2.3 33.2 ± 2.7 52.4 ± 0.5 35.1 ± 1.5 40.1 ± 0.9 CORAL [59] 46.7 ± 3.2 36.9 ± 4.3 49.5 ± 1.9 32.5 ± 0.7 41.4 ± 1.8 MMD [40] 49.1 ± 1.2 36.4 ± 4.8 50.4 ± 2.1 32.3 ± 1.5 42.0 ± 1.0 DANN [23] 44.3 ± 3.6 28.0 ± 1.5 47.9 ± 1.0 31.3 ± 0.6 37.9 ± 0.9 CDANN [44] 36.9 ± 6.4 32.7 ± 6.2 51.1 ± 1.3 33.5 ± 0.5 38.6 ± 2.3 MTL [6] 45.2 ± 2.6 31.0 ± 1.6 50.6 ± 1.1 34.9 ± 0.4 40.4 ± 1.0 SagNet [48] 36.3 ± 4.7 40.3 ± 2.0 52.5 ± 0.6 33.3 ± 1.3 40.6 ± 1.5 ARM [72] 41.5 ± 4.5 27.7 ± 2.4 50.9 ± 1.0 29.6 ± 1.5 37.4 ± 1.9 VREx [36] 48.0 ± 1.7 41.1 ± 1.5 51.8 ± 1.5 32.0 ± 1.2 43.2 ± 0.3 RSC [33] 42.8 ± 2.4 32.2 ± 3.8 49.6 ± 0.9 32.9 ± 1.2 39.4 ± 1.3 SelfReg [34] 46.1 ± 1.5 34.5 ± 1.6 49.8 ± 0.3 34.7 ± 1.5 41.3 ± 0.3 MixStyle [75] 50.6 ± 1.9 28.0 ± 4.5 52.1 ± 0.7 33.0 ± 0.2 40.9 ± 1.1 Fish [58] 46.3 ± 3.0 29.0 ± 1.1 52.7 ± 1.2 32.8 ± 1.0 40.2 ± 0.6 SD [51] 45.5 ± 1.9 33.2 ± 3.1 52.9 ± 0.7 36.4 ± 0.8 42.0 ± 1.0 CAD [53] 43.1 ± 2.6 31.1 ± 1.9 53.1 ± 1.6 34.7 ± 1.3 40.5 ± 0.4 CondCAD [53] 44.4 ± 2.9 32.9 ± 2.5 50.5 ± 1.3 30.8 ± 0.5 39.7 ± 0.4 Fishr [52] 49.9 ± 3.3 36.6 ± 0.9 49.8 ± 0.2 34.2 ± 1.3 42.6 ± 1.0 Ours 51.7 ± 2.4 37.6 ± 0.6 49.9 ± 0.6 33.6 ± 0.6 43.2 ± 0.5 Table 15. Average accuracies on the DomainNet [50] datasets using the default hyper-parameter settings in DomainBed [27]. clip info paint quick real sketch Average ERM [61] 50.4 ± 0.2 14.0 ± 0.2 40.3 ± 0.5 11.7 ± 0.2 52.0 ± 0.2 43.2 ± 0.3 35.3 ± 0.1 IRM [1] 43.2 ± 0.9 12.6 ± 0.3 35.0 ± 1.4 9.9 ± 0.4 43.4 ± 3.0 38.4 ± 0.4 30.4 ± 1.0 GroupGRO [55] 38.2 ± 0.5 13.0 ± 0.3 28.7 ± 0.3 8.2 ± 0.1 43.4 ± 0.5 33.7 ± 0.0 27.5 ± 0.1 Mixup [68] 48.9 ± 0.3 13.6 ± 0.3 39.5 ± 0.5 10.9 ± 0.4 49.9 ± 0.2 41.2 ± 0.2 34.0 ± 0.0 MLDG [38] 51.1 ± 0.3 14.1 ± 0.3 40.7 ± 0.3 11.7 ± 0.1 52.3 ± 0.3 42.7 ± 0.2 35.4 ± 0.0 CORAL [59] 51.2 ± 0.2 15.4 ± 0.2 42.0 ± 0.2 12.7 ± 0.1 52.0 ± 0.3 43.4 ± 0.0 36.1 ± 0.2 MMD [40] 16.6 ± 13.3 0.3 ± 0.0 12.8 ± 10.4 0.3 ± 0.0 17.1 ± 13.7 0.4 ± 0.0 7.9 ± 6.2 DANN [23] 45.0 ± 0.2 12.8 ± 0.2 36.0 ± 0.2 10.4 ± 0.3 46.7 ± 0.3 38.0 ± 0.3 31.5 ± 0.1 CDANN [44] 45.3 ± 0.2 12.6 ± 0.2 36.6 ± 0.2 10.3 ± 0.4 47.5 ± 0.1 38.9 ± 0.4 31.8 ± 0.2 MTL [6] 50.6 ± 0.2 14.0 ± 0.4 39.6 ± 0.3 12.0 ± 0.3 52.1 ± 0.1 41.5 ± 0.0 35.0 ± 0.0 SagNet [48] 51.0 ± 0.1 14.6 ± 0.1 40.2 ± 0.2 12.1 ± 0.2 51.5 ± 0.3 42.4 ± 0.1 35.3 ± 0.1 ARM [72] 43.0 ± 0.2 11.7 ± 0.2 34.6 ± 0.1 9.8 ± 0.4 43.2 ± 0.3 37.0 ± 0.3 29.9 ± 0.1 VREx [36] 39.2 ± 1.6 11.9 ± 0.4 31.2 ± 1.3 10.2 ± 0.4 41.5 ± 1.8 34.8 ± 0.8 28.1 ± 1.0 RSC [33] 39.5 ± 3.7 11.4 ± 0.8 30.5 ± 3.1 10.2 ± 0.8 41.0 ± 1.4 34.7 ± 2.6 27.9 ± 2.0 SelfReg [34] 47.9 ± 0.3 15.1 ± 0.3 41.2 ± 0.2 11.7 ± 0.3 48.8 ± 0.0 43.8 ± 0.3 34.7 ± 0.2 MixStyle [75] 49.1 ± 0.4 13.4 ± 0.0 39.3 ± 0.0 11.4 ± 0.4 47.7 ± 0.3 42.7 ± 0.1 33.9 ± 0.1 Fish [58] 51.5 ± 0.3 14.5 ± 0.2 40.4 ± 0.3 11.7 ± 0.5 52.6 ± 0.2 42.1 ± 0.1 35.5 ± 0.0 SD [51] 51.3 ± 0.3 15.5 ± 0.1 41.5 ± 0.3 12.6 ± 0.2 52.9 ± 0.2 44.0 ± 0.4 36.3 ± 0.2 CAD [53] 45.4 ± 1.0 12.1 ± 0.5 34.9 ± 1.1 10.2 ± 0.6 45.1 ± 1.6 38.5 ± 0.6 31.0 ± 0.8 CondCAD [53] 46.1 ± 1.0 13.3 ± 0.4 36.1 ± 1.4 10.7 ± 0.2 46.8 ± 1.3 38.7 ± 0.7 31.9 ± 0.7 Fishr [52] 47.8 ± 0.7 14.6 ± 0.2 40.0 ± 0.3 11.9 ± 0.2 49.2 ± 0.7 41.7 ± 0.1 34.2 ± 0.3 Ours 50.7 ± 0.7 13.9 ± 0.4 39.4 ± 0.5 11.9 ± 0.2 50.2 ± 0.3 43.5 ± 0.1 34.9 ± 0.1",
      "meta_data": {
        "arxiv_id": "2304.04494v2",
        "authors": [
          "Liang Chen",
          "Yong Zhang",
          "Yibing Song",
          "Ying Shan",
          "Lingqiao Liu"
        ],
        "published_date": "2023-04-10T10:12:38Z",
        "pdf_url": "https://arxiv.org/pdf/2304.04494v2.pdf"
      },
      "llm_extracted_info": {
        "main_contributions": "The main contributions are addressing the domain generalization (DG) problem by improving Test-Time Training (TTT) strategies. The work proposes an Improved Test-Time Adaptation (ITTA) method with two key components: a learnable consistency loss for the TTT task, which dynamically aligns with the main prediction task through learnable parameters and shared optimization direction, and the introduction of additional adaptive parameters in the trained model, which are exclusively updated during the test phase. ITTA achieves superior performance over state-of-the-art methods on various DG benchmarks for both multi-source and single-source tasks.",
        "methodology": "The methodology revolves around two main strategies: 1) A Learnable Consistency Loss for TTT: Instead of empirically defining an auxiliary objective, a learnable consistency loss (Lwcont) is introduced using a weight subnetwork (fw(·)) to measure consistency between an input and its augmented version (z, z'). The loss Lwcont = ||fw(z - z')||, where fw is implemented with stacked ReLU layers. This loss is enforced to align with the main cross-entropy loss (Lmain) by minimizing the difference between their normalized gradients (min w ||ˆgmain − ˆgwcont||). Training involves alternating updates for the feature extractor/classifier and the weight subnetwork. 2) Including Additional Adaptive Parameters: New adaptive blocks (fΘ), parameterized by Θ, are inserted after each block of the pretrained feature extractor fθ. During the test-time adaptation phase, only these additional adaptive parameters fΘ are updated using the learned consistency loss (min Θ ||fw(z - z')||), leaving the original model parameters unchanged. Test-time adaptation uses an online updating setting.",
        "experimental_setup": "ITTA was evaluated on five benchmark datasets for multi-source domain generalization: PACS, VLCS, OfficeHome, TerraInc, and DomainNet. For single-source domain generalization, the PACS dataset was used. The backbone model was an ImageNet-pretrained ResNet18 with 4 blocks as the feature extractor. The additional adaptive parameters (fΘ) consisted of 4 blocks, each implemented with 5 layers of learnable parameters. The weight subnetwork (fw) had 10 layers of learnable parameters. The classifier (fϕ) was an MLP layer. The weight parameter α for balancing main and consistency losses was set to 1. All experiments followed rigorous evaluation protocols, including 60 trials per source domain, 5,000 iteration steps per trial, and dynamic setting of random seeds, learning rates, batch size, and augmentation skills according to the DomainBed benchmark. Model selection was based on the 'training-domain validate set' method from validation samples. The primary augmentation strategy used was MixStyle [75], with additional experiments conducted using an affine transformation [41]. Test-time adaptation steps were primarily one, with ablation studies for more steps and episodic updating.",
        "limitations": "The primary limitation is the increased computational cost. Updating the learnable weight subnetwork (fw) requires computing second-order derivatives for gradient alignment (Eq. 5), which incurs additional computational burden (1 extra forward pass and 3 extra backward passes) compared to simpler TTT objectives. Consequently, ITTA demands more FLOPS and processing time during inference compared to a baseline model, though the model size increase is moderate.",
        "future_research_directions": "Future research will focus on simplifying the overall optimization process of ITTA and reducing its computational cost to make the method more efficient."
      }
    },
    {
      "title": "TTN: A Domain-Shift Aware Batch Normalization in Test-Time Adaptation",
      "abstract": "This paper proposes a novel batch normalization strategy for test-time\nadaptation. Recent test-time adaptation methods heavily rely on the modified\nbatch normalization, i.e., transductive batch normalization (TBN), which\ncalculates the mean and the variance from the current test batch rather than\nusing the running mean and variance obtained from the source data, i.e.,\nconventional batch normalization (CBN). Adopting TBN that employs test batch\nstatistics mitigates the performance degradation caused by the domain shift.\nHowever, re-estimating normalization statistics using test data depends on\nimpractical assumptions that a test batch should be large enough and be drawn\nfrom i.i.d. stream, and we observed that the previous methods with TBN show\ncritical performance drop without the assumptions. In this paper, we identify\nthat CBN and TBN are in a trade-off relationship and present a new test-time\nnormalization (TTN) method that interpolates the statistics by adjusting the\nimportance between CBN and TBN according to the domain-shift sensitivity of\neach BN layer. Our proposed TTN improves model robustness to shifted domains\nacross a wide range of batch sizes and in various realistic evaluation\nscenarios. TTN is widely applicable to other test-time adaptation methods that\nrely on updating model parameters via backpropagation. We demonstrate that\nadopting TTN further improves their performance and achieves state-of-the-art\nperformance in various standard benchmarks.",
      "full_text": "Published as a conference paper at ICLR 2023 TTN: A D OMAIN -SHIFT AWARE BATCH NORMALIZA - TION IN TEST-TIME ADAPTATION Hyesu Lim1,2∗, Byeonggeun Kim ∗, Jaegul Choo 2, Sungha Choi 1‡ 1Qualcomm AI Research†, 2KAIST ABSTRACT This paper proposes a novel batch normalization strategy for test-time adaptation. Recent test-time adaptation methods heavily rely on the modiﬁed batch normal- ization, i.e., transductive batch normalization (TBN), which calculates the mean and the variance from the current test batch rather than using the running mean and variance obtained from source data,i.e., conventional batch normalization (CBN). Adopting TBN that employs test batch statistics mitigates the performance degra- dation caused by the domain shift. However, re-estimating normalization statistics using test data depends on impractical assumptions that a test batch should be large enough and be drawn from i.i.d. stream, and we observed that the previous meth- ods with TBN show critical performance drop without the assumptions. In this paper, we identify that CBN and TBN are in a trade-off relationship and present a new test-time normalization(TTN) method that interpolates the standardization statistics by adjusting the importance between CBN and TBN according to the domain-shift sensitivity of each BN layer. Our proposed TTN improves model robustness to shifted domains across a wide range of batch sizes and in various realistic evaluation scenarios. TTN is widely applicable to other test-time adap- tation methods that rely on updating model parameters via backpropagation. We demonstrate that adopting TTN further improves their performance and achieves state-of-the-art performance in various standard benchmarks. 1 I NTRODUCTION When we deploy deep neural networks (DNNs) trained on the source domain into test environments (i.e., target domains), the model performance on the target domain deteriorates due to the domain shift from the source domain. For instance, in autonomous driving, a well-trained DNNs model may exhibit signiﬁcant performance degradation at test time due to environmental changes, such as camera sensors, weather, and region (Choi et al., 2021; Lee et al., 2022; Kim et al., 2022b). Test-time adaptation (TTA) has emerged to tackle the distribution shift between source and target domains during test time (Sun et al., 2020; Wang et al., 2020). Recent TTA approaches (Wang et al., 2020; Choi et al., 2022; Liu et al., 2021) address this issue by 1) (re-)estimating normaliza- tion statistics from current test input and 2) optimizing model parameters in unsupervised manner, such as entropy minimization (Grandvalet & Bengio, 2004; Long et al., 2016; Vu et al., 2019) and self-supervised losses (Sun et al., 2020; Liu et al., 2021). In particular, the former focused on the weakness of conventional batch normalization (CBN) (Ioffe & Szegedy, 2015) for domain shift in a test time. As described in Fig. 1(b), when standardizing target feature activations using source statistics, which are collected from the training data, the activations can be transformed into an un- intended feature space, resulting in misclassiﬁcation. To this end, the TTA approaches (Wang et al., 2020; Choi et al., 2022; Wang et al., 2022) have heavily depended on the direct use of test batch statistics to ﬁx such an invalid transformation in BN layers, called transductive BN (TBN) (Nado et al., 2020; Schneider et al., 2020; Bronskill et al., 2020) (see Fig. 1(c)). The approaches utilizing TBN showed promising results but have mainly been assessed in limited evaluation settings (Wang et al., 2020; Choi et al., 2022; Liu et al., 2021). For instance, such evalua- tion settings assume large test batch sizes (e.g., 200 or more) and a single stationary distribution shift ∗Work completed while at Qualcomm Technologies, Inc. ‡Corresponding author. †Qualcomm AI Research is an initiative of Qualcomm Technologies, Inc. 1 arXiv:2302.05155v2  [cs.CV]  18 Feb 2023Published as a conference paper at ICLR 2023 0 20 40 60 80 100 CBN TBN Ours TENT TENT+Ours SWR SWR+Ours Error rate (%) 200 64 16 4 2 1 TTN(Ours) TENT+TTN(Ours) SWR+TTN(Ours) Test batch size (a) Valid output using CBN (b) Invalid output using CBN (c) Valid output using TBN (d) Performance drops in small test batches using TBN ● Class A ● Class B Source mean● Source features Test features⨯ Test meanStandardize Figure 1: Trade-off between CBN & TBN.In conceptual illustrations (a), (b), and (c), the depicted standardization only considers making the feature distribution have a zero mean, disregarding mak- ing it have unit variance. When the source and test distributions are different, and the test batch size is large, (b) test features can be wrongly standardized when using CBN (Ioffe & Szegedy, 2015), but (c) TBN (Nado et al., 2020) can provide a valid output. (d) Error rates (↓) on shifted domains (CIFAR-10-C). TBN and TBN applied (TENT (Wang et al., 2020), SWR (Choi et al., 2022)) meth- ods suffer from severe performance drop when the batch size becomes small, while TTN (Ours) improves overall performance. (i.e., single corruption). Recent studies suggest more practical evaluation scenarios based on small batch sizes (Mirza et al., 2022; Hu et al., 2021; Khurana et al., 2021) or continuously changing data distribution during test time (Wang et al., 2022). We show that the performance of existing methods signiﬁcantly drops once their impractical assumptions of the evaluation settings are violated. For example, as shown in Fig. 1(d), TBN (Nado et al., 2020) and TBN applied methods suffer from severe performance drop when the test batch size becomes small, while CBN is irrelevant to the test batch sizes. We identify that CBN and TBN are in a trade-off relationship (Fig. 1), in the sense that one of each shows its strength when the other falls apart. To tackle this problem, we present a novel test-time normalization (TTN)strategy that controls the trade-off between CBN and TBN by adjusting the importance of source and test batch statistics according to the domain-shift sensitivity of each BN layer. Intuitively, we linearly interpolate be- tween CBN and TBN so that TBN has a larger weight than CBN if the standardization needs to be adapted toward the test data. We optimize the interpolating weight after the pre-training but before the test time, which we refer to as the post-training phase. Speciﬁcally, given a pre-trained model, we ﬁrst estimate channel-wise sensitivity of the afﬁne parameters in BN layers to domain shift by analyzing the gradients from the back-propagation of two input images, clean input and its aug- mented one (simulating unseen distribution). Afterward, we optimize the interpolating weight using the channel-wise sensitivity replacing BN with the TTN layers. It is noteworthy that none of the pre-trained model weights are modiﬁed, but we only train newly added interpolating weight. We empirically show that TTN outperforms existing TTA methods in realistic evaluation settings, i.e., with a wide range of test batch sizes for single, mixed, and continuously changing domain adaptation through extensive experiments on image classiﬁcation and semantic segmentation tasks. TTN as a stand-alone method shows compatible results with the state-of-the-art methods and com- bining our TTN with the baselines even boosts their performance in overall scenarios. Moreover, TTN applied methods ﬂexibly adapt to new target domains while sufﬁciently preserving the source knowledge. No action other than computing per batch statistics (which can be done simultaneously to the inference) is needed in test-time; TTN is compatible with other TTA methods without requir- ing additional computation cost. Our contributions are summarized as follows: • We propose a novel domain-shift aware test-time normalization (TTN) layer that combines source and test batch statistics using channel-wise interpolating weights considering the sensitivity to domain shift in order to ﬂexibly adapt to new target domains while preserving the well-trained source knowledge. 2Published as a conference paper at ICLR 2023 Per-batch        Frozen        Optimize S Standardize Ƹ𝑧 = 𝑧𝑖𝑛 −𝜇 𝜎 T Affine Transform 𝑧𝑜𝑢𝑡 = 𝛾⋅ Ƹ𝑧+𝛽 … … CBN CBN CBN … Pre-train (CBN) (a-1) Post-train (TTN)          Test time (TTN) (a) Overall procedure of train and test phases (b) Comparison of BN layers 𝑧𝑖𝑛 S Ƹ𝑧 T 𝑧𝑜𝑢𝑡 𝛾,𝛽+ 1−𝛼 𝜇𝑠,𝜎𝑠𝜇𝑖𝑛,𝜎𝑖𝑛 𝑧𝑖𝑛 S Ƹ𝑧 T 𝑧𝑜𝑢𝑡 𝜇𝑠,𝜎𝑠 𝛾,𝛽 CBN in test time TBN in test time (b-1) TTN(Ours) in post-train 𝑧𝑖𝑛 S Ƹ𝑧 T 𝑧𝑜𝑢𝑡 𝜇𝑖𝑛,𝜎𝑖𝑛 𝛾,𝛽 𝛼 𝑧𝑖𝑛 S Ƹ𝑧 T 𝑧𝑜𝑢𝑡 𝛾,𝛽+ 1−𝛼 𝜇𝑠,𝜎𝑠𝜇𝑖𝑛,𝜎𝑖𝑛 (b-2) TTN(Ours) in test time 𝛼 Figure 2: Method overview. (a) We introduce an additional training phase between pre-train and test time called (a-1) post-training phase. (b) Our proposed TTN layer combines per-batch statistics and frozen source statistics with interpolating weight α, which is (b-1) optimized in post-training phase and (b-2) ﬁxed in test time. • To show the broad applicability of our proposed TTN, which does not alter training or test- time schemes, we show that adding TTN to existing TTA methods signiﬁcantly improves the performance across a wide range of test batch sizes (from 200 to 1) and in three realistic evaluation scenarios; stationary, continuously changing, and mixed domain adaptation. • We evaluate our method through extensive experiments on image classiﬁcation using CIFAR-10/100-C, and ImageNet-C (Hendrycks & Dietterich, 2018) and semantic segmen- tation task using CityScapes (Cordts et al., 2016), BDD-100K (Yu et al., 2020), Mapil- lary (Neuhold et al., 2017), GTA V (Richter et al., 2016), and SYNTHIA (Ros et al., 2016). 2 M ETHODOLOGY In this section, we describe our method, the Test-Time Normalization(TTN) layer, whose design is suitable for test-time adaptation (TTA) in practical usages out of the large batch size and i.i.d assumptions during a test time. We ﬁrst deﬁne the problem setup in Section 2.1 and present our pro- posed TTN layers in Section 2.2. Finally, we discuss how we optimize TTN layers in Sections 2.3. 2.1 P ROBLEM SETUP Let the train and test data be DS and DT and the corresponding probability distributions be PS and PT, respectively, where DS and DT share the output space, i.e., {yi}∼D S = {yi}∼D T. The covariate shift in TTA is deﬁned asPS(x) ̸= PT(x) where PS(y|x) =PT(y|x) (Quinonero-Candela et al., 2008). A model, fθ, with parameters θ, is trained with a mini-batch, BS = {(xi,yi)}|BS| i=1 , from source data DS, where xi is an example and yi is the corresponding label. During the test, fθ encounters a test batch BT ∼DT, and the objective of TTA is correctly managing the test batch from the different distribution. To simulate more practical TTA, we mainly consider two modiﬁcations: (1) various test batch sizes, |BT|, where small batch size indicates small latency while handling the test data online, and (2) multi, N-target domains, DT = {DT,i}N i=1. Under this setting, each test batch BT is drawn by one of the test domains inDT, where DT may consist of a single target domain, multiple target domains, or mixture of target domains. 2.2 T EST-TIME NORMALIZATION LAYER We denote an input of a BN layer as z ∈RBCHW , forming a mini-batch size of B. The mean and variance of z are µand σ2, respectively, which are computed as follows: µc = 1 BHW B∑ b H∑ h W∑ w zbchw, σ 2 c = 1 BHW B∑ b H∑ h W∑ w (zbchw −µc)2, (1) where µand σ2 are in RC, and C, H, and W stand for the number of channels, dimension of height, and that of width, respectively. Based on µand σ2, the source statistics µs,σ2 s ∈RC are usually estimated with exponential moving average over the training data. 3Published as a conference paper at ICLR 2023 Conv CBN ReLU Conv ReLU Conv CBN ReLU FC… CBN 𝑥′ ℒ𝐶𝐸 (a-1) Gradient of affine parameters 𝛾,𝛽 ∇𝛾 (1),∇𝛽 (1) ∇𝛾 (2),∇𝛽 (2) ∇𝛾 (𝐿),∇𝛽 (𝐿) ∇𝛾′ (1),∇𝛽′ (1) ∇𝛾′ (2),∇𝛽′ (2) ∇𝛾′ (𝐿),∇𝛽′ (𝐿) Conv CBN ReLU Conv ReLU Conv CBN ReLU FC… CBN 𝑥 ℒ𝐶𝐸 (a-2) Prior 𝒜 (b-1) Initialize 𝛼 with prior 𝒜 … 1       0 (a) Obtain prior 𝒜 (b) Optimize 𝛼 Per-batch statistics        Frozen source statistics        Gradient flow Conv ReLU Conv ReLU Conv ReLU FC…𝑥′ ℒ𝐶𝐸 +ℒ𝑀𝑆𝐸 TTNTTNTTN 𝛼 1−𝛼 TTN 𝜇𝑠 (𝑙) 𝜇𝑖𝑛 (𝑙) 𝐶𝑙 𝛼(𝑙,𝑐) + 1−𝛼(𝑙,𝑐) … … ++ 𝛼(𝑙,𝐶𝑙) 1−𝛼(𝑙,𝐶𝑙) (b-2) Optimize 𝛼 C1 C2 C𝐿 Figure 3: Two stages in post-training phase. (a) Given a pre-trained model, which uses CBN, and its training data, we obtain prior knowledge of each BN layer. (a-1) We ﬁrst compute gradients of afﬁne parameters in each BN layer from clean x and augmented input x′and obtain the gradient distance score (Eq. 4). (a-2) For BN layers with larger distance score, we put more importance on current batch statistics than source statistics ( i.e., large α), and we deﬁne prior Aaccordingly (Eq. 5). (b) After obtaining prior A, we substitute BN layers from CBN to TTN.(b-1) Initializing α with prior A, (b-2) we optimize αusing CE and MSE loss (Eq. 6) with augmented training data x′. In BN layers, input z is ﬁrst standardized with statistics µand σ2 and then is scaled and shifted with learnable parameters γ and β in RC. The standardization uses current input batch statistics during training and uses estimated source statistics µs and σ2 s at test time (Fig. 2(b)). To address domain shifts in test time, we adjust the source statistics by combining the source and the test mini-batch statistics (Singh & Shrivastava, 2019; Summers & Dinneen, 2019) with a learnable interpolating weight α∈RC ranges [0,1]. Precisely, TTN standardizes a feature with ˜µ= αµ+ (1−α)µs, ˜σ2 = ασ2 + (1−α)σ2 s + α(1 −α)(µ−µs)2, (2) while using the same afﬁne parameters, γ and β. Note that we have different mixing ratios αc for every layer and channel. 2.3 P OST TRAINING Like Choi et al. (2022), we introduce an additional training phase, the post-training (after pre- training but before testing), to optimize the mixing parameters α in Eq. 2 (Fig. 2(a)). Note that all parameters except αare frozen and we have access to the labeled source data during the post- training. We ﬁrst obtain prior knowledge Aof αby identifying which layers and their channels are sensitive to domain shifts. Then, we optimizeαwith the prior knowledge and an additional objective term. The overall procedure is depicted in Fig. 3 and the pseudocode is provided in appendix A.3. Obtain Prior A. To identify which BN layers and corresponding channels are sensitive to domain shifts, we simulate the domain shifts by augmenting1 the clean image, i.e., original training data, and make a pair of (clean x, domain-shifted x′) images, where the semantic information is shared. To analyze in which layer and channel the standardization statistics should be corrected, we consider the standardized features ˆz(l,c) of z(l,c), for a channel index cat a layer l, whose input is clean x. We compare ˆz(l,c) to that of domain-shifted one, ˆz′(l,c) from x′. Since the pre-trained CBN uses the same µ(l,c) s and σ(l,c) s for both inputs, the difference between ˆz(l,c) and ˆz′(l,c) is caused by the domain discrepancy between xand x′. We argue that if the difference is signiﬁcant, the parameter at (l,c) is sensitive to the domain shift, i.e., intensely affected by the domain shift, and hence the standardization statistics at (l,c) should be adapted towards the shifted input. Drawing inspiration from Choi et al. (2022), we measure the domain-shift sensitivity by comparing gradients. Since the standardized feature ˆz is scaled and shifted by γ and β in each BN layer, we compare the gradients of afﬁne parameters γ and β, ∇γ and ∇β, respectively, to measure the dissimilarity of ˆzand ˆz′. As described in Fig. 3(a-1), we collect the ∇γ and ∇β using cross-entropy 1It is noteworthy that the post-training phase is robust to the choice of data augmentation types. Ablation study results and discussions are provided in the appendix B.4. 4Published as a conference paper at ICLR 2023 loss, LCE. To this end, we introduce a gradient distance score, d(l,c) ∈R for each channel cat layer las follows: s= 1 N N∑ i=1 gi ·g′ i ∥gi∥∥g′ i∥, (3) d(l,c) = 1−1 2 ( s(l,c) γ + s(l,c) β ) , (4) where (g,g′) is (∇(l,c) γ ,∇(l,c) γ′ ) and (∇(l,c) β ,∇(l,c) β′ ) for s(l,c) γ and s(l,c) β , respectively,N is the number of training data, and the resulting d(l,c) ∈[0,1]. Once we obtain sγ and sβ from Eq. 3, we conduct min-max normalization over all s(l,c) γ and s(l,c) β , before computing Eq. 4. To magnify the relative difference, we take the square as a ﬁnal step and denote the result as a prior A(Fig. 3(a-2)): A= [d(1,.),d(2,.),...,d (L,.)]2, (5) where d(l,.) = [d(l,c)]Cl c=1. Optimize α. The goal of optimizing α is to make the combined statistics correctly standardize the features when the input is sampled from an arbitrary target domain. After obtaining the prior A, we replace CBN with TTN layers while keeping the afﬁne parameters. Then, we initialize the interpolating weights α with A, which represents in which layer and channel the standardization statistics need to be adapted using test batch statistics (see Fig. 3(b-1)). To simulate distribution shifts, we use augmented training data. Expecting the model to make consistent predictions either given clean or augmented inputs, we use cross-entropy loss LCE. Furthermore, to prevent αfrom moving too far from the initial value A, we use mean-squared error loss LMSE between αand the prior A, i.e., LMSE = ∥α−A∥2 as a constraint. Total lossLcan be written asL= LCE +λLMSE (6), where λis a weighting hyperparameter (Details are provided in the appendix A.1 & B.1). 3 E XPERIMENTS In image classiﬁcation, we evaluate TTN for corruption robustness in realistic evaluation settings, i.e., where the test batch size can be variant and where the target domain can be either stationary, continuously changing, or mixed with multiple domains. Additionally, we further validate TTN on domain generalization benchmarks incorporating natural domain shifts ( e.g., changes in camera sensors, weather, time, and region) in semantic segmentation. 3.1 E XPERIMENTAL SETUP Given models pre-trained on clean source data, we optimize TTN parameter αwith the augmented source data in the post-training phase. Afterward, we evaluate our post-trained model on the cor- rupted target data. Implementation details are provided in the appendix A.1. Datasets and models. We use corruption benchmark datasets CIFAR-10/100-C and ImageNet-C, which consist of 15 types of common corruptions at ﬁve severity levels (Hendrycks & Dietterich, 2018). Each corruption is applied to test images of the clean datasets (CIFAR-10/100 and Ima- geNet). We use a training set of the clean dataset for post-training and the corrupted dataset for evaluation. As backbone models, we used WideResNet-40-2 (Hendrycks et al., 2019) trained on CIFAR-10/100, and ResNet-50 (He et al., 2016) trained on ImageNet. To validate our method in se- mantic segmentation, we conduct experiments on Cityscapes (Cordts et al., 2016), BDD-100K (Yu et al., 2020), Mapillary (Neuhold et al., 2017), GTA V (Richter et al., 2016), and SYNTHIA (Ros et al., 2016) datasets, in accordance with the experimental setup for domain generalization proposed in RobustNet (Choi et al., 2021). Baselines. To demonstrate that TTN successfully controls the trade-off between CBN and TBN, we compare TTN with (1) AdaptiveBN (Schneider et al., 2020), (2) α-BN (You et al., 2021) and (3) MixNorm (Hu et al., 2021), which combines or takes the moving average of the source and the test batch statistics with a pre-deﬁned hyperparameter ( i.e., a constant α). The following baselines are suggested on top of TBN (Nado et al., 2020); (4) TENT (Wang et al., 2020) optimizes BN afﬁne parameters via entropy minimization. (5) SWR (Choi et al., 2022) updates the entire model parame- ters considering the domain-shift sensitivity. (6) CoTTA (Wang et al., 2022) ensembles the output of 5Published as a conference paper at ICLR 2023 Table 1: Single domain adaptation on corruption benchmark. Error rate ( ↓) averaged over 15 corruptions with severity level 5 using WideResNet-40-2 as a backbone for each test batch size. We used reported results of MixNorm with ﬁxed parameters from the original paper and denoted as ∗. In appendix B.3, we provide variants of TTN, which show stronger performance for small test batch. CIFAR-10-C CIFAR-100-C Method 200 64 16 4 2 1 Avg. 200 64 16 4 2 1 Avg. Source (CBN)18.27 18.27 18.27 18.27 18.27 18.27 18.27 46.75 46.75 46.75 46.75 46.75 46.7546.75 Norm TBN 14.49 15.02 17.10 26.28 35.65 90.00 33.09 39.25 40.21 44.03 59.10 80.65 99.0460.38 AdaptiveBN12.21 12.31 12.89 14.51 15.79 16.14 13.98 36.56 36.85 38.19 41.18 43.2644.0140.01 α-BN 13.78 13.77 13.89 14.54 15.1615.4714.44 39.72 39.85 39.99 41.3442.6645.6441.53 MixNorm∗ 13.85 14.41 14.23 14.60 (B=5) - 15.0914.44 - - - - - - - Ours (TTN)11.6711.8012.13 13.93 15.8317.99 13.8935.5835.8436.7341.0846.6757.7142.27 Optim. TENT 12.08 14.78 16.90 25.61 35.69 90.00 32.51 35.52 39.90 43.78 59.02 80.68 99.0259.65 +Ours (TTN)11.2811.5212.04 13.95 15.8417.9413.77 35.1635.5736.5541.1846.6358.3342.24 SWR 10.26 13.51 16.61 27.33 40.48 90.04 33.04 32.6837.41 43.15 59.90 87.07 99.0559.88 +Ours (TTN)9.92 11.7713.41 18.02 24.0961.5623.13 32.8635.1338.6649.8060.7280.9049.68 Table 2: Continuously changing domain adaptation on corruption benchmark. Error rate (↓) averaged over 15 corruptions with severity level 5 using WideResNet-40-2 as backbone for each test batch size. We omitted ‘Norm’ methods results in this table since they are eqaul to that of Table 1. CIFAR-10-C CIFAR-100-C Method 200 64 16 4 2 1 Avg. 200 64 16 4 2 1 Avg. Source (CBN)18.27 18.27 18.27 18.27 18.27 18.2718.27 46.75 46.75 46.75 46.75 46.75 46.7546.75 Ours (TTN)11.6711.80 12.13 13.9315.8317.9913.89 35.58 35.84 36.73 41.08 46.67 57.71 42.27 Optim. CoTTA 12.46 14.60 21.26 45.69 58.87 90.0040.48 39.75 42.20 52.94 73.69 87.66 98.9965.87 TENT 12.54 13.52 15.69 26.23 35.77 90.0032.29 36.11 37.90 43.78 58.71 81.76 99.0459.55 +Ours (TTN)11.4411.60 12.08 16.1418.3622.4015.33 43.50 37.60 38.28 44.60 54.2980.63 49.82 SWR 11.04 11.53 13.90 23.99 34.02 90.0030.75 34.16 35.79 40.71 58.15 80.55 99.0362.56 +Ours (TTN)10.09 10.5111.28 14.2916.6784.1224.49 33.0934.07 36.15 42.41 53.63 93.08 48.74 augmented test inputs, updates the entire model parameters using a consistency loss between student and teacher models, and stochastically restores the pre-trained model. We refer to TBN, (1), (2), and (3) as normalization-based methods (Norm), the other as optimization-based methods (Optim.), and denote the pre-trained model using CBN as ‘source’. Evaluation scenarios. To show that TTN performs robust on various test batch sizes, we conduct experiments with test batch sizes of 200, 64, 16, 4, 2, and 1. We evaluate our method in three evalu- ation scenarios; single, continuously changing, and mixed domain adaptation. In the single domain adaptation, the model is optimized for one corruption type and then reset before adapting to the subsequent corruption, following the evaluation setting from TENT and SWR. In the continuously changing adaptation (Wang et al., 2022), the model is continuously adapted to 15 corruption types (w/o the reset), which is more realistic because it is impractical to precisely indicate when the data distribution has shifted in the real world. Finally, to simulate the non-stationary target domain where various domains coexist, we evaluate methods in the mixed domain adaptation setting, where a sin- gle batch contains multiple domains. We use a severity level of 5 (Hendrycks & Dietterich, 2018) for all experiments. It is noteworthy that we use a single checkpoint of TTN parameter αfor each dataset across all experimental settings. 3.2 E XPERIMENTS ON IMAGE CLASSIFICATION Tables 1, 2, and 3 show error rates on corruption benchmark datasets in three different evaluation scenarios; single domain, continuously changing, and mixed domain adaptation, respectively. Note that the performance of normalization-based methods in the single (Table 1) and in the continu- ously changing (Table 2) settings are identical. Tables 4 and 5 show the adaptation performance on the source and class imbalanced target domains, respectively. More results and discussions are provided in the appendix B, importantly, including results on ImageNet-C (B.5). Robustness to practical settings. In Table 1, 2, and 3, TTN and TTN applied methods show robust performance over the test batch size ranges from 200 to 1. Comparing with normalization-based baselines, we demonstrate that TTN, which uses channel-wisely optimized combining rateα, shows better results than deﬁning α as a constant hyperparameter, which can be considered as a special 6Published as a conference paper at ICLR 2023 Table 3: Mixed domain adaptation on corruption benchmark. Error rate (↓) of mixed domain with severity level 5 using WideResNet-40-2 as backbone for each test batch size. We used the reported results of MixNorm with ﬁxed parameters from the original paper and denoted them as ∗. CIFAR-10-C CIFAR-100-C Method 200 64 16 4 2 1 Avg. 200 64 16 4 2 1 Avg. Source (CBN)18.27 18.27 18.27 18.27 18.27 18.2718.27 46.75 46.75 46.75 46.75 46.75 46.7546.75 Norm TBN 14.99 15.29 17.38 26.65 35.59 90.0033.31 39.88 40.48 43.73 59.11 80.30 98.9160.40 AdaptiveBN12.62 12.48 12.97 14.59 15.74 16.0214.07 36.88 36.86 38.49 41.43 43.38 44.3140.23 α-BN 13.78 13.78 13.99 14.6115.07 15.2014.41 40.25 40.11 40.47 41.6442.39 43.8141.45 MixNorm∗ 18.80 18.80 18.80 18.80 18.80 18.8018.80 - - - - - - - Ours (TTN)12.1612.1912.3413.9615.5517.8314.00 36.2436.2336.8541.0145.8555.5241.95 Optim. TENT 14.33 14.97 17.30 26.07 35.37 90.0033.01 39.36 40.01 43.33 58.98 80.55 98.9260.19 +Ours (TTN)12.0212.0412.2013.7715.4216.4013.64 36.2936.2336.8941.3846.6557.9542.57 SWR 13.24 13.06 16.57 26.08 38.65 91.0359.54 37.84 37.93 44.37 59.50 78.66 98.9533.10 +Ours (TTN)11.8911.6513.3717.0523.5064.1050.29 36.4936.5139.6046.2058.2084.7623.59 case of TTN; TBN and α-BN corresponds to α = 1and 0.1, respectively. More comparisons with different constant αare provided in the appendix B.2. It is noteworthy that TTN as a stand-alone method favorably compares with optimization-based baselines in all three scenarios. Table 4: Source domain adaptation. Error rate (↓) on CIFAR-10 using WideResNet-40-2. Method Test batch size Avg.200 64 16 4 2 1 Source (CBN)4.92 4.92 4.92 4.92 4.92 4.924.92 Norm TBN 6.41 6.60 8.64 17.65 26.08 90.0025.90Ours (TTN)4.885.115.357.27 9.45 9.96 7.00 Optim. TENT 6.15 6.45 8.61 17.61 26.20 90.0032.2+Ours (TTN)4.935.115.327.22 9.3810.217.02 SWR 5.63 6.01 8.25 17.49 26.32 90.0025.62+Ours (TTN)4.795.025.516.68 7.91 9.34 6.54 Table 5: Class imbalanced target domain. Error rate (↓) averaged over 15 corruptions of CIFAR- 10-C with severity level 5 using WideResNet-40- 2. Details are provided in the appendix A.2. Method Test batch size Avg.200 64 16 4 2 1 Source (CBN)18.27 18.27 18.27 18.27 18.27 18.2718.27 TBN 77.60 76.66 77.72 78.59 77.84 90.0079.74Ours (TTN)35.7535.1334.9232.5128.6017.9930.82 Adopting TTN improves other TTA methods. We compare optimization-based methods with and without TTN layers. Since TENT, SWR, and CoTTA optimize model parameters on top of using TBN layers, they also suffer from performance drops when the test batch size becomes small. Adopting TTN reduces the dependency on large test batch size, i.e., makes robust to small batch size, and even improves their performance when using large test batch. Furthermore, in continual (Table 2) and mixed domain (Table 3) adaptation scenario, TENT and SWR shows higher error rate than in single domain (Table 1) adaptation. We interpret that because they update the model parameters based on the current output and predict the next input batch using the updated model, the model will not perform well if the consecutive batches have different corruption types ( i.e., mixed domain adaptation). Moreover, the error from the previous input batch propagates to the future input stream, and thus they may fall apart rapidly once they have a strongly wrong signal, which can happen in continual adaptation ( i.e., long-term adaptation without resetting). Applying TTN signiﬁcantly accelerates their model performance regardless of the evaluation scenarios. TTN preserves knowledge on source domain. In practice, data driven from the source domain (or a merely different domain) can be re-encountered in test time. We used clean domain test data in the single domain adaptation scenario to show how TTN and other TTA methods adapt to the seen source domain data (but unseen instance). As shown in Table 4, all baseline methods using TBN layers, show performance drops even with large batch sizes. We can conclude that it is still better to rely on source statistics collected from the large training data than using only current input statistics, even if its batch size is large enough to obtain reliable statistics ( i.e., 200). However, since TTN utilizes source statistics while leveraging the current input, TTN itself and TTN adopted methods well preserve the source knowledge compared to the TBN-based methods. With a batch size of 200, we observe that combining the source and a current test batch statistics outperforms the source model (see 3rd row of Table 4). TTN is robust to class imbalanced scenario. Heavily depending on current test batch statistics are especially vulnerable when the class labels are imbalanced (Boudiaf et al., 2022; Gong et al., 2022). To simulate this situation, we sorted test images in class label order and then sampled test batches following the sorted data order. In Table 5, we observe that TTN is more robust to the class imbalanced scenario than utilizing only test batch statistics (i.e., TBN). As explained in Section 3.5, 7Published as a conference paper at ICLR 2023 Table 6: Adaptation on DG benchmarks in semantic segmentation. mIoU(↑) on four unseen domains with test batch size of 2 using ResNet-50 based DeepLabV3+ as a backbone. Method(Cityscapes→) BDD-100K Mapillary GTA V SYNTHIA Cityscapes Source (Chen et al., 2018) 43.50 54.37 43.71 22.78 76.15 Norm TBN 43.12 47.61 42.51 25.71 72.94 Ours (TTN) 47.40 56.92 44.71 26.68 75.09 Optim. TENT 43.30 47.80 43.57 25.92 72.93 + Ours (TTN) 47.89 57.84 46.18 27.29 75.04 SWR 43.40 47.95 42.88 25.97 72.93 + Ours (TTN) 48.85 59.09 46.71 29.16 74.89 we are putting more importance on CBN than TBN, where semantic information is mainly handled, i.e., in deeper layers, so we can understand that TTN is less impacted by skewed label distribution. 3.3 E XPERIMENTS ON SEMANTIC SEGMENTATION We additionally conduct experiments on domain generalization (DG) benchmarks (Choi et al., 2021; Pan et al., 2018) for semantic segmentation, including natural domain shifts ( e.g., Cityscapes→BDD-100K), to demonstrate the broad applicability of TTN. Table 6 shows the results of evaluating the ResNet-50-based DeepLabV3+ (Chen et al., 2018) model trained on the Cityscapes training set using the validation set of real-world datasets such as Cityscapes, BDD-100K, and Map- illary, and synthetic datasets including GTA V and SYNTHIA. We employ a test batch size of 2 for test-time adaptation in semantic segmentation. We observe that even when exploiting test batch statistics for standardization in BN layers (TBN) or updating the model parameters on top of TBN (TENT, SWR) does not improve the model performance (i.e., perform worse than the source model), adopting TTN helps the model make good use of the strength of the test batch statistics. Implemen- tation details and additional results are provided in the appendix A.1 and B.7, respectively. 3.4 A BLATION STUDIES Prior Aregularizes αto be robust to overall test batch sizes. We conduct an ablation study on the importance of each proposed component,i.e., initializing αwith prior A, optimizing αusing CE and MSE losses, and the results are shown in Table 7. Using Afor initialization and MSE loss aims to optimize αfollowing our intuition that we discussed in Section 2.3. Optimizing αusing CE loss improves the overall performance, but without regularizing with MSE loss, αmay overﬁt to large batch size (rows 2 & 3). Initialization with Aor not does not show a signiﬁcant difference, but A provides a better starting point than random initialization when comparing the left and right of the 2nd row. We observe that when using MSE loss, regardless of initialization using A, the optimized αsufﬁciently reﬂects our intuition resulting in a low error rate to overall batch sizes (row 3). Table 7: Ablation study on importance of each component Method Test batch size Avg. Method Test batch size Avg.Init. CE MSE200 64 16 4 2 1 Init. CE MSE200 64 16 4 2 1 - - \u0013 13.36 13.43 13.85 15.50 17.43 20.0715.61 \u0013 - - 13.37 13.43 13.85 15.50 17.44 20.0715.61 - \u0013 - 11.64 11.7312.26 14.46 16.94 19.8814.49 \u0013 \u0013 - 11.73 11.82 12.23 14.18 16.41 19.2714.27 - \u0013 \u0013 11.6411.7812.21 13.97 15.86 18.0013.91 \u0013 \u0013 \u0013 11.67 11.80 12.13 13.93 15.83 17.9913.89 3.5 V ISUALIZATION OF α Fig. 4 shows the visualization of optimized αfor CIFAR-10 using WideResNet-40-2. We observe that α decreases from shallow to deep layers (left to right), which means CBN is more active in deeper layers, and TBN is vice versa. As shown in Table 4 and 6, CBN employing source statistics is superior to TBN when the distribution shift between source and target domains is small. Assum- ing that the αwe obtained is optimal, we can conjecture that CBN is more active ( i.e., αcloser to 0) in deeper layers because domain information causing the distribution shift has been diminished. In contrast, TBN has a larger weight ( i.e., α closer to 1) in shallower layers since the domain in- formation induces a large distribution shift. This interpretation is consistent with the observations of previous studies (Pan et al., 2018; Wang et al., 2021; Kim et al., 2022a) that style information mainly exists in shallower layers, whereas only content information remains in deeper layers. 8Published as a conference paper at ICLR 2023 𝐶𝑙 0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1 Channel-wise 𝛼 Channel mean 𝛼 per layer Channels in all layers1 2661 Figure 4: Optimized α. x- and y-axes indicate all channels in order from shallow to deep layers and the interpolating weight α, respectively. Cl denotes the channel size of layer l. 4 R ELATED WORK Test-time adaptation/training (TTA) aims to adapt models towards test data to overcome the per- formance degradation caused by distribution shifts (Sun et al., 2020; Wang et al., 2020). There are other related problems, unsupervised domain adaptation (UDA) (Sun & Saenko, 2016; Ganin et al., 2016) and source-free domain adaptation (SFDA) (Liang et al., 2020; Huang et al., 2021; Liu et al., 2021). Both UDA and SFDA have access to sufﬁciently large enough unlabeled target datasets, and their objective is to achieve high performance on that particular target domain. Unlike UDA and SFDA, TTA utilizes test data in an online manner. There are two key factors of recent approaches: adapting standardization statistics in normalization layers and adapting model parameters. Normalization in Test Time. Nado et al. (2020) suggested prediction-time BN, which uses test batch statistics for standardization and Schneider et al. (2020) introduced to adapt BN statistics by combining source and test batch statistics considering the the test batch size to mitigate the inter- mediate covariate shift. In this paper, we refer to the former method as TBN. Similarly, You et al. (2021) and Khurana et al. (2021) mixed the statistics using predeﬁned hyperparameter. Also, Mirza et al. (2022) and Hu et al. (2021) adapted the statistics using moving average while augmenting the input to form a pseudo test batch when only a single instance is given. The primary difference with the existing approaches is that we consider the channel-wise domain-shift sensitivity of BN layers to optimize the interpolating weights between CBN and TBN. Concurrently, Zou et al. (2022) pro- posed to adjust the standardization statistics using a learnable calibration strength and showed its effectiveness focusing on the semantic segmentation task. Optimization in Test Time.TENT (Wang et al., 2020), SWR (Choi et al., 2022), and CoTTA (Wang et al., 2022) updated model parameters while using TBN. TENT optimized afﬁne parameters in BN layers using entropy minimization while freezing the others. To maximize the adaptability, SWR updated the entire model parameters minimizing the entropy loss based on the domain-shift sensitivity. To stabilize the adaptation in continuously changing domains, CoTTA used consistency loss between student and teacher models and stochastically restored random parts of the pre-trained model. Liu et al. (2021) and Chen et al. (2022) suggested to update the model through contrastive learning. We focus on correcting the standardization statistics using domain-shift aware interpolating weight α. Similar to Choi et al. (2022), we measure the domain-shift sensitivity by comparing gradients. The principle difference is that we use channel-wise sensitivity when optimizing αin post-training, while SWR used layer-wise sensitivity regularizing the entire model update in test time. 5 C ONCLUSIONS This paper proposes TTN, a novel domain-shift aware batch normalization layer, which combines the beneﬁts of CBN and TBN that have a trade-off relationship. We present a strategy for mixing CBN and TBN based on the interpolating weight derived from the optimization procedure utilizing the sensitivity to domain shift and show that our method signiﬁcantly outperforms other normaliza- tion techniques in various realistic evaluation settings. Additionally, our method is highly practical because it can complement other optimization-based TTA methods. The oracle mixing ratio between CBN and TBN can vary depending on the domain gap difference. However, our proposed method employs a ﬁxed mixing ratio during test time, where the mixing ratio is optimized before model deployment. If we could ﬁnd the optimal mixing ratio according to the distribution shift during test time, we can expect further performance improvement. We consider it as future work. In this regard, our efforts encourage this ﬁeld to become more practical and inspire new lines of research. 9Published as a conference paper at ICLR 2023 REPRODUCIBILITY STATEMENT To ensure the reproducibility of our method, we provide the experimental setup in Section 3.1. Moreover, the details on implementation and evaluation settings can be found in the appendix A.1 and A.2, respectively. The pseudocode for overall training and testing scheme is provided in the appendix A.3. Together with related references and publicly available codes, we believe our paper contains sufﬁcient information for reimplementation. ACKNOWLEDGEMENT We would like to thank Kyuwoong Hwang, Simyung Chang, and Seunghan Yang of the Qualcomm AI Research team for their valuable discussions. REFERENCES Malik Boudiaf, Romain Mueller, Ismail Ben Ayed, and Luca Bertinetto. Parameter-free online test-time adaptation. In IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2022. John Bronskill, Jonathan Gordon, James Requeima, Sebastian Nowozin, and Richard Turner. Tas- knorm: Rethinking batch normalization for meta-learning. In International Conference on Ma- chine Learning (ICML). PMLR, 2020. Dian Chen, Dequan Wang, Trevor Darrell, and Sayna Ebrahimi. Contrastive test-time adaptation. In IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2022. Liang-Chieh Chen, Yukun Zhu, George Papandreou, Florian Schroff, and Hartwig Adam. Encoder- decoder with atrous separable convolution for semantic image segmentation. In Proceedings of the European conference on computer vision (ECCV), 2018. Sungha Choi, Sanghun Jung, Huiwon Yun, Joanne T Kim, Seungryong Kim, and Jaegul Choo. Robustnet: Improving domain generalization in urban-scene segmentation via instance selective whitening. In IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2021. Sungha Choi, Seunghan Yang, Seokeon Choi, and Sungrack Yun. Improving test-time adaptation via shift-agnostic weight regularization and nearest source prototypes. In European Conference on Computer Vision (ECCV), 2022. Marius Cordts, Mohamed Omran, Sebastian Ramos, Timo Rehfeld, Markus Enzweiler, Rodrigo Benenson, Uwe Franke, Stefan Roth, and Bernt Schiele. The cityscapes dataset for semantic urban scene understanding. In IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2016. Yaroslav Ganin, Evgeniya Ustinova, Hana Ajakan, Pascal Germain, Hugo Larochelle, Franc ¸ois Laviolette, Mario Marchand, and Victor Lempitsky. Domain-adversarial training of neural net- works. The journal of machine learning research, 2016. Taesik Gong, Jongheon Jeong, Taewon Kim, Yewon Kim, Jinwoo Shin, and Sung-Ju Lee. Note:robust continual test-time adaptation against temporal correlation. Advances in Neural In- formation Processing Systems (NeurIPS), 2022. Priya Goyal, Piotr Doll´ar, Ross Girshick, Pieter Noordhuis, Lukasz Wesolowski, Aapo Kyrola, An- drew Tulloch, Yangqing Jia, and Kaiming He. Accurate, large minibatch sgd: Training imagenet in 1 hour. arXiv preprint arXiv:1706.02677, 2017. Yves Grandvalet and Yoshua Bengio. Semi-supervised learning by entropy minimization. In Ad- vances in Neural Information Processing Systems (NeurIPS), 2004. Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image recog- nition. In IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2016. 10Published as a conference paper at ICLR 2023 Dan Hendrycks and Thomas Dietterich. Benchmarking neural network robustness to common cor- ruptions and perturbations. In International Conference on Learning Representations (ICLR), 2018. Dan Hendrycks, Norman Mu, Ekin Dogus Cubuk, Barret Zoph, Justin Gilmer, and Balaji Lakshmi- narayanan. Augmix: A simple data processing method to improve robustness and uncertainty. In International Conference on Learning Representations (ICLR), 2019. Xuefeng Hu, Gokhan Uzunbas, Sirius Chen, Rui Wang, Ashish Shah, Ram Nevatia, and Ser-Nam Lim. Mixnorm: Test-time adaptation through online normalization estimation. arXiv preprint arXiv:2110.11478, 2021. Jiaxing Huang, Dayan Guan, Aoran Xiao, and Shijian Lu. Model adaptation: Historical contrastive learning for unsupervised domain adaptation without source data. 2021. Sergey Ioffe and Christian Szegedy. Batch normalization: Accelerating deep network training by reducing internal covariate shift. InInternational Conference on Machine Learning (ICML), 2015. Ansh Khurana, Sujoy Paul, Piyush Rai, Soma Biswas, and Gaurav Aggarwal. Sita: Single image test-time adaptation. arXiv preprint arXiv:2112.02355, 2021. Byeonggeun Kim, Seunghan Yang, Jangho Kim, Hyunsin Park, Juntae Lee, and Simyung Chang. Domain Generalization with Relaxed Instance Frequency-wise Normalization for Multi-device Acoustic Scene Classiﬁcation. In Conference of the International Speech Communication Asso- ciation (INTERSPEECH), 2022a. Jin Kim, Jiyoung Lee, Jungin Park, Dongbo Min, and Kwanghoon Sohn. Pin the memory: Learn- ing to generalize semantic segmentation. In IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2022b. Diederik P Kingma and Jimmy Ba. Adam: A method for stochastic optimization. 2015. Suhyeon Lee, Hongje Seong, Seongwon Lee, and Euntai Kim. Wildnet: Learning domain general- ized semantic segmentation from the wild. In IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2022. Jian Liang, Dapeng Hu, and Jiashi Feng. Do we really need to access the source data? source hypothesis transfer for unsupervised domain adaptation. In International Conference on Machine Learning (ICML). PMLR, 2020. Yuejiang Liu, Parth Kothari, Bastien van Delft, Baptiste Bellot-Gurlet, Taylor Mordan, and Alexan- dre Alahi. Ttt++: When does self-supervised test-time training fail or thrive? Advances in Neural Information Processing Systems (NeurIPS), 2021. Mingsheng Long, Han Zhu, Jianmin Wang, and Michael I Jordan. Unsupervised domain adap- tation with residual transfer networks. In Advances in Neural Information Processing Systems (NeurIPS), 2016. Ilya Loshchilov and Frank Hutter. Sgdr: Stochastic gradient descent with warm restarts. 2017. M Jehanzeb Mirza, Jakub Micorek, Horst Possegger, and Horst Bischof. The norm must go on: Dynamic unsupervised domain adaptation by normalization. In IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2022. Zachary Nado, Shreyas Padhy, D Sculley, Alexander D’Amour, Balaji Lakshminarayanan, and Jasper Snoek. Evaluating prediction-time batch normalization for robustness under covariate shift. arXiv preprint arXiv:2006.10963, 2020. Gerhard Neuhold, Tobias Ollmann, Samuel Rota Bulo, and Peter Kontschieder. The mapillary vistas dataset for semantic understanding of street scenes. In International Conference on Computer Vision (ICCV), 2017. Xingang Pan, Ping Luo, Jianping Shi, and Xiaoou Tang. Two at once: Enhancing learning and generalization capacities via ibn-net. InEuropean Conference on Computer Vision (ECCV), 2018. 11Published as a conference paper at ICLR 2023 Joaquin Quinonero-Candela, Masashi Sugiyama, Anton Schwaighofer, and Neil D Lawrence. Dataset shift in machine learning. Mit Press, 2008. Stephan R. Richter, Vibhav Vineet, Stefan Roth, and Vladlen Koltun. Playing for data: Ground truth from computer games. In European Conference on Computer Vision (ECCV), 2016. German Ros, Laura Sellart, Joanna Materzynska, David Vazquez, and Antonio M Lopez. The synthia dataset: A large collection of synthetic images for semantic segmentation of urban scenes. In IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2016. Steffen Schneider, Evgenia Rusak, Luisa Eck, Oliver Bringmann, Wieland Brendel, and Matthias Bethge. Improving robustness against common corruptions by covariate shift adaptation. Ad- vances in Neural Information Processing Systems (NeurIPS), 2020. Saurabh Singh and Abhinav Shrivastava. Evalnorm: Estimating batch normalization statistics for evaluation. In International Conference on Computer Vision (ICCV), 2019. Cecilia Summers and Michael J Dinneen. Four things everyone should know to improve batch normalization. In International Conference on Learning Representations (ICLR), 2019. Baochen Sun and Kate Saenko. Deep coral: Correlation alignment for deep domain adaptation. In European Conference on Computer Vision (ECCV), 2016. Yu Sun, Xiaolong Wang, Zhuang Liu, John Miller, Alexei Efros, and Moritz Hardt. Test-time train- ing with self-supervision for generalization under distribution shifts. In International Conference on Machine Learning (ICML), 2020. Tuan-Hung Vu, Himalaya Jain, Maxime Bucher, Matthieu Cord, and Patrick P´erez. Advent: Adver- sarial entropy minimization for domain adaptation in semantic segmentation. InIEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2019. Dequan Wang, Evan Shelhamer, Shaoteng Liu, Bruno Olshausen, and Trevor Darrell. Tent: Fully test-time adaptation by entropy minimization. In International Conference on Learning Repre- sentations (ICLR), 2020. Qin Wang, Olga Fink, Luc Van Gool, and Dengxin Dai. Continual test-time domain adaptation. In IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2022. Yulin Wang, Zanlin Ni, Shiji Song, Le Yang, and Gao Huang. Revisiting locally supervised learning: an alternative to end-to-end training. In International Conference on Learning Representations (ICLR), 2021. Fuming You, Jingjing Li, and Zhou Zhao. Test-time batch statistics calibration for covariate shift. arXiv preprint arXiv:2110.04065, 2021. Fisher Yu, Haofeng Chen, Xin Wang, Wenqi Xian, Yingying Chen, Fangchen Liu, Vashisht Madha- van, and Trevor Darrell. Bdd100k: A diverse driving dataset for heterogeneous multitask learning. In IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2020. Yuliang Zou, Zizhao Zhang, Chun-Liang Li, Han Zhang, Tomas Pﬁster, and Jia-Bin Huang. Learn- ing instance-speciﬁc adaptation for cross-domain segmentation. European Conference on Com- puter Vision (ECCV), 2022. 12Published as a conference paper at ICLR 2023 A A PPENDIX : F OR REPRODUCIBILITY This section provides supplemental material for Section 2 and 3.1. A.1 I MPLEMENTATION DETAILS Datasets and Models. For CIFAR-10/100-C, we optimized α using augmented CIFAR-10/100 training set on the pre-trained WideResNet-40-2 (WRN-40) (Hendrycks et al., 2019). For ImageNet- C, we used augmented ImageNet training set (randomly sampled 64000 instances per epoch) on the pre-trained ResNet-50. Augmentation. Following the setting of SWR (Choi et al., 2022), we used color jittering, random invert and random grayscale when obtaining the prior A. When optimizing α, we followed the augmentation choice of CoTTA (Wang et al., 2022), which are color jittering, padding, random afﬁne, gaussian blur, center crop and random horizontal ﬂip. We excluded for gaussian noise to avoid any overlap with corruption type of common corruptions (Hendrycks & Dietterich, 2018). For ImageNet, we only used the same augmentation both for obtaining Aand optimizing αfollowing the SWR augmentation choices. Post-training. When obtaining prior, we used randomly selected 1024 samples from the training set, following the setting of SWR. We used Adam (Kingma & Ba, 2015) optimizer using a learning rate (LR) of 1e-3, which is decayed with cosine schedule (Loshchilov & Hutter, 2017) for 30 epochs and used 200 training batch for CIFAR-10/100. For ImageNet, we lowered LR to 2.5e-4 and used 64 batch size. For semantic segmentation task, we trained TTN using Cityscapes training set, and training batch size of 2. We resized the image height to 800 while preserving the original aspect ratio Cordts et al. (2016). We can terminate the training when MSE loss saturates (We observed that αdoes not show signiﬁcant difference after the MSE loss is saturated). We used the weighting hyperparmeter to MSE loss λas 1. Test time. For AdaptiveBN, which adjusts the interpolating weight using two factors: hyperpa- rameter N and test batch size n, we followed the suggested N, which is empirically obtained best hyperparameter from the original paper, for each n(Figure 11, ResNet architecture from Schneider et al. (2020)). In detail, we set N as 256, 128, 64, 32, and 16 for test batch size 200, 64, 16, 4, 2, and 1, which yields αas 0.44, 0.33, 0.2, 0.11, 0.06, and 0.06, respectively. For optimization-based TTA methods, we followed default setting in TENT, SWR, and CoTTA for test-time adaptation. We used LR of 1e-3 to test batch size of 200 for CIFAR-10/100-C in single domain (TENT, SWR) and continuously changing domain (CoTTA) scenarios. To avoid rapid error accumulation, we lowered LR to 1e-4 for TENT and SWR in continual and mixed do- main scenarios. Moreover, we updated model parameters after accumulating the gradients for 200 samples for CIFAR-10/100-C. In other words, we compute gradients per batch, but update, i.e., optimizer.step(), after seeing 200 data samples. Exceptionally, we used LR of 5e-5 for SWR and SWR+TTN in mixed domain setting. Additionally, in continuously changing and mixed domain scenarios, we used the stable version of SWR, which updates the model parameter based on the frozen source parameters instead of previously updated parameters (original SWR). For semantic segmentation, we set the test batch size as 2 and learning rate for optimization-based methods as 1e-6 for all datasets. For SWR, we set the importance of the regularization term λr as 500. The other hyperparameters are kept the same as Choi et al. (2022) choices. For all test-time adaptation, we used constant learning rate without scheduling. A.2 E VALUATION SCENARIO DETAILS Class imbalanced setting. In the main paper Table 5, we show the results under class imbalanced settings. In the setting, we sorted the test dataset of each corruption in the order of class labels, i.e., from class 0 to 9 for CIFAR-10-C. Then, we comprised test batches following the sorted order. Therefore, most batches consist of single class input data, which leads to biased test batch statistics. For larger batch size, the statistics are more likely to be extremely skewed, and that explains why error rates are higher with larger batch sizes than with the small ones. 13Published as a conference paper at ICLR 2023 A.3 P SEUDOCODE Pseudocode for post-training, i.e., obtaining Aand optimizing α, is provided in Algorithms 1 and 2, respectively, and that for test time is in Algorithm 3. Moreover, we provide PyTorch-friendly pseudocode for obtaining Ain Listing 1. Please see Section 2 for equations and terminologies used in the algorithms. Algorithm 1 Obtain prior A 1: Require: Pre-trained model fθ; source training data DS = (X,Y ) 2: Output: Prior A 3: for all (x,y) in DS do 4: Augment x: x′ 5: Collect gradients (∇γ,∇γ′ ) and (∇β,∇β′ ) from fθ using clean xand augmented x′ 6: end for 7: Compute gradient distance score dusing Eq. 4 8: Deﬁne prior Ausing Eq. 5 Algorithm 2 Post-train 1: Require: Pre-trained model fθ; source training data DS = (X,Y ); step size hyperparameter η; regularization weight hyperparameter λ 2: Output: Optimized interpolating weight α 3: Obtain prior Ausing Algorithm 1 4: Initialize αwith prior A 5: Replace all BN layers of fθ to TTN layers using αand Eq. 2 6: while not done do 7: Sample minibatches BS from DS 8: for all minibatches do 9: Augment all xin BS: x′ 10: Evaluate ∇αLusing fθ given {(x′ i,yi)}|BS| i=1 using Eq. 6 while adapting standardization statistics using Eq. 2 11: Update α←α−η∇αL 12: end for 13: end while Algorithm 3 Inference (Test time) 1: Require: Pre-trained model fθ; optimized α, target test data DT = (X); 2: Replace all BN layers of fθ to TTN layers using αand Eq. 2 3: Sample minibatches BT from DT 4: for all minibatches do 5: Make prediction using fθ given BT while adapting standardization statistics using Eq. 2 6: end for 1 def obtain_prior(model, train_data): 2 # make weight and bias of BN layers requires_grad=True, otherwise False 3 params = {n: p for n, p in model.named_parameters() if p.requires_grad} 4 5 grad_sim = {} 6 for x, y in train_data: 7 # collect gradients for clean and augmented input 8 grad_org = collect_grad(model, x, y, params) 9 grad_aug = collect_grad(model, augment(x), y, params) 10 11 # compute grad similarity 12 for n, p in params.items(): 13 grad_sim[n].data = cosine_sim(grad_org, grad_aug) 14 14Published as a conference paper at ICLR 2023 15 # average over data samples 16 for n, p in params.items(): 17 grad_sim[n].data /= len(train_data) 18 19 # min max normalization 20 max_grad = get_max_value(grad_sim) # scalar 21 min_grad = get_min_value(grad_sim) # scalar 22 grad_dist = {} 23 for n, p in grad_sim.items(): 24 grad_dist[n] = (p - min_grad) / (max_grad - min_grad) 25 26 prior = [] 27 j = 0 28 # integrate gradients of weight(gamma) and bias(beta) for each BN layer 29 for n, p in grad_dist.items(): 30 if \"weight\" in n: 31 prior.append(p) 32 elif \"bias\" in n: 33 prior[j] += p 34 prior[j] /= 2 35 prior[j] = (1-prior[j])**2 36 j += 1 37 38 return prior 39 40 def collect_grad(model, x, y, params): 41 model.zero_grad() 42 out = model(x) 43 loss = ce_loss(out, y) 44 loss.backward() 45 46 grad = {} 47 for n, p in params.items(): 48 grad[n].data = p.data 49 50 return grad Listing 1: PyTorch-friendly pseudo code for obtaining prior B A PPENDIX : E XPERIMENTAL RESULTS B.1 A BLATION STUDIES Channel-wise α. In Table 8, we compared different granularity levels of interpolating weightα, i.e., channel-wise, layer-wise, and a constant value with CIFAR-10-C and backbone WRN-40. We ob- served that channel-wise optimized αshows the best performance. We take average of the channel- wisely optimized α (the 1st row) over channels to make a layer-wise α (the 2nd row), and take average over all channels and layers to make a constant value (the 3rd row). αof the 1st and 2nd rows are visualized in the main paper Figure 4 colored in blue and red, respectively. The constant value α(the 3rd row) is 0.3988. We also optimized αlayer-wisely (the 4th row). Table 8: Ablation study on granularity level of α # Method Test batch size Avg. 200 64 16 4 2 1 1 Channel-wise (Optimized) 11.67 11.80 12.13 13.93 15.83 17.99 13.89 2 Layer-wise (Channel mean) 12.75 12.84 13.16 14.66 16.40 18.82 14.77 3 Constant (Mean) 12.07 12.21 13.05 16.72 20.04 21.26 15.89 4 Layer-wise (Optimized) 13.11 13.21 13.51 14.84 16.46 18.62 14.96 15Published as a conference paper at ICLR 2023 MSE loss strength λ. We empirically set the MSE loss strengthλin Eq. 6 as 1 through the ablation study using CIFAR-10-C and WRN-40 (Table 9). Using the MSE regularizer prevents the learnable αfrom moving too far from the prior, thus letting αfollow our intuition, i.e., putting smaller im- portance on the test batch statistics if the layer or channel is invariant to domain shifts. However, with too strong regularization (λ=10.0), the overall error rates are high, which means the αneeds to be sufﬁciently optimized. On the other hand, with too small regularization, the αmay overﬁt to the training batch size (B=200) and lose the generalizability to the smaller batch size. Table 9: MSE loss strength λ λ Test batch size Avg. 200 64 16 4 2 1 0.0 11.73 11.82 12.23 14.18 16.41 19.27 14.27 0.1 11.65 11.91 12.09 13.84 15.71 18.24 13.91 1.0 11.67 11.80 12.13 13.93 15.83 17.99 13.89 10.0 12.45 12.63 12.82 14.55 16.32 18.56 14.56 B.2 M ORE COMPARISONS ON CIFAR10-C Constant α. Table 10 shows results of simple baseline for normalization-based methods where the αis a constant value ranging from 0 to 1. α= 0 is identical to CBN (Ioffe & Szegedy, 2015), and α = 1 is identical to TBN (Nado et al., 2020). We observe that the lower α, i.e., using less test batch statistics, shows better performance for small test batch sizes. This observation is analogous to the ﬁnding of the previous work (Schneider et al., 2020). Table 10: Constant α. Error rate (↓) averaged over 15 corruptions of CIFAR-10-C (WRN-40). α Test batch size Avg. 200 64 16 4 2 1 0 18.26 18.39 18.26 18.26 18.25 18.25 18.28 0.1 13.95 14.1 14.05 14.65 15.14 15.45 14.56 0.2 12.46 12.66 12.89 14.30 15.53 15.64 13.91 0.3 12.05 12.29 12.72 15.18 17.35 17.42 14.50 0.4 12.13 12.41 13.12 16.69 19.81 20.51 15.78 0.5 12.42 12.78 13.73 18.32 22.52 24.88 17.44 0.6 12.88 13.32 14.48 20.02 25.17 31.97 19.64 0.7 13.37 13.9 15.23 21.75 27.91 46.65 23.14 0.8 13.82 14.37 15.94 23.44 30.59 77.15 29.22 0.9 14.18 14.8 16.58 24.94 33.12 89.81 32.24 1 14.50 15.15 17.10 26.29 35.67 90.00 33.12 B.3 V ARIANTS OF TTN FOR SMALL TEST BATCH SIZE Online TTN. TTN interpolating weight αcan also be adapted during test time. Table 11 shows the results of the TTN online version, which further optimizes the post-trained alpha using the entropy minimization and mean-squared error (MSE) loss between the updated alpha and the initial post- trained alpha. We followed entropy minimization loss used in TENT (Wang et al., 2020), and the MSE loss can be written as LMSE = ∥α−α0∥2, where α0 is the post-trained α. We set the learning rate as 1e-2, 2.5e-3, 5e-4, 1e-4, 5e-5, and 2.5e-5 by linearly decreasing according to the test batch size of 200, 64, 16, 4, 2, and 1 (Goyal et al., 2017). The online TTN shows improvements compared to the ofﬂine TTN in all three evaluation scenarios (single, continuously changing, and mixed). Scaled TTN. Following the observation from Table 10, we lowered the interpolating weight by mul- tiplying a constant scale value, ranging (0,1), to the optimized TTN α. In Table 11, we empirically set the scale value as 0.4. Dynamic training batch size. We observe that using the dynamic batch size in the post-training stage also improves the performance for small test batch sizes (2 or 1), while slightly deteriorating the performance for large test batch sizes (200 or 64). We randomly sampled training batch size from the range of [4,200] for each iteration. Other hyperparameters are kept as the same. 16Published as a conference paper at ICLR 2023 Table 11: TTN variants. Error rate (↓) averaged over 15 corruptions of CIFAR-10-C (WRN-40). Test batch size Avg. Method Eval. setting 200 64 16 4 2 1 TTN (ofﬂine, default) Single & Cont.11.67 11.80 12.13 13.93 15.83 17.99 13.89 TTN (ofﬂine, default) Mixed 12.16 12.19 12.34 13.96 15.55 17.83 14.00 TTN (online) Single 11.39 11.64 11.97 13.70 15.41 17.49 13.60 TTN (online) Cont. 11.67 11.96 12.15 13.90 15.67 17.72 13.85 TTN (online) Mixed 12.04 12.04 12.06 13.90 15.46 17.62 13.85 TTN (scaled) Single & Cont.13.20 13.38 13.35 13.88 14.54 15.17 13.92 TTN (scaled) Mixed 13.17 13.05 13.17 13.74 14.36 15.09 13.76 TTN (dynamic train batch size)Single & Cont.11.82 12.04 12.17 13.60 15.13 17.22 13.66 TTN (dynamic train batch size)Mixed 12.12 12.01 11.91 13.43 14.76 17.23 13.58 B.4 S TUDY ON AUGMENTATION TYPE TTN is robust to the data augmentation. We used data augmentation in the post-training phase to simulate domain shifts. The rationale for the simulation is to expose the model to different input domains. Especially when obtaining the prior, we compare the outputs from shifted domains with the clean (original) domain in order to analyze which part of the model is affected by the domain discrepancy. Therefore, changing the domain itself is what matters, not which domain the input is shifted to. We demonstrated this by conducting ablation studies by varying the augmentation type. We analyzed various augmentation types when obtaining prior and when optimizing alpha and the results are shown in Figure 5 (a) and (b), respectively. We use a true corruption, i.e., one of 15 corruption types in the corruption benchmark, as an augmentation type in the post-training phase to analyze how TTN works if the augmentation type and test corruption type are misaligned or perfectly aligned. Speciﬁcally, we used the true corruption when obtaining the prior while keeping the augmentation choices when optimizing alpha as described in the appendix A.1, and vice versa. Expectedly, the diagonal elements, i.e., where the same corruption type is used both for in post- training and in test time, tend to show the lowest error rate in Figure 5(b). The row-wise standard deviation is lower than 1 in most cases and even lower than 0.5 in Figure 5(a), which means the prior is invariant to the augmentation choice. Moreover, we observe that the average error rate over all elements, 11.74% in ablation for obtaining prior and 11.67% in ablation for optimizing alpha, is almost as same as TTN result 11.67% (See Table 14 and Figure 5). Moreover, we conducted an ablation study on the choice of the augmentation type using CIFAR-10-C and WRN-40 (Table12). We observe that obtaining prior and optimizing alpha steps are robust to the augmentation types. Table 12: Ablation study on augmentation choice. From left to right one augmentation type is added at a time. Default setting, which we used in all experiments, is colored in gray. Prioraugmentation type color jitter + grayscale + invert + gaussian blur + horizontal ﬂip 12.03 11.83 11.67 11.59 11.58 Optimizingαaugmentation type color jitter + padding + afﬁne + gaussian blur + horizontal ﬂip 11.78 11.78 11.7 11.70 11.67 B.5 R ESULTS ON IMAGE NET-C Table 13 shows the experimental results using ResNet-50 (He et al., 2016) on ImageNet- C (Hendrycks & Dietterich, 2018) dataset. We emphasized the effectiveness of our proposed method by showing the signiﬁcant improvement in large-scale dataset experiment. Similar to the results in CIFAR-10/100-C, TTN showed the best performance compared to normalization-based methods (TBN (Nado et al., 2020), AdaptiveBN Schneider et al. (2020), and α-BN (You et al., 2021)) and improved TTA performance when it is applied to optimization-based methods (TENT (Wang et al., 2020) and SWR (Choi et al., 2022)). During post-training, we used LR of 2.5e-4 and batch size of 64. In test time, we used the learning rate of 2.5e-4 for TENT following the setting from the original paper, and used 5e-6 for TENT+TTN. For SWR and SWR+TTN, we used learning rate of 2.5e-4 for B=64, and 2.5e-6 for B=16, 4, 2, and 1. We had to carefully tune the learning rate especially for SWR, since the method updates the 17Published as a conference paper at ICLR 2023 entire model parameters in an unsupervised manner and hence is very sensitive to the learning rate when the test batch size becomes small. Other hyperparameters and details are kept same (See the appendix A.1 for more details). Moreover, to avoid rapid accumulation, we stored gradients for sufﬁciently large test samples and then updated the model parameters (for example, we conducted adaptation in every 64 iterations in the case of batch size of 1) in both TENT and SWR. Table 13: Single domain adaptation on ImageNet-C (ResNet-50). Error rate (↓) averaged over 15 corruption types with severity level 5 is reported for each test batch size. Method Test batch size Avg. Error 64 16 4 2 1 Source (CBN)93.34 93.34 93.34 93.34 93.34 93.34 Norm TBN 74.24 76.81 85.74 95.35 99.86 86.40 AdaptiveBN 77.86 81.47 86.71 90.15 91.11 85.46 α-BN 86.06 86.32 87.16 88.33 90.45 87.66 Ours (TTN) 72.21 73.18 76.98 81.52 88.49 78.48 Optim. TENT 66.56 72.61 93.37 99.46 99.90 86.41 +Ours (TTN)71.42 72.45 76.66 81.89 91.00 78.68 SWR 64.41 74.19 84.30 93.05 99.86 83.16 +Ours (TTN)55.68 69.25 78.48 86.37 94.08 76.77 B.6 E RROR RATES OF EACH CORRUPTION In Table 14, we show the error rates of TTN for each corruption of CIFAR-10-C using the WRN-40 backbone. The averaged results over the corruptions are in Table 1. Table 14: Results of each corruption (CIFAR-10-C) batch sizegauss. shot impulse defocus glass motion zoom snow frost fog brightness contrast elastic pixel. jpeg.Avg. 200 14.81 12.78 17.32 7.37 17.87 8.51 7.23 10.29 9.88 11.29 6.06 8.36 13.42 14.89 14.94 11.6764 14.81 12.81 17.42 7.41 18.21 8.66 7.41 10.44 9.93 11.63 6.11 8.35 13.59 15.18 15.02 11.8016 15.23 13.00 17.98 7.71 18.46 8.95 7.68 10.85 10.17 12.21 6.25 8.54 13.95 15.67 15.3412.134 17.03 15.29 19.75 9.26 20.93 10.01 9.62 12.58 11.85 13.65 7.69 9.25 16.21 18.08 17.7013.932 19.21 16.80 21.86 10.70 23.74 11.39 11.92 14.00 13.39 15.38 8.95 10.13 18.92 20.68 20.4015.831 22.03 19.97 25.24 12.41 26.99 12.22 14.39 14.73 14.60 17.27 10.16 9.51 22.10 24.12 24.0917.99 B.7 S EGMENTATION RESULTS For more comparisons, we add segmentation results for TBN and TTN using test batch size (B) of 4 and 8 (Table 15). The results demonstrate that TTN is robust to the test batch sizes. In other words, the performance difference across the test batch size is small when using TTN (TTN with B=2, 4, and 8). The results are averaged over 3 runs (i.e., using 3 independently optimized α), and the standard deviation is denoted with a ±sign. We omitted the standard deviation for TBN, which is 0.0 for every result since no optimization is required. Since the backbone network is trained with a train batch size of 8, we assume that test batch statistics estimated from the test batch with B=8 are sufﬁciently reliable. Accordingly, TBN with B=8 shows compatible results. However, when B becomes small (i.e., in a more practical scenario), problematic test batch statistics are estimated, and thus TBN suffers from the performance drop while TTN keeps showing robust performance. It is worth noting that TTN outperforms TBN by 3.77% in average accuracy when B=2, i.e., in the most practical evaluation setting, and by 2.54% and 1.99% for B=4 and 8, respectively. Table 15: Adaptation on DG benchmarks in semantic segmentation. mIoU(↑) on four unseen domains with test batch size of 2, 4, and 8 using ResNet-50 based DeepLabV3+ as a backbone. Method(Cityscapes→) BDD-100K Mapillary GTA V SYNTHIA Cityscapes Norm TBN (B=2) 43.12 47.61 42.51 25.71 72.94 Ours (TTN)(B=2) 47.40(±0.02) 56.88(±0.04) 44.69(±0.03) 26.68(±0.01) 75.09(±0.01) TBN (B=4) 45.64 49.17 44.26 25.96 74.29 Ours (TTN)(B=4) 47.72(±0.01) 57.11(±0.01) 45.08(±0.02) 26.52(±0.01) 75.56(±0.01) TBN (B=8) 46.42 49.38 44.81 25.97 75.42 Ours (TTN)(B=8) 47.25(±0.02) 57.28(±0.02) 45.13(±0.03) 26.45(±0.01) 75.82(±0.01) 18Published as a conference paper at ICLR 2023 (b)Augmentation type used for optimizing alpha Test corruption type Error rate. Colored by normalized value (across test corruption type)avg.std.14.550.8612.640.6116.770.927.660.3117.750.738.760.297.490.2510.330.279.710.3212.401.466.110.098.260.4913.350.3714.581.2714.730.3911.67 (a)Augmentation type used for obtaining prior Test corruption type Error rate. Colored by normalized value (across test corruption type) avg.std.15.030.3412.790.2217.240.167.410.0617.910.148.480.087.260.0310.440.149.980.1411.550.106.050.048.570.4013.340.0715.211.5914.830.1111.74 Figure 5: True test corruption as augmentation. Each column represents the augmentation type used (a) when obtaining prior or (b) when optimizing α. Each row represents the test corruption type. The error rate (↓) of CIFAR-10-C with severity level 5 and test batch size 200 using WRN-40 is annotated in each element. Element (i,j) represents the error rate when j-th corruption type is used for augmenting the clean train data during post-training and tested on i-th corruption test set. The heatmap is colored by error rate normalized across the test corruption type (row-wise normalization). 19",
      "meta_data": {
        "arxiv_id": "2302.05155v2",
        "authors": [
          "Hyesu Lim",
          "Byeonggeun Kim",
          "Jaegul Choo",
          "Sungha Choi"
        ],
        "published_date": "2023-02-10T10:25:29Z",
        "pdf_url": "https://arxiv.org/pdf/2302.05155v2.pdf"
      },
      "llm_extracted_info": {
        "main_contributions": "The paper addresses the performance degradation of deep neural networks in test-time adaptation (TTA) due to domain shifts, particularly highlighting the limitations of Transductive Batch Normalization (TBN) under impractical assumptions like large and i.i.d test batches. It proposes Test-Time Normalization (TTN), a novel domain-shift aware batch normalization strategy that interpolates between Conventional Batch Normalization (CBN) and TBN. TTN uses channel-wise interpolating weights, optimized in a post-training phase, based on the domain-shift sensitivity of each BN layer. This approach improves model robustness across various batch sizes and realistic evaluation scenarios (stationary, continuously changing, and mixed domain adaptation), while also preserving source knowledge and improving the performance of other TTA methods.",
        "methodology": "The proposed TTN layer standardizes features by combining source statistics (mean \"µs\" and variance \"σ²s\") and current test mini-batch statistics (mean \"µ\" and variance \"σ²\") using a learnable channel-wise interpolating weight \"α\". Specifically, the combined statistics are \": \":\n\"˜µ = αµ + (1−α)µs\" and \"˜σ² = ασ² + (1−α)σ²s + α(1 −α)(µ−µs)². The method introduces a post-training phase (after pre-training, before testing) to optimize these \"α\" parameters while freezing all other model weights. This phase consists of two stages: 1) Obtaining a Prior A: By simulating domain shifts through data augmentation and comparing gradients of affine parameters (γ, β) between clean and augmented inputs, the method identifies BN layers/channels sensitive to domain shifts. A gradient distance score is computed and squared to form the prior \"A\". 2) Optimizing \"α\": \"α\" is initialized with \"A\" and then optimized using a combined loss function: cross-entropy loss (LCE) for consistent predictions on augmented data, and a mean-squared error loss (LMSE = ∥α−A∥²) to regularize \"α\" towards the prior.",
        "experimental_setup": "The method was evaluated on:Image Classification: CIFAR-10/100-C and ImageNet-C corruption benchmark datasets, using WideResNet-40-2 (for CIFAR) and ResNet-50 (for ImageNet) as backbone models.Semantic Segmentation: Cityscapes, BDD-100K, Mapillary, GTA V, and SYNTHIA domain generalization benchmarks, using a ResNet-50-based DeepLabV3+ model.Evaluation Scenarios: Single domain adaptation, continuously changing domain adaptation, and mixed domain adaptation, all with varying test batch sizes (from 200 down to 1). Additional evaluations included source domain adaptation and class-imbalanced target domains.Baselines for comparison included normalization-based methods (TBN, AdaptiveBN, α-BN, MixNorm) and optimization-based TTA methods (TENT, SWR, CoTTA), with comparisons also made against the source model (using CBN). Post-training details involved using Adam optimizer, cosine learning rate schedule, and specific data augmentation types for prior calculation and alpha optimization.",
        "limitations": "The proposed method employs a fixed mixing ratio (interpolating weight α) during test time. This ratio is optimized before model deployment and does not dynamically adjust based on the varying domain gaps that might be encountered during actual test-time inference.",
        "future_research_directions": "Future work could focus on dynamically determining the optimal mixing ratio for CBN and TBN according to the specific distribution shifts encountered during test time, rather than using a fixed, pre-optimized ratio. This would allow for even greater performance improvements and adaptability."
      }
    },
    {
      "title": "Active Test-Time Adaptation: Theoretical Analyses and An Algorithm",
      "abstract": "Test-time adaptation (TTA) addresses distribution shifts for streaming test\ndata in unsupervised settings. Currently, most TTA methods can only deal with\nminor shifts and rely heavily on heuristic and empirical studies.\n  To advance TTA under domain shifts, we propose the novel problem setting of\nactive test-time adaptation (ATTA) that integrates active learning within the\nfully TTA setting.\n  We provide a learning theory analysis, demonstrating that incorporating\nlimited labeled test instances enhances overall performances across test\ndomains with a theoretical guarantee. We also present a sample entropy\nbalancing for implementing ATTA while avoiding catastrophic forgetting (CF). We\nintroduce a simple yet effective ATTA algorithm, known as SimATTA, using\nreal-time sample selection techniques. Extensive experimental results confirm\nconsistency with our theoretical analyses and show that the proposed ATTA\nmethod yields substantial performance improvements over TTA methods while\nmaintaining efficiency and shares similar effectiveness to the more demanding\nactive domain adaptation (ADA) methods. Our code is available at\nhttps://github.com/divelab/ATTA",
      "full_text": "Published as a conference paper at ICLR 2024 ACTIVE TEST-TIME ADAPTATION : T HEORETICAL ANALYSES AND AN ALGORITHM Shurui Gui∗ Texas A&M University College Station, TX 77843 shurui.gui@tamu.edu Xiner Li* Texas A&M University College Station, TX 77843 lxe@tamu.edu Shuiwang Ji Texas A&M University College Station, TX 77843 sji@tamu.edu ABSTRACT Test-time adaptation (TTA) addresses distribution shifts for streaming test data in unsupervised settings. Currently, most TTA methods can only deal with minor shifts and rely heavily on heuristic and empirical studies. To advance TTA under domain shifts, we propose the novel problem setting of active test-time adaptation (ATTA) that integrates active learning within the fully TTA setting. We provide a learning theory analysis, demonstrating that incorporating limited labeled test instances enhances overall performances across test domains with a theoretical guarantee. We also present a sample entropy balancing for implementing ATTA while avoiding catastrophic forgetting (CF). We introduce a simple yet effective ATTA algorithm, known as SimATTA, using real-time sample selection techniques. Extensive experimental results confirm consistency with our theoretical analyses and show that the proposed ATTA method yields substantial performance improvements over TTA methods while maintaining efficiency and shares similar effectiveness to the more demanding active domain adaptation (ADA) methods. Our code is available at https://github.com/divelab/ATTA. 1 I NTRODUCTION Deep learning has achieved remarkable success across various fields, attaining high accuracy in numerous applications (Krizhevsky et al., 2017; Simonyan and Zisserman, 2014). Nonetheless, When training and test data follow distinct distributions, models often experience significant performance degradation during test. This phenomenon, known as the distribution shift or out-of-distribution (OOD) problem, is extensively studied within the context of both domain generalization (DG) (Gulra- jani and Lopez-Paz, 2020; Koh et al., 2021; Gui et al., 2022) and domain adaptation (DA) (Ganin et al., 2016; Sun and Saenko, 2016). While these studies involve intensive training of models with considerable generalization abilities towards target domains, they overlook an important application property; namely, continuous adaptivity to real-time streaming data under privacy, resource, and efficiency constraints. This gap leads to the emergence of test-time adaptation (TTA) tasks, targeting on-the-fly adaptation to continuous new domains during the test phase or application deployment. The study of TTA encompasses two main categories; namely test-time training (TTT) methods (Sun et al., 2020; Liu et al., 2021c) and fully test-time adaptation (FTTA) (Niu et al., 2023; Wang et al., 2021). The TTT pipeline incorporates retraining on the source data, whereas FTTA methods adapt arbitrary pre-trained models to the given test mini-batch by conducting entropy minimization, without access to the source data. Nevertheless, most TTA methods can only handle corrupted distribution shifts (Hendrycks and Dietterich, 2019b) (e.g., Gaussian noise,) and rely heavily on human intuition or empirical studies. To bridge this gap, our paper focuses on tackling significant domain distribution shifts in real time with theoretical insights. We investigate FTTA, which is more general and adaptable than TTT, particularly under data ac- cessibility, privacy, and efficiency constraints. Traditional FTTA aims at adapting a pre-trained model to streaming test-time data from diverse domains under unsupervised settings. However, recent works (Lin et al., 2022; Pearl, 2009) prove that it is theoretically infeasible to achieve OOD generalization without extra information such as environment partitions. Since utilizing environment partitions requires heavy pretraining, contradicting the nature of TTA, we are motivated to incorporate extra information in a different way,i.e., integrating a limited number of labeled test-time samples to alleviate distribution shifts, following the active learning (AL) paradigm (Settles, 2009). To this end, we propose the novel problem setting of active test-time adaptation (ATTA) by incorporating ∗Equal contributions 1 arXiv:2404.05094v1  [cs.LG]  7 Apr 2024Published as a conference paper at ICLR 2024 AL within FTTA. ATTA faces two major challenges; namely, catastrophic forgetting (CF) (Kemker et al., 2018; Li and Hoiem, 2017) and real-time active sample selection. CF problem arises when a model continually trained on a sequence of domains experiences a significant performance drop on previously learned domains, due to the inaccessibility of the source data and previous test data. Real-time active sample selection requires AL algorithms to select informative samples from a small buffer of streaming test data for annotation, without a complete view of the test distribution. In this paper, we first formally define the ATTA setting. We then provide its foundational analysis under the learning theory’s paradigm to guarantee the mitigation of distribution shifts and avoid CF. Aligned with our empirical validations, while the widely used entropy minimization (Wang et al., 2021; Grandvalet and Bengio, 2004) can cause CF, it can conversely become the key to preventing CF problems with our sample selection and balancing techniques. Building on the analyses, we then introduce a simple yet effective ATTA algorithm, SimATTA, incorporating balanced sample selections and incremental clustering. Finally, we conducted a comprehensive experimental study to evaluate the proposed ATTA settings with three different settings in the order of low to high requirement restrictiveness, i.e., TTA, Enhanced TTA, and Active Domain Adaptation (ADA). Intensive experiments indicate that ATTA jointly equips with the efficiency of TTA and the effectiveness of ADA, rendering an uncompromising real-time distribution adaptation direction. Comparison to related studies. Compared to TTA methods, ATTA requires extra active labels, but the failure of TTA methods (Sec. 5.1) and the theoretical proof of Lin et al. (2022); Pearl (2009) justify its necessity and rationality. Compared to active online learning, ATTA focuses on lightweight real-time fine-tuning without round-wise re-trainings as Saran et al. (2023) and emphasizes the importance of CF avoidance instead of resetting models and losing learned distributions. In fact, active online learning is partially similar to our enhanced TTA setting (Sec. 5.2. Compared to ADA methods (Prabhu et al., 2021; Ning et al., 2021), ATTA does not presuppose access to source data, model parameters, or pre-collected target samples. Furthermore, without this information, ATTA can still perform on par with ADA methods (Sec. 5.3). The recent source-free active domain adaptation (SFADA) method SALAD (Kothandaraman et al., 2023) still requires access to model parameter gradients, pre-collected target data, and training of additional networks. Our ATTA, in contrast, with non-regrettable active sample selection on streaming data, is a much lighter and more realistic approach distinct from ADA and SFADA. More related-work discussions are provided in Appx. C. 2 T HE ACTIVE TEST-TIME ADAPTATION FORMULATION TTA methods aim to solve distribution shifts by dynamically optimizing a pre-trained model based on streaming test data. We introduce the novel problem setting of Active Test-Time Adaptation (ATTA), which incorporates active learning during the test phase. In ATTA, the model continuously selects the most informative instances from the test batch to be labeled by an explicit or implicit oracle (e.g., human annotations, self-supervised signals) and subsequently learned by the model, aiming to improve future adaptations. Considering the labeling costs in real-world applications, a “budget” is established for labeled test instances. The model must effectively manage this budget distribution and ensure that the total number of label requests throughout the test phase does not surpass the budget. We now present a formal definition of the ATTA problem. Consider a pre-trained modelf(x; ϕ) with parameters ϕ trained on the source dataset DS = (x, y)|DS|, with each data sample x ∈ Xand a label y ∈ Y. We aim to adapt model parameters θ, initialized as ϕ, to an unlabeled test-time data stream. The streaming test data exhibit distribution shifts from the source data and varies continuously with time, forming multiple domains to which we must continuously adapt. The test phase commences at time step t = 1 and the streaming test data is formulated in batches. The samples are then actively selected, labeled (by the oracle) and collected as Dte(t) = ActAlg(Ute(t)), where ActAlg(·) denotes an active selection/labeling algorithm. The labeled samples Dte(t) are subsequently incorporated into the ATTA training setDtr(t). Finally, we conclude time step t by performing ATTA training, updating model parameters θ(t) using Dtr(t), with θ(t) initialized as the previous final state θ(t − 1). Definition 1 (The ATTA problem). Given a model f(x; θ), with parameters θ, initialized with parameters θ(0) = ϕ obtained by pre-training on source domain data, and streaming test data batches Ute(t) continually changing over time, the ATTA task aims to optimize the model at any time stept (with test phase commencing at t = 1) as θ(t)∗ := argmin θ(t) (E(x,y,t)∈Dtr(t)[ℓCE (f(x; θ(t)), y)] + E(x,t)∈Ute(t)[ℓU (f(x; θ(t)))]), (1) 2Published as a conference paper at ICLR 2024 where Dtr(t) = ( ∅, t = 0 Dtr(t − 1) ∪ Dte(t), t ≥ 1, s.t. |Dtr(t)| ≤ B, (2) Dte(t) = ActAlg(Ute(t)) is actively selected and labeled, ℓCE is the cross entropy loss, ℓU is an unsupervised learning loss, and B is the budget. 3 T HEORETICAL STUDIES In this section, we conduct an in-depth theoretical analysis of TTA based on learning theories. We mainly explore two questions: How can significant distribution shifts be effectively addressed under the TTA setting? How can we simultaneously combat the issue of CF? Sec. 3.1 provides a solution with theoretical guarantees to the first question, namely, active TTA (ATTA), along with the conditions under which distribution shifts can be well addressed. Sec. 3.2 answers the second question with an underexplored technique, i.e., selective entropy minimization, building upon the learning bounds established in Sec. 3.1. We further validate these theoretical findings through experimental analysis. Collectively, we present a theoretically supported ATTA solution that effectively tackles both distribution shift and CF. 3.1 A LLEVIATING DISTRIBUTION SHIFTS THROUGH ACTIVE TEST-TIME ADAPTATION Traditional TTA is performed in unsupervised or self-supervised context. In contrast, ATTA introduces supervision into the adaptation setting. In this subsection, we delve into learning bounds and establish generalization bounds to gauge the efficacy of ATTA in solving distribution shifts. We scrutinize the influence of active learning and evidence that the inclusion of labeled test instances markedly enhances overall performances across incremental test domains. Following Kifer et al. (2004), we examine statistical guarantees for binary classification. A hypothesis is a function h : X → {0, 1}, which can serve as the prediction function within this context. In the ATTA setting, the mapping ofh varies with time as h(x, t). We use H∆H-distance following Ben- David et al. (2010), which essentially provides a measure to quantify the distribution shift between two distributions D1 and D2, and can also be applied between datasets. The probability that an estimated hypothesis h disagrees with the true labeling function g : X → {0, 1} according to distribution D is defined as ϵ(h(t), g) = E(x)∼D[|h(x, t) − g(x)|], which we also refer to as the error or risk ϵ(h(t)). While the source data is inaccessible under ATTA settings, we consider the existence of source dataset DS for accurate theoretical analysis. Thus, we initialize Dtr as Dtr(0) = DS. For every time step t, the test and training data can be expressed asUte(t) and Dtr(t) = DS ∪Dte(1) ∪Dte(2) ∪···∪ Dte(t). Building upon two lemmas (provided in Appx. D), we establish bounds on domain errors under the ATTA setting when minimizing the empirical weighted error using the hypothesish at time t. Theorem 1. Let H be a hypothesis class of VC-dimension d. At time step t, for ATTA data domains DS, Ute(1), ··· , Ute(t), ··· , Si are unlabeled samples of sizem sampled from each of thet+1 domains respectively. The total number of samples in Dtr(t) is N and the ratio of sample numbers in each component is λ = (λ0, ··· , λt). If ˆh(t) ∈ Hminimizes the empirical weighted error ˆϵw(h(t)) with the weight vector w = (w0, ··· , wt) on Dtr(t), and h∗ j (t) = arg minh∈H ϵj(h(t)) is the optimal hypothesis on the jth domain, then for any δ ∈ (0, 1), with probability of at least 1 − δ, we have ϵj(ˆh(t)) ≤ ϵj(h∗ j (t)) + 2 tX i=0,i̸=j wi  1 2 ˆdH∆H(Si, Sj) + 2 s 2d log(2m) + log 2 δ m + γi   + 2C, where C = r\u0010Pt i=0 w2 i λi \u0011\u0010 d log(2N)−log(δ) 2N \u0011 and γi = minh∈H{ϵi(h(t)) + ϵj(h(t))}. For future test domains j = t + k (k >0), assuming k′ = argmink′∈{0,1,...t} dH∆H(D(k′), Ute(t + k)) and min dH∆H (D(k′), Ute(t + k)) ≤ δD, where 0 ≤ δD ≪ +∞, then ∀δ, with probability of at least 1 − δ, we have ϵt+k(ˆh(t)) ≤ ϵt+k(h∗ t+k(t)) + tX i=0 wi  ˆdH∆H(Si, Sk′ ) + 4 s 2d log(2m) + log 2 δ m + δD + 2γi   + 2C. The adaptation performance on a test domain is majorly bounded by the composition of (labeled) training data, estimated distribution shift, and ideal joint hypothesis performance, which correspond to C, ˆdH∆H(Si, Sj), and γi, respectively. The ideal joint hypothesis error γi gauges the inherent adaptability between domains. Further theoretical analysis are in Appx. D. 3Published as a conference paper at ICLR 2024 Figure 1: (a) Empirical validation of Thm. 1. We train a series of models on N = 2000 samples from the PACS (Li et al., 2017) dataset given differentλ0 and w0 and display the test domain loss of each model. Red points are the test loss minimums given a fixed λ0. The orange line is the reference where w0 = λ0. We observe that w0 with loss minimums are located closed to the orange line but slightly smaller than λ0, which validates our findings in Eq. (4). (b) Empirical analysis with an uncertainty balancing. Given source pre-trained models, we fine-tune the models on 500 samples with different λ0 and w0, and display the combined error surface of test and source error. Although a small λ0 is good for test domain error, it can lead to non-trivial source error exacerbation. Therefore, we can observe that the global loss minimum (green X) locates in a relatively high-λ0 region. If we consider the multiple test data distributions as a single test domain,i.e., St i=1 Ute(i), Thm. 1 can be reduced into bounds for the source domain error ϵS and test domain error ϵT . Given the optimal test/source hypothesis h∗ T (t) = arg minh∈H ϵT (h(t)) and h∗ S(t) = arg minh∈H ϵS(h(t)), we have |ϵT (ˆh(t)) − ϵT (h∗ T (t))| ≤w0A + s w2 0 λ0 + (1 − w0)2 1 − λ0 B, (3a) |ϵS(ˆh(t)) − ϵS(h∗ S(t))| ≤(1 − w0)A + s w2 0 λ0 + (1 − w0)2 1 − λ0 B, (3b) where the distribution divergence termA = ˆdH∆H(S0, ST )+4 q 2d log(2m)+log 2 δ m +2γ, the empirical gap term B = 2 q d log(2N)−log(δ) 2N , ST is sampled from St i=1 Ute(i), and γ = minh∈H{ϵ0(h(t)) + ϵT (h(t))}. Our learning bounds demonstrates the trade-off between the small amount of budgeted test-time data and the large amount of less relevant source data. Next, we provide an approximation of the condition necessary to achieve optimal adaptation performance, which is calculable from finite samples and can be readily applied in practical ATTA scenarios. Following Eq. (3.a), with approximatelyB = c1 p d/N, the optimal value w∗ 0 to tighten the test error bound is a function of λ0 and A: w∗ 0 = λ0 − s A2N c2 1d − A2Nλ0(1 − λ0), for λ 0 ≥ 1 − d A2N , (4) where c1 is a constant. Note that λ0 ≥ 1 − d A2N should be the satisfied condition in practical ATTA settings, where the budget is not sufficiently big while the source data amount is relatively large. The following theorem offers a direct theoretical guarantee that ATTA reduces the error bound on test domains in comparison to TTA without the integration of active learning. Theorem 2. Let H be a hypothesis class of VC-dimension d. For ATTA data domains DS, Ute(1), Ute(2), ··· , Ute(t), considering the test-time data as a single test domain St i=1 Ute(i), if ˆh(t) ∈ H minimizes the empirical weighted error ˆϵw(h(t)) with the weight vector w on Dtr(t), let the test error be upper-bounded with |ϵT (ˆh(t)) − ϵT (h∗ T (t))| ≤EBT (w, λ, N, t). Let w′ and λ′ be the weight and sample ratio vectors when no active learning is included, i.e., w′ and λ′ s.t. w′ 0 = λ′ 0 = 1 and w′ i = λ′ i = 0 for i ≥ 1, then for any λ ̸= λ′, there exists w s.t. EBT (w, λ, N, t) < EBT (w′, λ′, N, t). (5) Therefore, the incorporation of labeled test instances in ATTA theoretically enhances the overall performance across test domains, substantiating the significance of the ATTA setting in addressing distribution shifts. All proofs are provided in Appx. E. Finally, we support the theoretical findings with experimental analysis and show the numerical results of applying the principles on real-world datasets, as shown in Fig. 1. For rigorous analysis, note that our theoretical results rest on the underlying condition that N should at least be of the same scale as d, according to the principles of VC-dimension theory. The empirical alignment of our experiments with the theoretical framework can be attributed to the assumption that fine-tuning a model is roughly equivalent to learning a model with a relatively small d. Experiment details and other validations can be found in Appx. H. 4Published as a conference paper at ICLR 2024 3.2 M ITIGATING CATASTROPHIC FORGETTING WITH BALANCED ENTROPY MINIMIZATION Catastrophic forgetting (CF), within the realm of Test-Time Adaptation (TTA), principally manifests as significant declines in overall performance, most notably in the source domain. Despite the lack of well-developed learning theories for analyzing training with series data, empirical studies have convincingly illustrated the crucial role of data sequential arrangement in model learning, thereby accounting for the phenomenon of CF. Traditionally, the mitigation of CF in adaptation tasks involves intricate utilization of source domain data. However, under FTTA settings, access to the source dataset is unavailable, leaving the problem of CF largely unexplored in the data-centric view. Table 1: Correlation analysis of high/low en- tropy samples and domains. We use a source pre-trained model to select samples with low- est/highest entropy, and 1.retrain the model on 2000 samples; 2.fine-tune the model on 300 sam- ples. We report losses on source/test domains for each setting, showing that low-entropy samples form distributions close to the source domain. Sample type Retrain Fine-tune ϵS ϵT ϵS ϵT Low entropy 0.5641 0.8022 0.0619 1.8838 High entropy 2.5117 0.3414 0.8539 0.7725 To overcome this challenge of source dataset ab- sence, we explore the acquisition of “source-like” data. In TTA scenarios, it is generally assumed that the amount of source data is considerably large. We also maintain this assumption in ATTA, practically assuming the volume of source data greatly surpasses the test-time budget. As a re- sult, we can safely assume that the pre-trained model is well-trained on abundant source do- main data DS. Given this adequately trained source model, we can treat it as a “true” source data labeling function f(x; ϕ). The model es- sentially describes a distribution, Dϕ,S(X, Y) = {(x, ˆy) ∈ (X, Y) | ˆy = f(x; ϕ), x∈ DS}. The entropy of the model prediction is defined as H(ˆy) = −P c p(ˆyc) logp(ˆyc), ˆy = f(x; ϕ), where c denotes the class. Lower entropy indicates that the model assigns high probability to one of the classes, suggesting a high level of certainty or confidence in its prediction, which can be interpreted as the sample being well-aligned or fitting closely with the model’s learned distribution. In other words, the model recognizes the sample as being similar to those it was trained on. Thus entropy can be used as an indicator of how closely a sample x aligns with the model distribution Dϕ,S. Since the model distribution is approximately the source distribution, selecting (and labeling) low-entropy samples using f(x; ϕ) essentially provides an estimate of sampling from the source dataset. Therefore, in place of the inaccessible DS, we can feasibly include the source-like dataset into the ATTA training data at each time stept: Dϕ,S(t) = {(x, f(x; ϕ))|x ∈ Ute(t), H(f(x; ϕ)) < el}, (6) where el is the entropy threshold. The assumption that Dϕ,S(t) is an approximation of DS can be empirically validated, as shown by the numerical results on PACS in Tab. 1. In contrast, high-entropy test samples typically deviate more from the source data, from which we select Dte(t) for active labeling. Following the notations in Thm. 1, we are practically minimizing the empirical weighted error of hypothesis h(t) as ˆϵ′ w(h(t)) = tX j=0 wjˆϵj(h(t)) = w0 λ0N X x∈Dϕ,S(t) |h(x, t) − f(x; ϕ)| + tX j=1 wj λjN X x,y∈Dte(j) |h(x, t) − y|. (7) By substituting DS with Dϕ,S(t) in Thm. 1, the bounds of Thm. 1 continue to hold for the test domains. In the corollary below, we bound the source error for practical ATTA at each time stept. Corollary 3. At time step t, for ATTA data domains Dϕ,S(t), Ute(1), Ute(2), ··· , Ute(t), Si are unla- beled samples of size m sampled from each of the t + 1 domains respectively, and SS is unlabeled samples of size m sampled from DS. If ˆh(t) ∈ Hminimizes ˆϵ′ w(h(t)) while other conditions remain identical to Thm. 1, then ϵS(ˆh(t)) ≤ ϵS(h∗ S(t)) + tX i=0 wi  ˆdH∆H(Si, SS) + 4 s 2d log(2m) + log 2 δ m + 2γi   + 2C, with probability at least 1 − δ, where C follows Thm. 1 and γi = minh∈H{ϵi(h(t)) + ϵS(h(t))}. Further analysis and proofs are in Appx. D and E. The following corollary provides direct theoretical support that our strategy conditionally reduces the error bound on the source domain. Corollary 4. At time step t, for ATTA data domains Dϕ,S(t), Ute(1), Ute(2), ··· , Ute(t), suppose that ˆh(t) ∈ Hminimizes ˆϵw′(h(t)) under identical conditions to Thm. 2. Let’s denote the source error upper bound with |ϵS(ˆh(t)) − ϵS(h∗ S(t))| ≤EBS(w, λ, N, t). Let w′ and λ′ be the weight 5Published as a conference paper at ICLR 2024 <latexit sha1_base64=\"NxhXSyFABPQk4q8627/odirDspg=\">AAAB9XicbVDLSgMxFM34rPVVdekmWARXZab4WhbcuKzYF7S1ZNI7bWgmMyR3lDL0P9y4UMSt/+LOvzHTdqGtBwKHc87l3hw/lsKg6347K6tr6xubua389s7u3n7h4LBhokRzqPNIRrrlMwNSKKijQAmtWAMLfQlNf3ST+c1H0EZEqobjGLohGygRCM7QSg/3mIWFGtAaGOwVim7JnYIuE29OimSOaq/w1elHPAlBIZfMmLbnxthNmUbBJUzyncRAzPiIDaBtqWIhmG46vXpCT63Sp0Gk7VNIp+rviZSFxoxD3yZDhkOz6GXif147weC6mwoVJwiKzxYFiaQY0awC2hcaOMqxJYxrYW+lfMg042iLytsSvMUvL5NGueRdli7uysXK+byOHDkmJ+SMeOSKVMgtqZI64USTZ/JK3pwn58V5dz5m0RVnPnNE/sD5/AFnsJJq</latexit> Streaming Test <latexit sha1_base64=\"a41BOKrutEYSWO9+8CjkPZKHvb8=\">AAAB73icbVBNS8NAEJ3Ur1q/qh69BIvgqSTiR48FLx4r2A9oQ9lsN+3SzSbuToQQ+ie8eFDEq3/Hm//GTZuDtj4YeLw3w8w8PxZco+N8W6W19Y3NrfJ2ZWd3b/+genjU0VGiKGvTSESq5xPNBJesjRwF68WKkdAXrOtPb3O/+8SU5pF8wDRmXkjGkgecEjRSbzAhmKWzyrBac+rOHPYqcQtSgwKtYfVrMIpoEjKJVBCt+64To5cRhZwKNqsMEs1iQqdkzPqGShIy7WXze2f2mVFGdhApUxLtufp7IiOh1mnom86Q4EQve7n4n9dPMGh4GZdxgkzSxaIgETZGdv68PeKKURSpIYQqbm616YQoQtFElIfgLr+8SjoXdfe6fnV/WWs2ijjKcAKncA4u3EAT7qAFbaAg4Ble4c16tF6sd+tj0Vqyiplj+APr8wfpIY/e</latexit> ˆy <latexit sha1_base64=\"SJEOE2ZYxLL1SU/QahOlMH6fop4=\">AAAB8HicbVBNSwMxEM3Wr1q/qh69BItQL2VX/Oix4MVjBbettEvJptk2NMkuyaxQlv4KLx4U8erP8ea/MW33oK0PBh7vzTAzL0wEN+C6305hbX1jc6u4XdrZ3ds/KB8etUycasp8GotYd0JimOCK+cBBsE6iGZGhYO1wfDvz209MGx6rB5gkLJBkqHjEKQErPfr9DNi0Cuf9csWtuXPgVeLlpIJyNPvlr94gpqlkCqggxnQ9N4EgIxo4FWxa6qWGJYSOyZB1LVVEMhNk84On+MwqAxzF2pYCPFd/T2REGjORoe2UBEZm2ZuJ/3ndFKJ6kHGVpMAUXSyKUoEhxrPv8YBrRkFMLCFUc3srpiOiCQWbUcmG4C2/vEpaFzXvunZ1f1lp1PM4iugEnaIq8tANaqA71EQ+okiiZ/SK3hztvDjvzseiteDkM8foD5zPH2KnkB4=</latexit> U te ( t ) <latexit sha1_base64=\"7rdY0fXtveVAqOkqa7z+i6K3Rp0=\">AAAB+XicbVDLSsNAFJ34rPUVdelmsAh1UxLxUXBTcOOygn1AE8pkMmmHTiZh5qZQQv/EjQtF3Pon7vwbp20W2nrgwuGce7n3niAVXIPjfFtr6xubW9ulnfLu3v7BoX103NZJpihr0UQkqhsQzQSXrAUcBOumipE4EKwTjO5nfmfMlOaJfIJJyvyYDCSPOCVgpL5tR1WPhgncYQ+GDMhF3644NWcOvErcglRQgWbf/vLChGYxk0AF0brnOin4OVHAqWDTspdplhI6IgPWM1SSmGk/n18+xedGCXGUKFMS8Fz9PZGTWOtJHJjOmMBQL3sz8T+vl0FU93Mu0wyYpItFUSYwJHgWAw65YhTExBBCFTe3YjokilAwYZVNCO7yy6ukfVlzb2rXj1eVRr2Io4RO0RmqIhfdogZ6QE3UQhSN0TN6RW9Wbr1Y79bHonXNKmZO0B9Ynz9h0pLV</latexit> f ( · ; ✓ ) <latexit sha1_base64=\"ud3dFXm+F2nsLD2/MdusutzkLvU=\">AAAB9HicbVDLSgNBEJyNrxhfUY9eBoPgKeyKr2PAixchgnlAsoTZ2d5kyMzOOjMbDEu+w4sHRbz6Md78GyfJHjSxoKGo6qa7K0g408Z1v53Cyura+kZxs7S1vbO7V94/aGqZKgoNKrlU7YBo4CyGhmGGQztRQETAoRUMb6Z+awRKMxk/mHECviD9mEWMEmMlvysC+ZTdyRD4pNQrV9yqOwNeJl5OKihHvVf+6oaSpgJiQznRuuO5ifEzogyjHCalbqohIXRI+tCxNCYCtJ/Njp7gE6uEOJLKVmzwTP09kRGh9VgEtlMQM9CL3lT8z+ukJrr2MxYnqYGYzhdFKcdG4mkCOGQKqOFjSwhVzN6K6YAoQo3NaRqCt/jyMmmeVb3L6sX9eaV2nsdRREfoGJ0iD12hGrpFddRAFD2iZ/SK3pyR8+K8Ox/z1oKTzxyiP3A+fwCmlpH9</latexit> Model SimATTA <latexit sha1_base64=\"bhVea6W/pzUPuDRNfs2xbDF7qAk=\">AAAB73icbVC7SgNBFL3rM8ZX1NJmMAhWYTf4KgM2FhYRzAOSJcxOZpMhs7PrzF0hhPyEjYUitv6OnX/jbLKFJh4YOJxzD3PvCRIpDLrut7Oyura+sVnYKm7v7O7tlw4OmyZONeMNFstYtwNquBSKN1Cg5O1EcxoFkreC0U3mt564NiJWDzhOuB/RgRKhYBSt1L6jQRYd9Eplt+LOQJaJl5My5Kj3Sl/dfszSiCtkkhrT8dwE/QnVKJjk02I3NTyhbEQHvGOpohE3/mS275ScWqVPwljbp5DM1N+JCY2MGUeBnYwoDs2il4n/eZ0Uw2t/IlSSIlds/lGYSoIxyY4nfaE5Qzm2hDIt7K6EDammDG1FRVuCt3jyMmlWK95l5eK+Wq6d53UU4BhO4Aw8uIIa3EIdGsBAwjO8wpvz6Lw4787HfHTFyTNH8AfO5w/1SI/i</latexit> Labeling <latexit sha1_base64=\"7rdY0fXtveVAqOkqa7z+i6K3Rp0=\">AAAB+XicbVDLSsNAFJ34rPUVdelmsAh1UxLxUXBTcOOygn1AE8pkMmmHTiZh5qZQQv/EjQtF3Pon7vwbp20W2nrgwuGce7n3niAVXIPjfFtr6xubW9ulnfLu3v7BoX103NZJpihr0UQkqhsQzQSXrAUcBOumipE4EKwTjO5nfmfMlOaJfIJJyvyYDCSPOCVgpL5tR1WPhgncYQ+GDMhF3644NWcOvErcglRQgWbf/vLChGYxk0AF0brnOin4OVHAqWDTspdplhI6IgPWM1SSmGk/n18+xedGCXGUKFMS8Fz9PZGTWOtJHJjOmMBQL3sz8T+vl0FU93Mu0wyYpItFUSYwJHgWAw65YhTExBBCFTe3YjokilAwYZVNCO7yy6ukfVlzb2rXj1eVRr2Io4RO0RmqIhfdogZ6QE3UQhSN0TN6RW9Wbr1Y79bHonXNKmZO0B9Ynz9h0pLV</latexit> f ( · ; ✓ ) <latexit sha1_base64=\"DPrA95GNP27SFW5vSoLC/hYa644=\">AAAB9XicbVDLSsNAFJ3UV62vqks3g0Wom5KIj4KbghuXFewDmlgmk0k7dJIJMzdKCf0PNy4Uceu/uPNvnLZZaOuBC4dz7uXee/xEcA22/W0VVlbX1jeKm6Wt7Z3dvfL+QVvLVFHWolJI1fWJZoLHrAUcBOsmipHIF6zjj26mfueRKc1lfA/jhHkRGcQ85JSAkR7CqksDCdfYTYb8tF+u2DV7BrxMnJxUUI5mv/zlBpKmEYuBCqJ1z7ET8DKigFPBJiU31SwhdEQGrGdoTCKmvWx29QSfGCXAoVSmYsAz9fdERiKtx5FvOiMCQ73oTcX/vF4KYd3LeJykwGI6XxSmAoPE0whwwBWjIMaGEKq4uRXTIVGEggmqZEJwFl9eJu2zmnNZu7g7rzTqeRxFdISOURU56Ao10C1qohaiSKFn9IrerCfrxXq3PuatBSufOUR/YH3+AFKlkbs=</latexit> f ( · ; \u0000 ) <latexit sha1_base64=\"DPrA95GNP27SFW5vSoLC/hYa644=\">AAAB9XicbVDLSsNAFJ3UV62vqks3g0Wom5KIj4KbghuXFewDmlgmk0k7dJIJMzdKCf0PNy4Uceu/uPNvnLZZaOuBC4dz7uXee/xEcA22/W0VVlbX1jeKm6Wt7Z3dvfL+QVvLVFHWolJI1fWJZoLHrAUcBOsmipHIF6zjj26mfueRKc1lfA/jhHkRGcQ85JSAkR7CqksDCdfYTYb8tF+u2DV7BrxMnJxUUI5mv/zlBpKmEYuBCqJ1z7ET8DKigFPBJiU31SwhdEQGrGdoTCKmvWx29QSfGCXAoVSmYsAz9fdERiKtx5FvOiMCQ73oTcX/vF4KYd3LeJykwGI6XxSmAoPE0whwwBWjIMaGEKq4uRXTIVGEggmqZEJwFl9eJu2zmnNZu7g7rzTqeRxFdISOURU56Ao10C1qohaiSKFn9IrerCfrxXq3PuatBSufOUR/YH3+AFKlkbs=</latexit> f ( · ; \u0000 ) <latexit sha1_base64=\"ipQ+JKlINPDcPjrbUYUkqyyzp40=\">AAAB+nicbVC7TsMwFHXKq5RXCiOLRYXEQpVUvMZKLIxF0IfURpXj3LRWHSeyHVBV+iksDCDEypew8Te4aQZoOZKlo3Puy8dPOFPacb6twsrq2vpGcbO0tb2zu2eX91sqTiWFJo15LDs+UcCZgKZmmkMnkUAin0PbH13P/PYDSMVica/HCXgRGQgWMkq0kfp2+S6bdNqQoCUxQ4K+XXGqTga8TNycVFCORt/+6gUxTSMQmnKiVNd1Eu1NiNSMcpiWeqmChNARGUDXUEEiUN4kO32Kj40S4DCW5gmNM/V3x4RESo0j31RGRA/VojcT//O6qQ6vvAkTSapB0PmiMOVYx3iWAw6YBKr52BBCJTO3YjokklBt0iqZENzFLy+TVq3qXlTPb2uV+lkeRxEdoiN0glx0ieroBjVQE1H0iJ7RK3qznqwX6936mJcWrLznAP2B9fkDSAyT+w==</latexit> Source-Pretrained <latexit sha1_base64=\"ud3dFXm+F2nsLD2/MdusutzkLvU=\">AAAB9HicbVDLSgNBEJyNrxhfUY9eBoPgKeyKr2PAixchgnlAsoTZ2d5kyMzOOjMbDEu+w4sHRbz6Md78GyfJHjSxoKGo6qa7K0g408Z1v53Cyura+kZxs7S1vbO7V94/aGqZKgoNKrlU7YBo4CyGhmGGQztRQETAoRUMb6Z+awRKMxk/mHECviD9mEWMEmMlvysC+ZTdyRD4pNQrV9yqOwNeJl5OKihHvVf+6oaSpgJiQznRuuO5ifEzogyjHCalbqohIXRI+tCxNCYCtJ/Njp7gE6uEOJLKVmzwTP09kRGh9VgEtlMQM9CL3lT8z+ukJrr2MxYnqYGYzhdFKcdG4mkCOGQKqOFjSwhVzN6K6YAoQo3NaRqCt/jyMmmeVb3L6sX9eaV2nsdRREfoGJ0iD12hGrpFddRAFD2iZ/SK3pyR8+K8Ox/z1oKTzxyiP3A+fwCmlpH9</latexit> Model <latexit sha1_base64=\"5LNAmmVR/AN9Lc2T+FRV/is2yz8=\">AAAB8nicbVDLSgNBEJyNrxhfUY9eBoPgKewGX8eACB48RDAP2CxhdjKbDJmdWWZ6lbDkM7x4UMSrX+PNv3GS7EETCxqKqm66u8JEcAOu++0UVlbX1jeKm6Wt7Z3dvfL+QcuoVFPWpEoo3QmJYYJL1gQOgnUSzUgcCtYOR9dTv/3ItOFKPsA4YUFMBpJHnBKwkn+nnvCNBK2Sca9ccavuDHiZeDmpoByNXvmr21c0jZkEKogxvucmEGREA6eCTUrd1LCE0BEZMN9SSWJmgmx28gSfWKWPI6VtScAz9fdERmJjxnFoO2MCQ7PoTcX/PD+F6CrIuExSYJLOF0WpwKDw9H/c55pREGNLCNXc3orpkGhCwaZUsiF4iy8vk1at6l1Uz+9rlfpZHkcRHaFjdIo8dInq6BY1UBNRpNAzekVvDjgvzrvzMW8tOPnMIfoD5/MHKbiRJQ==</latexit> Low Entropy <latexit sha1_base64=\"vLgKkEyV9E/djVdgAkvKuOUQOTU=\">AAAB7nicbVDLSgMxFL1TX7W+qi7dBIvgqswUX8uCG5cV7QPaoWTSTBuaZEKSEcrQj3DjQhG3fo87/8a0nYW2HrhwOOde7r0nUpwZ6/vfXmFtfWNzq7hd2tnd2z8oHx61TJJqQpsk4YnuRNhQziRtWmY57ShNsYg4bUfj25nffqLasEQ+2omiocBDyWJGsHVS+wELxanplyt+1Z8DrZIgJxXI0eiXv3qDhKSCSks4NqYb+MqGGdaWEU6npV5qqMJkjIe066jEgpowm587RWdOGaA40a6kRXP190SGhTETEblOge3ILHsz8T+vm9r4JsyYVKmlkiwWxSlHNkGz39GAaUosnziCiWbuVkRGWGNiXUIlF0Kw/PIqadWqwVX18r5WqV/kcRThBE7hHAK4hjrcQQOaQGAMz/AKb57yXrx372PRWvDymWP4A+/zB19wj48=</latexit> Samples <latexit sha1_base64=\"wuZucU3JbeEJSquG2WgqGdYMCR8=\">AAAB83icbVDLSgMxFL3js9ZX1aWbYBFclZnia1kQocsK9gHtUDJppg3NJCHJCGXob7hxoYhbf8adf2PazkJbD1w4nHMv994TKc6M9f1vb219Y3Nru7BT3N3bPzgsHR23jEw1oU0iudSdCBvKmaBNyyynHaUpTiJO29H4bua3n6g2TIpHO1E0TPBQsJgRbJ3Uq7PhCN0Lq6Wa9Etlv+LPgVZJkJMy5Gj0S1+9gSRpQoUlHBvTDXxlwwxrywin02IvNVRhMsZD2nVU4ISaMJvfPEXnThmgWGpXwqK5+nsiw4kxkyRynQm2I7PszcT/vG5q49swY0KllgqyWBSnHFmJZgGgAdOUWD5xBBPN3K2IjLDGxLqYii6EYPnlVdKqVoLrytVDtVy7zOMowCmcwQUEcAM1qEMDmkBAwTO8wpuXei/eu/exaF3z8pkT+APv8wfIYpF9</latexit> High Entropy <latexit sha1_base64=\"vLgKkEyV9E/djVdgAkvKuOUQOTU=\">AAAB7nicbVDLSgMxFL1TX7W+qi7dBIvgqswUX8uCG5cV7QPaoWTSTBuaZEKSEcrQj3DjQhG3fo87/8a0nYW2HrhwOOde7r0nUpwZ6/vfXmFtfWNzq7hd2tnd2z8oHx61TJJqQpsk4YnuRNhQziRtWmY57ShNsYg4bUfj25nffqLasEQ+2omiocBDyWJGsHVS+wELxanplyt+1Z8DrZIgJxXI0eiXv3qDhKSCSks4NqYb+MqGGdaWEU6npV5qqMJkjIe066jEgpowm587RWdOGaA40a6kRXP190SGhTETEblOge3ILHsz8T+vm9r4JsyYVKmlkiwWxSlHNkGz39GAaUosnziCiWbuVkRGWGNiXUIlF0Kw/PIqadWqwVX18r5WqV/kcRThBE7hHAK4hjrcQQOaQGAMz/AKb57yXrx372PRWvDymWP4A+/zB19wj48=</latexit> Samples <latexit sha1_base64=\"1BO6D/gzkeZNQ7HNIaph5NqELCI=\">AAAB8nicbVDLSgMxFM3UV62vqks3wSK4KjPF17LgRncV7AOmQ8mkd9rQTDIkGaEM/Qw3LhRx69e482/MtLPQ1gOBwzn3kHtPmHCmjet+O6W19Y3NrfJ2ZWd3b/+genjU0TJVFNpUcql6IdHAmYC2YYZDL1FA4pBDN5zc5n73CZRmUjyaaQJBTEaCRYwSYyX/XlAFMQhD+KBac+vuHHiVeAWpoQKtQfWrP5Q0zdOUE619z01MkBFlGOUwq/RTDQmhEzIC31JBYtBBNl95hs+sMsSRVPYJg+fq70RGYq2ncWgnY2LGetnLxf88PzXRTZAxkaQGBF18FKUcG4nz+/GQKaCGTy0hVDG7K6Zjogg1tqWKLcFbPnmVdBp176p++dCoNS+KOsroBJ2ic+Sha9REd6iF2ogiiZ7RK3pzjPPivDsfi9GSU2SO0R84nz9y2ZFU</latexit> Incremental <latexit sha1_base64=\"Jmobmj50NeE6y3ftB4xt5xZD5Eg=\">AAAB8XicbVDLSgNBEOyNrxhfUY9eBoPgKewGX8dALh4jmAcmS5id9CZDZmeXmVkhLP6FFw+KePVvvPk3TpI9aGJBQ1HVTXdXkAiujet+O4W19Y3NreJ2aWd3b/+gfHjU1nGqGLZYLGLVDahGwSW2DDcCu4lCGgUCO8GkMfM7j6g0j+W9mSboR3QkecgZNVZ6aIhUG1Rcjgblilt15yCrxMtJBXI0B+Wv/jBmaYTSMEG17nluYvyMKsOZwKdSP9WYUDahI+xZKmmE2s/mFz+RM6sMSRgrW9KQufp7IqOR1tMosJ0RNWO97M3E/7xeasIbP+MySQ1KtlgUpoKYmMzeJ0OukBkxtYQyxe2thI2posymoEs2BG/55VXSrlW9q+rlXa1Sv8jjKMIJnMI5eHANdbiFJrSAgYRneIU3RzsvzrvzsWgtOPnMMfyB8/kDzgaQ+A==</latexit> Clustering <latexit sha1_base64=\"c4xrXg0yZYBSSDLHCxlf45OWNzg=\">AAAB7nicbVDLSgNBEOz1GeMr6tHLYBA8hd2Aj2PAi8eI5gHJEmYnnWTIzOwyMyuEJR/hxYMiXv0eb/6Nk2QPmljQUFR1090VJYIb6/vf3tr6xubWdmGnuLu3f3BYOjpumjjVDBssFrFuR9Sg4AoblluB7UQjlZHAVjS+nfmtJ9SGx+rRThIMJR0qPuCMWie1HqhMBJpeqexX/DnIKglyUoYc9V7pq9uPWSpRWSaoMZ3AT2yYUW05EzgtdlODCWVjOsSOo4pKNGE2P3dKzp3SJ4NYu1KWzNXfExmVxkxk5DoltSOz7M3E/7xOagc3YcZVklpUbLFokApiYzL7nfS5RmbFxBHKNHe3EjaimjLrEiq6EILll1dJs1oJriqX99VyrZrHUYBTOIMLCOAaanAHdWgAgzE8wyu8eYn34r17H4vWNS+fOYE/8D5/AF7Wj40=</latexit> Samples <latexit sha1_base64=\"eimCpRgfVxBfxhwCehIJdcsMsvY=\">AAAB8XicbVA9SwNBEJ2LXzF+RS1tFoNgFe5SRMtAGssI5gOTI+xt5pIle3vH7p4QjvwLGwtFbP03dv4bN8kVmvhg4PHeDDPzgkRwbVz32ylsbe/s7hX3SweHR8cn5dOzjo5TxbDNYhGrXkA1Ci6xbbgR2EsU0igQ2A2mzYXffUKleSwfzCxBP6JjyUPOqLHSY1Ok2qDicjwsV9yquwTZJF5OKpCjNSx/DUYxSyOUhgmqdd9zE+NnVBnOBM5Lg1RjQtmUjrFvqaQRaj9bXjwnV1YZkTBWtqQhS/X3REYjrWdRYDsjaiZ63VuI/3n91IS3fsZlkhqUbLUoTAUxMVm8T0ZcITNiZgllittbCZtQRZlNQZdsCN76y5ukU6t69Wr9vlZpuHkcRbiAS7gGD26gAXfQgjYwkPAMr/DmaOfFeXc+Vq0FJ585hz9wPn8AzSSQ9Q==</latexit> Clustering <latexit sha1_base64=\"JgGHFC5oztwX6+XjDtZWQo9C1hA=\">AAAB7nicbVA9TwJBEJ3DL8Qv1NJmIzGxIncUaImxscREwAQuZG8ZYMPe7mV3z4Rc+BE2Fhpj6++x89+4wBUKvmSSl/dmMjMvSgQ31ve/vcLG5tb2TnG3tLd/cHhUPj5pG5Vqhi2mhNKPETUouMSW5VbgY6KRxpHATjS5nfudJ9SGK/lgpwmGMR1JPuSMWid1biQbK2365Ypf9Rcg6yTISQVyNPvlr95AsTRGaZmgxnQDP7FhRrXlTOCs1EsNJpRN6Ai7jkoaowmzxbkzcuGUARkq7UpaslB/T2Q0NmYaR64zpnZsVr25+J/XTe3wOsy4TFKLki0XDVNBrCLz38mAa2RWTB2hTHN3K2FjqimzLqGSCyFYfXmdtGvVoF6t39cqDT+PowhncA6XEMAVNOAOmtACBhN4hld48xLvxXv3PpatBS+fOYU/8D5/AFOaj4U=</latexit> Anchors <latexit sha1_base64=\"eimCpRgfVxBfxhwCehIJdcsMsvY=\">AAAB8XicbVA9SwNBEJ2LXzF+RS1tFoNgFe5SRMtAGssI5gOTI+xt5pIle3vH7p4QjvwLGwtFbP03dv4bN8kVmvhg4PHeDDPzgkRwbVz32ylsbe/s7hX3SweHR8cn5dOzjo5TxbDNYhGrXkA1Ci6xbbgR2EsU0igQ2A2mzYXffUKleSwfzCxBP6JjyUPOqLHSY1Ok2qDicjwsV9yquwTZJF5OKpCjNSx/DUYxSyOUhgmqdd9zE+NnVBnOBM5Lg1RjQtmUjrFvqaQRaj9bXjwnV1YZkTBWtqQhS/X3REYjrWdRYDsjaiZ63VuI/3n91IS3fsZlkhqUbLUoTAUxMVm8T0ZcITNiZgllittbCZtQRZlNQZdsCN76y5ukU6t69Wr9vlZpuHkcRbiAS7gGD26gAXfQgjYwkPAMr/DmaOfFeXc+Vq0FJ585hz9wPn8AzSSQ9Q==</latexit> Clustering <latexit sha1_base64=\"JgGHFC5oztwX6+XjDtZWQo9C1hA=\">AAAB7nicbVA9TwJBEJ3DL8Qv1NJmIzGxIncUaImxscREwAQuZG8ZYMPe7mV3z4Rc+BE2Fhpj6++x89+4wBUKvmSSl/dmMjMvSgQ31ve/vcLG5tb2TnG3tLd/cHhUPj5pG5Vqhi2mhNKPETUouMSW5VbgY6KRxpHATjS5nfudJ9SGK/lgpwmGMR1JPuSMWid1biQbK2365Ypf9Rcg6yTISQVyNPvlr95AsTRGaZmgxnQDP7FhRrXlTOCs1EsNJpRN6Ai7jkoaowmzxbkzcuGUARkq7UpaslB/T2Q0NmYaR64zpnZsVr25+J/XTe3wOsy4TFKLki0XDVNBrCLz38mAa2RWTB2hTHN3K2FjqimzLqGSCyFYfXmdtGvVoF6t39cqDT+PowhncA6XEMAVNOAOmtACBhN4hld48xLvxXv3PpatBS+fOYU/8D5/AFOaj4U=</latexit> Anchors <latexit sha1_base64=\"KzBZ8R84UC9mpPFQBWeRHFxcqjw=\">AAAB7XicbVBNS8NAEJ3Ur1q/qh69LBbBU0mKVI8FLx4rmLbQhrLZbNq1m92wuxFK6H/w4kERr/4fb/4bt20O2vpg4PHeDDPzwpQzbVz32yltbG5t75R3K3v7B4dH1eOTjpaZItQnkkvVC7GmnAnqG2Y47aWK4iTktBtObud+94kqzaR4MNOUBgkeCRYzgo2VOn4aYUOH1ZpbdxdA68QrSA0KtIfVr0EkSZZQYQjHWvc9NzVBjpVhhNNZZZBpmmIywSPat1TghOogX1w7QxdWiVAslS1h0EL9PZHjROtpEtrOBJuxXvXm4n9ePzPxTZAzkWaGCrJcFGccGYnmr6OIKUoMn1qCiWL2VkTGWGFibEAVG4K3+vI66TTqXrPevG/UWldFHGU4g3O4BA+uoQV30AYfCDzCM7zCmyOdF+fd+Vi2lpxi5hT+wPn8AYuwjxQ=</latexit> Update <latexit sha1_base64=\"y2NH6tDs2GygUDqZYglGwvR4SpA=\">AAAB+nicbVBNSwMxEJ2tX7V+bfXoJVgEQSi7PVSPFS8eK9oPaEvJptk2NMkuSVYpa3+KFw+KePWXePPfmLZ70NYHA4/3ZpiZF8ScaeN5305ubX1jcyu/XdjZ3ds/cIuHTR0litAGiXik2gHWlDNJG4YZTtuxolgEnLaC8fXMbz1QpVkk780kpj2Bh5KFjGBjpb5bvMMi5lSjc3QlyShSuu+WvLI3B1olfkZKkKHed7+6g4gkgkpDONa643ux6aVYGUY4nRa6iaYxJmM8pB1LJRZU99L56VN0apUBCiNlSxo0V39PpFhoPRGB7RTYjPSyNxP/8zqJCS97KZNxYqgki0VhwpGJ0CwHNGCKEsMnlmCimL0VkRFWmBibVsGG4C+/vEqalbJfLVdvK6Wal8WRh2M4gTPw4QJqcAN1aACBR3iGV3hznpwX5935WLTmnGzmCP7A+fwBUnKTWg==</latexit> Samples + Anchors <latexit sha1_base64=\"u0BDOcH87PXd3DsT+o414+7cHnI=\">AAAB7XicbZC7SgNBFIbPxluMt6ilIINBsAq7FjGdARvLBMwFkhBmZ2eTMbMzy8ysEJaU9jYWitj6Cql8CDufwZdwcik0+sPAx/+fw5xz/JgzbVz308msrK6tb2Q3c1vbO7t7+f2DhpaJIrROJJeq5WNNORO0bpjhtBUriiOf06Y/vJrmzTuqNJPixoxi2o1wX7CQEWys1eiQQBrdyxfcojsT+gveAgqX75Pa1/3xpNrLf3QCSZKICkM41rrtubHpplgZRjgd5zqJpjEmQ9ynbYsCR1R309m0Y3RqnQCFUtknDJq5PztSHGk9inxbGWEz0MvZ1PwvaycmLHdTJuLEUEHmH4UJR0ai6eooYIoSw0cWMFHMzorIACtMjD1Qzh7BW175LzTOi16pWKq5hUoZ5srCEZzAGXhwARW4hirUgcAtPMATPDvSeXRenNd5acZZ9BzCLzlv33Yvk3g=</latexit> ··· <latexit sha1_base64=\"+7L/8ObZcl+JIZaSFhVO3t+lUUE=\">AAAB7XicbVDLSgNBEOyNrxhf8XHzMhiEeAm7ItFjQA8eI5gHJCHMTmaT0dnZZaZXCEv+wYsHRbz6P978GyebHDSxoKGo6qa7y4+lMOi6305uZXVtfSO/Wdja3tndK+4fNE2UaMYbLJKRbvvUcCkUb6BAydux5jT0JW/5j9dTv/XEtRGRusdxzHshHSoRCEbRSs2bvizjWb9YcituBrJMvDkp1Y6CDPV+8as7iFgScoVMUmM6nhtjL6UaBZN8UugmhseUPdIh71iqaMhNL82unZBTqwxIEGlbCkmm/p5IaWjMOPRtZ0hxZBa9qfif10kwuOqlQsUJcsVmi4JEEozI9HUyEJozlGNLKNPC3krYiGrK0AZUsCF4iy8vk+Z5xatWqnc2jQuYIQ/HcAJl8OASanALdWgAgwd4hld4cyLnxXl3PmatOWc+cwh/4Hz+AFjYkTs=</latexit> D l ( t ) <latexit sha1_base64=\"9C0bB8PYImk9DX0HLfGvGd44PFA=\">AAAB7XicbVDLSgNBEOyNrxhf8XHzMhiEeAm7ItFjQA8eI5gHJCHMTmaT0dnZZaZXCEv+wYsHRbz6P978GyebHDSxoKGo6qa7y4+lMOi6305uZXVtfSO/Wdja3tndK+4fNE2UaMYbLJKRbvvUcCkUb6BAydux5jT0JW/5j9dTv/XEtRGRusdxzHshHSoRCEbRSs2b/qiMZ/1iya24Gcgy8eakVDsKMtT7xa/uIGJJyBUySY3peG6MvZRqFEzySaGbGB5T9kiHvGOpoiE3vTS7dkJOrTIgQaRtKSSZ+nsipaEx49C3nSHFkVn0puJ/XifB4KqXChUnyBWbLQoSSTAi09fJQGjOUI4toUwLeythI6opQxtQwYbgLb68TJrnFa9aqd7ZNC5ghjwcwwmUwYNLqMEt1KEBDB7gGV7hzYmcF+fd+Zi15pz5zCH8gfP5A1K8kTc=</latexit> D h ( t ) <latexit sha1_base64=\"eNrtnhPGeU8n4BRDMStm5cjQ4ts=\">AAAB73icbVBNS8NAEJ34WetX1aOXxSJ4KkmR6rHQi8cK9gPaUDbbTbt0s4m7E6GE/gkvHhTx6t/x5r9x2+agrQ8GHu/NMDMvSKQw6Lrfzsbm1vbObmGvuH9weHRcOjltmzjVjLdYLGPdDajhUijeQoGSdxPNaRRI3gkmjbnfeeLaiFg94DThfkRHSoSCUbRS1zNIGlTKQansVtwFyDrxclKGHM1B6as/jFkacYVMUmN6npugn1GNgkk+K/ZTwxPKJnTEe5YqGnHjZ4t7Z+TSKkMSxtqWQrJQf09kNDJmGgW2M6I4NqveXPzP66UY3vqZUEmKXLHlojCVBGMyf54MheYM5dQSyrSwtxI2ppoytBEVbQje6svrpF2teLVK7b5arl/ncRTgHC7gCjy4gTrcQRNawEDCM7zCm/PovDjvzseydcPJZ87gD5zPH1Naj3k=</latexit> 1st Call <latexit sha1_base64=\"mxsL+XuWb2hqFND+pzTctrB1rcY=\">AAAB73icbVBNS8NAEJ34WetX1aOXxSJ4KkmR6rHQi8cK9gPaUDababt0s4m7G6GE/gkvHhTx6t/x5r9x2+agrQ8GHu/NMDMvSATXxnW/nY3Nre2d3cJecf/g8Oi4dHLa1nGqGLZYLGLVDahGwSW2DDcCu4lCGgUCO8GkMfc7T6g0j+WDmSboR3Qk+ZAzaqzUrcqQNKgQg1LZrbgLkHXi5aQMOZqD0lc/jFkaoTRMUK17npsYP6PKcCZwVuynGhPKJnSEPUsljVD72eLeGbm0SkiGsbIlDVmovycyGmk9jQLbGVEz1qveXPzP66VmeOtnXCapQcmWi4apICYm8+dJyBUyI6aWUKa4vZWwMVWUGRtR0Ybgrb68TtrViler1O6r5fp1HkcBzuECrsCDG6jDHTShBQwEPMMrvDmPzovz7nwsWzecfOYM/sD5/AE0o49l</latexit> 2nd Call <latexit sha1_base64=\"oSA1OFmXXL9y3PJtqoVxTIG9mto=\">AAAB8HicbVA9TwJBEJ3DL8Qv1NJmIzGxIncUaElCY2UwkQ8DF7K3zMGGvb3L7p6REH6FjYXG2Ppz7Pw3LnCFgi+Z5OW9mczMCxLBtXHdbye3sbm1vZPfLeztHxweFY9PWjpOFcMmi0WsOgHVKLjEpuFGYCdRSKNAYDsY1+d++xGV5rG8N5ME/YgOJQ85o8ZKD7f4ZEidCtEvltyyuwBZJ15GSpCh0S9+9QYxSyOUhgmqdddzE+NPqTKcCZwVeqnGhLIxHWLXUkkj1P50cfCMXFhlQMJY2ZKGLNTfE1MaaT2JAtsZUTPSq95c/M/rpia89qdcJqlByZaLwlQQE5P592TAFTIjJpZQpri9lbARVZQZm1HBhuCtvrxOWpWyVy1X7yqlWiWLIw9ncA6X4MEV1OAGGtAEBhE8wyu8Ocp5cd6dj2VrzslmTuEPnM8fSFeQCA==</latexit> Next Call Figure 2: Overview of the SimATTA framework. and sample ratio vectors when Dϕ,S(t) is not included, i.e., w′ and λ′ s.t. w′ 0 = λ′ 0 = 0 . If ˆdH∆H(DS, Dϕ,S(t)) < ˆdH∆H(DS, St i=1 Ute(i)), then for any λ ̸= λ′, there exists w s.t. EBS(w, λ, N, t) < EBS(w′, λ′, N, t). (8) Corollary 4 validates that the selected low-entropy samples can mitigate the CF problem under the assumption that these samples are source-like, which is also empirically validated in Fig. 1. Note that our strategy employs entropy minimization in a selective manner, aiming to solve CF rather than the main adaptation issue. While many FTTA works use entropy minimization to adapt across domains without guarantees, our use is more theoretically-sound. 4 A N ATTA ALGORITHM Building on our theoretical findings, we introduce a simple yet effective ATTA method, known as SimATTA, that innovatively integrates incremental clustering and selective entropy minimization techniques, as illustrated in Fig. 2. We start with an overview of our methodology, including the learning framework and the comprehensive sample selection strategies. We then proceed to discuss the details of the incremental clustering technique designed for real-time sample selections. 4.1 A LGORITHM OVERVIEW Let (x, y) be a labeled sample and f(·; θ) be our neural network, where ˆy = f(x; θ) and θ represents the parameters. We have a model pre-trained on source domains with the pre-trained parameters ϕ. We initialize model parameters as θ(0) = ϕ and aim to adapt the model f(·; θ) in real-time. During the test phase, the model continuously predicts labels for streaming-in test data and concurrently gets fine-tuned. We perform sample selection to enable active learning. As discussed in Sec. 3.2, we empirically consider informative high-entropy samples for addressing distribution shifts and source-like low-entropy samples to mitigate CF. As shown in Alg. 1, at each time step t, we first partition unlabeled test samples Ute(t) into high entropy and low entropy datasets, Uh(t) and Ul(t), using an entropy threshold. The source-pretrained model f(·; ϕ) is frozen to predict pseudo labels for low entropy data. We obtain labeled low-entropy data Dl(t) by labeling Ul(t) with f(·; ϕ) and combining it with Dl(t − 1). In contrast, the selection of high-entropy samples for active labeling is less straightforward. Since the complete test dataset is inaccessible for analyzing the target domain distribution, real-time sample selection is required. We design an incremental clustering sample selection technique to reduce sample redundancy and increase distribution coverage, detailed in Sec. 4.2. The incremental clustering algorithm outputs the labeled test samples Dh(t), also referred to as anchors, given Dh(t −1) and Uh(t). After sample selection, the model undergoes test-time training using the labeled test anchors Dh(t) and pseudo-labeled source-like anchors Dl(t). Following the analyses in Sec. 3.1, the training weights and sample numbers should satisfy w(t) ≈ λ(t) for Dh(t) and Dl(t) for optimal results. The analyses and results in Sec. 3.2 further indicate that balancing the source and target ratio is the key to mitigating CF. However, when source-like samples significantly outnumber test samples, the optimal w(t) for test domains can deviate from λ(t) according to Eq. (4). 4.2 I NCREMENTAL CLUSTERING We propose incremental clustering, a novel continual clustering technique designed to select informa- tive samples in unsupervised settings under the ATTA framework. The primary goal of this strategy is to store representative samples for distributions seen so far. Intuitively, we apply clusters to cover all seen distributions while adding new clusters to cover newly seen distributions. During this process with new clusters added, old clusters may be merged due to the limit of the cluster budget. Since 6Published as a conference paper at ICLR 2024 Algorithm 1 SIMATTA: A SIMPLE ATTA ALGORITHM Require: A fixed source pre-trained model f(·; ϕ) and a real-time adapting model f(·; θ(t)) with θ(0) = ϕ. Streaming test data Ute(t) at time step t. Entropy of predictions H(ˆy) = −P c p(ˆyc) logp(ˆyc). Low entropy and high entropy thresholds el and eh. The number of cluster centroid budget NC (t) at time step t. Centroid increase number k. Learning step size η. 1: for t = 1, . . . , Tdo 2: Model inference on Ute(t) using f(·; θ(t − 1)). 3: Dl(t) ← Dl(t − 1) ∪ {(x, f(x; ϕ))|x ∈ Ute(t), H(f(x; ϕ)) < el} 4: Uh(t) ← {x|x ∈ Ute(t), H(f(x; θ)) > eh} 5: Dh(t) ← Dh(t − 1) ∪ {(x, y)|∀x ∈ IC(Dh(t − 1), Uh(t), NC(t)), y= Oracle(x)} 6: λ(t) ← |Dl(t)|/(|Dl(t)| + |Dh(t)|), |Dh(t)|/(|Dl(t)| + |Dh(t)|) 7: w(t) ← GetW(λ(t)) ▷ Generally, GetW(λ(t)) = λ(t) is a fair choice. 8: θ(t) ← θ(t − 1) 9: for (xl, yl) in Dl and (xh, yh) in Dh do 10: θ(t) ← θ(t) − ηw0∇ℓCE (f(xl; θ(t)), yl) − η(1 − w0)∇ℓCE (f(xh; θ(t)), yh) 11: end for 12: NC (t + 1) ← UpdateCentroidNum(NC (t)) ▷ Naive choice: NC (t + 1) ← NC (t) + k. 13: end for clusters cannot be stored efficiently, we store the representative samples of clusters, named anchors, instead. In this work, we adopt weighted K-means (Krishna and Murty, 1999) as our base clustering method due to its popularity and suitability for new setting explorations. When we apply clustering with new samples, a previously selected anchor should not weigh the same as new samples since the anchor is a representation of a cluster,i.e., a representation of many samples. Instead, the anchor should be considered as a barycenter with a weight of the sum of its cluster’s sample weights. For a newly added cluster, its new anchor has the weight of the whole cluster. For clusters containing multiple old anchors, i.e., old clusters, the increased weights are distributed equally among these anchors. These increased weights are contributed by new samples that are close to these old anchors. Intuitively, this process of clustering is analogous to the process of planet formation. Where there are no planets, new planets (anchors) will be formed by the aggregation of the surrounding material (samples). Where there are planets, the matter is absorbed by the surrounding planets. This example is only for better understanding without specific technical meanings. Specifically, we provide the detailed Alg. 2 for incremental clustering. In each iteration, we apply weighted K-Means for previously selected anchors Danc and the new streaming-in unlabeled data Unew. We first extract all sample features using the model from the previous step f(·; θ(t − 1)), and then cluster these weighted features. The initial weights of the new unlabeled samples are 1, while anchors inherit weights from previous iterations. After clustering, clusters including old anchors are old clusters, while clusters only containing new samples are newly formed ones. For each new cluster, we select the centroid-closest sample as the new anchor to store. As shown in line 10 of Alg. 2, for both old and new clusters, we distribute the sample weights in this cluster as its anchors’ weights. With incremental clustering, although we can control the number of clusters in each iteration, we cannot control the number of new clusters/new anchors. This indirect control makes the increase of new anchors adaptive to the change of distributions, but it also leads to indirect budget control. Therefore, in experimental studies, we set the budget limit, but the actual anchor budget will not reach this limit. The overall extra storage requirement is O(B) since the number of saved unlabeled samples is proportional to the number of saved labeled samples (anchors). 5 E XPERIMENTAL STUDIES In this study, we aim to validate the effectiveness of our proposed method, as well as explore the various facets of the ATTA setting. Specifically, we design experiments around the following research questions: RQ1: Can TTA methods address domain distribution shifts? RQ2: Is ATTA as efficient as TTA? RQ3: How do the components of SimATTA perform? RQ4: Can ATTA perform on par with stronger Active Domain Adaptation (ADA) methods? We compare ATTA with three settings, TTA (Tab. 2), enhanced TTA (Tab. 3 and 5), and ADA (Tab. 4). Datasets. To assess the OOD performance of the TTA methods, we benchmark them using datasets from DomainBed (Gulrajani and Lopez-Paz, 2020) and Hendrycks and Dietterich (2019a). We employ PACS (Li et al., 2017), VLCS (Fang et al., 2013), Office-Home (Venkateswara et al., 2017), and Tiny-ImageNet-C datasets for our evaluations. For each dataset, we designate one domain as 7Published as a conference paper at ICLR 2024 Table 2: TTA comparisons on PACS and VLCS.This table includes the two data stream mentioned in the dataset setup and reports performances in accuracy. Results that outperform all TTA baselines are highlighted in bold font. N/A denotes the adaptations are not applied on the source domain. PACS Domain-wise data stream Post-adaptation Random data stream Post-adaptation P →A→ →C→ →S P A C S →1→ →2→ →3→ →4 P A C S BN w/o adapt 99.70 59.38 28.03 42.91 99.70 59.38 28.03 42.91 43.44 43.44 43.44 43.44 99.70 59.38 28.03 42.91BN w/ adapt 98.74 68.07 64.85 54.57 98.74 68.07 64.85 54.57 62.50 62.50 62.50 62.50 98.74 68.07 64.85 54.57 Tent (steps=1) N/A 67.29 64.59 44.67 97.60 66.85 64.08 42.58 56.35 54.09 51.83 48.58 97.19 63.53 60.75 41.56Tent (steps=10) N/A 67.38 57.85 20.23 62.63 34.52 40.57 13.59 47.36 31.01 22.84 20.33 50.78 23.68 20.95 19.62EATA N/A 67.04 64.72 50.27 98.62 66.50 62.46 48.18 57.31 56.06 58.17 59.78 98.62 69.63 65.70 54.26CoTTA N/A 65.48 62.12 53.17 98.62 65.48 63.10 53.78 56.06 54.33 57.16 57.42 98.62 65.97 62.97 54.62SAR (steps=1) N/A 66.75 63.82 49.58 98.32 66.94 62.93 45.74 56.78 56.35 56.68 56.70 98.44 68.16 64.38 52.53SAR (steps=10) N/A 69.38 68.26 49.02 96.47 62.16 56.19 54.62 53.51 51.15 51.78 45.60 94.13 56.64 56.02 36.37 SimATTA (B ≤300) N/A 76.86 70.90 75.39 98.80 84.47 82.25 81.52 69.47 76.49 82.45 82.22 98.98 84.91 83.92 86.00SimATTA (B ≤500) N/A 77.93 76.02 76.30 98.62 88.33 83.49 83.74 68.46 78.22 80.91 85.49 99.16 86.67 84.77 87.71 VLCS Domain-wise data stream Post-adaptation Random data stream Post-adaptation C →L→ →S→ →V C L S V →1→ →2→ →3→ →4 C L S V BN w/o adapt 100.00 33.55 41.10 49.05 100.00 33.55 41.10 49.05 41.23 41.23 41.23 41.23 100.00 33.55 41.10 49.05BN w/ adapt 85.16 37.31 33.27 52.16 85.16 37.31 33.27 52.16 40.91 40.91 40.91 40.91 85.16 37.31 33.27 52.16 Tent (steps=1) N/A 38.55 34.40 53.88 84.73 43.86 33.61 53.11 44.85 44.29 47.38 44.98 85.30 43.49 37.81 53.35Tent (steps=10) N/A 45.41 31.44 32.32 42.54 37.65 27.79 33.12 46.13 42.31 43.51 39.48 52.01 40.32 33.64 40.37EATA N/A 37.24 33.15 52.58 84.10 37.69 32.39 52.49 43.77 42.48 43.34 41.55 83.32 36.67 31.47 52.55CoTTA N/A 37.39 32.54 52.25 82.12 37.65 33.12 52.90 43.69 42.14 43.21 42.32 81.98 37.99 33.52 53.23SAR (steps=1) N/A 36.18 34.43 52.46 83.96 39.72 36.53 52.37 43.64 43.04 44.20 41.93 85.09 40.70 36.44 53.02SAR (steps=10) N/A 35.32 34.10 51.66 82.12 41.49 33.94 53.08 43.56 42.05 42.53 41.16 85.09 37.58 33.12 52.01 SimATTA (B ≤300) N/A 62.61 65.08 74.38 99.93 69.50 66.67 77.34 62.33 69.33 73.20 71.93 99.93 69.43 72.46 80.39SimATTA (B ≤500) N/A 63.52 68.01 76.13 99.51 70.56 73.10 78.35 62.29 70.45 73.50 72.02 99.43 70.29 72.55 80.18 the source domain and arrange the samples from the other domains to form the test data stream. For DomainBed datasets, we adopt two stream order strategies. The first order uses a domain-wise data stream, i.e., we finish streaming samples from one domain before starting streaming another domain. The second order is random, where we shuffle samples from all target domains and partition them into four splits 1, 2, 3, and 4, as shown in Tab. 2. More dataset details are provided in Appx. G.1. Baselines. For baseline models, we start with the common source-only models, which either utilize pre-calculated batch statistics (BN w/o adapt) or test batch statistics (BN w/ adapt). For comparison with other TTA methods, we consider four state-of-the-art TTA methods: Tent (Wang et al., 2021), EATA (Niu et al., 2022), CoTTA (Wang et al., 2022a), and SAR (Niu et al., 2023). The three of them except Tent provide extra design to avoid CF. To compare with ADA methods, we select algorithms that are partially comparable with our method, i.e., they should be efficient (e.g., uncertainty-based) without the requirements of additional networks. Therefore, we adopt random, entropy (Wang and Shang, 2014), k-means (Krishna and Murty, 1999), and CLUE (Prabhu et al., 2021) for comparisons. Settings. For TTA, we compare with general TTA baselines in streaming adaptation using the two aforementioned data streaming orders, domain-wise and random. We choose P in PACS and C in VLCS as source domains. For domain-wise data stream, we use order A → C → S for PACS and L → S → V for VLCS. We report the real-time adaptation accuracy results for each split of the data stream, as well as the accuracy on each domain after all adaptations through the data stream (under “post-adaptation” columns). Enhanced TTA is built on TTA with access to extra random sample labels. TTA baselines are further fine-tuned with these random samples. To further improve enhanced TTA, we use long-term label storage and larger unlabeled sample pools. To its extreme where the model can access the whole test set samples, the setting becomes similar to ADA, thus we also use ADA methods for comparisons. ADA baselines have access to all samples in the pre-collected target datasets but not source domain data, whereas our method can only access the streaming test data. 5.1 T HE FAILURE OF TEST-TIME ADAPTATION The failure of TTA methods on domain distribution shifts is one of the main motivations of the ATTA setting. As shown in Tab. 2, TTA methods cannot consistently outperform eventhe simplest baseline \"BN w/ adapt\" which uses test time batch statistics to make predictions, evidencing that current TTA methods cannot solve domain distribution shifts (RQ1). Additionally, Tent (step=10) exhibits significant CF issues, where \"step=10\" indicates 10 test-time training updates, i.e., 10 gradient backpropagation iterations. This failure of TTA methods necessitates the position of ATTA. In contrast, SimATTA, with a budget B less than 300, outperforms all TTA methods on both source and target domains by substantial margins. Moreover, compared to the source-only baselines, our method improves the target domain performances significantly with negligible source performance loss, showing that ATTA is a more practically effective setting for real-world distribution shifts. 5.2 E FFICIENCY & ENHANCED TTA SETTING COMPARISONS To validate the efficiency of ATTA and broaden the dataset choice, we conduct this study on Tiny- ImageNet-C which, though does not focus on domain shifts, is much larger than PACS and VLCS. we 8Published as a conference paper at ICLR 2024 Table 3: Comparisons with Enhanced TTA on Tiny-ImageNet-C (severity level 5). Tiny-ImageNet-C Time (sec)Noise Blur Weather Digital Gauss. Shot Impul. Defoc. Glass Motion Zoom Snow Frost Fog Contr. Elastic Pixel JPEG Avg. Tent (step=1) 68.83 9.32 11.97 8.86 10.43 7.00 12.20 14.34 13.58 15.46 13.55 3.99 13.31 17.79 18.61 12.17Tent (step=10) 426.90 0.86 0.63 0.52 0.52 0.55 0.54 0.50 0.50 0.50 0.50 0.50 0.50 0.50 0.50 0.54EATA 93.14 3.98 3.33 2.18 4.80 2.37 11.02 11.41 14.06 15.26 9.65 1.36 9.88 14.24 12.12 8.26CoTTA 538.78 5.63 7.12 6.31 8.05 5.74 9.68 10.55 11.75 12.00 11.15 4.17 5.35 7.82 8.90 8.16SAR (step=1) 113.76 8.90 3.11 1.67 1.55 1.47 1.35 1.19 1.03 1.04 0.93 0.83 1.00 0.74 0.77 1.83SAR (step=10) 774.11 2.67 3.26 2.38 1.64 1.85 2.49 3.16 3.81 2.72 3.12 0.81 3.47 4.04 1.76 2.66 SimATTA (step=10) 736.289.68 19.40 12.14 30.28 17.03 42.36 43.10 31.96 40.08 29.243.21 34.56 45.24 45.74 28.86 enhance the TTA setting by fine-tuning baselines on randomly selected labeled samples. Specifically, the classifier of ResNet18-BN is pre-adapted to the brightness corruption (source domain) before test-time adapting. SimATTA’s label budget is around 4,000, while all other TTA methods have budget 4,500 for randomly selected labeled samples. The data stream order is shown in Tab. 3. Time is measured across all corrupted images in the Noise and Blur noise types, and the values represent the average time cost for adapting 10,000 images. The results clearly evidence the efficiency of ATTA (RQ2), while substantially outperforming all enhanced TTA baselines. Simply accessing labeled samples cannot benefit TTA methods to match ATTA. With 10 training updates (step=10) for each batch, FTTA methods would suffer from severe CF problem. In contrast, ATTA covers a statistically significant distribution, achieving stronger performances with 10 training updates or even more steps till approximate convergences. In fact, longer training on Tent (step=10) leads to worse results (compared to step=1), which further motivates the design of the ATTA setting. The reason for higher absolute time cost in Tab. 3 is due to differences in training steps. In this experiment, SimATTA has a training step of 10, and similar time cost as SAR per step. Note that if the enhanced TTA setting is further improved to maintain distributions with a balanced CF mitigation strategy and an incremental clustering design, the design approaches ATTA. Specifically, we compare SimATTA with its variants as the ablation study (RQ3) in Appx. I.2. 5.3 C OMPARISONS TO A STRONGER SETTING : ACTIVE DOMAIN ADAPTATION Table 4: Comparisons to ADA baselines. Source domains are denoted as \"(S)\". Results are average accuracies (with standard deviations). PACS P (S) A C S Random (B= 300) 96.21 (0.80) 81.19 (0.48) 80.75 (1.27) 84.34 (0.18)Entropy (B= 300) 96.31 (0.64)88.00 (1.46)82.48 (1.71) 80.55 (1.01)Kmeans (B= 300) 93.71 (1.50) 79.31 (4.01) 79.64 (1.44) 83.92 (0.65)CLUE (B= 300) 96.69 (0.17)83.97 (0.57)84.77 (0.88) 86.91 (0.26) SimATTA (B ≤300) 98.89 (0.09)84.69 (0.22)83.09 (0.83)83.76 (2.24) VLCS C (S) L S V Random (B= 300) 96.21 (1.65) 66.67 (1.70) 70.72 (0.30) 72.14 (1.71)Entropy (B= 300) 97.74 (1.56) 69.29 (2.26)69.25 (4.77) 75.26 (3.07)Kmeans (B= 300) 98.61 (0.27)67.57 (1.64)70.77 (0.01)74.49 (0.97)CLUE (B= 300) 85.70 (10.09) 65.29 (1.49) 69.42 (2.64) 69.09 (6.05) SimATTA (B ≤300) 99.93 (0.00) 69.47 (0.03)69.57 (2.90)78.87 (1.53) In addtion to the above comparisons with (en- hanced) TTA, which necessitate the requirement of extra information in the ATTA setting, we com- pare ATTA with a stronger setting Active Domain Adaptation (ADA) to demonstrate another supe- riority of ATTA, i.e., weaker requirements for comparable performances (RQ4). ADA baselines are able to choose the global best active samples, while ATTA has to choose samples from a small sample buffer (e.g., a size of 100) and discard the rest. Tab. 4 presents the post-adaptation model per- formance results. All ADA results are averaged from 3 random runs, while ATTA results are the post-adaptation performances averaged from the two data stream orders. As can be observed, despite the lack of a pre-collected target dataset, SimATTA produces better or competitive results against ADA methods. Moreover, without source data access, SimATTA’s design for CF allows it to maintain superior source domain performances over ADA methods. Further experimental studies including the Office-Home dataset are provided in Appx. I. In conclusion, the significant improvement compared to weaker settings (TTA, enhanced TTA) and the comparable performance with the stronger setting, ADA, rendering ATTA a setting that is as efficient as TTA and as effective as ADA. This implies its potential is worthy of future explorations. 6 C ONCLUSION AND DISCUSSION There’s no denying that OOD generalization can be extremely challenging without certain information, often relying on various assumptions easily compromised by different circumstances. Thus, it’s prudent to seek methods to achieve significant improvements with minimal cost, e.g., DG methods leveraging environment partitions and ATTA methods using budgeted annotations. As justified in our theoretical and experimental studies, ATTA stands as a robust approach to achieve real-time OOD generalization. Although SimATTA sets a strong baseline for ATTA, there’s considerable scope for further investigation within the ATTA setting. One potential direction involves developing alternatives to prevent CF in ATTA scenarios. While selective entropy minimization on low-entropy samples has prove to be empirically effective, it relies on the quality of the pre-trained model and training on incorrectly predicted low-entropy samples may reinforce the errors. It might not be cost-effective to expend annotation budgets on low-entropy samples, but correcting them could be a viable alternative solution. We anticipate that our work will spur numerous further explorations in this field. 9Published as a conference paper at ICLR 2024 ACKNOWLEDGMENTS This work was supported in part by National Science Foundation grant IIS-2006861 and National Institutes of Health grant U01AG070112. REFERENCES Hana Ajakan, Pascal Germain, Hugo Larochelle, François Laviolette, and Mario Marchand. Domain- adversarial neural networks. arXiv preprint arXiv:1412.4446, 2014. Lucas Baier, Tim Schlör, Jakob Schöffer, and Niklas Kühl. Detecting concept drift with neural network model uncertainty. In Hawaii International Conference on System Sciences, 2021. URL https://api.semanticscholar.org/CorpusID:235731947. Shai Ben-David, John Blitzer, Koby Crammer, Alex Kulesza, Fernando Pereira, and Jennifer Wortman Vaughan. A theory of learning from different domains. Machine learning, 79:151–175, 2010. Davide Cacciarelli and Murat Kulahci. A survey on online active learning, 2023. Cheng Chen, Quande Liu, Yueming Jin, Qi Dou, and Pheng-Ann Heng. Source-free domain adaptive fundus image segmentation with denoised pseudo-labeling. In Medical Image Computing and Computer Assisted Intervention–MICCAI 2021: 24th International Conference, Strasbourg, France, September 27–October 1, 2021, Proceedings, Part V 24, pages 225–235. Springer, 2021. Li Chen, Tutian Tang, Zhitian Cai, Yang Li, Penghao Wu, Hongyang Li, Jianping Shi, Junchi Yan, and Yu Qiao. Level 2 autonomous driving on a single device: Diving into the devils of openpilot. arXiv preprint arXiv:2206.08176, 2022a. Weijie Chen, Luojun Lin, Shicai Yang, Di Xie, Shiliang Pu, and Yueting Zhuang. Self-supervised noisy label learning for source-free unsupervised domain adaptation. In 2022 IEEE/RSJ In- ternational Conference on Intelligent Robots and Systems (IROS) , pages 10185–10192. IEEE, 2022b. Yining Chen, Colin Wei, Ananya Kumar, and Tengyu Ma. Self-training avoids using spurious features under domain shift. Advances in Neural Information Processing Systems, 33:21061–21071, 2020. David A Cohn, Zoubin Ghahramani, and Michael I Jordan. Active learning with statistical models. Journal of artificial intelligence research, 4:129–145, 1996. Matthias De Lange, Rahaf Aljundi, Marc Masana, Sarah Parisot, Xu Jia, Aleš Leonardis, Gregory Slabaugh, and Tinne Tuytelaars. A continual learning survey: Defying forgetting in classification tasks. IEEE transactions on pattern analysis and machine intelligence, 44(7):3366–3385, 2021. Yuhe Ding, Lijun Sheng, Jian Liang, Aihua Zheng, and Ran He. Proxymix: Proxy-based mixup training with label refinery for source-free domain adaptation. arXiv preprint arXiv:2205.14566, 2022. Cian Eastwood, Ian Mason, Christopher KI Williams, and Bernhard Schölkopf. Source-free adaptation to measurement shift via bottom-up feature restoration. arXiv preprint arXiv:2107.05446, 2021. Jiahao Fan, Hangyu Zhu, Xinyu Jiang, Long Meng, Chen Chen, Cong Fu, Huan Yu, Chenyun Dai, and Wei Chen. Unsupervised domain adaptation by statistics alignment for deep sleep staging networks. IEEE Transactions on Neural Systems and Rehabilitation Engineering, 30:205–216, 2022. Chen Fang, Ye Xu, and Daniel N Rockmore. Unbiased metric learning: On the utilization of multiple datasets and web images for softening bias. In Proceedings of the IEEE International Conference on Computer Vision, pages 1657–1664, 2013. Yuqi Fang, Pew-Thian Yap, Weili Lin, Hongtu Zhu, and Mingxia Liu. Source-free unsupervised domain adaptation: A survey. arXiv preprint arXiv:2301.00265, 2022. Francois Fleuret et al. Uncertainty reduction for model adaptation in semantic segmentation. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 9613–9623, 2021. 10Published as a conference paper at ICLR 2024 Yaroslav Ganin and Victor Lempitsky. Unsupervised domain adaptation by backpropagation. In International conference on machine learning, pages 1180–1189. PMLR, 2015. Yaroslav Ganin, Evgeniya Ustinova, Hana Ajakan, Pascal Germain, Hugo Larochelle, François Laviolette, Mario Marchand, and Victor Lempitsky. Domain-adversarial training of neural networks. The journal of machine learning research, 17(1):2096–2030, 2016. Jakob Gawlikowski, Cedrique Rovile Njieutcheu Tassi, Mohsin Ali, Jongseok Lee, Matthias Humt, Jianxiang Feng, Anna Kruspe, Rudolph Triebel, Peter Jung, Ribana Roscher, et al. A survey of uncertainty in deep neural networks. arXiv preprint arXiv:2107.03342, 2021. Yves Grandvalet and Yoshua Bengio. Semi-supervised learning by entropy minimization. Advances in neural information processing systems, 17, 2004. Shurui Gui, Chaoyue Wang, Qihua Chen, and Dacheng Tao. Featureflow: Robust video interpolation via structure-to-texture generation. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 14004–14013, 2020. Shurui Gui, Xiner Li, Limei Wang, and Shuiwang Ji. GOOD: A graph out-of-distribution benchmark. In Thirty-sixth Conference on Neural Information Processing Systems Datasets and Benchmarks Track, 2022. URL https://openreview.net/forum?id=8hHg-zs_p-h. Ishaan Gulrajani and David Lopez-Paz. In search of lost domain generalization. arXiv preprint arXiv:2007.01434, 2020. Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image recognition. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 770–778, 2016. Dan Hendrycks and Thomas Dietterich. Benchmarking neural network robustness to common corruptions and perturbations. March 2019a. doi: 10.48550/ARXIV .1903.12261. Dan Hendrycks and Thomas Dietterich. Benchmarking neural network robustness to common corruptions and perturbations. arXiv preprint arXiv:1903.12261, 2019b. Steven CH Hoi, Rong Jin, Jianke Zhu, and Michael R Lyu. Semisupervised svm batch mode active learning with applications to image retrieval. ACM Transactions on Information Systems (TOIS), 27(3):1–29, 2009. Yihan Hu, Jiazhi Yang, Li Chen, Keyu Li, Chonghao Sima, Xizhou Zhu, Siqi Chai, Senyao Du, Tianwei Lin, Wenhai Wang, et al. Planning-oriented autonomous driving. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 17853–17862, 2023. Jiaxing Huang, Dayan Guan, Aoran Xiao, and Shijian Lu. Model adaptation: Historical contrastive learning for unsupervised domain adaptation without source data. Advances in Neural Information Processing Systems, 34:3635–3649, 2021. Masato Ishii and Masashi Sugiyama. Source-free domain adaptation via distributional alignment by matching batch normalization statistics. arXiv preprint arXiv:2101.10842, 2021. Yusuke Iwasawa and Yutaka Matsuo. Test-time classifier adjustment module for model-agnostic domain generalization. Advances in Neural Information Processing Systems, 34:2427–2440, 2021. Suyog Dutt Jain and Kristen Grauman. Active image segmentation propagation. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pages 2864–2873, 2016. Guoliang Kang, Lu Jiang, Yi Yang, and Alexander G Hauptmann. Contrastive adaptation network for unsupervised domain adaptation. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 4893–4902, 2019. Ashish Kapoor, Kristen Grauman, Raquel Urtasun, and Trevor Darrell. Active learning with gaussian processes for object categorization. In 2007 IEEE 11th international conference on computer vision, pages 1–8. IEEE, 2007. Neerav Karani, Ertunc Erdil, Krishna Chaitanya, and Ender Konukoglu. Test-time adaptable neural networks for robust medical image segmentation. Medical Image Analysis, 68:101907, 2021. 11Published as a conference paper at ICLR 2024 Ronald Kemker, Marc McClure, Angelina Abitino, Tyler Hayes, and Christopher Kanan. Measuring catastrophic forgetting in neural networks. In Proceedings of the AAAI conference on artificial intelligence, volume 32, 2018. Daniel Kifer, Shai Ben-David, and Johannes Gehrke. Detecting change in data streams. In VLDB, volume 4, pages 180–191. Toronto, Canada, 2004. James Kirkpatrick, Razvan Pascanu, Neil Rabinowitz, Joel Veness, Guillaume Desjardins, Andrei A Rusu, Kieran Milan, John Quan, Tiago Ramalho, Agnieszka Grabska-Barwinska, et al. Overcoming catastrophic forgetting in neural networks. Proceedings of the national academy of sciences, 114 (13):3521–3526, 2017. Pang Wei Koh, Shiori Sagawa, Henrik Marklund, Sang Michael Xie, Marvin Zhang, Akshay Bal- subramani, Weihua Hu, Michihiro Yasunaga, Richard Lanas Phillips, Irena Gao, et al. Wilds: A benchmark of in-the-wild distribution shifts. In International Conference on Machine Learning, pages 5637–5664. PMLR, 2021. Divya Kothandaraman, Sumit Shekhar, Abhilasha Sancheti, Manoj Ghuhan, Tripti Shukla, and Dinesh Manocha. Salad: Source-free active label-agnostic domain adaptation for classification, segmentation and detection. In Proceedings of the IEEE/CVF Winter Conference on Applications of Computer Vision, pages 382–391, 2023. K Krishna and M Narasimha Murty. Genetic k-means algorithm. IEEE Transactions on Systems, Man, and Cybernetics, Part B (Cybernetics), 29(3):433–439, 1999. Alex Krizhevsky, Ilya Sutskever, and Geoffrey E Hinton. Imagenet classification with deep convolu- tional neural networks. Communications of the ACM, 60(6):84–90, 2017. David Krueger, Ethan Caballero, Joern-Henrik Jacobsen, Amy Zhang, Jonathan Binas, Dinghuai Zhang, Remi Le Priol, and Aaron Courville. Out-of-distribution generalization via risk extrap- olation (REx). In International Conference on Machine Learning , pages 5815–5826. PMLR, 2021. Vinod K Kurmi, Venkatesh K Subramanian, and Vinay P Namboodiri. Domain impression: A source data free domain adaptation method. In Proceedings of the IEEE/CVF winter conference on applications of computer vision, pages 615–625, 2021. David D Lewis and Jason Catlett. Heterogeneous uncertainty sampling for supervised learning. In Machine learning proceedings 1994, pages 148–156. Elsevier, 1994. Aodong Li, Alex Boyd, Padhraic Smyth, and Stephan Mandt. Detecting and adapting to irregular distribution shifts in bayesian online learning. Advances in neural information processing systems, 34:6816–6828, 2021a. Da Li, Yongxin Yang, Yi-Zhe Song, and Timothy M Hospedales. Deeper, broader and artier domain generalization. In Proceedings of the IEEE international conference on computer vision, pages 5542–5550, 2017. Rui Li, Qianfen Jiao, Wenming Cao, Hau-San Wong, and Si Wu. Model adaptation: Unsupervised domain adaptation without source data. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 9641–9650, 2020. Xianfeng Li, Weijie Chen, Di Xie, Shicai Yang, Peng Yuan, Shiliang Pu, and Yueting Zhuang. A free lunch for unsupervised domain adaptive object detection without source data. In Proceedings of the AAAI Conference on Artificial Intelligence, volume 35, pages 8474–8481, 2021b. Zhizhong Li and Derek Hoiem. Learning without forgetting. IEEE transactions on pattern analysis and machine intelligence, 40(12):2935–2947, 2017. Jian Liang, Dapeng Hu, Ran He, and Jiashi Feng. Distill and fine-tune: Effective adaptation from a black-box source model. arXiv preprint arXiv:2104.01539, 1(3), 2021. Jian Liang, Dapeng Hu, Jiashi Feng, and Ran He. Dine: Domain adaptation from single and multiple black-box predictors. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 8003–8013, 2022. 12Published as a conference paper at ICLR 2024 Yong Lin, Shengyu Zhu, Lu Tan, and Peng Cui. Zin: When and how to learn invariance without environment partition? Advances in Neural Information Processing Systems, 35:24529–24542, 2022. Xiaofeng Liu, Fangxu Xing, Chao Yang, Georges El Fakhri, and Jonghye Woo. Adapting off-the- shelf source segmenter for target medical image segmentation. In Medical Image Computing and Computer Assisted Intervention–MICCAI 2021: 24th International Conference, Strasbourg, France, September 27–October 1, 2021, Proceedings, Part II 24, pages 549–559. Springer, 2021a. Xinyu Liu and Yixuan Yuan. A source-free domain adaptive polyp detection framework with style diversification flow. IEEE Transactions on Medical Imaging, 41(7):1897–1908, 2022. Yuang Liu, Wei Zhang, Jun Wang, and Jianyong Wang. Data-free knowledge transfer: A survey. arXiv preprint arXiv:2112.15278, 2021b. Yuejiang Liu, Parth Kothari, Bastien Van Delft, Baptiste Bellot-Gurlet, Taylor Mordan, and Alexandre Alahi. Ttt++: When does self-supervised test-time training fail or thrive? Advances in Neural Information Processing Systems, 34:21808–21820, 2021c. Mingsheng Long, Yue Cao, Jianmin Wang, and Michael Jordan. Learning transferable features with deep adaptation networks. In International conference on machine learning, pages 97–105. PMLR, 2015. David Lopez-Paz and Marc’Aurelio Ranzato. Gradient episodic memory for continual learning. Advances in neural information processing systems, 30, 2017. Chaochao Lu, Yuhuai Wu, José Miguel Hernández-Lobato, and Bernhard Schölkopf. Invariant causal representation learning for out-of-distribution generalization. In International Conference on Learning Representations, 2021. Xinhong Ma, Junyu Gao, and Changsheng Xu. Active universal domain adaptation. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 8968–8977, 2021. Haitao Mao, Lun Du, Yujia Zheng, Qiang Fu, Zelin Li, Xu Chen, Shi Han, and Dongmei Zhang. Source free unsupervised graph domain adaptation. arXiv preprint arXiv:2112.00955, 2021. Christoforos Mavrogiannis, Francesca Baldini, Allan Wang, Dapeng Zhao, Pete Trautman, Aaron Steinfeld, and Jean Oh. Core challenges of social robot navigation: A survey. ACM Transactions on Human-Robot Interaction, 12(3):1–39, 2023. Zachary Nado, Shreyas Padhy, D Sculley, Alexander D’Amour, Balaji Lakshminarayanan, and Jasper Snoek. Evaluating prediction-time batch normalization for robustness under covariate shift. arXiv preprint arXiv:2006.10963, 2020. Munan Ning, Donghuan Lu, Dong Wei, Cheng Bian, Chenglang Yuan, Shuang Yu, Kai Ma, and Yefeng Zheng. Multi-anchor active domain adaptation for semantic segmentation. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 9112–9122, 2021. Shuaicheng Niu, Jiaxiang Wu, Yifan Zhang, Yaofo Chen, Shijian Zheng, Peilin Zhao, and Mingkui Tan. Efficient test-time model adaptation without forgetting. In International conference on machine learning, pages 16888–16905. PMLR, 2022. Shuaicheng Niu, Jiaxiang Wu, Yifan Zhang, Zhiquan Wen, Yaofo Chen, Peilin Zhao, and Mingkui Tan. Towards stable test-time adaptation in dynamic wild world. InThe Eleventh International Con- ference on Learning Representations, 2023. URL https://openreview.net/forum?id=g2YraF75Tj. Sinno Jialin Pan, Ivor W Tsang, James T Kwok, and Qiang Yang. Domain adaptation via transfer component analysis. IEEE transactions on neural networks, 22(2):199–210, 2010. Adam Paszke, Sam Gross, Francisco Massa, Adam Lerer, James Bradbury, Gregory Chanan, Trevor Killeen, Zeming Lin, Natalia Gimelshein, Luca Antiga, et al. Pytorch: An imperative style, high-performance deep learning library. Advances in neural information processing systems, 32, 2019. 13Published as a conference paper at ICLR 2024 Vishal M Patel, Raghuraman Gopalan, Ruonan Li, and Rama Chellappa. Visual domain adaptation: A survey of recent advances. IEEE signal processing magazine, 32(3):53–69, 2015. Judea Pearl. Causality. Cambridge university press, 2009. Fabian Pedregosa, Gaël Varoquaux, Alexandre Gramfort, Vincent Michel, Bertrand Thirion, Olivier Grisel, Mathieu Blondel, Peter Prettenhofer, Ron Weiss, Vincent Dubourg, et al. Scikit-learn: Machine learning in python. the Journal of machine Learning research, 12:2825–2830, 2011. Jonas Peters, Peter Bühlmann, and Nicolai Meinshausen. Causal inference by using invariant prediction: identification and confidence intervals. Journal of the Royal Statistical Society: Series B (Statistical Methodology), 78(5):947–1012, 2016. Jonas Peters, Dominik Janzing, and Bernhard Schölkopf. Elements of causal inference: foundations and learning algorithms. The MIT Press, 2017. Viraj Prabhu, Arjun Chandrasekaran, Kate Saenko, and Judy Hoffman. Active domain adaptation via clustering uncertainty-weighted embeddings. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 8505–8514, 2021. Elan Rosenfeld, Pradeep Ravikumar, and Andrej Risteski. The risks of invariant risk minimization. arXiv preprint arXiv:2010.05761, 2020. Shiori Sagawa, Pang Wei Koh, Tatsunori B Hashimoto, and Percy Liang. Distributionally robust neural networks for group shifts: On the importance of regularization for worst-case generalization. arXiv preprint arXiv:1911.08731, 2019. Shibani Santurkar, Dimitris Tsipras, Andrew Ilyas, and Aleksander Madry. How does batch normal- ization help optimization? Advances in neural information processing systems, 31, 2018. Akanksha Saran, Safoora Yousefi, Akshay Krishnamurthy, John Langford, and Jordan T. Ash. Streaming active learning with deep neural networks. In Andreas Krause, Emma Brunskill, Kyunghyun Cho, Barbara Engelhardt, Sivan Sabato, and Jonathan Scarlett, editors, Proceedings of the 40th International Conference on Machine Learning, volume 202 of Proceedings of Machine Learning Research, pages 30005–30021. PMLR, 23–29 Jul 2023. URL https://proceedings.mlr. press/v202/saran23a.html. Harald Schafer, Eder Santana, Andrew Haden, and Riccardo Biasini. A commute in data: The comma2k19 dataset, 2018. Tobias Scheffer, Christian Decomain, and Stefan Wrobel. Active hidden markov models for informa- tion extraction. In Advances in Intelligent Data Analysis: 4th International Conference, IDA 2001 Cascais, Portugal, September 13–15, 2001 Proceedings 4, pages 309–318. Springer, 2001. Steffen Schneider, Evgenia Rusak, Luisa Eck, Oliver Bringmann, Wieland Brendel, and Matthias Bethge. Improving robustness against common corruptions by covariate shift adaptation. Advances in Neural Information Processing Systems, 33:11539–11551, 2020. Burr Settles. Active learning literature survey. 2009. Karen Simonyan and Andrew Zisserman. Very deep convolutional networks for large-scale image recognition. arXiv preprint arXiv:1409.1556, 2014. Jong-Chyi Su, Yi-Hsuan Tsai, Kihyuk Sohn, Buyu Liu, Subhransu Maji, and Manmohan Chandraker. Active adversarial domain adaptation. In Proceedings of the IEEE/CVF Winter Conference on Applications of Computer Vision, pages 739–748, 2020. Baochen Sun and Kate Saenko. Deep coral: Correlation alignment for deep domain adaptation. In European conference on computer vision, pages 443–450. Springer, 2016. Yu Sun, Xiaolong Wang, Zhuang Liu, John Miller, Alexei Efros, and Moritz Hardt. Test-time training with self-supervision for generalization under distribution shifts. In International conference on machine learning, pages 9229–9248. PMLR, 2020. 14Published as a conference paper at ICLR 2024 Yi-Hsuan Tsai, Wei-Chih Hung, Samuel Schulter, Kihyuk Sohn, Ming-Hsuan Yang, and Manmohan Chandraker. Learning to adapt structured output space for semantic segmentation. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 7472–7481, 2018. Eric Tzeng, Judy Hoffman, Trevor Darrell, and Kate Saenko. Simultaneous deep transfer across domains and tasks. In Proceedings of the IEEE international conference on computer vision, pages 4068–4076, 2015. Eric Tzeng, Judy Hoffman, Kate Saenko, and Trevor Darrell. Adversarial discriminative domain adaptation. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 7167–7176, 2017. Hemanth Venkateswara, Jose Eusebio, Shayok Chakraborty, and Sethuraman Panchanathan. Deep hashing network for unsupervised domain adaptation. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 5018–5027, 2017. Sudheendra Vijayanarasimhan and Ashish Kapoor. Visual recognition and detection under bounded computational resources. In 2010 IEEE Computer Society Conference on Computer Vision and Pattern Recognition, pages 1006–1013. IEEE, 2010. Dan Wang and Yi Shang. A new active labeling method for deep learning. In 2014 International joint conference on neural networks (IJCNN), pages 112–119. IEEE, 2014. Dequan Wang, Evan Shelhamer, Shaoteng Liu, Bruno Olshausen, and Trevor Darrell. Tent: Fully test- time adaptation by entropy minimization. InInternational Conference on Learning Representations, 2021. URL https://openreview.net/forum?id=uXl3bZLkr3c. Mei Wang and Weihong Deng. Deep visual domain adaptation: A survey. Neurocomputing, 312: 135–153, 2018. Qin Wang, Olga Fink, Luc Van Gool, and Dengxin Dai. Continual test-time domain adaptation. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 7201–7211, 2022a. Rui Wang, Zuxuan Wu, Zejia Weng, Jingjing Chen, Guo-Jun Qi, and Yu-Gang Jiang. Cross-domain contrastive learning for unsupervised domain adaptation. IEEE Transactions on Multimedia , 2022b. Garrett Wilson and Diane J Cook. A survey of unsupervised deep domain adaptation. ACM Transactions on Intelligent Systems and Technology (TIST), 11(5):1–46, 2020. Binhui Xie, Longhui Yuan, Shuang Li, Chi Harold Liu, Xinjing Cheng, and Guoren Wang. Active learning for domain adaptation: An energy-based approach. InProceedings of the AAAI Conference on Artificial Intelligence, volume 36, pages 8708–8716, 2022. Zhao Xu, Kai Yu, V olker Tresp, Xiaowei Xu, and Jizhi Wang. Representative sampling for text classification using support vector machines. In Advances in Information Retrieval: 25th European Conference on IR Research, ECIR 2003, Pisa, Italy, April 14–16, 2003. Proceedings 25, pages 393–407. Springer, 2003. Baoyao Yang, Hao-Wei Yeh, Tatsuya Harada, and Pong C Yuen. Model-induced generalization error bound for information-theoretic representation learning in source-data-free unsupervised domain adaptation. IEEE Transactions on Image Processing, 31:419–432, 2021a. Guanglei Yang, Hao Tang, Zhun Zhong, Mingli Ding, Ling Shao, Nicu Sebe, and Elisa Ricci. Transformer-based source-free domain adaptation. arXiv preprint arXiv:2105.14138, 2021b. Jianfei Yang, Xiangyu Peng, Kai Wang, Zheng Zhu, Jiashi Feng, Lihua Xie, and Yang You. Divide to adapt: Mitigating confirmation bias for domain adaptation of black-box predictors. arXiv preprint arXiv:2205.14467, 2022. H Yao, Yuhong Guo, and Chunsheng Yang. Source-free unsupervised domain adaptation with surrogate data generation. In Proceedings of NeurIPS 2021 Workshop on Distribution Shifts: Connecting Methods and Applications, 2021. 15Published as a conference paper at ICLR 2024 Hao-Wei Yeh, Baoyao Yang, Pong C Yuen, and Tatsuya Harada. Sofa: Source-data-free feature alignment for unsupervised domain adaptation. InProceedings of the IEEE/CVF Winter Conference on Applications of Computer Vision, pages 474–483, 2021. Fuming You, Jingjing Li, and Zhou Zhao. Test-time batch statistics calibration for covariate shift. arXiv preprint arXiv:2110.04065, 2021. Hu Yu, Jie Huang, Yajing Liu, Qi Zhu, Man Zhou, and Feng Zhao. Source-free domain adaptation for real-world image dehazing. In Proceedings of the 30th ACM International Conference on Multimedia, pages 6645–6654, 2022. Haojian Zhang, Yabin Zhang, Kui Jia, and Lei Zhang. Unsupervised domain adaptation of black-box source models. arXiv preprint arXiv:2101.02839, 2021. Marvin Zhang, Sergey Levine, and Chelsea Finn. Memo: Test time robustness via adaptation and augmentation. Advances in Neural Information Processing Systems, 35:38629–38642, 2022a. Yifan Zhang, Xue Wang, Kexin Jin, Kun Yuan, Zhang Zhang, Liang Wang, Rong Jin, and Tieniu Tan. Adanpc: Exploring non-parametric classifier for test-time adaptation. In International Conference on Machine Learning, pages 41647–41676. PMLR, 2023. Yizhe Zhang, Shubhankar Borse, Hong Cai, and Fatih Porikli. Auxadapt: Stable and efficient test-time adaptation for temporally consistent video semantic segmentation. In Proceedings of the IEEE/CVF Winter Conference on Applications of Computer Vision, pages 2339–2348, 2022b. Bowen Zhao, Chen Chen, and Shu-Tao Xia. Delta: degradation-free fully test-time adaptation. arXiv preprint arXiv:2301.13018, 2023a. Hao Zhao, Yuejiang Liu, Alexandre Alahi, and Tao Lin. On pitfalls of test-time adaptation. In International Conference on Machine Learning (ICML), 2023b. Chunting Zhou, Xuezhe Ma, Paul Michel, and Graham Neubig. Examining and combating spurious features under distribution shift. In International Conference on Machine Learning, pages 12857– 12867. PMLR, 2021. 16Published as a conference paper at ICLR 2024 Active Test-Time Adaptation: Foundational Analyses and An Algorithm Supplementary Material A B ROADER IMPACTS The field of domain generalization primarily concentrates on enhancing a model’s generalization abilities by preparing it thoroughly before deployment. However, it is equally important for deep learning applications to have the capacity for real-time adaptation, as no amount of preparation can account for all possible scenarios. Consequently, domain generalization and test-time adaptation are complementary strategies: the former is more weighty and extensive, while the latter is more agile, lightweight and privacy-friendly. This work delves into the development of a real-time model adaptation strategy that can be applied to any pre-trained models, including large language models, to enhance their adaptive capabilities. Our research does not involve any human subjects or dataset releases, nor does it raise any ethical concerns. Since this work does not directly tie to specific applications, we do not foresee any immediate negative societal impacts. Nonetheless, we acknowledge that any technological advancement may carry potential risks, and we encourage the continued assessment of the broader impacts of real-time adaptation methodologies in various contexts. B FAQ & D ISCUSSIONS To facilitate the reviewing process, we summarize the answers to the questions that arose during the discussion of an earlier version of this paper. The major updates of this version are reorganized theoretical studies, incremental clustering details, experimental reorganization, and additional datasets and settings . We include more related field comparisons to distinguish different settings. We also cover the position of this paper in literature and the main claims of this paper. Finally, we will frankly acknowledge the limitations of this paper, explain and justify the scope of coverage, and provide possible future directions. Q1: What is the relationship between the proposed ATTA protocol and stream based active learning (Saran et al., 2023)? A: We would like to discuss the difference between our work and the referenced work. 1. Real-time Training Distinction: Saran et al. (2023) doesn’t operate in real-time capacity. This is evident from their experiments, where their model is trained only after completing a round. In contrast, our work involves training the model post each batch. This positions Saran et al. (2023)’s work as an intrinsic active learning technique, while our approach leans towards TTA methods. 2. Continual Training Nuance: Following the point above, Saran et al. (2023) stands out of the scope of continual training. As they mentioned ‘each time new data are acquired, the ResNet is reset to the ImageNet pre-trained weights before being updated‘, Saran et al. (2023) starts afresh with each iteration and is out of scope for CF discussions. Contrarily, our model is continuously trained on varying distributions, compelling us to address the CF issue while preserving advantages derived from various stored distributions. 3. Comparative Complexity: Given the aforementioned distinctions, it’s evident that our task presents a greater challenge compared to theirs. In addition, we have included comparisons with stronger active learning settings in Sec. 5.3. Q2: What are the insights from the theoretically foundational analysis? A: 1. It sets a well-defined formulation and grounded theoretical framework for the ATTA setting. 2. While entropy minimizations can cause CF, balancing the learning rate and number of high/low entropy samples is conversely the key solution to both distribution shifts and 17Published as a conference paper at ICLR 2024 CF by corresponding benefits. Though adding low-entropy data is intuitive, it is crucial in that this simple operation can make methods either too conservative or too aggressive without the correct balancing conditions. 3. The studies in Sec. 3.1 directly present a feasible and guaranteed solution for imple- menting ATTA to tackle shifts while avoiding CF. The aligned empirical validations of Sec. 3.2 also instruct the implementation of SimATTA. Q3: In test-time adaptation, one important issue is that the number of testing samples in a batch may be small, which means the sample size m will also be very small. May it affect the theorem and make them become very loose? A: We consider this issue jointly from theoretical and empirical validations. 1. It is true that the theoretical bounds can be loose given a small size of m unlabeled test samples. This situation of the error bound is mathematically ascribed to the quotient between the VC-dimension d of the hypothesis class and m. Under the VC-dimension theory, the ResNet18 model we adopt should have d ≫ m. However, practically we perform fine-tuning on pre-trained models instead of training from scratch, which significantly reduces the scale of parameter update. In this case, an assumption can be established that fine-tuning a model is roughly equivalent to learning a model with a relatively small d (Appx. H). This assumption is potentially underpinned by the empirical alignment of our validation experiments with the theoretical framework (Fig. 1). To this end, experiments indicate thatd and m are practically of similar scale for our settings. This prevents our theoretical bounds from being very loose and meaningless in reality. 2. Regarding cases that our assumption does not apply, this issue would appear inevitable, since it is rigorously inherent in the estimation error of our streaming and varying test distributions. The distribution of a test stream can be hardly monitored when only a limited batch is allowed, which we consider as a limitation of TTA settings. Moreover, this issue directly implies the necessity of using a buffer for unlabeled samples. A good practice is to maintain a relatively comparable sample buffer scale. Q4: What distribution shifts can ATTA solve? A: We would like to follow (but not limited to) the work (Zhao et al., 2023b) to discuss the distribution shifts ATTA can solve. 1. As elucidated in Sec. 3.1 and Sec. 5, ATTA can solve domain generalization shifts. Domain generalization shifts include complex shifts on the joint data distribution P(X, Y), given X as the covariates and Y as the label variable. Since P(X, Y) = P(X)P(Y |X), ATTA can handle covariate shift (P(X)), label shift (P(Y )), and conditional shift (P(Y |X)). The shifts on both covariate and conditional distributions can cover the shift on labels, but they (covariate + conditional shifts) are more complicated than pure label shifts, where only the marginal label distribution changes while the conditional distribution remains. Note that the conditional shifts are generally caused by spurious correlations, where the independent causal mechanism assumption (Pearl, 2009) holds or no concept drifts exist. 2. In our framework, the distribution support of X at different time steps can be different, but we don’t cover the situation where the support of Y changes, i.e., class-incremental problems. Q5: It is unclear how many samples are selected in each minibatch of testing samples. How the total budget is distributed across the whole testing data stream? A: The number of selected samples for each minibatch is decided jointly by the incremental clustering and the cluster centroid number NC (t). Intuitively, this sample selection is a dynamic process, with NC (t) restricting the budget and incremental clustering performing sample selection. For each batch, we increase applicable clustering centroids as a maximum limit, while the exact number of the selected samples is given by the incremental clustering by how many clusters are located in the scope of new distributions. e.g., if the incoming batch does not introduce new data distributions, then we select zero samples even with increased NC (t). In contrast, if the incoming batch contains data located in multiple new distributions, the incremental clustering tends to select more samples than the NC (t) limit, thus forcing to merging of multiple previous clusters into one new cluster. 18Published as a conference paper at ICLR 2024 The incremental clustering is detailed in Sec. 4.2, and NC (t) is naively increased by a constant hyper-parameter k. Therefore, the budget is adaptively distributed according to the data streaming distribution with budgets controlled by k, which is also the reason why we compare methods under a budget limit. Q6: Could compared methods have access to a few ground-truth labels as well? Making other algorithms be able to use the same amount of ground-truth labels randomly will produce fairer comparisons. A: 1. The enhanced TTA setting is exactly the setup we provide to produce fairer comparisons. See Tab. 3 and Tab. 5 for comparison results. 2. ATTA also compares to a stronger setting ADA which can access the whole test datasets multiple times. Table 5: The table demonstrates the comparisons on PACS where all enhanced TTA baselines have 300 budgets to randomly select labeled samples. The training steps of these labeled samples are the same as the original TTA method training steps. For accumulated sample selection, please refer to our ablation studies. Method Domain-wise data stream A VG Random data stream A VG P→ →A→ →C→ →S P A C S 1 2 3 4 P A C S Source onlyBN w/o adapt 99.70 59.38 28.03 42.91 99.70 59.38 28.03 42.91 43.44 43.44 43.44 43.44 99.70 59.38 28.03 42.91BN w/ adapt 98.74 68.07 64.85 54.57 98.74 68.07 64.85 54.57 62.50 62.50 62.50 62.50 98.74 68.07 64.85 54.57 TTA Tent (steps=1) N/A 70.07 68.43 64.42 97.72 74.17 72.61 68.92 61.20 62.36 66.59 67.32 98.14 74.37 70.26 66.07Tent (steps=10) N/A 76.27 63.78 49.35 59.46 38.62 48.46 55.03 56.20 53.22 52.55 55.55 58.32 47.56 60.75 58.00EATA N/A 69.53 66.94 61.42 98.56 69.38 66.60 64.83 60.34 59.81 64.38 65.02 98.68 73.78 68.30 59.74CoTTA N/A 66.55 63.14 59.91 90.12 61.67 66.68 67.68 57.26 57.36 63.46 65.64 92.22 71.53 70.44 62.41SAR (steps=1) N/A 66.60 63.78 50.34 98.38 67.87 64.04 49.48 57.21 56.06 56.78 57.14 98.38 68.80 64.59 53.02SAR (steps=10) N/A 69.09 66.55 49.07 96.23 62.50 59.34 46.53 49.76 52.74 48.51 49.06 95.39 57.13 54.61 38.76 Ours (B ≤300) N/A 76.86 70.90 75.39 98.80 84.47 82.25 81.52 69.47 76.49 82.45 82.22 98.98 84.91 83.92 86.00 Q7: What is the position of ATTA? A: Comparisons with different settings are challenging. In this work, the design of our experiments (Sec. 5) is to overcome this challenge by comparing both weaker settings and stronger settings. While the significant performance over weaker settings renders the necessity of extra information, the comparable performance with stronger settings provides the potential to relax restricted requirements. Intuitively, ATTA is the most cost-effective option in the consideration of both efficiency and effectiveness. We further provide the following ATTA summary: ATTA, which incorporates active learning in FTTA, is the light, real-time, source-free, widely applicable setting to achieve high generalization performances for test-time adaptation. 1. Necessity: From the causality perspective, new information is necessary (Lin et al., 2022; Pearl, 2009; Peters et al., 2017) to attain generalizable over distribution shifts which are insurmountable within the current TTA framework. 2. Effectiveness: Compared to FTTA methods, ATTA produces substantially better perfor- mances, on-par with the costly active domain adaptation (ADA) methods as shown in Table 3 in the paper. 3. Efficiency: Relative to ADA methods, ATTA possesses superior efficiency, similar to general FTTA methods, as shown in Tab. 3. 4. Applicability: ATTA is a model-agnostic setting. (1) Compared to domain generalization methods, ATTA do not require re-training and has the potential to apply to any pre-trained models. One interesting future direction is designing ATTA methods for large language models (LLMs), where re-trainings are extremely expensive and source data may be in- accessible. (2) Compared to FTTA methods, ATTA can protect model parameters from corrupting while learning new distributions by fine-tuning pre-trained models, rendering it more feasible and practical. In comparison with existing works, ATTA is motivated to mitigate the limitations of previous settings: 1. FTTA: Limited generalization performance. 19Published as a conference paper at ICLR 2024 2. TTT: Not source-free; limited generalization performance. 3. ADA & domain adaptation/generalization: Expensive re-trainings; limited applicability to pre-trained models. 4. Online active learning: It does not maintain and protect adaptation performances for multiple distributions in one model and does not consider the CF problem. Q8: What is the potential practical utility of ATTA? A: 1. Empirically, our method can generally finish a round of sample selection/training of 100 frames in 5s, i.e., 20 frames per sec, which is more than enough to handle multiple practical situations. Experiments on time complexity are provided in Tab. 3, where SimATTA has comparable time efficiency. 2. As a case analysis, the autopilot system (Hu et al., 2023; Chen et al., 2022a) presents an application scenario requiring high-speed low-latency adaptations, while these adaptations are largely underexplored. When entering an unknown environment, e.g., a construction section, a system of ATTA setting can require the driver to take over the wheel. During the period of manual operation when the driver is handling the wheel, steering signals are generated, and the in-car system quickly adaptations. The system doesn’t need to record 60 frames per second, since only the key steering operations and the corresponding dash cam frames are necessary, which can be handled by ATTA algorithms processing at 20 frames per sec. In this case, the human annotations are necessary and indirect. ATTA makes use of this information and adapts in the short term instead of collecting videos and having a long-round fine-tuning (Schafer et al., 2018). 3. In addition, many scenarios applicable for ATTA are less speed-demanding than the case above. One example is a personalized chatbot that subtly prompts and gathers user labels during user interaction. In a home decoration setting, applications can request that users scan a few crucial areas to ensure effective adaptation. Social robots (Mavrogiannis et al., 2023), e.g., vacuum robots, often require users to label critical obstacles they’ve encountered. 4. Compared with ADA, ATTA stands out as the tailored solution for the above scenarios. It does not require intensive retraining or server-dependent fine-tuning, offering both speed and computational efficiency. Meanwhile, akin to other TTA methods, ATTA also ensures user privacy. While it might marginally exceed the cost of standard TTA methods, the superior generalization ability makes it a compelling choice and justifies the additional expense. Q9: What can be covered by this paper? A: This paper endeavors to establish the foundational framework for a novel setting referred to as ATTA. We target (1) positioning the ATTA setting, (2) solving the two major and basic challenges of ATTA,i.e., the mitigation of distribution shifts and the avoidance of catastrophic forgetting (CF). We achieve the first goal by building the problem formulation and analyses, and further providing extensive qualitative and well-organized experimental comparisons with TTA, enhanced TTA, and ADA settings. These efforts position ATTA as the most cost-effective option between TTA and ADA, where ATTA inherits the efficiency of TTA and the effectiveness of ADA. With our theoretical analyses and the consistent algorithm design, we validate the success of our second goal through significant empirical performances. Q10: What are not covered by this paper? A: Constructing a new setting involves multifaceted complexities. Although there are various potential applications discussed above including scaling this setting up for large models and datasets, we cannot cover them in this single piece of work. There are three main reasons. First, the topics covered by a single paper are limited. Formally establishing ATTA setting and addressing its major challenges of ATTA takes precedence over exploring practical applications. Secondly, given the interrelations between ATTA and other settings, our experimental investigations are predominantly comparative, utilizing the most representative datasets from TTA and domain adaptation to showcase persuasive results. Thirdly, many practical applications necessitate task-specific configurations, rendering them unsuitable for establishing a universal learning setting. While the current focus is on laying down the foundational aspects of ATTA, the exploration of more specialized applications remains a prospective avenue for future work in the ATTA domain. 20Published as a conference paper at ICLR 2024 C R ELATED WORKS The development of deep learning witnesses various applications (He et al., 2016; Gui et al., 2020). To tackle OOD problem, various domain generalization works emerge (Krueger et al., 2021; Sagawa et al., 2019). C.1 U NSUPERVISED DOMAIN ADAPTATION Unsupervised Domain Adaptation (UDA) (Pan et al., 2010; Patel et al., 2015; Wilson and Cook, 2020; Wang and Deng, 2018) aims at mitigating distribution shifts between a source domain and a target domain, given labeled source domain samples and unlabeled target samples. UDA methods generally rely on feature alignment techniques to eliminate distribution shifts by aligning feature distributions between source and target domains. Typical feature alignment techniques include discrepancy minimization (Long et al., 2015; Sun and Saenko, 2016; Kang et al., 2019) and adversarial training (Ganin and Lempitsky, 2015; Tsai et al., 2018; Ajakan et al., 2014; Ganin et al., 2016; Tzeng et al., 2015; 2017). Nevertheless, alignments are normally not guaranteed to be correct, leading to the alignment distortion problem as noted by Ning et al. (2021). Source-free Unsupervised Domain Adaptation (SFUDA) (Fang et al., 2022; Liu et al., 2021b) algorithms aim to adapt a pre-trained model to unlabeled target domain samples without access to source samples. Based on whether the algorithm can access model parameters, these algorithms are categorized into white-box and black-box methods. White-box SFUDA typically considers data recovery (generation) and fine-tuning methods. The former focuses on recovering source- like data (Ding et al., 2022; Yao et al., 2021), e.g., training a Generative Adversarial Network (GAN) (Kurmi et al., 2021; Li et al., 2020), while the latter employs various techniques (Mao et al., 2021), such as knowledge distillation (Chen et al., 2022b; Liu and Yuan, 2022; Yang et al., 2021b; Yu et al., 2022), statistics-based domain alignment (Ishii and Sugiyama, 2021; Liu et al., 2021a; Fan et al., 2022; Eastwood et al., 2021), contrastive learning (Huang et al., 2021; Wang et al., 2022b), and uncertainty-based adaptation (Gawlikowski et al., 2021; Fleuret et al., 2021; Chen et al., 2021; Li et al., 2021b). Black-box SFUDA cannot access model parameters and often relies on self-supervised knowledge distillation (Liang et al., 2022; 2021), pseudo-label denoising (Zhang et al., 2021; Yang et al., 2022), or generative distribution alignment (Yeh et al., 2021; Yang et al., 2021a). C.2 T EST-TIME ADAPTATION Test-time Adaptation (TTA), especially Fully Test-time Adaptation (FTTA) algorithms (Wang et al., 2021; Iwasawa and Matsuo, 2021; Karani et al., 2021; Nado et al., 2020; Schneider et al., 2020; Wang et al., 2022a; Zhao et al., 2023a; Niu et al., 2022; Zhang et al., 2022a; Niu et al., 2023; You et al., 2021; Zhang et al., 2022b), can be considered as realistic and lightweight methods for domain adaptation. Built upon black-box SFUDA, FTTA algorithms eliminate the requirement of a pre-collected target dataset and the corresponding training phase. Instead, they can only access an unlabeled data stream and apply real-time adaptation and training. In addition to FTTA, Test-time Training (TTT) (Sun et al., 2020; Liu et al., 2021c) often relies on appending the original network with a self-supervised task. TTT methods require retraining on the source dataset to transfer information through the self-supervised task. Although they do not access the source dataset during the test-time adaptation phase, TTT algorithms are not off-the-shelf source-free methods. TTA is a promising and critical direction for real-world applications, but current entropy minimization-based methods can be primarily considered as feature calibrations that require high-quality pseudo-labels. This requirement, however, can be easily violated under larger distribution shifts. Current TTA algorithms, inheriting UDA drawbacks, cannot promise good feature calibration results, which can be detrimental in real-world deployments. For instance, entropy minimization on wrongly predicted target domain samples with relatively low entropy can only exacerbate spurious correla- tions (Chen et al., 2020). Without extra information, this problem may be analogous to applying causal inference without intervened distributions, which is intrinsically unsolvable (Peters et al., 2016; Pearl, 2009). This paper aims to mitigate this issue with minimal labeled target domain samples. To minimize the cost, we tailor active learning techniques for TTA settings. It is worth noting that a recent work AdaNPC (Zhang et al., 2023) is essentially a domain gener- alization method with a TTA phase attached, while our ATTA is built based on the FTTA setting. Specifically, Current FTTA methods and our work cannot access the source domain. In contrast, 21Published as a conference paper at ICLR 2024 AdaNPC accesses source data to build its memory bank, circumventing the catastrophic forgetting problem. Furthermore, AdaNPC requires multiple source domains and training before performing TTA. Thus AdaNPC uses additional information on domain labels and retraining resources for its memory bank, undermining the merits of FTTA. Regarding theoretical bounds, their target domain is bounded by source domain error and model estimations (in big-O expression), while we consider active sample learning and time variables for varying test distributions. C.3 C ONTINUAL DOMAIN ADAPTATION Many domain adaptation methods focus on improving target domain performance, neglecting the performance on the source domain, which leads to the CF problem (Kemker et al., 2018; Kirkpatrick et al., 2017; Li and Hoiem, 2017; Lopez-Paz and Ranzato, 2017; De Lange et al., 2021; Wang et al., 2022a; Niu et al., 2022). This issue arises when a neural network, after being trained on a sequence of domains, experiences a significant degradation in its performance on previously learned domains as it continues to learn new domains. Continual learning, also known as lifelong learning, addresses this problem. Recent continual domain adaptation methods have made significant progress by employing gradient regularization, random parameter restoration, buffer sample mixture, and more. Although the CF problem is proposed in the continual learning field, it can occur in any source-free OOD settings since the degradation caused by CF is attributed to the network’s parameters being updated to optimize performance on new domains, which may interfere with the representations learned for previous domains. C.4 A CTIVE DOMAIN ADAPTATION Active Domain Adaptation (ADA) (Prabhu et al., 2021; Ning et al., 2021; Su et al., 2020; Ma et al., 2021; Xie et al., 2022) extends semi-supervised domain adaptation with active learning strate- gies (Cohn et al., 1996; Settles, 2009), aiming to maximize target domain performance with a limited annotation budget. Therefore, the key challenge of active learning algorithms is selecting the most informative unlabeled data in target domains (Kapoor et al., 2007). Sample selection strategies are of- ten based on uncertainty (Lewis and Catlett, 1994; Scheffer et al., 2001), diversity (Jain and Grauman, 2016; Hoi et al., 2009), representativeness (Xu et al., 2003), expected error minimization (Vijaya- narasimhan and Kapoor, 2010), etc. Among these methods, uncertainty and diversity-based methods are simple and computationally efficient, making them the most suitable choices to tailor for TTA settings. Adapting these strategies is non-trivial because, compared to typical active domain adaptation, our proposed Active Test-time Adaptation (ATTA) setting does not provide access to source data, model parameters, or pre-collected target samples. This requirement demands that our active sample selection algorithm select samples for annotation during data streaming. Consequently, this active sampling selection process is non-regrettable, i.e., we can only meet every sample once in a short period. To avoid possible confusion, compared to the recent Source-free Active Domain Adaptation (SFADA) method SALAD (Kothandaraman et al., 2023), we do not require access to model parameter gradients, training additional neural networks, or pre-collected target datasets. Therefore, our ATTA setting is quite different, much lighter, and more realistic than ADA and SFADA. C.5 A CTIVE ONLINE LEARNING The most related branch of active online learning (AOL) (Cacciarelli and Kulahci, 2023) is active online learning on drifting data stream (Zhou et al., 2021; Baier et al., 2021; Li et al., 2021a). Generally, these methods include two components, namely, detection and adaptation. Compared with ATTA, there are several distinctions. First, this line of studies largely focuses on the distribution shift detection problem, while ATTA focuses on multi-domain adaptations. Second, AOL on drifting data stream aims to detect and adapt to one current distribution in the stream, without considering preserving the adaptation abilities of multiple past distributions by maintaining and fine-tuning the original pre-trained models. In contrast, ATTA’s goal is to achieve the OOD generalization optimums adaptable across multiple source and target distributions, leading to the consideration of CF problems. Third, while AOL requires one-by-one data input and discard, ATTA maintains a buffer for incoming data before selection decisions. This is because ATTA targets maintaining the original model without corrupting and replacing it, such that making statistically meaningful and high-quality decisions is 22Published as a conference paper at ICLR 2024 critical for ATTA. In contrast, AOL allows resetting and retraining new models, whose target is more lean to cost saving and one-by-one manner. D F URTHER THEORETICAL STUDIES In this section, we refine the theoretical studies with supplement analysis and further results. We use the H-divergence and H∆H-distance definitions following (Ben-David et al., 2010). Definition 2 (H-divergence). For a function class H and two distributions D1 and D2 over a domain X, the H-divergence between D1 and D2 is defined as dH(D1, D2) = sup h∈H |Px∼D1 [h(x) = 1] − Px∼D2 [h(x) = 1]|. The H∆H-distance is defined base on H-divergence. We use the H∆H-distance definition follow- ing (Ben-David et al., 2010). Definition 3 (H∆H-distance). For two distributions D1 and D2 over a domain X and a hypothesis class H, the H∆H-distance between D1 and D2 w.r.t. H is defined as dH∆H(D1, D2) = sup h,h′∈H Px∼D1 [h(x) ̸= h′(x)] + Px∼D2 [h(x) ̸= h′(x)]. (9) The H∆H-distance essentially provides a measure to quantify the distribution shift between two distributions. It measures the maximum difference of the disagreement between two hypotheses in H for two distributions, providing a metrics to quantify the distribution shift between D1 and D2. H-divergence and H∆H-distance have the advantage that they can be applied between datasets, i.e., estimated from finite samples. Specifically, let S1, S2 be unlabeled samples of size m sampled from D1 and D2; then we have estimated H∆H-distance ˆdH(S1, S2). This estimation can be bounded based on Theorem 3.4 of Kifer et al. (2004), which we state here for completeness. Theorem 5. Let A be a collection of subsets of some domain measure space, and assume that the VC-dimension is some finite d. Let P1 and P2 be probability distributions over that domain and S1, S2 finite samples of sizes m1, m2 drawn i.i.d. according P1, P2 respectively. Then Pm1+m2 [|ϕA(S1, S2) − ϕA(P1, P2)| > ϵ] ≤ (2m)de−m1ϵ2/16 + (2m)de−m2ϵ2/16, (10) where Pm1+m2 is the m1 + m2’th power of P - the probability that P induces over the choice of samples. Theorem 5 bounds the probability for relativized discrepancy, and its applications in below lemmas and Theorem 1 help us bound the quantified distribution shifts between domains. The probability, according to a distribution D, that an estimated hypothesis h disagrees with the true labeling function g : X → {0, 1} is defined as ϵ(h(t), g) = E(x)∼D[|h(x, t) − g(x)|], which we also refer to as the error or risk ϵ(h(t)). While the source domain dataset is inaccessible under ATTA settings, we consider the existence of the source dataset DS for the purpose of accurate theoretical analysis. Thus, we initialize Dtr(0) as DS, i.e., Dtr(0) = DS. For every time step t, the test and training data can be expressed as Ute(t) and Dtr(t) = DS ∪ Dte(1) ∪ Dte(2) ∪ ··· ∪Dte(t). (11) We use N to denote the total number of samples in Dtr(t) and λ = (λ0, λ1, ··· , λt) to represent the ratio of sample numbers in each component subset. In particular, we have |DS| |Dtr(t)| = λ0, |Dte(1)| |Dtr(t)| = λ1, ··· , |Dte(t)| |Dtr(t)| = λt, (12) where Pt i=0 λi = 1. Therefore, at time step t, the model has been trained on labeled data Dtr(t), which contains t + 1 components consisting of a combination of data from the source domain and multiple test-time domains. For each domain the model encounters, DS, Ute(1), Ute(2), ··· , Ute(t), let ϵj(h(t)) denote the error of hypothesis h at time t on the jth domain. Specifically, ϵ0(h(t)) = ϵS(h(t)) represents the error of h(t) on the source data DS, and ϵj(h(t)) for j ≥ 1 denotes the error of h(t) on test data Ute(j). Our optimization minimizes a convex combination of training error over the labeled samples from all domains. Formally, given the vector w = (w0, w1, ··· , wt) of domain error 23Published as a conference paper at ICLR 2024 weights with Pt j=0 wj = 1 and the sample number from each component Nj = λjN, we minimize the empirical weighted error of h(t) as ˆϵw(h(t)) = tX j=0 wjˆϵj(h(t)) = tX j=0 wj Nj X Nj |h(x, t) − g(x)|. (13) Note that w, λ and N are also functions of t, which we omit for simplicity. We now establish two lemmas as the preliminary for Theorem 1. In the following lemma, we bound the difference between the weighted error ϵw(h(t)) and the domain error ϵj(h(t)). Lemma 6. Let H be a hypothesis space of VC-dimension d. At time step t, let the ATTA data domains be DS, Ute(1), Ute(2), ··· , Ute(t), and Si be unlabeled samples of size m sampled from each of the t + 1 domains respectively. Then for any δ ∈ (0, 1), for every h ∈ Hminimizing ϵw(h(t)) on Dtr(t), we have |ϵw(h(t)) − ϵj(h(t))| ≤ tX i=0,i̸=j wi  1 2 ˆdH∆H(Si, Sj) + 2 s 2d log(2m) + log 2 δ m + γi  , with probability of at least 1 − δ, where γi = minh∈H{ϵi(h(t)) + ϵj(h(t))}. In the following lemma, we provide an upper bound on the difference between the true and empirical weighted errors ϵw(h(t)) and ˆϵw(h(t)). Lemma 7. Let H be a hypothesis class. For Dtr(t) = DS ∪ Dte(1) ∪ ··· ∪Dte(t) at time t, if the total number of samples in Dtr(t) is N, and the ratio of sample numbers in each component is λj, then for any δ ∈ (0, 1) and h ∈ H, with probability of at least 1 − δ, we have P[|ϵw(h(t)) − ˆϵw(h(t))| ≥ϵ] ≤ 2 exp   −2Nϵ2/( tX j=0 w2 j λj ) ! . Thus, as wj deviates from λj, the feasible approximation ˆϵw(h(t)) with a finite number of labeled samples becomes less reliable. The proofs for both lemmas are provided in Appx. E. Building upon the two preceding lemmas, we proceed to derive bounds on the domain errors under the ATTA setting when minimizing the empirical weighted error using the hypothesis h at time t. Lemma 6 bounds the difference between the weighted error ϵw(h(t)) and the domain error ϵj(h(t)), which is majorly influenced by the estimatedH∆H-distance and the quality of discrepancy estimation. During the ATTA process, the streaming test data can form multiple domains and distributions. However, if we consider all data during the test phase as a single test domain,i.e., St i=1 Ute(i), we can simplify Lemma 6 to obtain an upper bound for the test error ϵT as |ϵw(h(t)) − ϵT (h(t))| ≤w0  1 2 ˆdH∆H(S0, ST ) + 2 s 2d log(2m) + log 2 δ m + γ  , (14) where γ = min h∈H{ϵ0(h(t)) + ϵT (h(t))}, and ST is sampled from St i=1 Ute(i). To understand Lamma 7, we need to understand Hoeffding’s Inequality, which we state below as a Proposition for completeness. Proposition 8 (Hoeffding’s Inequality). Let X be a set, D1, . . . , Dt be probability distributions on X, and f1, . . . , ft be real-valued functions on X such that fi : X → [ai, bi] for i = 1, . . . , t. Then for any ϵ >0, P  \f\f\f\f\f 1 t tX i=1 fi(x) − 1 t tX i=1 Ex∼Di[fi(x)] \f\f\f\f\f ≥ ϵ ! ≤ 2 exp   − 2t2ϵ2 Pt i=1(bi − ai)2 ! (15) where E[fi(x)] is the expected value of fi(x). Lamma 7 provides an upper bound on the difference between the true and empirical weighted errors ϵw(h(t)) and ˆϵw(h(t)). Thus, as wj deviates from λj, the feasible approximation ˆϵw(h(t)) with a finite number of labeled samples becomes less reliable. Building upon the two preceding lemmas, we proceed to derive bounds on the domain errors under the ATTA setting when minimizing the empirical weighted error using the hypothesis h at time t. Theorem 1 essentially bounds the performance of ATTA on the source and each test domains. The adaptation performance on a test domain is majorly 24Published as a conference paper at ICLR 2024 bounded by the composition of (labeled) training data, estimated distribution shift, and ideal joint hypothesis performance, which correspond to C, ˆdH∆H(Si, Sj), and γi, respectively. The ideal joint hypothesis error γi gauges the inherent adaptability between domains. If we consider the multiple data distributions during the test phase as a single test domain, i.e., St i=1 Ute(i), Theorem 1 can be reduced into bounds for the source domain error ϵS and test domain error ϵT . With the optimal test/source hypothesis h∗ T (t) = arg min h∈H ϵT (h(t)) and h∗ S(t) = arg minh∈H ϵS(h(t)), |ϵT (ˆh(t)) − ϵT (h∗ T (t))| ≤w0A + s w2 0 λ0 + (1 − w0)2 1 − λ0 B, (16a) |ϵS(ˆh(t)) − ϵS(h∗ S(t))| ≤(1 − w0)A + s w2 0 λ0 + (1 − w0)2 1 − λ0 B, (16b) where the distribution divergence termA = ˆdH∆H(S0, ST )+4 q 2d log(2m)+log 2 δ m +2γ, the empirical gap term B = 2 q d log(2N)−log(δ) 2N , ST is sampled from St i=1 Ute(i), and γ = minh∈H{ϵ0(h(t)) + ϵT (h(t))}. Our learning bounds demonstrates the trade-off between the small amount of budgeted test-time data and the large amount of less relevant source data. Next, we provide an approximation of the condition necessary to achieve optimal adaptation performance, which is calculable from finite samples and can be readily applied in practical ATTA scenarios. Following Eq. (16.a), with approximately B = c1 p d/N, the optimal value w∗ 0 to tighten the test error bound is a function of λ0 and A: w∗ 0 = λ0 − s A2N c2 1d − A2Nλ0(1 − λ0), for λ 0 ≥ 1 − d A2N , (17) where c1 is a constant. Note that λ0 ≥ 1 − d A2N should be the satisfied condition in practical ATTA settings, where the budget is not sufficiently big while the source data amount is relatively large. When the budget is sufficiently large or the source data amount is not sufficiently large compared to the distribution shift A, the optimal w∗ 0 for the test error bound is w∗ 0 = 0, i.e., using no source data since possible error reduction from the data addition is always less than the error increase caused by large divergence between the source data and the test data. Theorem 2 offers a direct theoretical guarantee that ATTA reduces the error bound on test domains in comparison to TTA without the integration of active learning. Following Theorem 1, when no active learning is included during TTA,i.e., w0 = λ0 = 1, the upper boundw0A+ q w2 0 λ0 + (1−w0)2 1−λ0 B ≥ A+B; when enabling ATTA, withw0 = λ0 ̸= 1, we can easily achieve an upper bound w0A + B < A+ B. Therefore, the incorporation of labeled test instances in ATTA theoretically enhances the overall performance across test domains, substantiating the significance of the ATTA setting in addressing distribution shifts. Entropy quantifies the amount of information contained in a probability distribution. In the context of a classification model, lower entropy indicates that the model assigns high probability to one of the classes, suggesting a high level of certainty or confidence in its prediction. When a model assigns low entropy to a sample, this high confidence can be interpreted as the sample being well-aligned or fitting closely with the model’s learned distribution. In other words, the model “recognizes” the sample as being similar to those it was trained on, hence the high confidence in its prediction. While entropy is not a direct measure of distributional distance, it can be used as an indicator of how closely a sample aligns with the model’s learned distribution. This interpretation is more about model confidence and the implied proximity rather than a strict mathematical measure of distributional distance. The pre-trained model is well-trained on abundant source domain data, and thus the model distribution is approximately the source distribution. Selecting low-entropy samples using essentially provides an estimate of sampling from the source dataset. Thus, Dϕ,S(t), based on well-aligned with the model’s learned distribution is an approximation of DS. When we consider the CF problem and feasibly include the source-like dataset Dϕ,S(t) into the ATTA training data in place of the inaccessible DS in Eq. (11), we can also derive bounds on the domain errors under this practical ATTA setting when minimizing the empirical weighted errorϵ′ w(h(t)) using the hypothesis h at time t, similar to Theorem 1. Let H be a hypothesis class of VC-dimension d. At time step t, for ATTA data domainsDϕ,S(t), Ute(1), Ute(2), ··· , Ute(t), Si are unlabeled samples of size m sampled from each of the t + 1 domains respectively. The total number of samples in Dtr(t) is 25Published as a conference paper at ICLR 2024 N and the ratio of sample numbers in each component is λi. If ˆh(t) ∈ Hminimizes the empirical weighted error ˆϵ′ w(h(t)) with the weight vector w on Dtr(t), and h∗ j (t) = arg minh∈H ϵj(h(t)) is the optimal hypothesis on the jth domain, then for any δ ∈ (0, 1), we have ϵj(ˆh(t)) ≤ ϵj(h∗ j (t)) + 2 tX i=0,i̸=j wi  1 2 ˆdH∆H(Si, Sj) + 2 s 2d log(2m) + log 2 δ m + γi   + 2C with probability of at least 1 − δ, where C = r\u0010Pt i=0 w2 i λi \u0011\u0010 d log(2N)−log(δ) 2N \u0011 and γi = minh∈H{ϵi(h(t)) + ϵj(h(t))}. Other derived results following Theorem 1 also apply for this practical ATTA setting. Further empirical validations for our theoretical results are provided in Appx. H. E P ROOFS This section presents comprehensive proofs for all the lemmas, theorems, and corollaries mentioned in this paper, along with the derivation of key intermediate results. Lemma 6. Let H be a hypothesis space of VC-dimension d. At time step t, let the ATTA data domains be DS, Ute(1), Ute(2), ··· , Ute(t), and Si be unlabeled samples of size m sampled from each of the t + 1 domains respectively. Then for any δ ∈ (0, 1), for every h ∈ Hminimizing ϵw(h(t)) on Dtr(t), we have |ϵw(h(t)) − ϵj(h(t))| ≤ tX i=0,i̸=j wi  1 2 ˆdH∆H(Si, Sj) + 2 s 2d log(2m) + log 2 δ m + γi  , with probability of at least 1 − δ, where γi = minh∈H{ϵi(h(t)) + ϵj(h(t))}. Proof. First we prove that given unlabeled samples of size m S1, S2 sampled from two distributions D1 and D2, we have dH∆H(D1, D2) ≤ ˆdH∆H(S1, S2) + 4 s 2d log(2m) + log 2 δ m . (18) We start with Theorem 3.4 of Kifer et al. (2004): Pm1+m2 [|ϕA(S1, S2) − ϕA(P1, P2)| > ϵ] ≤ (2m)de−m1ϵ2/16 + (2m)de−m2ϵ2/16. (19) In Eq. 19, ’d’ is the VC-dimension of a collection of subsets of some domain measure space A, while in our case, d is the VC-dimension of hypothesis space H. Following (Ben-David et al., 2010), the H∆H space is the set of disagreements between every two hypotheses inH, which can be represented as a linear threshold network of depth 2 with 2 hidden units. Therefore, the VC-dimension of H∆H is at most twice the VC-dimension of H, and the VC-dimension of our domain measure space is 2d for Eq. 19 to hold. Given δ ∈ (0, 1), we set the upper bound of the inequality to δ, and solve for ϵ: δ = (2m)2de−m1ϵ2/16 + (2m)2de−m2ϵ2/16. We rewrite the inequality as δ (2m)2d = e−m1ϵ2/16 + e−m2ϵ2/16; taking the logarithm of both sides, we get log δ (2m)2d = −m1 ϵ2 16 + log(1 +e−(m1−m2) ϵ2 16 ). 26Published as a conference paper at ICLR 2024 Assuming m1 = m2 = m and defining a = ϵ2 16 , we have log δ (2m)2d = −ma + log 2; rearranging the equation, we then get ma + log(δ/2) = 2d log(2m). Now, we can solve for a: a = 2d log(2m) + log 2 δ m . Recall that a = ϵ2 16 , so we get: ϵ = 4√a ϵ = 4 s 2d log(2m) + log 2 δ m . With probability of at least 1 − δ, we have |ϕA(S1, S2) − ϕA(P1, P2)| ≤4 s 2d log(2m) + log 2 δ m ; therefore, dH∆H(D1, D2) ≤ ˆdH∆H(S1, S2) + 4 s 2d log(2m) + log 2 δ m . (20) Now we prove Lemma 6. We use the triangle inequality for classification error in the derivation. For the domain error of hypothesis h at time t on the jth domain ϵj(h(t)), given the definition of ϵw(h(t)), |ϵw(h(t)) − ϵj(h(t))| = | tX i=0 wiϵi(h(t)) − ϵj(h(t))| ≤ tX i=0 wi|ϵi(h(t)) − ϵj(h(t))| ≤ tX i=0 wi(|ϵi(h(t)) − ϵi(h(t), h∗ i (t))| + |ϵi(h(t), h∗ i (t)) − ϵj(h(t), h∗ i (t))| + |ϵj(h(t), h∗ i (t)) − ϵj(h(t))|) ≤ tX i=0 wi(ϵi(h∗ i (t)) + |ϵi(h(t), h∗ i (t)) − ϵj(h(t), h∗ i (t))| + ϵj(h∗ i (t))) ≤ tX i=0 wi(γi + |ϵi(h(t), h∗ i (t)) − ϵj(h(t), h∗ i (t))|), where γi = minh∈H{ϵi(h(t)) + ϵj(h(t))}. By the definition of H∆H-distance and our proved Eq. 20, |ϵi(h(t), h∗ i (t)) − ϵj(h(t), h∗ i (t))| ≤sup h,h′∈H |ϵi(h(t), h′(t)) − ϵj(h(t), h′(t))| = sup h,h′∈H Px∼Di[h(x) ̸= h′(x)] + Px∼Dj [h(x) ̸= h′(x)] = 1 2dH∆H(Di, Dj) ≤ 1 2 ˆdH∆H(Si, Sj) + 2 s 2d log(2m) + log 2 δ m , 27Published as a conference paper at ICLR 2024 where Di, Dj denote the ith and jth domain. Therefore, |ϵw(h(t)) − ϵj(h(t))| ≤ tX i=0 wi(γi + |ϵi(h(t), h∗ i (t)) − ϵj(h(t), h∗ i (t))|) ≤ tX i=0 wi(γi + 1 2dH∆H(Di, Dj)) ≤ tX i=0 wi(γi + 1 2 ˆdH∆H(Si, Sj) + 2 s 2d log(2m) + log 2 δ m ). Since ϵi(h(t)) − ϵj(h(t)) = 0 when i = j, we derive |ϵw(h(t)) − ϵj(h(t))| ≤ tX i=0,i̸=j wi  1 2 ˆdH∆H(Si, Sj) + 2 s 2d log(2m) + log 2 δ m + γi  , with probability of at least 1 − δ, where γi = minh∈H{ϵi(h(t)) + ϵj(h(t))}. This completes the proof. Lemma 7. Let H be a hypothesis class. For Dtr(t) = DS ∪ Dte(1) ∪ ··· ∪Dte(t) at time t, if the total number of samples in Dtr(t) is N, and the ratio of sample numbers in each component is λj, then for any δ ∈ (0, 1) and h ∈ H, with probability of at least 1 − δ, we have P[|ϵw(h(t)) − ˆϵw(h(t))| ≥ϵ] ≤ 2 exp   −2Nϵ2/( tX j=0 w2 j λj ) ! . Proof. We apply Hoeffding’s Inequality in our proof: P  \f\f\f\f\f 1 t tX i=1 fi(x) − 1 t tX i=1 Ex∼Di[fi(x)] \f\f\f\f\f ≥ ϵ ! ≤ 2 exp   − 2t2ϵ2 Pt i=1(bi − ai)2 ! . (21) In the jth domain, there are λjN samples. With the true labeling function g(x), for each of the λjN samples x, let there be a real-valued function fi(x) fi(x) = wj λj |h(x, t) − g(x)|, where fi(x) ∈ [0, wj λj ]. Incorporating all the domains, we get ˆϵw(h(t)) = tX j=0 wjˆϵj(h(t)) = tX j=0 wj λjN X λjN |h(x, t) − g(x)| = 1 N tX j=0 λjNX i=1 fi(x), which corresponds to the 1 t Pt i=1 fi(x) part in Hoeffding’s Inequality. Due to the linearity of expectations, we can calculate the sum of expectations as 1 N tX j=0 λjNX i=1 E[fi(x)] = 1 N ( tX j=0 λjN wj λj ϵj(h(t))) = tX j=0 wjϵj(h(t)) = ϵw(h(t)), which corresponds to the 1 t Pt i=1 Ex∼Di[fi(x)] part in Hoeffding’s Inequality. Therefore, we can apply Hoeffding’s Inequality as P[|ϵw(h(t)) − ˆϵw(h(t))| ≥ϵ] ≤ 2 exp   −2N2ϵ2/( NX i=0 range2(fi(x))) ! = 2 exp   −2N2ϵ2/( tX j=0 λjN(wj λj )2) ! = 2 exp   −2Nϵ2/( tX j=0 w2 j λj ) ! . This completes the proof. 28Published as a conference paper at ICLR 2024 Theorem 1. Let H be a hypothesis class of VC-dimension d. At time step t, for ATTA data domains DS, Ute(1), Ute(2), ··· , Ute(t), Si are unlabeled samples of size m sampled from each of the t + 1 domains respectively. The total number of samples in Dtr(t) is N and the ratio of sample numbers in each component is λi. If ˆh(t) ∈ Hminimizes the empirical weighted error ˆϵw(h(t)) with the weight vector w on Dtr(t), and h∗ j (t) = arg minh∈H ϵj(h(t)) is the optimal hypothesis on the jth domain, then for any δ ∈ (0, 1), with probability of at least 1 − δ, we have ϵj(ˆh(t)) ≤ ϵj(h∗ j (t)) + 2 tX i=0,i̸=j wi  1 2 ˆdH∆H(Si, Sj) + 2 s 2d log(2m) + log 2 δ m + γi   + 2C, where C = r\u0010Pt i=0 w2 i λi \u0011\u0010 d log(2N)−log(δ) 2N \u0011 and γi = minh∈H{ϵi(h(t)) + ϵj(h(t))}. For future test domains j = t + k (k >0), assuming k′ = argmink′∈{0,1,...t} dH∆H(D(k′), Ute(t + k)) and min dH∆H (D(k′), Ute(t + k)) ≤ δD, where 0 ≤ δD ≪ +∞, then ∀δ, with probability of at least 1 − δ, we have ϵt+k(ˆh(t)) ≤ ϵt+k(h∗ t+k(t)) + tX i=0 wi  ˆdH∆H(Si, Sk′ ) + 4 s 2d log(2m) + log 2 δ m + δD + 2γi   + 2C. Proof. First we prove that for any δ ∈ (0, 1) and h ∈ H, with probability of at least 1 − δ, we have |ϵw(h(t)) − ˆϵw(h(t))| ≤ vuut  tX i=0 w2 i λi !\u0012d log(2N) − log(δ) 2N \u0013 . (22) We apply Theorem 3.2 of Kifer et al. (2004) and Lemma 7, P[|ϵw(h(t)) − ˆϵw(h(t))| ≥ϵ] ≤ (2N)d exp   −2Nϵ2/( tX j=0 w2 j λj ) ! . Given δ ∈ (0, 1), we set the upper bound of the inequality to δ, and solve for ϵ: δ = (2N)d exp   −2Nϵ2/( tX j=0 w2 j λj ) ! . We rewrite the inequality as δ (2N)d = e −2Nϵ2/(Pt j=0 w2 j λj ) , taking the logarithm of both sides, we get log δ (2N)d = −2Nϵ2/( tX j=0 w2 j λj ). Rearranging the equation, we then get ϵ2 = ( tX j=0 w2 j λj )d log(2N) − log(δ) 2N . Therefore, with probability of at least 1 − δ, we have |ϵw(h(t)) − ˆϵw(h(t))| ≤ vuut  tX i=0 w2 i λi !\u0012d log(2N) − log(δ) 2N \u0013 . (23) 29Published as a conference paper at ICLR 2024 Based on Eq. 23, we now prove Theorem 1. For the empirical domain error of hypothesis h at time t on the jth domain ϵj(ˆh(t)), applying Lemma 6, Eq. 23, and the definition of h∗ j (t), we get ϵj(ˆh(t)) ≤ ϵw(ˆh(t)) + tX i=0,i̸=j wi  1 2 ˆdH∆H(Si, Sj) + 2 s 2d log(2m) + log 2 δ m + γi   ≤ ˆϵw(ˆh(t)) + vuut  tX i=0 w2 i λi !\u0012d log(2N) − log(δ) 2N \u0013 + tX i=0,i̸=j wi  1 2 ˆdH∆H(Si, Sj) + 2 s 2d log(2m) + log 2 δ m + γi   ≤ ˆϵw(h∗ j (t)) + vuut  tX i=0 w2 i λi !\u0012d log(2N) − log(δ) 2N \u0013 + tX i=0,i̸=j wi  1 2 ˆdH∆H(Si, Sj) + 2 s 2d log(2m) + log 2 δ m + γi   ≤ ϵw(h∗ j (t)) + 2 vuut  tX i=0 w2 i λi !\u0012d log(2N) − log(δ) 2N \u0013 + tX i=0,i̸=j wi  1 2 ˆdH∆H(Si, Sj) + 2 s 2d log(2m) + log 2 δ m + γi   ≤ ϵj(h∗ j (t)) + 2 vuut  tX i=0 w2 i λi !\u0012d log(2N) − log(δ) 2N \u0013 + 2 tX i=0,i̸=j wi  1 2 ˆdH∆H(Si, Sj) + 2 s 2d log(2m) + log 2 δ m + γi   = ϵj(h∗ j (t)) + 2 tX i=0,i̸=j wi  1 2 ˆdH∆H(Si, Sj) + 2 s 2d log(2m) + log 2 δ m + γi   + 2C with probability of at least 1 − δ, where C = r\u0010Pt i=0 w2 i λi \u0011\u0010 d log(2N)−log(δ) 2N \u0011 and γi = minh∈H{ϵi(h(t)) + ϵj(h(t))}. For future test domains j = t + k where k > 0, we have the assumption that k′ = argmink′∈{0,1,...t} dH∆H(D(k′), Ute(t + k)) and min dH∆H(D(k′), Ute(t + k)) ≤ δD. Here, we slightly abuse the notation D(k′) to represent Ds if k′ = 0 and Ute(k′) if k′ > 0. Then we get ϵt+k(ˆh(t)) ≤ ϵw(ˆh(t)) + tX i=0 wi  1 2 ˆdH∆H(Si, St+k) + 2 s 2d log(2m) + log 2 δ m + γi   ≤ ϵw(ˆh(t)) + tX i=0 wi  1 2( ˆdH∆H(Si, Sk′ ) + ˆdH∆H(Sk′ , St+k)) + 2 s 2d log(2m) + log 2 δ m + γi   ≤ ϵw(ˆh(t)) + tX i=0 wi  1 2 ˆdH∆H(Si, Sk′ ) + 1 2δD + 2 s 2d log(2m) + log 2 δ m + γi   ≤ ˆϵw(ˆh(t)) + vuut  tX i=0 w2 i λi !\u0012d log(2N) − log(δ) 2N \u0013 + tX i=0 wi  1 2 ˆdH∆H(Si, Sk′ ) + 1 2δD + 2 s 2d log(2m) + log 2 δ m + γi   30Published as a conference paper at ICLR 2024 ≤ ˆϵw(h∗ t+k(t)) + vuut  tX i=0 w2 i λi !\u0012d log(2N) − log(δ) 2N \u0013 + tX i=0 wi  1 2 ˆdH∆H(Si, Sk′ ) + 1 2δD + 2 s 2d log(2m) + log 2 δ m + γi   ≤ ϵw(h∗ t+k(t)) + 2 vuut  tX i=0 w2 i λi !\u0012d log(2N) − log(δ) 2N \u0013 + tX i=0 wi  1 2 ˆdH∆H(Si, Sk′ ) + 1 2δD + 2 s 2d log(2m) + log 2 δ m + γi   ≤ ϵt+k(h∗ t+k(t)) + 2 vuut  tX i=0 w2 i λi !\u0012d log(2N) − log(δ) 2N \u0013 + 2 tX i=0 wi  1 2 ˆdH∆H(Si, Sk′ ) + 1 2δD + 2 s 2d log(2m) + log 2 δ m + γi   = ϵt+k(h∗ t+k(t)) + tX i=0 wi  ˆdH∆H(Si, Sk′ ) + 4 s 2d log(2m) + log 2 δ m + δD + 2γi   + 2C. with probability of at least 1−δ, where C = r\u0010Pt i=0 w2 i λi \u0011\u0010 d log(2N)−log(δ) 2N \u0011 , γi = minh∈H{ϵi(h(t))+ ϵt+k(h(t))}, and 0 ≤ δD ≪ +∞. This completes the proof. Theorem 2. Let H be a hypothesis class of VC-dimension d. For ATTA data domains DS, Ute(1), Ute(2), ··· , Ute(t), considering the test-time data as a single test domain St i=1 Ute(i), if ˆh(t) ∈ H minimizes the empirical weighted error ˆϵw(h(t)) with the weight vector w on Dtr(t), let the test error be upper-bounded with |ϵT (ˆh(t)) − ϵT (h∗ T (t))| ≤EBT (w, λ, N, t). Let w′ and λ′ be the weight and sample ratio vectors when no active learning is included, i.e., w′ and λ′ s.t. w′ 0 = λ′ 0 = 1 and w′ i = λ′ i = 0 for i ≥ 1, then for any λ ̸= λ′, there exists w s.t. EBT (w, λ, N, t) < EBT (w′, λ′, N, t). (24) Proof. From Theorem 1, we can derive the bound for the test error where the test-time data are considered as a single test domain: |ϵT (ˆh(t)) − ϵT (h∗ T (t))| ≤EBT (w, λ, N, t) = w0( ˆdH∆H(S0, ST ) + 4 s 2d log(2m) + log 2 δ m + 2γ) + 2 s w2 0 λ0 + (1 − w0)2 1 − λ0 r d log(2N) − log(δ) 2N ; and we simplify the above equation as |ϵT (ˆh(t)) − ϵT (h∗ T (t))| ≤w0A + s w2 0 λ0 + (1 − w0)2 1 − λ0 B, (25) where the distribution divergence termA = ˆdH∆H(S0, ST )+4 q 2d log(2m)+log 2 δ m +2γ, the empirical gap term B = 2 q d log(2N)−log(δ) 2N , ST is sampled from St i=1 Ute(i), and γ = minh∈H{ϵ0(h(t)) + ϵT (h(t))}. Since we have s w2 0 λ0 + (1 − w0)2 1 − λ0 = s (w0 − λ0)2 λ0(1 − λ0) + 1 ≥ 1, (26) 31Published as a conference paper at ICLR 2024 where Formula 26 obtains the minimum value if and only if w0 = λ0; when enabling ATTA with any λ0 ̸= 1, we can get EBT (w, λ, N, t) = w0A + s w2 0 λ0 + (1 − w0)2 1 − λ0 B ≥ w0A + B, (27) where the minimum value EBT (w, λ, N, t)min = w0A + B can be obtained with condition w0 = λ0 ̸= 1. When no active learning is included, i.e., for weight and sample ratio vectors w′ and λ′, w′ 0 = λ′ 0 = 1 and w′ i = λ′ i = 0 for i ≥ 1, we have EBT (w′, λ′, N, t) = w′ 0A + s w′2 0 λ′ 0 + (1 − w′ 0)2 1 − λ′ 0 B = A + B. (28) Since for EBT (w, λ, N, t)min = w0A + B, w0 < 1 and A, B >0 hold, we derive EBT (w, λ, N, t)min = w0A + B < A+ B = EBT (w′, λ′, N, t). (29) This completes the proof. Corollary 3. At time step t, for ATTA data domains Dϕ,S(t), Ute(1), Ute(2), ··· , Ute(t), Si are unla- beled samples of size m sampled from each of the t + 1 domains respectively, and SS is unlabeled samples of size m sampled from DS. If ˆh(t) ∈ Hminimizes ˆϵ′ w(h(t)) while other conditions remain identical to Theorem 1, then ϵS(ˆh(t)) ≤ ϵS(h∗ S(t)) + tX i=0 wi  ˆdH∆H(Si, SS) + 4 s 2d log(2m) + log 2 δ m + 2γi   + 2C, with probability at least 1 − δ, where C follows Theorem 1 and γi = minh∈H{ϵi(h(t)) + ϵS(h(t))}. Proof. For the empirical source error on DS of hypothesis h at time t, similar to Theorem 1, we apply Lemma 6, Eq. 23 to get ϵS(ˆh(t)) ≤ ϵw(ˆh(t)) + tX i=0 wi  1 2 ˆdH∆H(Si, SS) + 2 s 2d log(2m) + log 2 δ m + γi   ≤ ˆϵw(ˆh(t)) + vuut  tX i=0 w2 i λi !\u0012d log(2N) − log(δ) 2N \u0013 + tX i=0 wi  1 2 ˆdH∆H(Si, SS) + 2 s 2d log(2m) + log 2 δ m + γi   ≤ ˆϵw(h∗ S(t)) + vuut  tX i=0 w2 i λi !\u0012d log(2N) − log(δ) 2N \u0013 + tX i=0 wi  1 2 ˆdH∆H(Si, SS) + 2 s 2d log(2m) + log 2 δ m + γi   ≤ ϵw(h∗ S(t)) + 2 vuut  tX i=0 w2 i λi !\u0012d log(2N) − log(δ) 2N \u0013 + tX i=0 wi  1 2 ˆdH∆H(Si, SS) + 2 s 2d log(2m) + log 2 δ m + γi   ≤ ϵS(h∗ S(t)) + 2 vuut  tX i=0 w2 i λi !\u0012d log(2N) − log(δ) 2N \u0013 + 2 tX i=0 wi  1 2 ˆdH∆H(Si, SS) + 2 s 2d log(2m) + log 2 δ m + γi   32Published as a conference paper at ICLR 2024 = ϵS(h∗ S(t)) + tX i=0 wi  ˆdH∆H(Si, SS) + 4 s 2d log(2m) + log 2 δ m + 2γi   + 2C with probability of at least 1 − δ, where C = r\u0010Pt i=0 w2 i λi \u0011\u0010 d log(2N)−log(δ) 2N \u0011 and γi = minh∈H{ϵi(h(t)) + ϵS(h(t))}. This completes the proof. Corollary 4. At time step t, for ATTA data domains Dϕ,S(t), Ute(1), Ute(2), ··· , Ute(t), suppose that ˆh(t) ∈ Hminimizes ˆϵw′(h(t)) under identical conditions to Theorem 2. Let’s denote the source error upper bound with |ϵS(ˆh(t)) − ϵS(h∗ S(t))| ≤EBS(w, λ, N, t). Let w′ and λ′ be the weight and sample ratio vectors when Dϕ,S(t) is not included, i.e., w′ and λ′ s.t. w′ 0 = λ′ 0 = 0 . If ˆdH∆H(DS, Dϕ,S(t)) < ˆdH∆H(DS, St i=1 Ute(i)), then for any λ ̸= λ′, there exists w s.t. EBS(w, λ, N, t) < EBS(w′, λ′, N, t). (30) Proof. From Theorem 1, considering the test-time data as a single test domain, we can derive the bound for the source error on DS: |ϵS(ˆh(t)) − ϵS(h∗ S(t))| ≤EBS(w, λ, N, t) = w0( ˆdH∆H(S0, SS) + 4 s 2d log(2m) + log 2 δ m + 2γ) + (1 − w0)( ˆdH∆H(SS, ST ) + 4 s 2d log(2m) + log 2 δ m + 2γ′) + 2 s w2 0 λ0 + (1 − w0)2 1 − λ0 r d log(2N) − log(δ) 2N , where ST is sampled fromSt i=1 Ute(i), γ = minh∈H{ϵ0(h(t))+ϵS(h(t))}, and γ′ = minh∈H{ϵT (h(t))+ ϵS(h(t))}. We have s w2 0 λ0 + (1 − w0)2 1 − λ0 = s (w0 − λ0)2 λ0(1 − λ0) + 1 ≥ 1, (31) where the equality and the minimum value are obtained if and only if w0 = λ0. When Dϕ,S(t) is not included,i.e., with the weight and sample ratio vectorsw′ and λ′ s.t. w′ 0 = λ′ 0 = 0, using the empirical gap term B = 2 q d log(2N)−log(δ) 2N , we have EBS(w′, λ′, N, t) = ˆdH∆H(SS, ST ) + 4 s 2d log(2m) + log 2 δ m + 2γ′ + s w2 0 λ0 + (1 − w0)2 1 − λ0 B = ˆdH∆H(SS, ST ) + 4 s 2d log(2m) + log 2 δ m + 2γ′ + B. When Dϕ,S(t) is included with λ0 ̸= 0, EBS(w, λ, N, t) = w0( ˆdH∆H(S0, SS) + 4 s 2d log(2m) + log 2 δ m + 2γ) + (1 − w0)( ˆdH∆H(SS, ST ) + 4 s 2d log(2m) + log 2 δ m + 2γ′) + s w2 0 λ0 + (1 − w0)2 1 − λ0 B ≤ w0( ˆdH∆H(S0, SS) + 4 s 2d log(2m) + log 2 δ m + 2γ) + (1 − w0)( ˆdH∆H(SS, ST ) + 4 s 2d log(2m) + log 2 δ m + 2γ′) + B, 33Published as a conference paper at ICLR 2024 Algorithm 2 INCREMENTAL CLUSTERING (IC) Require: Given previously selected anchors, new unlabeled samples, and the cluster budget as Danc, Unew, and NC . Global anchor weights wanc = (wanc 1 , . . . , wanc |Danc|)⊤. 1: For simplicity, we consider anchor weights wanc as a global vector. 2: function IC(Danc, Unew, NC ) 3: wsp ← Concat(wanc, 1⊤ |Unew|) ▷ Assign all new samples with weight 1. 4: Φ ← Extract the features from the penultimate layer of model f on x ∈ Danc ∪ Unew in order. 5: clusters ← Weighted-K-Means(Φ, wsp, NC) 6: new_clusters ← {clusteri | ∀clusteri ∈ clusters, ∀x ∈ Danc, x /∈ clustersi} 7: Xnew_anchors ← {the closest sample x to the centroid of clusteri | ∀clusteri ∈ new_clusters} 8: Xanchors ← {x ∈ Danc} ∪Xnew_anchors 9: wanc ← Concat(wanc, 0⊤ |Xnew_anchors|) ▷ Initialize new anchor weights. 10: for wanc i ∈ wanc, wanc i ← wanc i + # sample of clusterj # anchor in clusterj , wanc i ∈ clusterj ▷ Weight accumulation. 11: Return Xanchors 12: end function where the minimum value can be obtained with condition w0 = λ0 ̸= 0. In practical learning scenarios, we generally assume adaptation tasks are solvable; therefore, there should be a prediction function that performs well on two distinct domains. In this case, γ and γ′ should be relatively small, so we can assume γ ≈ γ′. If ˆdH∆H(S0, SS) < ˆdH∆H(SS, ST ), then we have EBS(w, λ, N, t)min = w0( ˆdH∆H(S0, SS) + 4 s 2d log(2m) + log 2 δ m + 2γ) + (1 − w0)( ˆdH∆H(SS, ST ) + 4 s 2d log(2m) + log 2 δ m + 2γ′) + B < ˆdH∆H(SS, ST ) + 4 s 2d log(2m) + log 2 δ m + 2γ′ + B = EBS(w′, λ′, N, t). Therefore, we derive EBS(w, λ, N, t)min < EBS(w′, λ′, N, t). (32) This completes the proof. F I NCREMENTAL CLUSTERING F.1 A LGORITHM DETAILS We provide the detailed algorithm for incremental clustering as Alg. 2. F.2 V ISUALIZATION To better illustrate the incremental clustering algorithm, we provide visualization results on PACS to demonstrate the process. As shown in Fig. 3, the initial step of IC is a normal K-Means clustering step, and ten anchors denoted as \"X\" are selected. The weights of all samples in a clusters is aggregated into the corresponding anchor’s weight. Therefore, these ten samples (anchors) are given larger sizes visually (i.e., larger weights) than that of other new test samples in the first IC step (Fig. 4). During the first IC step, several distributions are far away from the existed anchors and form clusters 1,7,9 and 10, which leads to 4 new selected anchors. While the number of cluster centroid is only increased by 1, 4 of the existing anchors are clustered into the same cluster 8 (purple). Thus IC produces 4 new anchors instead of 1. Similarly, in the second IC step (Fig. 5), the new streaming-in test samples introduce a new distribution; IC produces 3 new clusters (4, 8, and 11) and the corresponding number of anchors to cover them. The number of centroid is only increased by 1, which implies that there are two original-cluster-merging events. More IC step visualization results are provided in Fig. 6 and 7. 34Published as a conference paper at ICLR 2024 Figure 3: Initial IC step: normal clustering. Left: Clustering results. Right: Selecting new anchors. Figure 4: The first IC step. Left: Weighted clustering results. Right: Selecting new anchors. Figure 5: The second IC step. Left: Weighted clustering results. Right: Selecting new anchors. 35Published as a conference paper at ICLR 2024 Figure 6: The third IC step. Left: Weighted clustering results. Right: Selecting new anchors. Figure 7: The fourth IC step. Left: Weighted clustering results. Right: Selecting new anchors. 36Published as a conference paper at ICLR 2024 G E XPERIMENT DETAILS In this section, we provide more experimental details including the details of the datasets and training settings. G.1 D ETAILS ABOUT THE DATASETS We adopt datasets PACS, VLCS, and Office-Home from DomainBed (Gulrajani and Lopez-Paz, 2020) with the same domain splits. All available licenses are mentioned below. • PACS (Li et al., 2017) includes four domains: art, cartoons, photos, and sketches. PACS is a 7-class classification dataset with 9,991 images of dimension (3, 224, 224). • VLCS (Fang et al., 2013) contains photographic domains: Caltech101, LabelMe, SUN09, and VOC2007. This dataset includes 10,729 images of dimension (3, 224, 224) with 5 classes. • Office-Home (Venkateswara et al., 2017) is a 65-class dataset, including domains: art, clipart, product, and real. VLCS includes 10,729 images of dimension (3, 224, 244). (License) • Tiny-ImageNet-C is a 200-class dataset, including 15 corrupt types. Tiny-ImageNet-C includes 150,000 images of dimension (3, 224, 244). Since the class number 200 is less than ImageNet (1000), the model’s last layer classifier needs to be adapted. In this work, we use the brightness corruption domain to adapt. In the source pretraining phase, we adopt the most ImageNet-like domain as our source domain. For PACS and Office-Home, we use domains \"photos\" and \"real\" as the source domains, respectively, while for VLCS, Caltech101 is assigned to apply the source pretraining. We freeze the random seeds to generate the sample indices order for the two test data streams, namely, the domain-wise data stream and the random data stream. For PACS, the domain-wise data stream inputs samples from domain art, cartoons, to sketches, while we shuffle all samples from these three domains in the random data stream. For VLCS, we stream the domains in the order: LabelMe, SUN09, and VOC2007, as the domain-wise data stream. For Office-Home, the domain-wise data stream order becomes art, clipart, and product. G.2 T RAINING AND OPTIMIZATION SETTINGS In this section, we extensively discuss the model architectures, optimization settings, and method settings. G.2.1 A RCHITECTURES PACS & VLCS. We adopt ResNet-18 as our model encoder followed by a linear classifier. The initial parameters of ResNet-18 are ImageNet pre-trained weights. In our experiment, we remove the Dropout layer since we empirically found that using the Dropout layer might degrade the optimization process when the sample number is small. The specific implementation of the network is closely aligned with the implementation in DomainBed (Gulrajani and Lopez-Paz, 2020). Office-Home. We employ ResNet-50 as our model encoder for Office-Home. Except for the architecture, the other model settings are aligned with the ResNet-18. Tiny-ImageNet-C ResNet-18 is adapted from ImageNet to Tiny-ImageNet-C by training the last linear layer. G.2.2 T RAINING & OPTIMIZATION In this section, we describe the training configurations for both the source domain pre-training and test-time adaptation procedures. Source domain pre-training. For the PACS and VLCS datasets, models are fine-tuned on the selected source domains for 3,000 iterations. The Adam optimizer is utilized with a learning rate 37Published as a conference paper at ICLR 2024 of 10−4. In contrast, for the Office-Home dataset, the model is fine-tuned for a longer duration of 10,000 iterations with a slightly adjusted learning rate of 5 × 10−5. Test-time adaptation. For test-time adaptation across PACS and VLCS, the pre-trained source model is further fine-tuned using the SGD optimizer with a learning rate of 10−3. While on Office-Home and Tiny-ImageNet-C, a learning rate of 10−4 is adopted. For all TTA baselines, barring specific exceptions, we faithfully adhere to the original implementation settings. A noteworthy exception is the EATA method, which requires a cosine similarity threshold. The default threshold of the original EATA implementation was not suitable for the three datasets used in our study, necessitating an adjustment. We empirically set this threshold to 0.5 for training. Unlike Tent and SAR, which only require the optimization of batch normalization layers (Santurkar et al., 2018), SimATTA allows the training of all parameters in the networks. In experiments, we use a tolerance count (tol) to control the training process. SimATTA will stop updating once the loss does not descrease for more than 5 steps. However, for Tiny-ImageNet-C, SimATTA uses ‘steps=10‘ for time comparisons since other methods apply at most 10 steps. G.2.3 M ETHOD SETTINGS Tent. In our experiments, we apply the official implementation of Tent1. Specifically, we evaluate Tent with 1 test-time training step and 10 steps, respectively. EATA.Our EATA implementation follows its official code2. In our experiments, EATA has 2000 fisher training samples, E0 = 0.4 × log(# class), ϵ <0.5. CoTTA. For CoTTA, we strictly follow all the code and settings from its official implementation3. SAR. With SAR’s official implementation4, we set E0 = 0 .4 × log(# class) and e0 = 0 .1 in our experiments. ADA baselines. For ADA baselines, we follow the architecture of the official implementation of CLUE (Prabhu et al., 2021)5. SimATTA Implementation. Our implementation largely involves straightforward hyperparameter settings. The higher entropy bound eh = 10−2 should exceed the lower entropy bound el, but equal values are acceptable. Empirically, the lower entropy bound el can be set to 10−3 for VLCS and Office-Home, or 10−4 for PACS. The choice of el is largely dependent on the number of source-like samples obtained. A lower el may yield higher-accuracy low-entropy samples, but this could lead to unstable training due to sample scarcity. Though experimentation with different hyperparameters is encouraged, our findings suggest that maintaining a non-trivial number of low-entropy samples and setting an appropriateλ0 are of primary importance. If λ0 < 0.5, CF may ensue, which may negate any potential improvement. Regarding the management of budgets, numerous strategies can be adopted. In our experiments, we utilized a simple hyperparameter k, varying from 1 to 3, to regulate the increasing rate of budget consumption. This strategy is fairly elementary and can be substituted by any adaptive techniques. G.3 S OFTWARE AND HARDWARE We conduct our experiments with PyTorch (Paszke et al., 2019) and scikit-learn (Pedregosa et al., 2011) on Ubuntu 20.04. The Ubuntu server includes 112 Intel(R) Xeon(R) Gold 6258R CPU @2.70GHz, 1.47TB memory, and NVIDIA A100 80GB PCIe graphics cards. The training process costs graphics memory less than 10GB, and it requires CPU computational resources for scikit-learn K-Means clustering calculations. Our implementation also includes a GPU-based PyTorch K-Means method for transferring calculation loads from CPUs to GPUs. However, for consistency, the results of our experiments are obtained with the original scikit-learn K-Means implementation. 1https://github.com/DequanWang/tent 2https://github.com/mr-eggplant/EATA 3https://github.com/qinenergy/cotta 4https://github.com/mr-eggplant/SAR 5https://github.com/virajprabhu/CLUE 38Published as a conference paper at ICLR 2024 Figure 8: Target loss surface on 2000 samples without source pre-training. The red points denote the loss minimum for a fixed λ0. The orange line denote the place where w0 = λ0. Figure 9: Target loss surface on 2000 samples with source pre-training. H E MPIRICAL VALIDATIONS FOR THEORETICAL ANALYSIS In this section, we undertake empirical validation of our learning theory, which encompasses multiple facets awaiting verification. In contemporary computer vision fields, pre-trained models play a pivotal role, and performance would significantly decline without the use of pre-trained features. The learning theory suggests that given the vast VC-dimension of complete ResNets, without substantial data samples, the training error cannot be theoretically tight-bounded. However, we show empirically in the following experiments that fine-tuning pre-trained models is behaviorally akin to training a model with a low VC-dimension. Training on 2000 Samples Without Source Domain Pre-training. For an ImageNet pre-trained ResNet-18 model, we trained it using 2000 samples from the PACS dataset. To ascertain the optimal value w∗ 0 in Equation 4, we trained multiple models for different w0 and λ0 pairings. For each pair, we derived the target domain loss (from art, cartoons, and sketches) post-training and plotted this loss on the z-axis. With w0 and λ0 serving as the xy-axes, we drafted the target domain loss ϵT surface in Figure 8. As the results show, given a λ0, the optimal w∗ 0 typically aligns with the line λ0 = w0, with a slight downward shift, which aligns with Equation 4. 39Published as a conference paper at ICLR 2024 Figure 10: Target loss surface on 500 samples with source pre-training. Figure 11: Source loss surface on 500 samples with source pre-training. 40Published as a conference paper at ICLR 2024 Figure 12: Target and source loss surface on 500 samples with source pre-training. Table 6: TTA comparisons on Office-Home. This table includes the two data stream settings mentioned in the dataset setup and reports performances in accuracy. Results that outperform all TTA baselines are highlighted in bold font. N/A denotes the adaptations are not applied on the source domain. Office-Home Domain-wise data stream Post-adaptation Random data stream Post-adaptation R →A→ →C→ →P R A C P 1 2 3 4 R A C P BN w/o adapt 93.78 42.93 37.62 59.90 93.78 42.93 37.62 59.90 46.82 46.82 46.82 46.82 93.78 42.93 37.62 59.90BN w/ adapt 92.38 49.69 39.43 63.53 92.38 49.69 39.43 63.53 50.88 50.88 50.88 50.88 92.38 49.69 39.43 63.53 Tent (steps=1) N/A 49.61 39.31 63.87 92.47 49.57 39.89 63.89 49.95 50.27 50.23 52.06 92.40 49.24 39.68 63.98Tent (steps=10) N/A 49.61 39.04 61.41 87.08 44.79 38.37 60.49 50.05 49.31 48.74 47.79 85.31 42.85 37.89 58.71EATA N/A 49.65 39.04 63.53 91.60 49.61 38.65 63.48 49.73 50.27 49.45 51.07 91.05 49.11 38.26 62.99CoTTA N/A 49.61 38.76 61.84 87.81 44.95 35.92 59.04 49.84 49.84 48.95 50.43 86.99 43.68 34.73 57.56SAR (steps=1) N/A 49.65 39.24 63.53 92.45 49.73 39.36 63.69 49.84 50.05 49.91 51.67 92.38 49.57 39.50 63.87SAR (steps=10) N/A 49.53 38.81 61.50 88.94 46.15 37.04 59.41 50.09 50.30 49.77 49.22 89.14 46.23 36.31 59.45 SimATTA (B ≤300) N/A 56.20 48.38 71.66 95.75 60.07 52.62 74.70 58.57 60.88 62.91 63.67 95.89 62.01 54.98 74.70SimATTA (B ≤500) N/A 58.71 51.11 74.36 96.03 62.05 57.41 76.98 58.85 62.63 63.41 64.31 95.91 63.78 57.87 77.09 Training on 2000 Samples with Source Domain Pre-training. To further assess the effects of source pre-training, we repeated the same experiment on a source pre-trained ResNet-18. The results are depicted in Figure 9. This experiment provides empirical guidance on selecting w0 in source domain pre-trained situations. The findings suggest that the optimal w∗ 0 non-trivially shifts away from the line λ0 = w0 towards lower-value regions. Considering the source pre-training process as using a greater quantity of source domain samples, it implies that when the number of source samples greatly exceeds target samples, a lower w0 can enhance target domain results. Training on 500 Samples with Source Domain Pre-training. We proceed to fine-tune the source domain pre-trained ResNet-18 using only 500 samples, thereby simulating active TTA settings. We train models with various w0 and λ0 pairings, then graph the target domain losses, source domain losses, and the combined losses. As shown in Figure 10, the target losses still comply with our theoretical deductions where the local minima are close to the line λ0 = w0 and marginally shift towards lower values. Considering the challenge of CF, the source domain results in Figure 11 suggest a reverse trend compared to the target domain, where lower λ0 and w0 values yield superior target domain results but inferior source domain results. Thus, to curb CF, the primary strategy is to maintain a relatively higher λ0. When considering both target and source domains, a balance emerges as depicted in Figure 12. The global minimum is located in the middle region, demonstrating the trade-off between the target domain and source domain performance. I A DDITIONAL EXPERIMENT RESULTS In this section, we provide additional experiment results. The Office-Home results and ablation studies will be presented in a similar way as the main paper. In the full results Sec. I.3, we will post more detailed experimental results with specific budget numbers and intermediate performance during the test-time adaptation. 41Published as a conference paper at ICLR 2024 Table 7: Comparisons to ADA baselines on Office-Home. The source domain is denoted as \"(S)\" in the table. Results are average accuracies with standard deviations). Office-Home R (S) A C P Random (B = 300) 95.04 (0.20) 57.54 (1.16) 53.43 (1.17) 73.46 (0.97) Entropy (B = 300) 94.39 (0.49) 61.21 (0.71) 56.53 (0.71) 72.31 (0.28) Kmeans (B = 300) 95.09 (0.14) 57.37 (0.90) 51.74 (1.34) 71.81 (0.39) CLUE (B = 300) 95.20 (0.23) 60.18 (0.98) 58.05 (0.43) 73.72 (0.70) Ours (B ≤300) 95.82 (0.07) 61.04 (0.97) 53.80 (1.18) 74.70 (0.00) I.1 R ESULTS ON OFFICE -HOME We conduct experiments on Office-Home and get the test-time performances and post-adaptation performances for two data streams. As shown in Tab. 6, SimATTA can outperform all TTA baselines with huge margins. Compared to ADA baselines under the source-free settings, as shown in Tab. 7, SimATTA obtains comparable results. I.2 A BLATION STUDIES Figure 13: Ablation study on PACS and VLCS.\"IC=0\" denotes removing incremental clustering (IC) selection. \"LE=0\" denotes removing the low-entropy (LE) sample training. Domain-wise stream and random stream are applied on first and second rows, respectively. The accuracy values are averaged across all splits/domains. In this section, we explore three variations of our method to examine the individual impacts of its components. The first variant replaces the incremental clustering selection with entropy selection, 42Published as a conference paper at ICLR 2024 where only the samples with the highest entropy are chosen. The second variant eliminates low- entropy sample training. The third variation combines the first and second variants. We perform this ablation study on the PACS and VLCS as outlined in Fig. 13. We denote the use of incremental clustering (IC) and low-entropy training (LE) respectively as IC=1 and LE=1. The experiments essentially reveals the effectiveness of incremental clustering and low-entropy- sample training. As we have detailed in Sec. 3.2, these techniques are designed to to select informative samples, increase distribution coverage, and mitigate catastrophic forgetting. These designs appositely serve the ATTA setting where the oracle has costs and the budget is limited. Therefore, their effectiveness is prominent particularly when the budget is small. As the results show, when the budget B ≤100 or B ≤300, removing the components observably impairs performances. When B gets large, more active samples cover a larger distribution; thus the performance gap from random selection and informative selection gets smaller. In the extreme case where B → ∞, all samples are selected and thus the superiority of our meticulously-designed techniques are not manifested. Specifically, our analysis yields several insights. First, SimATTA (LE=1, IC=1) comprehensively outperforms other variants on both datasets, different streams, and different budgets. Second, variants without low-entropy training (LE=0, IC=0/1) easily fail to produce stable results (e.g., domain-wise stream in VLCS). Third, SimATTA’s performance surpasses this variant on PACS’s domain-wise stream clearly especially when the budgets are low. This indicates these variants fail to retrieve the most informative style shift (PACS’s shifts) samples, which implies the advantage of incremental clustering when the budget is tight. In addition, these results show that IC has its unique advantage on domain-wise streams where distributions change abruptly instead of random streams. Therefore, compared to PACS’s domain- wise stream results, the reason for the smaller performance improvement of SimATTA over the variant (LE=1, IC=0) on VLCS’s domain-wise stream is that images in VLCS are all photos that do not include those severe style shifts in PACS (i.e., art, cartoons, and sketches). That is, when the shift is not severe, we don’t need IC to cover very different distributions, and selecting samples using entropy can produce good results. In brief, IC is extraordinary for severe distribution shifts and quick adaptation. It is worth mentioning that low budget comparison is essential to show the informative sample retrieval ability, since as the budget increases, all AL techniques will tend to perform closely. I.3 C OMPLETE EXPERIMENT RESULTS We provide complete experimental results in this section. As shown in Tab. 8, we present the full results for two data streams. The test-time adaptation accuracies are shown in the \"Current domain\" row, while the \"Budgets\" row denotes the used budget by the end of the domain. The rest four rows denote the four domain test results by the end of the real-time adaptation of the current domain, where the first column results are the test accuracy before the test-time adaptation phase. N/A represents \"do not apply\". Table 8: Tent (steps=1) on PACS. Tent (steps=1) Domain-wise data stream Random data stream P →A→ → C→ → S 1 → → 2→ → 3→ → 4→ Current domain N/A 67.29 64.59 44.67 56.35 54.09 51.83 48.58 Budgets N/A N/A N/A N/A N/A N/A N/A N/A P 99.70 98.68 98.38 97.60 98.56 98.08 97.72 97.19 A 59.38 69.09 68.95 66.85 68.07 67.33 65.58 63.53 C 28.03 64.04 65.19 64.08 64.85 65.19 62.97 60.75 S 42.91 53.65 47.39 42.58 54.57 49.83 44.13 41.56 J C HALLENGES AND PERSPECTIVES Despite advancements, test-time adaptation continues to pose considerable challenges. As previously discussed, without supplementary information and assumptions, the ability to guarantee model generalization capabilities is limited. However, this is not unexpected given that recent progress 43Published as a conference paper at ICLR 2024 Table 9: Tent (steps=10) on PACS. Tent (steps=10) Domain-wise data stream Random data stream P →A→ → C→ → S 1 → → 2→ → 3→ → 4→ Current domain N/A 67.38 57.85 20.23 47.36 31.01 22.84 20.33 Budgets N/A N/A N/A N/A N/A N/A N/A N/A P 99.70 95.45 87.43 62.63 93.83 81.32 65.39 50.78 A 59.38 64.94 55.03 34.52 55.32 40.28 28.27 23.68 C 28.03 55.89 56.70 40.57 54.52 39.68 27.22 20.95 S 42.91 36.96 26.27 13.59 32.25 23.16 20.95 19.62 Table 10: EATA on PACS. EATA Domain-wise data stream Random data stream P →A→ → C→ → S 1 → → 2→ → 3→ → 4→ Current domain N/A 67.04 64.72 50.27 57.31 56.06 58.17 59.78 Budgets N/A N/A N/A N/A N/A N/A N/A N/A P 99.70 98.62 98.50 98.62 98.68 98.62 98.50 98.62 A 59.38 68.90 68.16 66.50 68.65 68.95 69.34 69.63 C 28.03 63.74 65.36 62.46 65.19 66.00 65.57 65.70 S 42.91 54.01 52.89 48.18 55.71 55.64 54.09 54.26 Table 11: CoTTA on PACS. CoTTA Domain-wise data stream Random data stream P →A→ → C→ → S 1 → → 2→ → 3→ → 4→ Current domain N/A 65.48 62.12 53.17 56.06 54.33 57.16 57.42 Budgets N/A N/A N/A N/A N/A N/A N/A N/A P 99.70 98.68 98.62 98.62 98.62 98.62 98.56 98.62 A 59.38 65.82 65.87 65.48 66.02 65.87 66.31 65.97 C 28.03 62.63 63.05 63.10 63.01 62.88 63.01 62.97 S 42.91 53.88 54.03 53.78 54.67 55.31 55.10 54.62 Table 12: SAR (steps=1) on PACS. SAR (steps=1) Domain-wise data stream Random data stream P →A→ → C→ → S 1 → → 2→ → 3→ → 4→ Current domain N/A 66.75 63.82 49.58 56.78 56.35 56.68 56.70 Budgets N/A N/A N/A N/A N/A N/A N/A N/A P 99.70 98.68 98.50 98.32 98.74 98.56 98.50 98.44 A 59.38 68.02 68.07 66.94 67.87 68.65 68.55 68.16 C 28.03 62.84 64.97 62.93 63.82 64.89 64.46 64.38 S 42.91 53.47 52.07 45.74 54.92 55.46 53.68 52.53 Table 13: SAR (steps=10) on PACS. SAR (steps=10) Domain-wise data stream Random data stream P →A→ → C→ → S 1 → → 2→ → 3→ → 4→ Current domain N/A 69.38 68.26 49.02 53.51 51.15 51.78 45.60 Budgets N/A N/A N/A N/A N/A N/A N/A N/A P 99.70 98.20 95.39 96.47 97.13 97.78 97.72 94.13 A 59.38 72.36 66.60 62.16 62.74 64.94 66.11 56.64 C 28.03 63.44 68.30 56.19 59.77 61.73 62.03 56.02 S 42.91 53.37 44.59 54.62 41.00 49.66 48.79 36.37 44Published as a conference paper at ICLR 2024 Table 14: SimATTA (B ≤300) on PACS. SimATTA (B ≤300) Domain-wise data stream Random data stream P →A→ → C→ → S 1 → → 2→ → 3→ → 4→ Current domain N/A 76.86 70.90 75.39 69.47 76.49 82.45 82.22 Budgets N/A 75 145 223 66 142 203 267 P 99.70 98.44 98.86 98.80 97.96 98.68 99.04 98.98 A 59.38 80.71 82.32 84.47 73.97 80.52 81.10 84.91 C 28.03 48.12 82.00 82.25 72.35 81.06 83.36 83.92 S 42.91 32.78 56.25 81.52 79.49 83.10 84.78 86.00 Table 15: SimATTA (B ≤500) on PACS. SimATTA (B ≤500) Domain-wise data stream Random data stream P →A→ → C→ → S 1 → → 2→ → 3→ → 4→ Current domain N/A 77.93 76.02 76.30 68.46 78.22 80.91 85.49 Budgets N/A 121 230 358 102 221 343 425 P 99.70 98.92 98.86 98.62 98.20 99.46 99.10 99.16 A 59.38 87.01 87.60 88.33 73.39 79.20 84.91 86.67 C 28.03 54.78 83.96 83.49 68.43 74.40 84.22 84.77 S 42.91 46.37 63.53 83.74 81.34 81.04 86.66 87.71 Table 16: Tent (steps=1) on VLCS. Tent (steps=1) Domain-wise data stream Random data stream C →L→ → S→ → V 1 → → 2→ → 3→ → 4→ Current domain N/A 38.55 34.40 53.88 44.85 44.29 47.38 44.98 Budgets N/A N/A N/A N/A N/A N/A N/A N/A C 100.00 84.81 85.44 84.73 84.95 85.16 85.80 85.30 L 33.55 40.02 43.11 43.86 39.68 41.98 43.11 43.49 S 41.10 33.39 35.41 33.61 36.29 37.90 38.27 37.81 V 49.08 53.20 54.06 53.11 53.76 54.18 53.76 53.35 Table 17: Tent (steps=10) on VLCS. Tent (steps=10) Domain-wise data stream Random data stream C →L→ → S→ → V 1 → → 2→ → 3→ → 4→ Current domain N/A 45.41 31.44 32.32 46.13 42.31 43.51 39.48 Budgets N/A N/A N/A N/A N/A N/A N/A N/A C 100.00 73.07 48.34 42.54 74.13 62.19 56.54 52.01 L 33.55 46.61 38.44 37.65 44.88 45.93 43.41 40.32 S 41.10 31.75 28.82 27.79 35.37 36.14 35.28 33.64 V 49.08 48.05 40.14 33.12 50.50 44.49 42.48 40.37 Table 18: EATA on VLCS. EATA Domain-wise data stream Random data stream C →L→ → S→ → V 1 → → 2→ → 3→ → 4→ Current domain N/A 37.24 33.15 52.58 43.77 42.48 43.34 41.55 Budgets N/A N/A N/A N/A N/A N/A N/A N/A C 100.00 85.16 85.02 84.10 84.73 84.52 84.10 83.32 L 33.55 37.16 37.24 37.69 37.09 36.78 36.90 36.67 S 41.10 33.39 33.49 32.39 33.33 32.54 31.84 31.47 V 49.08 51.87 52.16 52.49 52.07 52.43 52.64 52.55 45Published as a conference paper at ICLR 2024 Table 19: CoTTA on VLCS. CoTTA Domain-wise data stream Random data stream C →L→ → S→ → V 1 → → 2→ → 3→ → 4→ Current domain N/A 37.39 32.54 52.25 43.69 42.14 43.21 42.32 Budgets N/A N/A N/A N/A N/A N/A N/A N/A C 100.00 81.55 81.98 82.12 82.61 82.47 82.12 81.98 L 33.55 37.20 37.91 37.65 38.48 38.22 38.40 37.99 S 41.10 30.71 32.78 33.12 34.00 33.70 33.97 33.52 V 49.08 52.01 52.64 52.90 53.64 53.14 53.08 53.23 Table 20: SAR (steps=1) on VLCS. SAR (steps=1) Domain-wise data stream Random data stream C →L→ → S→ → V 1 → → 2→ → 3→ → 4→ Current domain N/A 36.18 34.43 52.46 43.64 43.04 44.20 41.93 Budgets N/A N/A N/A N/A N/A N/A N/A N/A C 100.00 84.31 84.17 83.96 85.09 85.23 85.23 85.09 L 33.55 35.62 38.29 39.72 38.55 39.34 40.21 40.70 S 41.10 33.24 36.41 36.53 34.37 35.62 36.29 36.44 V 49.08 51.75 52.61 52.37 52.90 52.75 53.05 53.02 Table 21: SAR (steps=10) on VLCS. SAR (steps=10) Domain-wise data stream Random data stream C →L→ → S→ → V 1 → → 2→ → 3→ → 4→ Current domain N/A 35.32 34.10 51.66 43.56 42.05 42.53 41.16 Budgets N/A N/A N/A N/A N/A N/A N/A N/A C 100.00 83.96 83.04 82.12 84.03 84.24 85.23 85.09 L 33.55 34.07 35.92 41.49 39.53 38.37 37.65 37.58 S 41.10 31.93 34.89 33.94 35.19 32.94 33.88 33.12 V 49.08 51.33 51.51 53.08 52.78 52.34 51.78 52.01 Table 22: SimATTA (B ≤300) on VLCS. SimATTA (B ≤300) Domain-wise data stream Random data stream C →L→ → S→ → V 1 → → 2→ → 3→ → 4→ Current domain N/A 62.61 65.08 74.38 62.33 69.33 73.20 71.93 Budgets N/A 79 175 272 71 135 208 262 C 100.00 99.51 98.52 99.93 99.86 99.79 100.00 99.93 L 33.55 68.11 69.92 69.50 62.61 66.64 68.45 69.43 S 41.10 55.24 68.89 66.67 65.54 69.29 71.79 72.46 V 49.08 66.08 70.94 77.34 73.79 76.87 78.82 80.39 Table 23: SimATTA (B ≤500) on VLCS. SimATTA (B ≤500) Domain-wise data stream Random data stream C →L→ → S→ → V 1 → → 2→ → 3→ → 4→ Current domain N/A 63.52 68.01 76.13 62.29 70.45 73.50 72.02 Budgets N/A 113 266 446 107 203 283 356 C 100.00 99.29 98.59 99.51 99.93 99.86 99.86 99.43 L 33.55 62.95 70.63 70.56 66.57 67.09 67.24 70.29 S 41.10 51.31 73.83 73.10 65.33 71.79 72.91 72.55 V 49.08 59.36 71.65 78.35 73.58 77.84 80.01 80.18 46Published as a conference paper at ICLR 2024 Table 24: Tent (steps=1) on Office-Home. Tent (steps=1) Domain-wise data stream Random data stream R →A→ → C→ → P 1 → → 2→ → 3→ → 4→ Current domain N/A 49.61 39.31 63.87 49.95 50.27 50.23 52.06 Budgets N/A N/A N/A N/A N/A N/A N/A N/A R 96.44 92.33 92.36 92.47 92.38 92.45 92.45 92.40 A 57.07 49.73 49.73 49.57 49.69 49.73 49.57 49.24 C 44.97 39.27 39.54 39.89 39.45 39.68 39.73 39.68 P 73.15 63.60 63.66 63.89 63.60 63.82 63.93 63.98 Table 25: Tent (steps=10) on Office-Home. Tent (steps=10) Domain-wise data stream Random data stream R →A→ → C→ → P 1 → → 2→ → 3→ → 4→ Current domain N/A 49.61 39.04 61.41 50.05 49.31 48.74 47.79 Budgets N/A N/A N/A N/A N/A N/A N/A N/A R 96.44 91.99 89.14 87.08 92.08 90.80 88.59 85.31 A 57.07 49.94 46.77 44.79 49.44 48.21 45.69 42.85 C 44.97 38.58 39.11 38.37 40.18 40.02 38.63 37.89 P 73.15 63.28 61.03 60.49 64.36 63.64 61.12 58.71 Table 26: EATA on Office-Home. EATA Domain-wise data stream Random data stream R →A→ → C→ → P 1 → → 2→ → 3→ → 4→ Current domain N/A 49.65 39.04 63.53 49.73 50.27 49.45 51.07 Budgets N/A N/A N/A N/A N/A N/A N/A N/A R 96.44 92.36 92.17 91.60 92.38 92.22 91.71 91.05 A 57.07 49.57 49.53 49.61 49.69 49.40 49.36 49.11 C 44.97 39.08 39.01 38.65 39.27 39.01 38.42 38.26 P 73.15 63.42 63.42 63.48 63.51 63.37 63.33 62.99 Table 27: CoTTA on Office-Home. CoTTA Domain-wise data stream Random data stream R →A→ → C→ → P 1 → → 2→ → 3→ → 4→ Current domain N/A 49.61 38.76 61.84 49.84 49.84 48.95 50.43 Budgets N/A N/A N/A N/A N/A N/A N/A N/A R 96.44 90.38 88.02 87.81 90.48 89.37 88.00 86.99 A 57.07 48.58 45.53 44.95 47.34 46.35 44.62 43.68 C 44.97 36.66 35.58 35.92 37.55 36.40 35.44 34.73 P 73.15 60.40 57.74 59.04 61.12 59.63 58.35 57.56 Table 28: SAR (steps=1) on Office-Home. SAR (steps=1) Domain-wise data stream Random data stream R →A→ → C→ → P 1 → → 2→ → 3→ → 4→ Current domain N/A 49.65 39.24 63.53 49.84 50.05 49.91 51.67 Budgets N/A N/A N/A N/A N/A N/A N/A N/A R 96.44 92.38 92.31 92.45 92.40 92.36 92.36 92.38 A 57.07 49.65 49.57 49.73 49.69 49.61 49.57 49.57 C 44.97 39.34 39.22 39.36 39.34 39.56 39.47 39.50 P 73.15 63.51 63.51 63.69 63.60 63.71 63.71 63.87 47Published as a conference paper at ICLR 2024 Table 29: SAR (steps=10) on Office-Home. SAR (steps=10) Domain-wise data stream Random data stream R →A→ → C→ → P 1 → → 2→ → 3→ → 4→ Current domain N/A 49.53 38.81 61.50 50.09 50.30 49.77 49.22 Budgets N/A N/A N/A N/A N/A N/A N/A N/A R 96.44 92.20 92.06 88.94 92.40 92.47 91.53 89.14 A 57.07 49.40 49.77 46.15 49.81 50.02 48.91 46.23 C 44.97 39.20 38.63 37.04 39.50 39.29 38.65 36.31 P 73.15 63.53 62.69 59.41 64.18 64.18 62.83 59.45 Table 30: SimATTA (B ≤300) on Office-Home. SimATTA (B ≤300) Domain-wise data stream Random data stream R →A→ → C→ → P 1 → → 2→ → 3→ → 4→ Current domain N/A 56.20 48.38 71.66 58.57 60.88 62.91 63.67 Budgets N/A 75 187 277 79 147 216 278 R 96.44 95.43 95.43 95.75 95.91 95.96 96.01 95.89 A 57.07 57.56 59.50 60.07 58.34 59.91 61.15 62.01 C 44.97 42.25 52.46 52.62 51.66 52.30 54.75 54.98 P 73.15 68.84 70.13 74.70 72.45 73.10 74.50 74.70 Table 31: SimATTA (B ≤500) on Office-Home. SimATTA (B ≤500) Domain-wise data stream Random data stream R →A→ → C→ → P 1 → → 2→ → 3→ → 4→ Current domain N/A 58.71 51.11 74.36 58.85 62.63 63.41 64.31 Budgets N/A 107 284 440 126 248 361 467 R 96.44 95.69 95.71 96.03 96.26 96.19 95.87 95.91 A 57.07 61.43 61.43 62.05 58.18 61.15 61.52 63.78 C 44.97 46.41 57.73 57.41 53.17 55.14 56.79 57.87 P 73.15 70.74 71.98 76.98 73.51 74.18 75.78 77.09 48Published as a conference paper at ICLR 2024 in deep learning heavily relies on large-scale data. Consequently, two promising paths emerge: establishing credible assumptions and leveraging additional information. Firstly, developing credible assumptions can lead to comprehensive comparisons across various stud- ies. Given that theoretical guarantees highlight the inherent differences between methods primarily based on the application limits of their assumptions, comparing these assumptions becomes critical. Without such comparative studies, empirical evaluations may lack precise guidance and explanation. Secondly, while we acknowledge the value of real-world data (observations), discussions surrounding the use of extra information remain pertinent. Considerations include the strategies to acquire this supplementary information and the nature of the additional data needed. Despite the myriad of works on domain generalization, domain adaptation, and test-time adaptation, a comprehensive survey or benchmark encapsulating the aforementioned comparisons remains an unmet need. Moreover, potential future directions for out-of-distribution generalization extend beyond domain generalization and test-time adaptation. One promising avenue is bridging the gap between causal inference and deep learning, for instance, through causal representation learning. In conclusion, our hope is that this work not only offers a novel practical setting and algorithm but also illuminates meaningful future directions and research methodologies that can benefit the broader scientific community. 49",
      "meta_data": {
        "arxiv_id": "2404.05094v1",
        "authors": [
          "Shurui Gui",
          "Xiner Li",
          "Shuiwang Ji"
        ],
        "published_date": "2024-04-07T22:31:34Z",
        "pdf_url": "https://arxiv.org/pdf/2404.05094v1.pdf"
      },
      "llm_extracted_info": {
        "main_contributions": "This paper introduces Active Test-Time Adaptation (ATTA), a novel problem setting that integrates active learning into fully Test-Time Adaptation (TTA) to address significant distribution shifts in streaming test data and mitigate catastrophic forgetting (CF). The key contributions include a learning theory analysis demonstrating that incorporating limited labeled test instances enhances performance with theoretical guarantees, and the proposal of SimATTA, a simple yet effective ATTA algorithm that employs real-time sample selection and balanced entropy minimization. SimATTA achieves substantial performance improvements over TTA methods and comparable effectiveness to Active Domain Adaptation (ADA) methods, while maintaining efficiency and avoiding CF.",
        "methodology": "The ATTA problem is formally defined, aiming to optimize a pre-trained model on streaming test data by actively selecting and labeling informative instances within a budget. The theoretical framework analyzes how ATTA alleviates distribution shifts through active learning and combats catastrophic forgetting (CF) using a selective entropy minimization approach. Specifically, low-entropy samples (well-aligned with the source-pretrained model) are pseudo-labeled to act as 'source-like' data, helping to prevent CF. High-entropy samples are actively selected for annotation. The SimATTA algorithm implements this by partitioning unlabeled test samples into high and low entropy sets. High-entropy samples are selected using an incremental clustering technique (based on weighted K-means) to obtain diverse and representative 'anchors' for oracle labeling. The model is then fine-tuned using these labeled anchors and the pseudo-labeled low-entropy samples, with balanced weighting to optimize for both target domain performance and CF avoidance.",
        "experimental_setup": "Experiments were conducted to validate ATTA's effectiveness and efficiency, and to compare it against various baselines. Datasets used include PACS, VLCS, Office-Home (from DomainBed), and Tiny-ImageNet-C, known for evaluating out-of-distribution (OOD) performance. One domain was designated as the source, and others formed test data streams (domain-wise or random order). Baselines included source-only models (BN w/o adapt, BN w/ adapt), state-of-the-art TTA methods (Tent, EATA, CoTTA, SAR), and Active Domain Adaptation (ADA) methods (Random, Entropy, K-means, CLUE). Models were ResNet-18 or ResNet-50, initialized with ImageNet pre-trained weights. Evaluation was based on accuracy, with metrics reported for current domain adaptation and post-adaptation performance on all domains. Ablation studies investigated the impact of incremental clustering and low-entropy sample training.",
        "limitations": "The theoretical bounds can be loose with a small number of unlabeled test samples (`m`), although fine-tuning pre-trained models is empirically found to behave like training a model with a relatively small VC-dimension (`d`). The current framework does not cover situations where the support of `Y` (labels) changes, such as class-incremental problems. The selective entropy minimization for CF prevention relies on the quality of the pre-trained model, and training on incorrectly predicted low-entropy samples may reinforce errors. The cost-effectiveness of expending annotation budgets on low-entropy samples, versus correcting them, is an open question. The current paper focuses on foundational aspects and does not cover scaling ATTA for large models (e.g., LLMs) or extensive task-specific practical applications.",
        "future_research_directions": "Future research could focus on developing alternative strategies to prevent catastrophic forgetting (CF) in ATTA scenarios, especially considering the potential pitfalls of selective entropy minimization. Investigating the cost-effectiveness of annotating low-entropy samples and exploring alternative solutions, such as correcting them, is another promising direction. Bridging the gap between causal inference and deep learning could lead to new avenues for OOD generalization. Scaling ATTA methods for large models and datasets, such as large language models (LLMs), where retraining is computationally expensive and source data may be inaccessible, represents a significant future area of exploration. Furthermore, applying and tailoring ATTA to more specialized real-world applications is also a valuable direction."
      }
    },
    {
      "title": "DELTA: DEGRADATION-FREE FULLY TEST-TIME ADAPTATION",
      "abstract": "Fully test-time adaptation aims at adapting a pre-trained model to the test\nstream during real-time inference, which is urgently required when the test\ndistribution differs from the training distribution. Several efforts have been\ndevoted to improving adaptation performance. However, we find that two\nunfavorable defects are concealed in the prevalent adaptation methodologies\nlike test-time batch normalization (BN) and self-learning. First, we reveal\nthat the normalization statistics in test-time BN are completely affected by\nthe currently received test samples, resulting in inaccurate estimates. Second,\nwe show that during test-time adaptation, the parameter update is biased\ntowards some dominant classes. In addition to the extensively studied test\nstream with independent and class-balanced samples, we further observe that the\ndefects can be exacerbated in more complicated test environments, such as\n(time) dependent or class-imbalanced data. We observe that previous approaches\nwork well in certain scenarios while show performance degradation in others due\nto their faults. In this paper, we provide a plug-in solution called DELTA for\nDegradation-freE fuLly Test-time Adaptation, which consists of two components:\n(i) Test-time Batch Renormalization (TBR), introduced to improve the estimated\nnormalization statistics. (ii) Dynamic Online re-weighTing (DOT), designed to\naddress the class bias within optimization. We investigate various test-time\nadaptation methods on three commonly used datasets with four scenarios, and a\nnewly introduced real-world dataset. DELTA can help them deal with all\nscenarios simultaneously, leading to SOTA performance.",
      "meta_data": {
        "arxiv_id": "2301.13018v1",
        "authors": [
          "Bowen Zhao",
          "Chen Chen",
          "Shu-Tao Xia"
        ],
        "published_date": "2023-01-30T15:54:00Z",
        "pdf_url": "https://arxiv.org/pdf/2301.13018v1.pdf"
      }
    },
    {
      "title": "Robust Test-Time Adaptation in Dynamic Scenarios",
      "abstract": "Test-time adaptation (TTA) intends to adapt the pretrained model to test\ndistributions with only unlabeled test data streams. Most of the previous TTA\nmethods have achieved great success on simple test data streams such as\nindependently sampled data from single or multiple distributions. However,\nthese attempts may fail in dynamic scenarios of real-world applications like\nautonomous driving, where the environments gradually change and the test data\nis sampled correlatively over time. In this work, we explore such practical\ntest data streams to deploy the model on the fly, namely practical test-time\nadaptation (PTTA). To do so, we elaborate a Robust Test-Time Adaptation (RoTTA)\nmethod against the complex data stream in PTTA. More specifically, we present a\nrobust batch normalization scheme to estimate the normalization statistics.\nMeanwhile, a memory bank is utilized to sample category-balanced data with\nconsideration of timeliness and uncertainty. Further, to stabilize the training\nprocedure, we develop a time-aware reweighting strategy with a teacher-student\nmodel. Extensive experiments prove that RoTTA enables continual testtime\nadaptation on the correlatively sampled data streams. Our method is easy to\nimplement, making it a good choice for rapid deployment. The code is publicly\navailable at https://github.com/BIT-DA/RoTTA",
      "full_text": "Robust Test-Time Adaptation in Dynamic Scenarios Longhui Yuan Binhui Xie Shuang Li \f School of Computer Science and Technology, Beijing Institute of Technology {longhuiyuan,binhuixie,shuangli}@bit.edu.cn Abstract Test-time adaptation (TTA) intends to adapt the pre- trained model to test distributions with only unlabeled test data streams. Most of the previous TTA methods have achieved great success on simple test data streams such as independently sampled data from single or multiple distri- butions. However, these attempts may fail in dynamic sce- narios of real-world applications like autonomous driving, where the environments gradually change and the test data is sampled correlatively over time. In this work, we ex- plore such practical test data streams to deploy the model on the fly, namely practical test-time adaptation (PTTA). To do so, we elaborate a Robust Test-Time Adaptation (RoTTA) method against the complex data stream in PTTA. More specifically, we present a robust batch normalization scheme to estimate the normalization statistics. Meanwhile, a memory bank is utilized to sample category-balanced data with consideration of timeliness and uncertainty. Further, to stabilize the training procedure, we develop a time-aware reweighting strategy with a teacher-student model. Exten- sive experiments prove that RoTTA enables continual test- time adaptation on the correlatively sampled data streams. Our method is easy to implement, making it a good choice for rapid deployment. The code is publicly available at https://github.com/BIT-DA/RoTTA 1. Introduction In recent years, many machine learning problems have made considerable headway with the success of deep neu- ral networks [13, 22, 33, 38]. Unfortunately, the perfor- mance of deep models drops significantly when training data and testing data come from different distributions [59], which limits their utility in real-world applications. To re- duce the distribution shift, a handful of works focus on transfer learning field [56], in particular, domain adapta- tion (DA) [17, 42, 45, 48, 69, 72] or domain generalization (DG) [40, 41, 52, 71, 83], in which one or more different but \fCorresponding author Test data stream Continual TTANon-i.i.d.TTAPractical  TTACategoryDistribution Fully TTA Correlation samplingDistributionchanging Figure 1. We consider the practical test-time adaptation (TTA) setup and compare it with related ones. First, Fully TTA [70] adapts models on a fixed test distribution with an independently sampled test stream. Then, on this basis, Continual TTA [73] takes the continually changing distributions into consideration. Next, Non-i.i.d. TTA [19] tries to tackle the correlatively sampled test streams on a single test distribution, where the label distribution among a batch of data deviates from that of the test distribution. To be more practical, Practical TTA strives to connect both worlds: distribution changing and correlation sampling. related labeled datasets (a.k.a. source domain) are collected to help the model generalize well to unlabeled or unseen samples in new datasets (a.k.a. target domain). While both DA and DG have extensively studied the problem of distribution shifts, they typically assume acces- sibility to the raw source data. However, in many practical scenarios like personal consumption records, the raw data should not be publicly available due to data protection reg- ulations. Further, existing methods have to perform heavy backward computation, resulting in unbearable training costs. Test-time adaptation (TTA) [3,11,16,24,26,54,65,81] attempts to address the distribution shift online at test time with only unlabeled test data streams. Unequivocally, TTA has drawn widespread attention in a variety of applications, e.g., 2D/3D visual recognition [2, 29, 49, 65, 82], multi- modality [63, 64] and document understanding [15]. Prior TTA studies [7, 20, 70, 73] mostly concentrate on a simple adaptation scenario, where test samples are inde- pendently sampled from a fixed target domain. To name a few, Sun et al. [65] adapt to online test samples drawn from a constant or smoothly changing distribution with an auxil- iary self-supervised task. Wang et al. [70] adapt to a fixed arXiv:2303.13899v1  [cs.CV]  24 Mar 2023Table 1. Comparison between our proposed practical test-time adaptation (PTTA) and related adaptation settings. Setting Adaptation StageAvailable Data Test Data Stream Train Test Source Target Distribution Sampling Protocol Domain Adaptation ! % ! ! - - Domain Generalization ! % ! % - - Test-Time Training [65] ! ! ! ! stationary independently Fully Test-Time Adaptation [70] % ! % ! stationary independently Continual Test-Time Adaptation [73]% ! % ! continually changing independently Non-i.i.d. Test-Time Adaptation [5, 19]% ! % ! stationary correlatively Practical Test-Time Adaptation (Ours)% ! % ! continually changing correlatively target distribution by performing entropy minimization on- line. However, such an assumption is violated when the test environments change frequently [73]. Later on, Boudiaf et al. [5] and Gonget al. [19] consider the temporal correlation ship within test samples. For example, in autonomous driv- ing, test samples are highly correlated over time as the car will follow more vehicles on the highway or will encounter more pedestrians in the streets. More realistically, the data distribution changes as the surrounding environment alerts in weather, location, or other factors. In a word, distribution change and data correlation occur simultaneously in reality. Confronting continually changing distributions, tradi- tional algorithms like pseudo labeling or entropy minimiza- tion become more unreliable as the error gradients cumu- late. Moreover, the high correlation among test samples re- sults in the erroneous estimation of statistics for batch nor- malization and collapse of the model. Driven by this analy- sis, adapting to such data streams will encounter two major obstacles: 1) incorrect estimation in the batch normaliza- tion statistics leads to erroneous predictions of test samples, consequently resulting in invalid adaptation; 2) the model will easily or quickly overfit to the distribution caused by the correlative sampling. Thus, such dynamic scenarios are pressing for a new TTA paradigm to realize robust adapta- tion. In this work, we launch a more realistic TTA setting, where distribution changing and correlative sampling oc- cur simultaneously at the test phase. We call this Practical Test-Time Adaptation, or briefly,PTTA. To understand more clearly the similarities and differences between PTTA and the previous setups, we visualize them in Figure 1 and sum- marize them in Table 1. To conquer this challenging prob- lem, we propose a Robust Test-Time Adaptation (RoTTA) method, which consists of three parts: 1) robust statistics es- timation, 2) category-balanced sampling considering time- liness and uncertainty and 3) time-aware robust training. More concretely, we first replace the erroneous statistics of the current batch with global ones maintained by the expo- nential moving average. It is a more stable manner to esti- mate the statistics in BatchNorm layers. Then, we simulate a batch of independent-like data in memory with category- balanced sampling while considering the timeliness and un- certainty of the buffered samples. That is, samples that are newer and less uncertain are kept in memory with higher priority. With this batch of category-balanced, timely and confident samples, we can obtain a snapshot of the current distribution. Finally, we introduce a time-aware reweight- ing strategy that considers the timeliness of the samples in the memory bank, with a teacher-student model to perform robust adaptation. With extensive experiments, we demon- strate that RoTTA can robustly adapt in the practical setup, i.e., PTTA. In a nutshell, our contributions can be summarized as: • We propose a new test-time adaptation setup that is more suitable for real-world applications, namely practical test-time adaptation (PTTA). PTTA considers both distribution changing and correlation sampling. • We benchmark the performance of prior methods in PTTA and uncover that they only consider one aspect of the problem, resulting in ineffective adaptation. • We propose a robust test-time adaptation method (RoTTA), which has a more comprehensive considera- tion of PTTA challenges. Ease of implementation and effectiveness make it a practical deployment option. • We extensively demonstrate the practicality of PTTA and the effectiveness of RoTTA on common TTA benchmarks [23], i.e., CIFAR-10-C and CIFAR-100- C and a large-scale DomainNet [58] dataset. RoTTA obtains state-of-the-art results, outperforming the best baseline by a large margin (reducing the averaged classification error by over 5.9%, 5.5% and 2.2% on CIFAR-10-C, CIFAR-100-C and DomainNet, respec- tively). 2. Related Work Domain adaptation (DA) studies the problem of transfer- ring the knowledge learned from a labeled source dataset to an unlabeled target dataset [8, 17, 43, 51, 67, 68]. Represen- tative techniques include latent distribution alignment [48, 77], adversarial training [17, 62], or self-training [75, 85]. The limitation of this setting, however, is that an unlabeled test dataset (target domain) is needed at training time, in addition to a labeled training dataset (source domain). Ac- cordingly, it might fail to handle more practical scenariosFeature 𝐹Robust batch normalization (RBN)Update𝜇௚, 𝜎௚ଶNormalizeFeature𝐹′Update bank with current sample  Training lossℒ௥in Eq. (7) Teacher StudentAdaptation with RBNMemorybankEMA 𝑡A stream of online dataUpdateTest timeCorrelationsamplingStrong & weakaugmentation flowDistributionsCategoryTeacherMajor classhas highest ℋin majorRemoveAddWhen ℋ>ℋSamples to beadded& removed Figure 2. Framework overview. Firstly, we replace the batch normalization layer with RBN which robustly normalizes the feature map. During the inference of the online test stream of PTTA, we utilize the predictions of samples to maintain a memory bank by category- balanced sampling with timeliness and uncertainty. Finally, we use the category-balanced, timely and confident data in the memory bank combined with a robust loss to adapt the model at test time. like test-time adaptation. Our practical test-time adaptation setting can be viewed as performing correlatively sample adaptation on the fly. It is worth noting that standard domain adaptation techniques might collapse when only continual data streams from multiple target domains are accessible. Domain generalization (DG) assumes that multiple source domains are available for model training and tries to learn models that can generalize well to any unseen domains [4, 26,40,41,52,84]. A broad spectrum of methodologies based on data augmentation [78, 84], meta-learning [14, 40], or domain alignment [50,52] has made great progress. In con- trast, this work instead aims to improve the performance of source pre-trained models at the test time by using unla- beled online data streams from multiple continually chang- ing target domains. Continual learning (CL) (also known as incremental learning, life-long learning) addresses the problem of learn- ing a model for many tasks sequentially without forgetting knowledge obtained from the preceding tasks. [1, 6, 31, 37, 60]. CL methods can often be categorized into replay- based [60, 66] and regularization-based [31, 44] methods. Ideas from continual learning are also adopted for continu- ous domain adaptation approaches [34, 74] In our work, we share the same motivation as CL and point out that prac- tical test-time adaptation (PTTA) also suffers catastrophic forgetting (i.e., performance degradation on new test sam- ples due to correlation sampling), which makes test-time adaptation approaches are unstable to deploy. Test-time adaptation (TTA) focus on more challenging settings where only source model and unlabeled target data are available [9, 18, 27, 28, 35, 46, 61]. A similar paradigm is source-free domain adaptation (SFDA) [10, 36, 47, 79], which also requires no access to the training (source) data. To name a few, Liang et al . [45] fit the source hypoth- esis by exploiting the information maximization and self- supervised pseudo-labeling. Kundu et al. [35] formalize a unified solution that explores SFDA without any category- gap knowledge. To fully utilize any arbitrary pre-trained model, Sun et al. [65] propose conducting adaptation on the fly with an auxiliary self-supervised task. Later on, Wanget al. [70] take a source pre-trained model and adapt it to the test data by updating a few trainable parameters in Batch- Norm layers [25] using entropy minimization [21]. While standard TTA has been widely studied in many tasks [2, 20, 63, 64, 70, 82], the fact remains that both dis- tribution changing [73] and data correlation sampling [19] has only been considered in isolation. For example, Gong et al. [19] propose instance-aware batch normalization and prediction-balanced reservoir sampling to address the chal- lenges of correlatively sampled test streams, however, it does not consider unstable adaptation resulting from long- term adaptation on continually changing distributions. On the other hand, Wang et al. [73] assume that the target test data is streamed from a continually changing environment and continually adapt an off-the-shelf source pre-trained model to the current test data. In this work, we launch PTTA, a more practical TTA setting to connect both worlds: distribution changing and correlation sampling. 3. Method 3.1. Problem Definition and Motivation Given a model fθ0 with parameter θ0 pre-trained on source domain DS = {(xS, yS)}, the proposed practical test-time adaptation (PTTA) aims to adapt fθ0 to a stream of online unlabeled samples X0, X1, ...,XT , where Xt is a batch of highly correlated samples from the distribution Ptest that changes with time t continually. More specifi- cally, at test time, with time going on, the test distribution Ptest changes continually as P0, P1, ...,P∞. At time step t, we will receive a batch of unlabeled and correlated samplesmotion distribution changing snow time  Distributions and Labels of PTTA T est Stream uniform 10 1 0.1 0.01 0.001 Dirichlet Parameter  Figure 3. Illustration of the labels and distributions of the test stream of CIFAR10-C under the setup PTTA. And we adopt Dirichlet distribution to simulate the process of correlative sam- pling. It is clear that as the concentration parameter δ decreases, the correlation among sampled data increases, which is reflected in the increasing aggregation of categories. Xt from Ptest. Next, Xt is fed into the model fθt and the model needs to adapt itself to the current test data streams and make predictions fθt (Xt) on the fly. As a matter of fact, this setup is largely driven the prac- tical demands of deploying models in dynamic scenarios. Taking for example the case of autonomous driving men- tioned in § 1, test samples are highly correlated and the data distribution changes continually with the weather or loca- tion. Another example is the situation of intelligent moni- toring, the camera will continuously capture more people at certain times, such as after work, but fewer of them during work time. Meanwhile, the light condition changes con- tinually from day to night. The deployed model should be robustly adapted in such dynamic scenarios. In a word, dis- tribution change and data correlation often happen simul- taneously in the real world. For this reason, existing TTA methods [7,9,19,28,70,73,81] might become unstable when the test stream is sampled from such dynamic scenarios. To obtain the test stream of PTTA, we adopt Dirich- let Distribution with parameter δ to simulate the correla- tion among test samples. We present the test data streams corresponding to different values of δ on the CIFAR10-C dataset in Figure 3. We can observe that the smaller δ is, the higher the correlation will be. For the sake of unity, we set δ = 0.1 as the default for all experiments. In the follow- ing, we present a robust test-time adaptation framework for the practical test-time adaptation setup defined above. An overview of our RoTTA is illustrated in Figure 2. 3.2. Robust Test-Time Adaptation Motivated by the fact that the statistics of current batch data, which are commonly used in previous TTA meth- ods [7, 20, 65, 70, 73], become unreliable when they en- counter correlative test data streams, we first turn to the global robust statistics for normalization. Then, to effec- tively adapt to the current distribution, we maintain a mem- ory bank by category-balanced sampling with considering timeliness and uncertainty, which captures a more stable snapshot of the distribution. Finally, we utilize the teacher- student model and design a timeliness-based reweighting strategy to train the model robustly. Robust batch normalization (RBN). Batch Normaliza- tion (BN) [25] is a widely-used training technique as it can accelerate the training and convergence speed of networks and stabilize the training process by reducing the risk of gradient explosion and vanishing. Given the feature map F ∈ RB×C×H×W as the input for a BN layer when train- ing, the channel-wise mean µ ∈ RC and variance σ2 ∈ RC are calculated as follows: µc = 1 BHW BX b=1 HX h=1 WX w=1 F(b,c,h,w) , (1) σ2 c = 1 BHW BX b=1 HX h=1 WX w=1 (F(b,c,h,w) − µc)2 . (2) Then the feature map is normalized and refined in a channel-wise manner as BN (F(b,c,h,w); µ, σ2) =γc F(b,c,h,w) − µc √σ2c + ϵ + βc , (3) where γ, β∈ RC are learnable parameters in the layer and ϵ > 0 is a constant for numerical stability. Meanwhile, during training, the BN layer maintains a group of global running mean and running variance (µs, σ2 s) for inference. Due to the domain shift at test time, the global statis- tics (µs, σ2 s) normalize test features inaccurately, causing significant performance degradation. To tackle the prob- lem above, some methods [55, 70, 73] use the statistics of the current batch to perform normalization. Unfortunately, when the test samples have a high correlation under PTTA setup, the statistics of the current batch also fail to correctly normalize the feature map, as demonstrated in Figure 4c. Specifically, the performance of BN [53] decreases rapidly as the data correlation increases. Based on the analysis above, we propose a robust batch normalization (RBN) module, which maintains a group of global statistics (µg, σ2 g) to normalize the feature map ro- bustly. Before the whole test-time adaptation, (µg, σ2 g) is initialized as the running mean and variance (µs, σ2 s) of the pre-trained model. When adapting the model, we update the global statistics first by exponential moving average as µg = (1− α)µg + αµ , (4) σ2 g = (1− α)σ2 g + ασ2 , (5) where (µ, σ2) is the statistics of the buffered samples in the memory bank. Then we normalize and affine the feature as Eq. (3) with (µg, σ2 g). When inferring for test samples, we directly utilize (µg, σ2 g) to calculate the output as Eq (3). Al- though simple, RBN is effective enough to tackle the prob- lem of normalization on test streams of PTTA.Category-balanced sampling with timeliness and uncer- tainty (CSTU). In the PTTA setup, the correlation among test samples Xt at time t leads to a deviation between the observed distribution bPtest and the test distribution Ptest. Specifically, the marginal label distribution p(y|t) tends to differ from p(y). Continuously learning with Xt over time t can lead to model adaptation to an unreliable distribution bPtest, resulting in ineffective adaptation and an increased risk of model collapse. To address this issue, we propose a category-balanced memory bank M with a capacity of N, which takes into account the timeliness and uncertainty of samples when up- dating. In particular, we adopt the predictions of test sam- ples as pseudo labels to guide the update ofM. Meanwhile, to guarantee the balance among categories, we distribute the capacity of M equally to each category, and samples of the major categories will be replaced first (refer to lines 5-9 in Algorithm 1). Furthermore, due to the continually changing test distribution, old samples in M are limited in value, and could even impair the ability of the model to adapt to the current distribution. Additionally, samples of high uncer- tainty always produce erroneous gradient information that can hinder model adaptation, as suggested by [55]. With this in mind, we attach each sample in M with a group of heuristics (A, U), where A, initialized as 0 and in- creasing with time t, is the age of the sample, and U the un- certainty calculated as the entropy of the prediction. Next, we combine the timeliness and uncertainty to calculate a heuristic score, i.e., category-balanced sampling with time- liness and uncertainty (CSTU), as follows: H = λt 1 1 + exp(−A/N) + λu U log C , (6) where λt and λu make the trade-off between timeliness and uncertainty, and for simplicity, λt and λu are set to 1.0 for all experiments, andC is the number of categories. We sum- marize our sampling algorithm in Algorithm 1. With CSTU, we can obtain a robust snapshot of the current test distribu- tion Ptest, and effectively adapt the model to it. Robust training with timeliness. Actually, after replacing BN layers with our RBN and obtaining the memory bank selected via CSTU, we can directly adopt the widely used techniques like pseudo labeling or entropy minimization to perform test-time adaptation. However, we notice that too old or unreliable instances still have the opportunity to stay in M since keeping the category balance is assigned the top priority. In addition, too aggressive updates of the model will make the category balance ofM unreliable, resulting in unstable adaptation. Meanwhile, error accumulation caused by the distribution change also makes the aforementioned approaches unworkable. To further reduce the risk of error gradients information from old and unreliable instances and stabilize the adapta- tion, we turn to the robust unsupervised learning method Algorithm 1: CSTU for one test sample. 1 Input: a test sample x and the teacher model fθT . 2 Define: memory bank M and its capacity N, number of classes C, per class occupation O ∈RC, total occupation Ω, classes to pop instance D. 3 Infer as p(y|x) =Softmax(fθT (x)). 4 Calculate the predicted category of x as ˆy = arg maxc p(c|x), the uncertainty as Ux = −PC c=1 p(c|x) log(p(c|x)), the age as Ax = 0, and the heuristic score Hx of x with Eq (6) 5 if Oˆy < N C then 6 if Ω <N: Search range D = ∅. 7 else: Search range D = {j|j = arg maxc Oc} 8 else 9 Search range D = {ˆy} 10 if D is ∅ then 11 Add (x, ˆy, Hx, Ux) into M. 12 else 13 Find the instance (ˆx, yˆx, Aˆx, Uˆx) with the highest value in Eq (6) Hˆx among D. 14 if Hx < Hˆx then 15 Remove (ˆx, yˆx, Aˆx, Uˆx) from M. 16 Add (x, ˆy, Hx, Ux) into M. 17 else 18 Discard x. 19 Increase the age of all instances in M. teacher-student model and propose a timeliness reweight- ing strategy. In addition, for the sake of time efficiency and stability, only affine parameters in RBN are trained during adaptation. At time step t, after inferring for the correlated data Xt with the teacher model fθT t and updating the memory bank M with Xt, we begin updating the student model fθS t and the teacher model fθT t . Firstly, we update parameters of stu- dent model θS t → θS t+1 by minimizing the following loss: Lr = 1 Ω ΩX i=1 L(xM i , Ai; θT t , θS t ) , (7) where Ω = |M| is the total occupation of the memory bank, and xM i and Ai(i = 1, ..., Ω) are instances in the memory bank and their age respectively. Subsequently, the teacher model is updated by exponential moving average as θT t+1 = (1− ν)θT t + νθS t+1 . (8) To calculate the loss value of an instancexM i from the mem- ory bank, the timeliness reweighting term is computed as E(Ai) = exp(−Ai/N) 1 + exp(−Ai/N) , (9)where Ai is the age of xM i , and N is the capacity of the bank. And then we calculate the cross entropy between the soft-max prediction pS(y|x′′ i ) of the strong-augmented view x′′ i from the student model and that pT (y|x′ i) of the weak- augmented view 1 x′ i from the teacher model as follows: ℓ(x′ i, x′′ i ) =−1 C CX c=1 pT (c|x′ i) logpS(c|x′′ i ) . (10) Finally, equipped with Eq. (9) and Eq. (10), the right-hand side of Eq. (7) reduces to L(xM i , Ai; θT t , θS t ) =E(Ai)ℓ(x′ i, x′′ i ) . (11) To sum up, equipped with RBN, CSTU, and robust training with timeliness, our RoTTA is capable of effectively adapt- ing any pre-trained models in dynamic scenarios. 4. Experiments 4.1. Setup Datasets. CIFAR10-C and CIFAR100-C [23] are the com- monly used TTA benchmarks to testify the robustness un- der corruptions. Both of them are obtained by applying 15 kinds of corruption with 5 different degrees of severity on their clean test images of original datasets CIFAR10 and CIFAR100 respectively. CIFAR10/CIFAR100 [32] have 50,000/10,000 training/test images, all of which fall into 10/100 categories. DomainNet [58] is the largest and hard- est dataset to date for domain adaptation and consists of about 0.6 million images with 345 classes. It consists of six different domains including Clipart (clp), Infograph (inf), Painting (pnt), Quickdraw (qdr), Real (rel), and Sketch (skt). We first pre-train a source model on the train set in one of six domains and testify all baseline methods on the test set of the remaining five domains. Implementation details. All experiments are conducted with PyTorch [57] framework. In the case of robustness to corruption, following the previous methods [55, 70, 73], we obtain the pre-trained model from RobustBench bench- mark [12], including the WildResNet-28 [80] for CIFAR10 → CIFAR10-C, and the ResNeXt-29 [76] for CIFAR100 → CIFAR100-C. Then, we change the test corruption at the highest severity 5 one by one to simulate that the test distri- bution continually changes with time in PTTA. And in the case of generalization under the huge domain gap, we train a ResNet-101 [22] by standard classification loss for each domain in DomainNet and adapt them continually to differ- ent domains except the source domain. Meanwhile, we uti- lize the Dirichlet distribution to simulate the correlatively sampled test stream for all datasets. For optimization, we adopt Adam [30] optimizer with learning rate 1.0 × 10−3, 1Weak augmentation is ReSize+CenterCrop. Strong augmentation is a combination nine operations like Clip, ColorJitter, and RandomAffine. β = 0.9. For a fair comparison, we set the batch size for all methods as 64 and the capacity of the memory bank of RoTTA as N = 64. Concerning the hyperparameters, we adopt a unified set of values for RoTTA across all experi- ments including α = 0.05, ν = 0.001, λt = 1.0, λu = 1.0, and δ = 0.1. More details are provided in the appendix. 4.2. Comparisons with the State-of-the-arts Robustness under corruptions. The classification error on CIFAR10→CIFAR10-C and CIFAR100→CIFAR100-C are shown in Table 2 and Table 3 respectively. We change the type of the current corruption at the highest severity 5 as time goes on, and sample data correlatively for infer- ence and adaptation simultaneously. The same test stream is shared across all compared methods. From Table 2 and Table 3, we can see that RoTTA achieves the best performance compared to previous meth- ods. Moreover, RoTTA has a significant performance gain to the second-best method that 5.9% improvement on CIFAR10 →CIFAR10-C and 5.5% improvement on CIFAR100→CIFAR100-C respectively, verifying the effec- tiveness of RoTTA to adapt the model under PTTA. In more detail, we can observe that BN [53], PL [39], TENT [70] and CoTTA [73] negatively adapt the model to the test streams of both datasets compared to Source (−6.5 ∼ −46.4%). This is attributed to the fact that these methods overlook the issues posed by correlation sampling, which can result in highly correlated data within a batch. As a consequence, traditional normalization statistics may be ineffective in appropriately normalizing the feature maps. Equipped with RBN and CSTU, RoTTA no longer suffers from this issue. Meanwhile, in Table 3, if focus on the adaptation procedure, we can see that the performance of PL [39], TENT [70] and NOTE [19] becomes worse and worse, and eventually, the model even collapses (error rate > 97%). This reveals that the impact of error accumula- tion on long-term adaptation can be catastrophic. To tackle this problem, RoTTA turns to robustly adapt the model with timeliness reweighting and confident samples in the mem- ory bank, and superior performance throughout the adapta- tion process demonstrates its effectiveness. In addition, we find that although LAME [5] never tunes the parameters of the model, it is still a competi- tive baseline for example it achieves the second-best result on CIFAR100→CIFAR100-C. However, its performance is very dependent on the performance of the pre-trained model e.g. negligible improvement on difficult corruptions (shot, gaussian, pixelate). On the contrary, our RoTTA is more flexible and achieves better and more robust results. Generalization under domain shift. We also evalu- ate RoTTA under a more challenging dataset DomainNet, where we continually adapt a source pre-trained model to correlatively sampled test streams of the rest domains. AsTable 2. Average classification error of the task CIFAR10→ CIFAR10-C while continually adapting to different corruptions at the highest severity 5 with correlatively sampled test stream under the proposed setup PTTA. Time t− − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − → Method motionsnow fog shot defocuscontrastzoom brightnessfrost elasticglass gaussianpixelatejpeg impulse Avg. Source 34.8 25.1 26.0 65.7 46.9 46.7 42.0 9.3 41.3 26.6 54.3 72.3 58.5 30.3 72.9 43.5BN [53] 73.2 73.4 72.7 77.2 73.7 72.5 72.9 71.0 74.1 77.7 80.0 76.9 75.5 78.3 79.0 75.2PL [39] 73.9 75.0 75.6 81.0 79.9 80.6 82.0 83.2 85.3 87.3 88.3 87.5 87.5 87.5 88.2 82.9TENT [70] 74.3 77.4 80.1 86.2 86.7 87.3 87.9 87.4 88.2 89.0 89.2 89.0 88.3 89.7 89.2 86.0LAME [5] 29.5 19.0 20.3 65.3 42.4 43.4 36.8 5.4 37.2 18.6 51.2 73.2 57.0 22.6 71.3 39.5CoTTA [73]77.1 80.6 83.1 84.4 83.9 84.2 83.1 82.6 84.4 84.2 84.5 84.6 82.7 83.8 84.9 83.2NOTE [19] 18.0 22.1 20.6 35.6 26.9 13.6 26.5 17.3 27.2 37.0 48.3 38.8 42.6 41.9 49.7 31.1 RoTTA 18.1 21.3 18.8 33.6 23.6 16.5 15.1 11.2 21.9 30.7 39.6 26.8 33.7 27.8 39.5 25.2(+5.9) Table 3. Average classification error of the task CIFAR100 → CIFAR100-C while continually adapting to different corruptions at the highest severity 5 with correlatively sampled test stream under the proposed setup PTTA. Time t− − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − → Method motionsnow fog shot defocuscontrastzoom brightnessfrost elasticglass gaussianpixelatejpeg impulse Avg. Source 30.8 39.5 50.3 68.0 29.3 55.1 28.8 29.5 45.8 37.2 54.1 73.0 74.7 41.2 39.4 46.4BN [53] 48.5 54.0 58.9 56.2 46.4 48.0 47.0 45.4 52.9 53.4 57.1 58.2 51.7 57.1 58.8 52.9PL [39] 50.6 62.1 73.9 87.8 90.8 96.0 94.8 96.4 97.4 97.2 97.4 97.4 97.3 97.4 97.4 88.9TENT [70] 53.3 77.6 93.0 96.5 96.7 97.5 97.1 97.5 97.3 97.2 97.1 97.7 97.6 98.0 98.3 92.8LAME [5] 22.4 30.4 43.9 66.3 21.3 51.7 20.6 21.8 39.6 28.0 48.7 72.8 74.6 33.1 32.3 40.5CoTTA [73]49.2 52.7 56.8 53.0 48.7 51.7 49.4 48.7 52.5 52.2 54.3 54.9 49.6 53.4 56.2 52.2NOTE [19] 45.7 53.0 58.2 65.6 54.2 52.0 59.8 63.5 74.8 91.8 98.1 98.3 96.8 97.0 98.2 73.8 RoTTA 31.8 36.7 40.9 42.1 30.0 33.6 27.9 25.4 32.3 34.0 38.8 38.7 31.3 38.0 42.9 35.0(+5.5) Table 4. Average classification error of DomainNet while continually adapting to different domains with correlatively sampled test stream. Timet− − − − − − − − − − − − − − − − − − →Timet− − − − − − − − − − − − − − − − − − →Timet− − − − − − − − − − − − − − − − − − →Timet− − − − − − − − − − − − − − − − − − →Sourceclp inf pnt qdr rel sktAvg. BN clp inf pnt qdr rel sktAvg. PL clp inf pnt qdr rel sktAvg.TENTclp inf pnt qdr rel sktAvg. clp N/A 83.9 65.4 88.6 48.0 59.1 69.0clp N/A 88.6 70.7 90.5 65.4 67.0 76.5clp N/A 94.5 98.9 99.5 99.7 99.7 98.5clp N/A 87.5 71.9 94.2 96.2 98.9 89.7inf 61.8 N/A 66.9 96.0 50.0 70.6 69.1inf 68.6 N/A 74.2 96.2 69.9 76.8 77.1inf 82.6 N/A 99.2 99.6 99.7 99.3 96.1inf 68.6 N/A 75.0 97.3 95.9 98.7 87.1pnt 56.5 83.7 N/A 94.2 42.6 63.4 68.1pnt 60.8 87.9 N/A 94.3 62.3 68.7 74.8pnt 78.6 99.4 N/A 99.7 99.6 99.7 95.4pnt 61.7 87.1 N/A 96.4 95.3 98.8 87.8qdr 89.2 99.0 98.6 N/A 95.0 92.3 94.8qdr 80.3 97.7 92.6 N/A 88.7 88.1 89.5qdr 81.7 99.5 99.6 N/A 99.7 99.8 96.1qdr 78.9 97.1 91.6 N/A 89.2 88.7 89.1rel 49.4 80.4 51.5 93.4 N/A 63.3 67.6rel 57.9 87.1 63.1 94.3 N/A 70.8 74.6rel 73.5 99.4 99.2 99.6 N/A 99.7 94.3rel 57.8 86.4 68.1 96.9 N/A 96.7 81.2skt 47.5 88.2 62.9 87.1 51.8 N/A 67.5skt 50.4 87.6 64.6 89.6 63.1 N/A 71.1skt 64.8 99.2 99.4 99.7 99.7 N/A 92.6skt 51.9 87.2 69.1 95.3 97.3 N/A 80.1Avg.60.9 87.0 69.1 91.9 57.5 69.7 72.7Avg.63.6 89.8 73.0 93.0 69.9 74.3 77.3Avg.76.2 98.4 99.3 99.6 99.7 99.6 95.5Avg.63.8 89.0 75.1 96.0 94.8 96.4 85.8 Timet− − − − − − − − − − − − − − − − − − →Timet− − − − − − − − − − − − − − − − − − →Timet− − − − − − − − − − − − − − − − − − →Timet− − − − − − − − − − − − − − − − − − →LAMEclp inf pnt qdr rel sktAvg.COTTAclp inf pnt qdr rel sktAvg.NOTEclp inf pnt qdr rel sktAvg.RoTTAclp inf pnt qdr rel sktAvg. clp N/A 82.2 64.5 87.7 46.9 58.9 68.0clp N/A 90.6 77.9 89.3 76.3 72.7 81.4clp N/A 89.2 73.0 94.8 98.4 99.4 91.0clp N/A 85.5 62.0 82.0 49.3 59.8 67.7inf 60.1 N/A 65.7 95.4 48.5 69.4 67.8inf 74.5 N/A 82.0 95.7 80.2 81.5 82.8inf 75.4 N/A 78.7 98.7 98.1 99.5 90.1inf 61.8 N/A 63.7 91.5 52.5 67.6 67.4pnt 55.8 81.5 N/A 93.3 41.3 62.1 66.8pnt 66.3 89.8 N/A 93.4 74.0 75.4 79.8pnt 64.7 89.8 N/A 97.8 98.4 99.2 90.0pnt 53.3 84.1 N/A 89.1 47.3 61.4 67.0qdr 88.3 99.1 99.0 N/A 94.9 92.2 94.7qdr 82.3 98.2 94.6 N/A 92.5 90.1 91.5qdr 74.7 97.2 92.2 N/A 93.5 99.6 91.4qdr 77.5 97.0 89.8 N/A 80.3 82.2 85.3rel 48.0 79.3 50.1 91.6 N/A 60.2 65.8rel 64.0 90.3 73.2 93.5 N/A 77.6 79.7rel 61.3 89.2 68.9 98.8 N/A 99.2 83.5rel 49.1 82.3 50.3 88.0 N/A 61.1 66.2skt 45.6 87.1 59.5 83.9 49.9 N/A 65.2skt 56.1 89.2 71.9 89.2 73.5 N/A 76.0skt 55.2 89.7 70.1 96.9 98.3 N/A 82.0skt 42.6 83.7 54.4 80.9 47.5 N/A 61.8Avg.59.6 85.8 67.8 90.4 56.3 68.6 71.4Avg.68.6 91.6 79.9 92.2 79.3 79.5 81.9Avg.66.3 91.0 76.6 97.4 97.3 99.4 88.0Avg.56.8 86.5 64.0 86.3 55.4 66.469.2(+2.2) shown in Table 4, consistent with the previous analysis, most of the methods include BN [53], PL [39], TENT [70], CoTTA [73] and NOTE [19] even perform worse than the Source model ( −4.6 ∼ −22.8%). RoTTA consistently achieves the best performance and has 2.2% gain than the second method LAME [5], demonstrating RoTTA’s effec- tiveness again. 4.3. Ablation Study Effect of each component. To further investigate the effi- cacy of each component, we replace each part with the nor- mally used solutions to obtain three variants: (1) RoTTA w/o RBN, replace RBN with test-time BN in TENT [70]; (2) RoTTA w/o CSTU, directly adapt the model on test stream; (3) RoTTA w/o robust training (RT), directly adapt the model only with entropy minimization. As shown in Table 5, we can observe that significant performance degra- dation occurs for all variants, proving that every part of our proposed method is valid for PTTA. Take one com- ponent for a detailed example, without RBN robustly nor- malizing feature maps, the performance of RoTTA drops 50.2% and 16.3% on CIFAR10-C and CIFAR100-C respec- tively, proving that RBN is robust enough to tackle the prob- lem of normalization of correlatively sampled data streams. CSTU enables RoTTA to adapt to a more stable distribu- tion by maintaining a timely and confident snapshot of the test distribution. Meanwhile, robust training with timeliness greatly reduces the accumulation of errors. Every compo- nent behaves significantly to enable effective adaptation un- der PTTA. Effect of the distribution changing order. To exclude the effect of a fixed order of distribution changing, we con- ducted experiments on ten different sequences of changes on CIFAR10-C and CIFAR100-C with independently andBN PL TENT LAME CoTTA NOTE RoTTA0 10 20 30 40 50 60 70 80Classification error (%) Source CIFAR-10  CIFAR-10-C Independent Correlative (a) CIFAR10-C. BN PL TENT LAME CoTTA NOTE RoTTA0 20 40 60 80Classification error (%) Source CIFAR-100  CIFAR-100-C Independent Correlative (b) CIFAR100-C. uniform 10 1 0.1 0.01 0.001 30 40 50 60 70 80 90 100Classification error (%) Source BN PL TENT LAME CoTTA NOTE RoTTA (c) δ. 16 32 64 128 256 512 40 50 60 70 80 90 100Classification error (%) Source BN PL TENT LAME CoTTA NOTE RoTTA (d) Batch size. Figure 4. (a) & (b) we adapt the model continually to different corruptions of 10 different orders with independently and correlatively sampled test streams on CIFAR10-C and CFAR100-C respectively and report their average classification error. (c) & (d) we verify the effect of δ and batch size to different methods on CIFAR100-C respectively. Table 5. Classification error of different variants of our RoTTA. Variant CIFAR10-C CIFAR100-C Avg. RoTTA w/o RBN 75.4 51.3 63.4 RoTTA w/o CSTU 47.1 46.3 46.7 RoTTA w/o RT 78.2 95.0 81.6 RoTTA 25.2 35.0 30.1 correlatively sampled test streams respectively. As shown in Figure 4a and 4b, no matter what kind of setup, RoTTA can achieve excellent results. The detailed results on the correlatively sampled test streams are shown in Table 6, RoTTA achieves 4.3% and 4.7% progress on CIFAR10- C and CIFAR100-C respectively. This shows that RoTTA can adapt the model robustly and effectively in long-term scenarios where distribution continually changes and test streams are sampled either independently or correlatively, making it a good choice for model deployment. Effect of Dirichlet concentration parameter δ. We vary the value of δ on CIFAR100-C and compare RoTTA with other approaches in Figure 4c. As the value of δ increases, the performance of BN [53], PL [39], TENT [70] and CoTTA [73] drops quickly, because they never consider the increasing correlation among test samples. NOTE [19] is stable to correlatively sampled test streams but does not consider the distribution changing, causing ineffective adaptation. Meanwhile, the higher correlation between test samples will make the propagation of labels more accurate, which is why the result of LAME [5] slightly improves. Fi- nally, excellent and stable results once again prove the sta- bility and effectiveness of RoTTA. Effect of batch size. In real scenarios, considering deploy- ment environments may use different test batch sizes, we conduct experiments with different values of test batch sizes and results are shown in Figure 4d. For a fair comparison, we control the frequency of updating the model of RoTTA so that the number of samples involved in back-propagation is the same. As the batch size increases, we can see that all of the compared methods have a significant improvement except for lame which has a slight decrease. This is be- cause the number of categories in a batch increases with the Table 6. Average classification error of tasks CIFAR10 → CIFAR10-C and CIFAR100 → CIFAR100-C while continually adapting to different corruptions of 10 different orders at the high- est severity 5 with correlatively sampled test stream. Method CIFAR10-C CIFAR100-C Avg. Source 43.5 46.4 46.9 BN [53] 75.2 52.9 64.1 PL [39] 75.2 52.9 60.1 TENT [70] 82.3 93.2 87.8 LAME [5] 39.5 40.6 40.1 NOTE [19] 30.5 76.1 53.3 CoTTA [73] 83.1 52.8 67.9 RoTTA 26.2(+4.3) 35.9(+4.7) 31.1(+9.0) increasing batch size, causing the overall correlation to be- come lower but the propagation of labels to become more difficult. Most significantly, RoTTA achieves the best re- sults across different batch sizes, demonstrating its robust- ness in dynamic scenarios once again. 5. Conclusion This work proposes a more realistic TTA setting where distribution changing and correlative sampling occur si- multaneously at the test phase, namely Practical Test-Time Adaptation (PTTA). To tackle the problems of PTTA, we propose Robust Test-Time Adaptation (RoTTA) method against the complex data stream. More specifically, a group of robust statistics for the normalization of feature maps is estimated by robust batch normalization. Meanwhile, a memory bank is adopted to capture a snapshot of the test distribution by category-balanced sampling with consider- ing timeliness and uncertainty. Further, we develop a time- aware reweighting strategy with a teacher-student model to stabilize the adaptation process. Extensive experiments and ablation studies are conducted to verify the robustness and effectiveness of the proposed method. We believe this work will pave the way for thinking about adapting models into real-world applications by test-time adaptation algorithm. Acknowledgements. This paper was supported by National Key R&D Program of China (No. 2021YFB3301503), and also supported by the National Natural Science Foundation of China under Grant No. 61902028.References [1] Rahaf Aljundi, Min Lin, Baptiste Goujaud, and Yoshua Ben- gio. Gradient based sample selection for online continual learning. In NeurIPS, pages 11816–11825, 2019. 3 [2] Fatemeh Azimi, Sebastian Palacio, Federico Raue, J ¨orn Hees, Luca Bertinetto, and Andreas Dengel. Self-supervised test-time adaptation on video data. In WACV, pages 2603– 2612, 2022. 1, 3 [3] Mathilde Bateson, Herve Lombaert, and Ismail Ben Ayed. Test-time adaptation with shape moments for image segmen- tation. In MICCAI, pages 736–745, 2022. 1 [4] Gilles Blanchard, Gyemin Lee, and Clayton Scott. General- izing from several related classification tasks to a new unla- beled sample. In NeurIPS, pages 2178–2186, 2011. 3 [5] Malik Boudiaf, Romain Mueller, Ismail Ben Ayed, and Luca Bertinetto. Parameter-free online test-time adaptation. In CVPR, pages 8344–8353, 2022. 2, 6, 7, 8, 13, 14, 15, 16, 17 [6] Francisco M Castro, Manuel J Mar ´ın-Jim´enez, Nicol´as Guil, Cordelia Schmid, and Karteek Alahari. End-to-end incre- mental learning. In ECCV, pages 233–248, 2018. 3 [7] Dian Chen, Dequan Wang, Trevor Darrell, and Sayna Ebrahimi. Contrastive test-time adaptation. In CVPR, pages 295–305, 2022. 1, 4 [8] Yuhua Chen, Wen Li, Christos Sakaridis, Dengxin Dai, and Luc Van Gool. Domain adaptive faster r-cnn for object de- tection in the wild. In CVPR, pages 3339–3348, 2018. 2 [9] Zhixiang Chi, Yang Wang, Yuanhao Yu, and Jin Tang. Test- time fast adaptation for dynamic scene deblurring via meta- auxiliary learning. In CVPR, pages 9137–9146, 2021. 3, 4 [10] Boris Chidlovskii, St ´ephane Clinchant, and Gabriela Csurka. Domain adaptation in the absence of source domain data. In KDD, pages 451–460, 2016. 3 [11] Sungha Choi, Seunghan Yang, Seokeon Choi, and Sun- grack Yun. Improving test-time adaptation via shift-agnostic weight regularization and nearest source prototypes. In ECCV, pages 440–458, 2022. 1 [12] Francesco Croce, Maksym Andriushchenko, Vikash Se- hwag, Edoardo Debenedetti, Nicolas Flammarion, Mung Chiang, Prateek Mittal, and Matthias Hein. Robustbench: a standardized adversarial robustness benchmark. In Neurips, 2021. 6 [13] Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner, Mostafa Dehghani, Matthias Minderer, Georg Heigold, Syl- vain Gelly, Jakob Uszkoreit, and Neil Houlsby. An image is worth 16x16 words: Transformers for image recognition at scale. In ICLR, 2021. 1 [14] Ying-Jun Du, Jun Xu, Huan Xiong, Qiang Qiu, Xiantong Zhen, Cees G. M. Snoek, and Ling Shao. Learning to learn with variational information bottleneck for domain general- ization. In ECCV, pages 200–216, 2020. 3 [15] Sayna Ebrahimi, Sercan ¨O. Arik, and Tomas Pfister. Test- time adaptation for visual document understanding. CoRR, abs/2206.07240, 2022. 1 [16] Yossi Gandelsman, Yu Sun, Xinlei Chen, and Alexei A Efros. Test-time training with masked autoencoders. In NeurIPS, 2022. 1 [17] Yaroslav Ganin, Evgeniya Ustinova, Hana Ajakan, Pas- cal Germain, Hugo Larochelle, Franc ¸ois Laviolette, Mario Marchand, and Victor S. Lempitsky. Domain-adversarial training of neural networks. J. Mach. Learn. Res., 17:59:1– 59:35, 2016. 1, 2 [18] Yunhe Gao, Xingjian Shi, Yi Zhu, Hao Wang, Zhiqiang Tang, Xiong Zhou, Mu Li, and Dimitris N. Metaxas. Vi- sual prompt tuning for test-time domain adaptation. CoRR, abs/2210.04831, 2022. 3 [19] Taesik Gong, Jongheon Jeong, Taewon Kim, Yewon Kim, Jinwoo Shin, and Sung-Ju Lee. Robust continual test- time adaptation: Instance-aware BN and prediction-balanced memory. In NeurIPS, 2022. 1, 2, 3, 4, 6, 7, 8, 13, 14, 15, 16, 17 [20] Sachin Goyal, Mingjie Sun, Aditi Raghunathan, and J Zico Kolter. Test time adaptation via conjugate pseudo-labels. In NeurIPS, 2022. 1, 3, 4 [21] Yves Grandvalet and Yoshua Bengio. Semi-supervised learning by entropy minimization. In NeurIPS, pages 529– 536, 2004. 3 [22] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image recognition. In CVPR, pages 770–778, 2016. 1, 6 [23] Dan Hendrycks and Thomas G. Dietterich. Benchmarking neural network robustness to common corruptions and per- turbations. In ICLR, 2019. 2, 6 [24] Hengguan Huang, Xiangming Gu, Hao Wang, Chang Xiao, Hongfu Liu, and Ye Wang. Extrapolative continuous-time bayesian neural network for fast training-free test-time adap- tation. In NeurIPS, 2022. 1 [25] Sergey Ioffe and Christian Szegedy. Batch normalization: Accelerating deep network training by reducing internal co- variate shift. In ICML, pages 448–456, 2015. 3, 4 [26] Yusuke Iwasawa and Yutaka Matsuo. Test-time classifier ad- justment module for model-agnostic domain generalization. In NeurIPS, pages 2427–2440, 2021. 1, 3 [27] Vidit Jain and Erik Learned-Miller. Online domain adapta- tion of a pre-trained cascade of classifiers. In CVPR, pages 577–584, 2011. 3 [28] Minguk Jang and Sae-Young Chung. Test-time adaptation via self-training with nearest neighbor information. CoRR, abs/2207.10792, 2022. 3, 4 [29] Junho Kim, Inwoo Hwang, and Young Min Kim. Ev-tta: Test-time adaptation for event-based object recognition. In CVPR, pages 17724–17733, 2022. 1 [30] Diederik P. Kingma and Jimmy Ba. Adam: A method for stochastic optimization. In ICLR, 2015. 6 [31] James Kirkpatrick, Razvan Pascanu, Neil C. Rabinowitz, Joel Veness, Guillaume Desjardins, Andrei A. Rusu, Kieran Milan, John Quan, Tiago Ramalho, Agnieszka Grabska- Barwinska, Demis Hassabis, Claudia Clopath, Dharshan Ku- maran, and Raia Hadsell. Overcoming catastrophic forget- ting in neural networks. CoRR, abs/1612.00796, 2016. 3 [32] Alex Krizhevsky, Geoffrey Hinton, et al. Learning multiple layers of features from tiny images. 2009. 6[33] Alex Krizhevsky, Ilya Sutskever, and Geoffrey E. Hinton. Imagenet classification with deep convolutional neural net- works. In NeurIPS, pages 1097–1105, 2012. 1 [34] Ananya Kumar, Tengyu Ma, and Percy Liang. Understand- ing self-training for gradual domain adaptation. In ICML, pages 5468–5479, 2020. 3 [35] Jogendra Nath Kundu, Naveen Venkat, Rahul M. V ., and R. Venkatesh Babu. Universal source-free domain adapta- tion. In CVPR, pages 4543–4552, 2020. 3 [36] Vinod K Kurmi, Venkatesh K Subramanian, and Vinay P Namboodiri. Domain impression: A source data free do- main adaptation method. In WACV, pages 615–625, 2021. 3 [37] Matthias De Lange, Rahaf Aljundi, Marc Masana, Sarah Parisot, Xu Jia, Ales Leonardis, Gregory G. Slabaugh, and Tinne Tuytelaars. A continual learning survey: Defying for- getting in classification tasks. IEEE Trans. Pattern Anal. Mach. Intell., 44(7):3366–3385, 2022. 3 [38] Yann LeCun, Yoshua Bengio, and Geoffrey E. Hinton. Deep learning. Nat., 521(7553):436–444, 2015. 1 [39] Dong-Hyun Lee et al. Pseudo-label: The simple and effi- cient semi-supervised learning method for deep neural net- works. In Workshop on challenges in representation learn- ing, ICML, volume 3, page 896, 2013. 6, 7, 8, 12, 14, 15, 16, 17 [40] Da Li, Yongxin Yang, Yi-Zhe Song, and Timothy M. Hospedales. Learning to generalize: Meta-learning for do- main generalization. In AAAI, pages 3490–3497, 2018. 1, 3 [41] Haoliang Li, Sinno Jialin Pan, Shiqi Wang, and Alex C. Kot. Domain generalization with adversarial feature learning. In CVPR, pages 5400–5409, 2018. 1, 3 [42] Shuang Li, Binhui Xie, Qiuxia Lin, Chi Harold Liu, Gao Huang, and Guoren Wang. Generalized domain conditioned adaptation network. IEEE Trans. Pattern Anal. Mach. Intell., 44(8):4093–4109, 2022. 1 [43] Shuang Li, Mixue Xie, Kaixiong Gong, Chi Harold Liu, Yulin Wang, and Wei Li. Transferable semantic augmen- tation for domain adaptation. In CVPR, pages 11516–11525, 2021. 2 [44] Zhizhong Li and Derek Hoiem. Learning without forgetting. IEEE Trans. Pattern Anal. Mach. Intell., 40(12):2935–2947, 2018. 3 [45] Jian Liang, Dapeng Hu, and Jiashi Feng. Do we really need to access the source data? source hypothesis transfer for un- supervised domain adaptation. In ICML, pages 6028–6039, 2020. 1, 3 [46] Yuejiang Liu, Parth Kothari, Bastien van Delft, Baptiste Bellot-Gurlet, Taylor Mordan, and Alexandre Alahi. TTT++: when does self-supervised test-time training fail or thrive? In NeurIPS, pages 21808–21820, 2021. 3 [47] Yuang Liu, Wei Zhang, and Jun Wang. Source-free do- main adaptation for semantic segmentation. In CVPR, pages 1215–1224, 2021. 3 [48] Mingsheng Long, Yue Cao, Zhangjie Cao, Jianmin Wang, and Michael I. Jordan. Transferable representation learning with deep adaptation networks. IEEE Trans. Pattern Anal. Mach. Intell., 41(12):3071–3085, 2019. 1, 2 [49] Wenao Ma, Cheng Chen, Shuang Zheng, Jing Qin, Huimao Zhang, and Qi Dou. Test-time adaptation with calibration of medical image classification nets for label distribution shift. In MICCAI, pages 313–323, 2022. 1 [50] Divyat Mahajan, Shruti Tople, and Amit Sharma. Domain generalization using causal matching. In ICML, pages 7313– 7324, 2021. 3 [51] Yishay Mansour, Mehryar Mohri, and Afshin Rostamizadeh. Domain adaptation: Learning bounds and algorithms. In COLT, 2009. 2 [52] Krikamol Muandet, David Balduzzi, and Bernhard Sch¨olkopf. Domain generalization via invariant fea- ture representation. In ICML, pages 10–18, 2013. 1, 3 [53] Zachary Nado, Shreyas Padhy, D. Sculley, Alexander D’Amour, Balaji Lakshminarayanan, and Jasper Snoek. Evaluating prediction-time batch normalization for robust- ness under covariate shift. CoRR, abs/2006.10963, 2020. 4, 6, 7, 8, 12, 14, 15, 16, 17 [54] Shuaicheng Niu, Jiaxiang Wu, Yifan Zhang, Yaofo Chen, Shijian Zheng, Peilin Zhao, and Mingkui Tan. Efficient test- time model adaptation without forgetting. In ICML, pages 16888–16905, 2022. 1 [55] Shuaicheng Niu, Jiaxiang Wu, Yifan Zhang, Yaofo Chen, Shijian Zheng, Peilin Zhao, and Mingkui Tan. Efficient test- time model adaptation without forgetting. In ICML, volume 162, pages 16888–16905, 2022. 4, 5, 6 [56] Sinno Jialin Pan and Qiang Yang. A survey on transfer learning. IEEE Trans. Knowl. Data Eng., 22(10):1345–1359, 2010. 1 [57] Adam Paszke, Sam Gross, Francisco Massa, Adam Lerer, James Bradbury, Gregory Chanan, Trevor Killeen, Zeming Lin, Natalia Gimelshein, Luca Antiga, et al. Pytorch: An imperative style, high-performance deep learning library. In NeurIPS, pages 8024–8035, 2019. 6 [58] Xingchao Peng, Qinxun Bai, Xide Xia, Zijun Huang, Kate Saenko, and Bo Wang. Moment matching for multi-source domain adaptation. In ICCV, pages 1406–1415, 2019. 2, 6 [59] Joaquin Quinonero-Candela, Masashi Sugiyama, Anton Schwaighofer, and Neil D Lawrence. Dataset shift in ma- chine learning. 2008. 1 [60] Sylvestre-Alvise Rebuffi, Alexander Kolesnikov, Georg Sperl, and Christoph H. Lampert. icarl: Incremental classi- fier and representation learning. InCVPR, pages 5533–5542, 2017. 3 [61] Amelie Royer and Christoph H Lampert. Classifier adapta- tion at prediction time. In CVPR, pages 1401–1409, 2015. 3 [62] Kuniaki Saito, Kohei Watanabe, Yoshitaka Ushiku, and Tat- suya Harada. Maximum classifier discrepancy for unsuper- vised domain adaptation. In CVPR, pages 3723–3732, 2018. 2 [63] Inkyu Shin, Yi-Hsuan Tsai, Bingbing Zhuang, Samuel Schulter, Buyu Liu, Sparsh Garg, In So Kweon, and Kuk- Jin Yoon. MM-TTA: multi-modal test-time adaptation for 3d semantic segmentation. In CVPR, pages 16907–16916, 2022. 1, 3[64] Manli Shu, Weili Nie, De-An Huang, Zhiding Yu, Tom Goldstein, Anima Anandkumar, and Chaowei Xiao. Test- time prompt tuning for zero-shot generalization in vision- language models. In NeurIPS, 2022. 1, 3 [65] Yu Sun, Xiaolong Wang, Zhuang Liu, John Miller, Alexei Efros, and Moritz Hardt. Test-time training with self- supervision for generalization under distribution shifts. In ICML, pages 9229–9248, 2020. 1, 2, 3, 4 [66] Rishabh Tiwari, KrishnaTeja Killamsetty, Rishabh K. Iyer, and Pradeep Shenoy. GCR: gradient coreset based replay buffer selection for continual learning. In CVPR, pages 99– 108, 2022. 3 [67] Yi-Hsuan Tsai, Wei-Chih Hung, Samuel Schulter, Ki- hyuk Sohn, Ming-Hsuan Yang, and Manmohan Chandraker. Learning to adapt structured output space for semantic seg- mentation. In CVPR, pages 7472–7481, 2018. 2 [68] Eric Tzeng, Judy Hoffman, Trevor Darrell, and Kate Saenko. Simultaneous deep transfer across domains and tasks. In ICCV, pages 4068–4076, 2015. 2 [69] Eric Tzeng, Judy Hoffman, Kate Saenko, and Trevor Darrell. Adversarial discriminative domain adaptation. In CVPR, pages 2962–2971, 2017. 1 [70] Dequan Wang, Evan Shelhamer, Shaoteng Liu, Bruno A. Ol- shausen, and Trevor Darrell. Tent: Fully test-time adaptation by entropy minimization. In ICLR, 2021. 1, 2, 3, 4, 6, 7, 8, 12, 13, 14, 15, 16, 17 [71] Jindong Wang, Cuiling Lan, Chang Liu, Yidong Ouyang, Tao Qin, Wang Lu, Yiqiang Chen, Wenjun Zeng, and Philip Yu. Generalizing to unseen domains: A survey on domain generalization. IEEE Trans. Knowl. Data Eng., 2022. 1 [72] Mei Wang and Weihong Deng. Deep visual domain adapta- tion: A survey. Neurocomputing, 312:135–153, 2018. 1 [73] Qin Wang, Olga Fink, Luc Van Gool, and Dengxin Dai. Con- tinual test-time domain adaptation. In CVPR, pages 7191– 7201, 2022. 1, 2, 3, 4, 6, 7, 8, 13, 14, 15, 16, 17 [74] Markus Wulfmeier, Alex Bewley, and Ingmar Posner. Incre- mental adversarial domain adaptation for continually chang- ing environments. In ICRA, pages 4489–4495, 2018. 3 [75] Binhui Xie, Shuang Li, Mingjia Li, Chi Harold Liu, Gao Huang, and Guoren Wang. Sepico: Semantic-guided pixel contrast for domain adaptive semantic segmentation. IEEE Trans. Pattern Anal. Mach. Intell., pages 1–17, 2023. 2 [76] Saining Xie, Ross Girshick, Piotr Doll ´ar, Zhuowen Tu, and Kaiming He. Aggregated residual transformations for deep neural networks. In CVPR, pages 5987–5995, 2017. 6 [77] Ruijia Xu, Guanbin Li, Jihan Yang, and Liang Lin. Larger norm more transferable: An adaptive feature norm approach for unsupervised domain adaptation. In ICCV, pages 1426– 1435, 2019. 2 [78] Zhenlin Xu, Deyi Liu, Junlin Yang, Colin Raffel, and Marc Niethammer. Robust and generalizable visual representation learning via random convolutions. In ICLR, 2021. 3 [79] Shiqi Yang, Yaxing Wang, Joost van de Weijer, Luis Herranz, and Shangling Jui. Generalized source-free domain adapta- tion. In ICCV, pages 8978–8987, 2021. 3 [80] Sergey Zagoruyko and Nikos Komodakis. Wide residual net- works. In BMVC, 2016. 6 [81] Marvin Mengxin Zhang, Sergey Levine, and Chelsea Finn. MEMO: Test time robustness via adaptation and augmenta- tion. In NeurIPS, 2022. 1, 4 [82] Yizhe Zhang, Shubhankar Borse, Hong Cai, and Fatih Porikli. Auxadapt: Stable and efficient test-time adaptation for temporally consistent video semantic segmentation. In WACV, pages 2633–2642, 2022. 1, 3 [83] Kaiyang Zhou, Ziwei Liu, Yu Qiao, Tao Xiang, and Chen Change Loy. Domain generalization: A survey. IEEE Trans. Pattern Anal. Mach. Intell., 2022. 1 [84] Kaiyang Zhou, Yongxin Yang, Yu Qiao, and Tao Xiang. Do- main generalization with mixstyle. In ICLR, 2021. 3 [85] Yang Zou, Zhiding Yu, BVK Vijaya Kumar, and Jinsong Wang. Unsupervised domain adaptation for semantic seg- mentation via class-balanced self-training. In ECCV, pages 289–305, 2018. 26. Appendix 6.1. Discussion Societal impact. RoTTA enables adapting pre-trained models on continually changing distributions with correl- atively sampled test streams without any more raw data or label requirements. Thus, our work may have a positive im- pact on communities to effectively deploy and adapt models in various real-world scenarios, which is economically and environmentally friendly. And since no training data is re- quired, this protects data privacy and has potential commer- cial value. We carry out experiments on benchmark datasets and do not notice any societal issues. It does not involve sensitive attributes. Future work. Our work suggests a few promising direc- tions for future work. Firstly, the proposed RoTTA is a preliminary attempt to perform test-time adaptation for the more realistic test stream under the setup PTTA. One could experiment to improve the algorithm by replacing some parts of RoTTA. More importantly, we hope that with this work, we can open a path to the original goal of test-time adaptation, which is performing test-time adaptation in real- world scenarios. Thus, one could improve PTTA to make it more realistic. Limitations. RoTTA achieves excellent performance on various tasks under the setup PTTA as demonstrated in Sec- tion 4 in the main paper, but we still find some limitations of it. Firstly, the adopted robust batch normalization (RBN) is a naive solution to the normalization of the correlatively sampled batch of data. This requires careful design of the value of α in RBN. Secondly, we observe that during the adaptation procedure of some methods like PL [39] and TENT [70], the model collapse finally. Although we de- sign many strategies to stabilize the adaptation and model collapse never occurs in the experiments of RoTTA, we are still missing a way to recover the model from the collapse state as a remedy. Thirdly, category similarity is only one kind of correlation. Although we conduct experiments on different datasets with Dirichlet distribution to simulate cor- relatively sampled test streams, we still need to validate our approach in some real-world scenarios. 6.2. Sensitivity to different hyper-parameters In this section, we conduct a detailed sensitivity analy- sis of the hyperparameters involved in RoTTA. All experi- ments are conducted on CIFAR100→CIFAR100-C, and the corruptions changes as motion, snow, fog, shot, defocus, contrast, zoom, brightness, frost, elastic, glass, gaussian, pixelate, jpeg, and impulse, and test streams are sampled correlatively with the Dirichlet parameter δ = 0.1. When we investigate the sensitivity to a specific hyperparameter, other hyperparameters are fixed to the default values, i.e., λt = 1.0, λu = 1.0, α = 0.05, and ν = 0.001, for all experiments. Table 7. Classification error with different value of λt/λu. λt/λu 0.0/2.0 0.5/1.5 1.0/1.0 1.5/ 0.5 2.0/ 0.0 CIFAR100-C 57.5 36.9 35.0 35.9 38.9 Trade-off between timeliness and uncertainty. When updating the memory bank, we take the timeliness and uncertainty of samples into account simultaneously, and λt and λu will make a trade-off between them. In Table 7, we show the results of RoTTA with varying λt/λu, i.e., λt/λu ∈ {0.0/2.0, 0.5/1.5, 1.0/1.0, 1.5/0.5, 2.0/0.0}. When we consider both of them, the results are relatively stable (35.0-36.9%). When we only think about one side, the performance drops significantly. For example, when we set λt/λu = 0.0/2.0 which means only considering uncer- tainty, the performance drops 22.5%. That’s because some confident samples get stuck in the memory bank, making it not work the way we design it. Table 8. Classification error with varying α α 0.5 0.1 0.05 0.01 0.005 0.001 CIFAR100-C 39.0 36.0 35.0 36.0 38.1 41.5 Sensitivity to α. We show the results of RoTTA with vary- ing α, i.e., α ∈ {0.5, 0.1, 0.05, 0.01, 0.005, 0.001} in Ta- ble 8. A larger value of α means updating the global statis- tics faster and vice versa. We can see that RoTTA achieves competitive results (35.0 − 36.0%) at appropriate values of α, i.e., α ∈ {0.1, 0.05, 0.01}. Updating too aggressively or too gently can lead to unreliable estimates of statistics. Table 9. Classification error with varying ν ν 0.05 0.01 0.005 0.001 0.0005 0.0001 CIFAR100-C 44.8 39.1 37.1 35.0 37.6 43.6 Sensitivity to ν. We show the results of RoTTA with vary- ing ν, i.e., ν ∈ {0.05, 0.01, 0.005, 0.001, 0.0005, 0.0001} in Table 9. As we can see, the best performance is achieved at ν = 0.001. Updating the teacher model too quickly or too slowly can cause performance degradation. 6.3. Additional experiment details and results 6.3.1 Compared methods BN [53] utilizes statistics of the current batch of data to nor- malize their feature maps without tuning any parameters. PL [39] is based on BN [53], and adopts pseudo labels to train the affine parameters in BN layers.TENT [70] is the first to propose fully test-time adaptation. It adopts test-time batch normalization and utilizes entropy minimization to train the affine parameters of BN layers. We reimplement it following the released code https:// github.com/DequanWang/tent. LAME [5] adapts the output of the pre-trained model by optimizing a group of latent variables without tuning any in- ner parts of the model. We reimplement it following the re- leased code https://github.com/fiveai/LAME. CoTTA [73] considers performing test-time adapta- tion on continually changing distributions and pro- pose augmentation-averaged pseudo-labels and stochastic restoration to address error accumulation and catastrophic forgetting. We reimplement it following the released code https://github.com/qinenergy/cotta. NOTE [19] proposes instance-aware normalization and prediction-balanced reservoir sampling to stable the adapta- tion on temporally correlated test streams. We reimplement it following the released code https://github.com/ TaesikGong/NOTE. 6.3.2 Simulate correlatively sampling As we described in the scenarios of autonomous driving that the car will follow more vehicles on the highway or will en- counter more pedestrians on the sidewalk, so we use the same category to simulate correlation. From a macro point of view, the test distribution Ptest changes continually as P0, P1, ...,P∞. During the period when Ptest = Pt, we adopt Dirichlet distribution to simulate correlatively sam- pled test stream. More specifically, we consider dividing samples of C classes into T slots. Firstly, we utilize Dirich- let distribution with parameter γ to generate the partition criterion q ∈ RC×T . Then for each class c, we split samples into T parts according to qc and assign each part to each slot respectively. Finally, we concatenate all slots to sim- ulate the correlatively sampled test stream for Ptest = Pt. And as Ptest changes, we use the above method again to generate the test stream. 6.3.3 Detailed results of different orders We report the average classification error of ten different distribution changing orders in Table 6 of the main pa- per. And then we present the specific results here, includ- ing Table 10, 11, 12, 13, 14, 15, 16, 17, 18, and 19 for CIFAR10→CIFAR10-C and Table 20, 21, 22, 23, 24, 25, 26, 27, 28, and 29 for CIFAR100 →CIFAR100-C. We can see consistently superior performance of RoTTA. One thing to mention is that on DomainNet we use alphabetical order to determine the order of domain changes.Table 10. Average classification error of the task CIFAR10→ CIFAR10-C while continually adapting to different corruptions at the highest severity 5 with correlatively sampled test stream under the proposed setup PTTA. Time t− − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − → Method brightnesspixelategaussianmotionzoom glass impulsejpeg defocuselasticshot frost snow fog contrast Avg. Source 9.3 58.5 72.3 34.8 42.0 54.3 72.9 30.3 46.9 26.6 65.7 41.3 25.1 26.0 46.7 43.5BN [53] 71.1 75.2 76.8 74.2 73.7 80.1 79.3 77.5 73.8 77.7 77.2 73.3 73.8 72.7 71.7 75.2PL [39] 71.7 75.9 80.2 78.4 80.2 85.2 85.3 85.4 85.1 86.7 87.9 87.9 88.1 88.3 87.9 83.6TENT [70] 71.6 75.9 81.3 80.5 82.3 85.6 87.1 87.0 87.1 88.1 88.2 87.8 87.9 88.3 88.2 84.4LAME [5] 5.4 56.8 73.1 29.1 37.0 50.5 71.4 22.3 42.8 18.6 65.5 37.3 18.8 20.4 43.6 39.5CoTTA [73] 75.0 79.8 83.1 83.4 83.2 84.0 84.5 83.2 83.5 83.3 83.6 83.0 83.0 83.4 83.7 82.6NOTE [19] 10.1 29.9 47.1 23.4 28.4 48.4 46.1 41.8 26.9 36.1 37.5 25.0 25.0 23.2 14.2 30.9 RoTTA 10.4 26.6 37.5 23.9 17.0 40.9 39.7 30.1 18.0 29.9 30.1 23.6 21.7 17.6 19.0 25.7(+5.2) Table 11. Average classification error of the task CIFAR10→ CIFAR10-C while continually adapting to different corruptions at the highest severity 5 with correlatively sampled test stream under the proposed setup PTTA. Time t− − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − → Method jpeg shot zoom frost contrastfog defocuselasticgaussianbrightnessglass impulsepixelatesnow motion Avg. Source 30.3 65.7 42.0 41.3 46.7 26.0 46.9 26.6 72.3 9.3 54.3 72.9 58.5 25.1 34.8 43.5BN [53] 77.6 75.8 73.4 74.1 73.1 72.5 72.9 77.1 77.2 72.2 79.9 79.9 75.5 74.6 72.9 75.2PL [39] 77.6 77.1 76.6 78.3 77.5 79.8 82.0 84.8 86.1 83.5 87.8 87.1 86.5 85.6 85.7 82.4TENT [70] 78.5 78.2 79.2 81.8 84.8 84.8 86.4 87.3 87.9 86.7 87.3 87.8 87.2 87.5 87.1 84.8LAME [5] 22.5 65.2 37.0 37.1 44.0 20.3 41.7 18.7 72.8 5.2 51.2 71.5 57.0 19.0 29.4 39.5CoTTA [73]78.5 81.0 82.8 84.1 84.9 83.4 83.5 83.5 84.5 83.3 84.7 84.6 83.0 84.4 83.4 83.3NOTE [19]35.4 36.1 22.1 21.3 11.6 24.8 24.5 36.0 37.7 18.4 49.0 47.4 43.9 30.4 29.2 31.2 RoTTA 33.2 33.3 19.8 24.1 24.9 20.5 16.2 31.7 28.4 11.8 43.1 36.9 32.5 20.7 20.6 26.5(+4.7) Table 12. Average classification error of the task CIFAR10→ CIFAR10-C while continually adapting to different corruptions at the highest severity 5 with correlatively sampled test stream under the proposed setup PTTA. Time t− − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − → Method contrastdefocusgaussianshot snow frost glass zoom elasticjpeg pixelatebrightnessimpulsemotion fog Avg. Source 46.7 46.9 72.3 65.7 25.1 41.3 54.3 42.0 26.6 30.3 58.5 9.3 72.9 34.8 26.0 43.5BN [53] 72.3 72.6 76.9 77.1 74.8 73.5 80.0 73.2 77.4 78.6 76.4 71.0 79.1 73.9 71.5 75.2PL [39] 72.4 75.3 80.7 82.6 83.3 83.5 86.6 85.7 86.6 88.4 87.5 86.6 88.3 88.2 86.8 84.1TENT [70] 73.5 77.9 85.5 86.9 87.6 87.8 88.3 87.7 88.6 89.2 88.5 88.5 89.3 88.6 88.6 86.4LAME [5] 43.5 42.3 73.1 65.3 19.2 37.3 51.1 36.8 18.5 22.5 56.9 5.5 71.1 29.1 20.5 39.5CoTTA [73]79.4 80.3 83.8 83.9 83.9 83.4 85.0 83.2 85.1 84.3 83.9 83.3 84.7 83.9 82.5 83.4NOTE [19] 9.6 21.8 40.1 31.0 25.5 22.6 44.8 22.8 33.2 39.4 33.2 18.1 50.0 28.3 29.8 30.0 RoTTA 18.4 17.9 38.4 31.9 23.3 19.8 40.7 17.4 31.4 29.8 27.8 11.3 43.8 19.7 18.8 26.0(+4.0) Table 13. Average classification error of the task CIFAR10→ CIFAR10-C while continually adapting to different corruptions at the highest severity 5 with correlatively sampled test stream under the proposed setup PTTA. Time t− − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − → Method shot fog glass pixelatesnow elasticbrightnessimpulsedefocusfrost contrastgaussianmotionjpeg zoom Avg. Source 65.7 26.0 54.3 58.5 25.1 26.6 9.3 72.9 46.9 41.3 46.7 72.3 34.8 30.3 42.0 43.5BN [53] 76.4 72.0 80.4 76.2 74.8 77.0 71.1 79.6 73.8 74.4 73.0 77.0 72.5 78.3 72.5 75.3PL [39] 77.0 73.3 82.4 79.8 81.0 82.3 79.5 84.4 82.7 83.5 83.5 85.5 84.8 87.0 84.5 82.1TENT [70]76.9 74.6 82.3 81.7 82.0 84.9 84.8 87.3 86.6 87.3 87.6 89.2 88.3 88.9 87.3 84.6LAME [5] 65.3 20.6 50.9 56.7 19.2 18.8 5.4 71.8 42.8 37.2 43.3 73.2 29.4 22.6 36.9 39.6CoTTA [73]77.4 77.6 83.8 81.9 82.2 82.6 80.4 83.3 82.3 81.5 82.7 82.6 81.1 82.9 81.0 81.6NOTE [19]34.0 20.9 43.1 36.6 24.0 36.4 12.1 48.0 25.9 23.9 13.4 38.1 25.0 43.2 24.2 29.9 RoTTA 35.0 21.1 43.9 29.2 22.1 29.7 10.8 44.6 25.3 22.7 24.6 29.4 26.9 34.4 16.1 27.7(+2.2) Table 14. Average classification error of the task CIFAR10→ CIFAR10-C while continually adapting to different corruptions at the highest severity 5 with correlatively sampled test stream under the proposed setup PTTA. Time t− − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − → Method pixelateglass zoomsnow fog impulsebrightnessmotionfrost jpeg gaussianshot contrastdefocus elastic Avg. Source 58.5 54.3 42.0 25.1 26.0 72.9 9.3 34.8 41.3 30.3 72.3 65.7 46.7 46.9 26.6 43.5BN [53] 76.0 79.6 73.3 75.2 72.9 79.8 71.1 73.5 74.1 78.6 77.4 76.1 72.0 73.8 76.4 75.3PL [39] 76.7 81.3 77.4 80.3 81.2 86.3 83.3 85.9 86.2 87.7 88.1 88.4 87.4 87.6 87.7 84.4TENT [70] 76.4 80.2 77.8 81.2 83.0 87.1 85.6 87.2 87.6 88.7 88.6 88.9 88.5 88.6 88.2 85.2LAME [5] 56.9 50.7 37.0 19.0 20.3 71.5 5.4 29.2 37.2 22.5 73.0 65.3 43.8 42.4 18.7 39.5CoTTA [73]77.1 83.6 84.1 84.8 84.4 85.2 84.0 84.3 84.9 84.9 85.0 84.7 85.3 84.4 84.3 84.1NOTE [19] 27.8 52.2 24.5 22.3 21.6 44.5 14.5 21.3 25.9 42.5 38.8 36.0 16.7 28.1 40.6 30.5 RoTTA 25.9 43.3 17.7 22.1 20.2 41.5 12.2 22.9 22.5 31.2 33.8 26.0 31.4 17.7 27.6 26.4(+4.1)Table 15. Average classification error of the task CIFAR10→ CIFAR10-C while continually adapting to different corruptions at the highest severity 5 with correlatively sampled test stream under the proposed setup PTTA. Time t− − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − → Method motionsnow fog shot defocuscontrastzoom brightnessfrost elasticglass gaussianpixelatejpeg impulse Avg. Source 34.8 25.1 26.0 65.7 46.9 46.7 42.0 9.3 41.3 26.6 54.3 72.3 58.5 30.3 72.9 43.5BN [53] 73.2 73.4 72.7 77.2 73.7 72.5 72.9 71.0 74.1 77.7 80.0 76.9 75.5 78.3 79.0 75.2PL [39] 73.9 75.0 75.6 81.0 79.9 80.6 82.0 83.2 85.3 87.3 88.3 87.5 87.5 87.5 88.2 82.9TENT [70] 74.3 77.4 80.1 86.2 86.7 87.3 87.9 87.4 88.2 89.0 89.2 89.0 88.3 89.7 89.2 86.0LAME [5] 29.5 19.0 20.3 65.3 42.4 43.4 36.8 5.4 37.2 18.6 51.2 73.2 57.0 22.6 71.3 39.5CoTTA [73]77.1 80.6 83.1 84.4 83.9 84.2 83.1 82.6 84.4 84.2 84.5 84.6 82.7 83.8 84.9 83.2NOTE [19] 18.0 22.1 20.6 35.6 26.9 13.6 26.5 17.3 27.2 37.0 48.3 38.8 42.6 41.9 49.7 31.1 RoTTA 18.1 21.3 18.8 33.6 23.6 16.5 15.1 11.2 21.9 30.7 39.6 26.8 33.7 27.8 39.5 25.2(+5.9) Table 16. Average classification error of the task CIFAR10→ CIFAR10-C while continually adapting to different corruptions at the highest severity 5 with correlatively sampled test stream under the proposed setup PTTA. Time t− − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − → Method frost impulsejpeg contrastzoom glass pixelatesnow defocusmotionbrightnesselasticshot fog gaussian Avg. Source 41.3 72.9 30.3 46.7 42.0 54.3 58.5 25.1 46.9 34.8 9.3 26.6 65.7 26.0 72.3 43.5BN [53] 73.8 79.1 77.9 73.0 73.7 80.1 75.7 74.4 73.7 74.0 71.7 77.0 75.9 72.8 76.2 75.3PL [39] 74.2 80.9 80.4 79.5 81.8 85.9 83.9 85.1 84.7 85.9 85.9 86.7 87.2 87.0 87.8 83.8TENT [70]73.9 80.3 81.8 81.6 83.6 86.3 85.6 85.7 86.4 87.7 87.4 88.8 88.8 88.5 88.4 85.0LAME [5] 37.4 71.8 22.4 43.5 37.0 50.5 57.0 19.0 42.8 29.1 5.4 18.7 65.2 20.4 72.9 39.5CoTTA [73]76.5 82.2 82.8 85.0 82.9 85.0 83.0 82.9 83.5 83.4 82.6 83.7 83.2 83.3 83.6 82.9NOTE [19]21.1 41.4 36.3 10.2 21.7 46.7 37.5 26.4 26.1 21.4 14.3 37.9 38.5 24.4 40.7 29.6 RoTTA 22.2 44.9 35.2 18.8 19.7 41.5 28.5 23.2 21.2 18.6 12.4 30.0 27.4 20.0 31.2 26.3(+3.3) Table 17. Average classification error of the task CIFAR10→ CIFAR10-C while continually adapting to different corruptions at the highest severity 5 with correlatively sampled test stream under the proposed setup PTTA. Time t− − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − → Method defocusmotionzoom shot gaussianglass jpeg fog contrastpixelatefrost snow brightnesselastic impulse Avg. Source 46.9 34.8 42.0 65.7 72.3 54.3 30.3 26.0 46.7 58.5 41.3 25.1 9.3 26.6 72.9 43.5BN [53] 72.8 72.7 73.3 77.2 77.3 80.0 77.6 72.6 73.3 76.6 73.8 74.1 70.3 77.5 79.0 75.2PL [39] 73.2 74.6 76.5 81.7 82.8 84.6 85.1 84.6 86.2 86.4 86.1 87.1 86.8 88.4 88.1 83.5TENT [70] 73.7 74.3 77.1 82.5 84.3 86.9 87.4 86.6 88.0 88.5 88.1 88.5 88.4 89.4 88.9 84.8LAME [5] 42.5 29.3 37.0 65.3 73.2 50.5 22.5 20.5 43.5 56.9 37.1 18.9 5.4 18.5 71.3 39.5CoTTA [73]76.3 79.8 82.4 83.3 83.8 84.5 83.1 82.7 84.7 82.9 83.0 83.3 81.4 83.8 83.8 82.6NOTE [19] 18.5 18.8 23.6 36.5 33.7 47.8 38.6 22.8 13.0 40.0 29.2 26.3 17.5 44.0 52.9 30.9 RoTTA 17.0 17.5 16.5 33.8 33.3 42.7 29.4 18.0 19.6 29.5 20.7 22.1 11.5 29.5 38.1 25.3(+5.6) Table 18. Average classification error of the task CIFAR10→ CIFAR10-C while continually adapting to different corruptions at the highest severity 5 with correlatively sampled test stream under the proposed setup PTTA. Time t− − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − → Method glass zoom impulsefog snow jpeg gaussianfrost shot brightnesscontrastmotionpixelatedefocus elastic Avg. Source 54.3 42.0 72.9 26.0 25.1 30.3 72.3 41.3 65.7 9.3 46.7 34.8 58.5 46.9 26.6 43.5BN [53] 79.7 72.3 79.8 73.2 74.7 77.7 76.6 73.2 77.1 72.2 73.0 73.3 75.5 73.8 76.4 75.2PL [39] 79.6 73.2 81.3 77.3 79.1 83.0 83.2 83.0 85.5 84.3 87.0 86.9 86.4 86.5 87.6 82.9TENT [70] 79.5 74.1 84.2 82.2 84.5 86.5 86.7 85.9 87.2 86.6 86.8 87.3 86.9 87.4 87.3 84.9LAME [5] 50.8 36.9 71.3 20.6 19.2 22.4 72.5 37.2 65.4 5.2 43.3 29.1 57.0 42.4 18.7 39.5CoTTA [73]81.5 79.4 85.2 84.1 84.5 84.2 84.8 84.0 84.8 83.2 85.2 83.8 83.2 84.6 83.6 83.7NOTE [19]45.0 21.2 42.3 21.0 21.6 38.4 36.4 21.4 33.1 16.7 14.6 25.4 43.5 29.1 38.5 29.9 RoTTA 42.6 17.6 48.1 23.9 21.9 32.6 32.1 20.7 30.2 12.0 21.9 20.0 33.7 16.4 28.1 26.8(+3.1) Table 19. Average classification error of the task CIFAR10→ CIFAR10-C while continually adapting to different corruptions at the highest severity 5 with correlatively sampled test stream under the proposed setup PTTA. Time t− − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − → Method contrastgaussiandefocuszoom frost glass jpeg fog pixelateelasticshot impulsesnow motion brightness Avg. Source 46.7 72.3 46.9 42.0 41.3 54.3 30.3 26.0 58.5 26.6 65.7 72.9 25.1 34.8 9.3 43.5BN [53] 72.4 76.2 73.2 73.7 73.6 80.0 77.6 72.6 76.4 77.7 77.2 79.9 73.8 73.9 70.0 75.2PL [39] 73.0 78.2 76.7 79.7 81.6 85.6 86.0 85.3 87.2 88.2 88.3 88.9 88.5 89.2 88.2 84.3TENT [70] 73.6 80.9 83.1 85.6 87.1 88.5 88.8 88.4 89.2 89.3 89.0 89.0 89.3 89.9 89.1 86.7LAME [5] 43.5 73.2 42.3 37.0 37.2 50.5 22.5 20.5 57.0 18.6 65.5 71.5 18.8 29.1 5.6 39.5CoTTA [73]79.5 81.4 83.4 83.6 83.9 85.0 84.0 82.8 84.8 84.8 84.5 84.7 84.1 84.4 82.8 83.6NOTE [19] 9.6 43.6 26.5 24.8 23.9 46.9 38.0 23.4 34.0 41.2 41.5 45.0 27.6 25.8 19.0 31.4 RoTTA 18.4 36.0 21.1 15.6 23.0 41.7 30.8 19.1 34.1 31.1 31.3 39.9 26.0 18.8 12.8 26.6(+4.8)Table 20. Average classification error of the task CIFAR100 → CIFAR100-C while continually adapting to different corruptions at the highest severity 5 with correlatively sampled test stream under the proposed setup PTTA. Time t− − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − → Method brightnesspixelategaussianmotionzoom glass impulsejpeg defocuselasticshot frost snow fog contrast Avg. Source 29.5 74.7 73.0 30.8 28.8 54.1 39.4 41.2 29.3 37.2 68.0 45.8 39.5 50.3 55.1 46.4BN [53] 46.5 52.0 58.6 47.4 47.4 57.6 58.2 56.9 47.0 53.4 56.0 52.5 53.1 57.7 49.1 52.9PL [39] 48.5 60.7 77.1 85.9 91.5 95.5 95.8 96.6 96.8 96.9 97.3 97.5 97.6 97.7 97.9 88.9TENT [70] 49.8 69.4 92.2 96.0 96.7 97.3 97.5 97.9 97.5 97.9 98.0 98.2 98.2 98.2 98.2 92.2LAME [5] 21.7 75.1 72.7 22.9 20.6 49.0 32.1 33.3 21.2 28.0 66.8 40.0 30.6 43.9 51.3 40.6CoTTA [73] 46.8 48.4 54.7 48.7 48.6 53.5 55.4 52.8 49.8 51.8 53.5 52.9 54.1 56.7 53.6 52.1NOTE [19] 42.6 53.0 69.9 52.1 53.3 70.4 73.1 76.7 80.8 96.0 97.7 97.1 96.6 97.2 95.8 76.8 RoTTA 28.4 37.3 44.6 31.9 28.3 41.8 43.6 39.9 28.0 35.2 38.2 33.7 33.0 39.5 31.0 35.6(+5.0) Table 21. Average classification error of the task CIFAR100 → CIFAR100-C while continually adapting to different corruptions at the highest severity 5 with correlatively sampled test stream under the proposed setup PTTA. Time t− − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − → Method jpeg shot zoom frost contrastfog defocuselasticgaussianbrightnessglass impulsepixelatesnow motion Avg. Source 41.2 68.0 28.8 45.8 55.1 50.3 29.3 37.2 73.0 29.5 54.1 39.4 74.7 39.5 30.8 46.4BN [53] 58.3 56.8 47.8 51.8 48.9 57.3 46.8 53.5 57.8 45.5 57.1 58.5 51.7 53.3 48.8 52.9PL [39] 59.4 66.3 74.9 87.5 94.2 95.5 96.2 97.1 97.4 97.2 97.5 97.7 98.0 98.2 98.2 90.4TENT [70] 62.0 79.3 91.7 95.8 96.9 97.0 97.4 97.7 97.6 97.7 97.9 97.9 98.0 97.9 97.9 93.5LAME [5] 33.6 66.7 21.1 39.9 50.6 43.9 21.0 28.6 72.5 21.6 48.6 32.5 74.5 30.6 22.5 40.6CoTTA [73]54.6 54.1 49.6 52.1 52.7 58.0 50.3 53.3 55.0 49.1 55.4 55.7 51.0 54.6 52.1 53.2NOTE [19]60.4 63.0 49.9 55.7 47.0 65.2 59.4 76.6 90.9 87.2 96.8 97.0 97.3 96.7 96.8 76.0 RoTTA 43.9 45.3 31.0 37.3 35.7 41.2 27.7 34.8 39.7 26.6 39.5 41.9 32.0 33.0 30.5 36.0(+4.6) Table 22. Average classification error of the task CIFAR100 → CIFAR100-C while continually adapting to different corruptions at the highest severity 5 with correlatively sampled test stream under the proposed setup PTTA. Time t− − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − → Method contrastdefocusgaussianshot snow frost glass zoom elasticjpeg pixelatebrightnessimpulsemotion fog Avg. Source 55.1 29.3 73.0 68.0 39.5 45.8 54.1 28.8 37.2 41.2 74.7 29.5 39.4 30.8 50.3 46.4BN [53] 49.4 47.2 58.6 56.2 52.7 52.0 57.9 46.1 54.4 57.7 50.5 46.2 58.2 47.6 58.5 52.9PL [39] 54.8 64.2 83.3 92.4 95.5 96.5 96.9 96.4 97.2 97.4 97.8 97.8 97.9 97.7 98.0 90.9TENT [70] 60.2 83.1 95.2 96.5 96.9 97.3 97.0 97.3 97.8 97.8 97.6 97.9 97.8 97.9 98.1 93.9LAME [5] 51.3 21.3 72.7 66.3 30.2 40.0 48.6 20.9 27.7 33.3 75.0 21.5 32.2 22.5 43.8 40.5CoTTA [73]52.1 48.6 55.1 52.7 53.4 51.9 55.9 49.2 53.2 52.8 49.2 49.7 56.2 50.7 58.1 52.6NOTE [19] 39.5 45.9 68.8 61.8 57.4 58.5 71.4 66.5 80.8 90.9 94.2 94.9 97.0 95.5 96.6 74.6 RoTTA 41.7 30.5 44.9 40.5 35.4 34.1 40.5 28.2 34.5 39.5 31.1 26.7 43.3 31.4 38.8 36.1(+4.4) Table 23. Average classification error of the task CIFAR100 → CIFAR100-C while continually adapting to different corruptions at the highest severity 5 with correlatively sampled test stream under the proposed setup PTTA. Time t− − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − → Method shot fog glass pixelatesnow elasticbrightnessimpulsedefocusfrost contrastgaussianmotionjpeg zoom Avg. Source 68.0 50.3 54.1 74.7 39.5 37.2 29.5 39.4 29.3 45.8 55.1 73.0 30.8 41.2 28.8 46.4BN [53] 57.5 58.6 58.5 50.5 52.7 53.1 45.9 57.9 47.0 51.5 47.8 58.2 48.2 57.1 47.7 52.8PL [39] 59.5 72.9 85.1 89.6 94.5 96.8 97.1 97.9 97.8 98.0 98.3 98.2 98.0 98.0 98.2 92.0TENT [70]60.3 81.4 95.0 96.6 97.0 97.3 97.3 97.7 97.7 97.7 97.8 97.7 97.6 97.6 97.9 93.8LAME [5] 66.4 43.2 49.0 75.2 30.2 28.5 21.6 32.5 21.2 39.5 52.0 72.8 22.3 33.1 20.5 40.5CoTTA [73]54.5 58.4 55.6 50.0 53.9 53.4 50.3 56.7 51.3 53.2 53.7 56.1 52.0 54.5 51.5 53.7NOTE [19]61.8 60.2 63.4 55.6 59.8 65.9 58.6 75.1 77.8 93.8 94.2 97.0 95.0 95.5 94.4 76.5 RoTTA 45.5 44.5 43.5 35.6 35.1 35.7 26.2 44.0 29.7 34.2 32.0 40.7 31.4 39.4 27.7 36.3(+4.2) Table 24. Average classification error of the task CIFAR100 → CIFAR100-C while continually adapting to different corruptions at the highest severity 5 with correlatively sampled test stream under the proposed setup PTTA. Time t− − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − → Method pixelateglass zoomsnow fog impulsebrightnessmotionfrost jpeg gaussianshot contrastdefocus elastic Avg. Source 74.7 54.1 28.8 39.5 50.3 39.4 29.5 30.8 45.8 41.2 73.0 68.0 55.1 29.3 37.2 46.4BN [53] 51.7 58.6 47.8 52.9 57.1 58.2 45.9 47.6 52.9 57.8 57.5 56.7 49.5 46.1 54.0 52.9PL [39] 52.4 68.0 73.4 87.9 93.7 96.1 95.7 96.0 96.5 96.7 97.5 97.7 97.7 97.3 97.7 89.6TENT [70] 53.5 77.8 91.1 96.0 97.0 97.6 97.4 97.6 97.9 98.1 98.1 98.0 98.1 97.9 98.1 92.9LAME [5] 74.8 48.2 21.1 30.6 43.4 32.5 21.6 23.0 39.6 33.3 72.7 66.5 51.5 20.7 27.5 40.5CoTTA [73]49.3 55.1 49.1 52.9 56.8 55.7 49.5 50.0 53.6 53.4 54.9 53.9 53.8 50.1 53.5 52.8NOTE [19] 52.2 64.9 47.5 57.0 61.9 67.3 60.4 67.8 77.4 90.6 97.1 96.8 92.8 95.9 96.6 75.1 RoTTA 36.4 44.4 29.7 36.5 41.0 44.1 26.8 29.5 33.0 40.3 40.3 38.2 33.9 28.5 34.9 35.8(+4.7)Table 25. Average classification error of the task CIFAR100 → CIFAR100-C while continually adapting to different corruptions at the highest severity 5 with correlatively sampled test stream under the proposed setup PTTA. Time t− − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − → Method motionsnow fog shot defocuscontrastzoom brightnessfrost elasticglass gaussianpixelatejpeg impulse Avg. Source 30.8 39.5 50.3 68.0 29.3 55.1 28.8 29.5 45.8 37.2 54.1 73.0 74.7 41.2 39.4 46.4BN [53] 48.5 54.0 58.9 56.2 46.4 48.0 47.0 45.4 52.9 53.4 57.1 58.2 51.7 57.1 58.8 52.9PL [39] 50.6 62.1 73.9 87.8 90.8 96.0 94.8 96.4 97.4 97.2 97.4 97.4 97.3 97.4 97.4 88.9TENT [70] 53.3 77.6 93.0 96.5 96.7 97.5 97.1 97.5 97.3 97.2 97.1 97.7 97.6 98.0 98.3 92.8LAME [5] 22.4 30.4 43.9 66.3 21.3 51.7 20.6 21.8 39.6 28.0 48.7 72.8 74.6 33.1 32.3 40.5CoTTA [73]49.2 52.7 56.8 53.0 48.7 51.7 49.4 48.7 52.5 52.2 54.3 54.9 49.6 53.4 56.2 52.2NOTE [19] 45.7 53.0 58.2 65.6 54.2 52.0 59.8 63.5 74.8 91.8 98.1 98.3 96.8 97.0 98.2 73.8 RoTTA 31.8 36.7 40.9 42.1 30.0 33.6 27.9 25.4 32.3 34.0 38.8 38.7 31.3 38.0 42.9 35.0(+5.5) Table 26. Average classification error of the task CIFAR100 → CIFAR100-C while continually adapting to different corruptions at the highest severity 5 with correlatively sampled test stream under the proposed setup PTTA. Time t− − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − → Method frost impulsejpeg contrastzoom glass pixelatesnow defocusmotionbrightnesselasticshot fog gaussian Avg. Source 45.8 39.4 41.2 55.1 28.8 54.1 74.7 39.5 29.3 30.8 29.5 37.2 68.0 50.3 73.0 46.4BN [53] 52.9 58.8 57.6 48.2 47.4 57.6 50.9 52.4 47.0 47.2 45.1 54.0 56.4 57.7 58.2 52.8PL [39] 56.9 73.3 86.7 94.4 95.8 97.3 97.2 97.4 97.6 97.4 97.7 97.6 97.8 98.3 98.1 92.2TENT [70]60.1 84.2 95.7 97.2 97.4 97.9 97.8 98.0 98.1 98.2 98.3 98.4 98.4 98.4 98.4 94.4LAME [5] 39.9 32.4 33.4 51.4 20.6 49.0 74.4 31.3 21.2 22.6 21.9 28.1 66.9 43.9 72.5 40.6CoTTA [73]51.5 55.3 54.3 51.8 49.4 55.3 50.7 54.2 51.4 50.6 49.5 53.6 55.0 57.1 55.8 53.0NOTE [19]51.6 60.9 60.3 45.4 54.3 70.8 68.8 75.0 75.7 87.1 94.7 95.6 96.7 96.4 97.2 75.4 RoTTA 40.0 46.3 42.8 36.4 29.2 42.3 33.2 34.4 28.4 29.2 26.4 34.5 38.5 39.8 39.3 36.0(+4.6) Table 27. Average classification error of the task CIFAR100 → CIFAR100-C while continually adapting to different corruptions at the highest severity 5 with correlatively sampled test stream under the proposed setup PTTA. Time t− − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − → Method defocusmotionzoom shot gaussianglass jpeg fog contrastpixelatefrost snow brightnesselastic impulse Avg. Source 29.3 30.8 28.8 68.0 73.0 54.1 41.2 50.3 55.1 74.7 45.8 39.5 29.5 37.2 39.4 46.4BN [53] 47.1 48.6 47.8 56.2 57.6 57.6 57.6 57.5 48.7 50.6 51.8 53.2 46.9 53.5 58.8 52.9PL [39] 48.8 58.7 69.9 88.0 95.1 96.6 96.7 96.9 97.4 97.4 98.2 98.2 98.2 98.3 98.5 89.1TENT [70] 51.0 67.6 85.8 95.9 97.2 97.5 97.2 97.7 98.1 97.9 97.7 97.7 98.0 98.0 98.2 91.7LAME [5] 21.2 22.8 21.1 66.3 72.8 49.0 33.3 44.8 51.7 74.9 39.8 31.2 21.3 27.3 32.3 40.6CoTTA [73]48.4 48.8 48.2 52.9 54.0 53.8 52.7 57.2 52.6 48.6 51.8 53.9 49.4 52.3 56.0 52.0NOTE [19] 45.1 46.7 49.1 67.3 65.5 69.4 75.5 80.3 83.8 96.0 97.6 97.1 96.1 97.9 98.7 77.7 RoTTA 29.6 31.3 28.8 43.9 41.5 41.3 40.9 39.8 32.1 32.6 33.1 33.0 26.5 34.5 42.9 35.4(+5.2) Table 28. Average classification error of the task CIFAR100 → CIFAR100-C while continually adapting to different corruptions at the highest severity 5 with correlatively sampled test stream under the proposed setup PTTA. Time t− − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − → Method glass zoom impulsefog snow jpeg gaussianfrost shot brightnesscontrastmotionpixelatedefocus elastic Avg. Source 54.1 28.8 39.4 50.3 39.5 41.2 73.0 45.8 68.0 29.5 55.1 30.8 74.7 29.3 37.2 46.4BN [53] 58.8 47.7 59.2 57.6 52.7 56.9 58.2 52.0 56.7 45.5 47.8 48.2 51.7 46.1 54.0 52.9PL [39] 60.1 59.5 75.1 85.7 91.5 94.6 96.5 97.1 97.4 97.3 98.0 97.7 97.9 97.8 97.7 89.6TENT [70] 61.6 71.5 91.0 95.9 96.6 97.1 96.9 97.3 97.4 97.2 97.9 98.0 98.1 97.9 97.8 92.8LAME [5] 48.6 20.6 32.3 44.4 30.2 33.6 72.4 40.0 66.3 21.6 52.0 22.8 74.6 20.7 27.5 40.5CoTTA [73]56.4 48.9 56.1 57.8 54.1 54.2 56.2 53.6 55.4 50.0 53.6 51.6 51.2 50.7 54.4 53.6NOTE [19]62.5 46.3 61.5 61.1 58.6 68.4 76.1 78.3 92.0 93.4 96.1 95.4 96.2 95.8 96.4 78.5 RoTTA 45.5 30.0 45.9 42.6 35.3 41.8 42.2 34.5 40.2 27.3 31.3 30.2 32.7 28.1 34.9 36.2(+4.3) Table 29. Average classification error of the task CIFAR100 → CIFAR100-C while continually adapting to different corruptions at the highest severity 5 with correlatively sampled test stream under the proposed setup PTTA. Time t− − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − → Method contrastgaussiandefocuszoom frost glass jpeg fog pixelateelasticshot impulsesnow motion brightness Avg. Source 55.1 73.0 29.3 28.8 45.8 54.1 41.2 50.3 74.7 37.2 68.0 39.4 39.5 30.8 29.5 46.4BN [53] 49.5 58.8 47.0 46.5 52.2 57.6 57.6 57.6 51.7 53.5 56.0 58.5 53.1 47.6 46.3 52.9PL [39] 53.6 70.4 76.0 85.1 91.2 95.2 96.0 97.0 96.9 97.3 97.3 97.6 97.5 97.6 97.7 89.8TENT [70] 60.2 89.1 95.0 96.2 96.9 97.0 96.5 97.0 97.0 97.2 97.6 97.8 97.5 97.9 97.7 94.0LAME [5] 51.3 72.5 21.5 21.0 39.6 49.0 33.3 44.8 74.8 28.0 66.8 32.5 30.6 22.5 21.4 40.6CoTTA [73]52.3 55.3 49.5 48.1 52.1 54.8 52.7 56.9 50.6 52.6 53.7 55.8 54.6 50.6 50.5 52.7NOTE [19] 39.1 64.7 48.9 50.6 59.1 70.1 71.7 75.0 85.2 95.7 96.9 98.4 96.0 95.9 94.9 76.1 RoTTA 41.4 46.2 30.5 28.5 36.0 40.9 40.5 39.6 33.0 35.0 38.2 43.1 33.9 30.7 27.1 36.3(+4.3)",
      "meta_data": {
        "arxiv_id": "2303.13899v1",
        "authors": [
          "Longhui Yuan",
          "Binhui Xie",
          "Shuang Li"
        ],
        "published_date": "2023-03-24T10:19:14Z",
        "pdf_url": "https://arxiv.org/pdf/2303.13899v1.pdf"
      },
      "llm_extracted_info": {
        "main_contributions": "The paper introduces Practical Test-Time Adaptation (PTTA), a new and more realistic TTA setup that simultaneously addresses distribution changing and correlative sampling in test data streams. It benchmarks existing TTA methods under PTTA, revealing their limitations. To overcome these challenges, the paper proposes Robust Test-Time Adaptation (RoTTA), a comprehensive method demonstrating state-of-the-art performance on CIFAR-10-C, CIFAR-100-C, and DomainNet, reducing averaged classification error significantly.",
        "methodology": "RoTTA addresses PTTA challenges with three main components. First, Robust Batch Normalization (RBN) is introduced to stably estimate normalization statistics, replacing current batch statistics with global ones maintained by exponential moving average from buffered samples. Second, a Category-Balanced Sampling with Timeliness and Uncertainty (CSTU) memory bank is utilized. This memory bank samples data to maintain category balance while prioritizing newer and less uncertain samples based on a heuristic score. Third, robust training with timeliness is implemented using a teacher-student model. The student model is updated by minimizing a loss on the memory bank, and the teacher model is updated via an exponential moving average of the student's parameters. A time-aware reweighting strategy assigns lower weights to older samples in the loss calculation to stabilize adaptation, and only affine parameters in RBN are trained.",
        "experimental_setup": "Experiments were conducted on CIFAR-10-C and CIFAR-100-C datasets for robustness under corruptions, where corruptions change sequentially at severity 5 to simulate distribution shifts. For generalization under large domain gaps, the DomainNet dataset was used, adapting a source pre-trained model to correlatively sampled streams of five other domains. Correlative sampling was simulated using a Dirichlet distribution with parameter δ=0.1. Pre-trained models included WildResNet-28 for CIFAR-10-C, ResNeXt-29 for CIFAR-100-C, and ResNet-101 for DomainNet. The Adam optimizer was used with a learning rate of 1.0e-3, a batch size of 64, and a memory bank capacity of N=64. RoTTA used unified hyperparameters: α=0.05, ν=0.001, λt=1.0, λu=1.0, and δ=0.1. Performance was measured by classification error, and ablation studies validated each component's efficacy. Sensitivity analyses were performed for Dirichlet parameter δ, batch size, and the hyperparameters λt/λu, α, and ν. Comparisons were made against BN, PL, TENT, LAME, CoTTA, and NOTE.",
        "limitations": "The Robust Batch Normalization (RBN) is considered a naive solution for normalizing correlatively sampled data, requiring careful tuning of its parameter α. The current method lacks a mechanism to recover the model from a collapsed state, though RoTTA aims to prevent such occurrences. Furthermore, the evaluation of correlation primarily focused on category similarity simulated by Dirichlet distribution, indicating a need for validation in more diverse real-world scenarios.",
        "future_research_directions": "Future work could focus on improving existing components of the RoTTA algorithm, potentially by designing more sophisticated solutions for robust batch normalization or memory management. More importantly, there's a direction to enhance the Practical Test-Time Adaptation (PTTA) setup itself to further increase its realism, aiming to bridge the gap towards deploying models effectively in complex, dynamic real-world applications."
      }
    },
    {
      "title": "Post-hoc estimators for learning to defer to an expert"
    },
    {
      "title": "Navigating Scaling Laws: Compute Optimality in Adaptive Model Training",
      "abstract": "In recent years, the state-of-the-art in deep learning has been dominated by\nvery large models that have been pre-trained on vast amounts of data. The\nparadigm is very simple: investing more computational resources (optimally)\nleads to better performance, and even predictably so; neural scaling laws have\nbeen derived that accurately forecast the performance of a network for a\ndesired level of compute. This leads to the notion of a `compute-optimal'\nmodel, i.e. a model that allocates a given level of compute during training\noptimally to maximize performance. In this work, we extend the concept of\noptimality by allowing for an `adaptive' model, i.e. a model that can change\nits shape during training. By doing so, we can design adaptive models that\noptimally traverse between the underlying scaling laws and outpace their\n`static' counterparts, leading to a significant reduction in the required\ncompute to reach a given target performance. We show that our approach\ngeneralizes across modalities and different shape parameters.",
      "full_text": "NAVIGATING SCALING LAWS: COMPUTE OPTIMALITY IN ADAPTIVE MODEL TRAINING Sotiris Anagnostidis∗, Gregor Bachmann∗, Imanol Schlag, Thomas Hofmann Department of Computer Science ETH Z¨urich ABSTRACT In recent years, the state-of-the-art in deep learning has been dominated by very large models that have been pre-trained on vast amounts of data. The paradigm is very simple: investing more computational resources (optimally) leads to better performance, and even predictably so; neural scaling laws have been derived that accurately forecast the performance of a network for a desired level of compute. This leads to the notion of a ‘compute-optimal’ model, i.e. a model that allocates a given level of compute during training optimally to maximize performance. In this work, we extend the concept of optimality by allowing for an ‘adaptive’ model, i.e. a model that can change its shape during training. By doing so, we can de- sign adaptive models that optimally traverse between the underlying scaling laws and outpace their ‘static’ counterparts, leading to a significant reduction in the re- quired compute to reach a given target performance. We show that our approach generalizes across modalities and different shape parameters. 1 I NTRODUCTION Deep learning has gradually undergone a paradigm shift, where instead of training specialized mod- els for a given task, a so-called frontier model is prompted/few-shoted/fine-tuned for different de- sired downstream tasks. Frontier models are typically defined by their large-scale architectures, often rooted in the Transformer architecture (Vaswani et al., 2017), and their exposure to extensive and diverse data during their pre-training process, yielding remarkable advancements in both natu- ral language understanding (OpenAI, 2023; K¨opf et al., 2023) and computer vision tasks (Dehghani et al., 2023a; Chen et al., 2023b). An inherent and pivotal feature of such models lies in their scal- ability, whereby their performance can be reliably predicted as a power law across the number of parameters and the volume of data or computational resources utilized (Cortes et al., 1993; Hestness et al., 2017; Rosenfeld et al., 2019; Kaplan et al., 2020). These principles are succinctly encap- sulated by the neural scaling laws that motivate the choice of a particular model and dataset size given a fixed budget of training compute (Hoffmann et al., 2022). The ability to accurately pre- dict performance offers an undeniable reassurance in the often uncertain world of deep learning. It nevertheless, introduces an intimidating realization; Given a training scheme, a fixed further improvement in performance requires exponentially more compute or parameters. Finding solutions to address this issue becomes increasingly paramount, as staying competitive in the realm of deep learning increasingly depends on the availability of substantial computational resources. Delving deeper into the preceding statement, we highlight a pivotal assumption: the shape of the model, and therefore the number of FLOPs for a forward pass, remainsfixed throughout the training process. By ‘shape’ we refer to any characteristic of a model that can be smoothly varied throughout training without leading to strong deterioration in performance. Such a static approach (i.e. where every model shape remains fixed) may however not always be optimal. For example, it has already been observed that the optimal model size grows smoothly with the loss target and the compute budget (Kaplan et al., 2020). ∗Equal contribution. Correspondence to {sanagnos,gregorb}@ethz.ch. 1 arXiv:2311.03233v3  [cs.LG]  23 May 2024(a) Patch sizes affect how ViTs process input images. 100 101 102 GFLOPs 60 40 30 24 20 15 12 10 8 6 4 Patch size (b) ViT-BFLOPs. Figure 1: Patch sizes define (left) how images are processed, while (right) impacting the compute of a forward pass. This paper challenges the assumption of a static model outlined above and explores adaptable train- ing methodologies designed to surpass conventional scaling laws . In other words, our aim is to achieve equivalent performance for a specified model with fewer computational resources (FLOPs) than initially projected. To that end, we adapt the shape of the model throughout training, allowing the optimal traversal between different scaling laws. This enables us to leverage the optimality of all shape configurations in different regions of compute, leading to a more efficient scaling of the model. We train Vision Transformers (Dosovitskiy et al., 2020) and Language Models (Radford et al., 2019) and showcase how an adaptive training scheme can lead to substantial training FLOPs reduction, in some cases more than 50%. Our contributions can be summarized as follows: • We introduce a simple and effective strategy to choose when to adapt a model and traverse scaling laws, opting for the one that leads to the faster descent, i.e. maximum performance gain for the same amount of compute. • We showcase the efficiency of our approach by optimally scheduling the patch size for ViTs as well as the context size for language models, leading to significant reductions in the required amount of compute to reach optimal performance. • We further confirm the validity of our approach by adapting other shape parameters such as width, batch size, and overall training objective of a Vision Transformer. 2 R ELATED WORK Neural scaling laws (Cortes et al., 1993), describe how a neural network’s performance varies as a power law E = a(P + d)b + c where P can be either the number of parameters in the model, the number of training samples or simply the number of FLOPs used for training (Rosenfeld et al., 2019). Recently, scaling laws have been successfully demonstrated in a range of different appli- cations, including language (Kaplan et al., 2020; Hoffmann et al., 2022) and vision (Zhai et al., 2022; Bachmann et al., 2023), as well as numerous learning settings, including supervised training, generative modeling (Henighan et al., 2020) and transfer learning (Hernandez et al., 2021). The pre- dictive power of scaling laws has also been leveraged to determine compute-optimal models before training; the size of the Chinchilla model and the number of training tokens were chosen based on the underlying scaling law and indeed, Chinchilla outperformed its larger but sub-optimally trained counterpart Gopher (Hoffmann et al., 2022). The training of state-of-the-art models has also been guided by scaling laws built from training runs of smaller models (OpenAI, 2023; Team et al., 2023). In this paper, we focus on the Transformer architecture and evaluate our methodology on both vision and natural language tasks. While the Transformer has been the default model in language for years, ViTs have more recently established themselves as the predominant vision architecture for large-scale pretraining tasks (Dehghani et al., 2023a). Different from convolutions, a ViT initially partitions the input image into patches and processes these through self-attention and MLP blocks. This lack of inductive bias (Smith et al., 2023) can be partially overcome through the introduction of ‘soft’ inductive bias, which proves to be beneficial, especially during the early phase of their training (d’Ascoli et al., 2021). Similarly to their counterparts in natural language processing, ViTs also exhibit predictable scaling behavior (Zhai et al., 2022; Dehghani et al., 2023a; Alabdulmohsin et al., 2023). 2In this work, we delve into models that feature adaptive ‘shape’ parameters, specifically focusing on the patch size for image processing and model width. The concept of training with different patch sizes, which Beyer et al. (2023) have explored, leads to a model robust to various patch sizes. Another common approach involves pre-training a ViT model at a lower resolution, followed by fine-tuning at a higher resolution while maintaining the same patch size (Dosovitskiy et al., 2020; Zhai et al., 2022; Alabdulmohsin et al., 2023). Analogously, large language models (LLMs) can be pre-trained with a shorter and fixed context length and subsequently fine-tuned on longer ones (Chen et al., 2023c;a; Tworkowski et al., 2023). Another way to change the model is by adding new parameters. Expanding a model under com- posable function-preserving operations has been a case of study for a long time in machine learn- ing (Ash, 1989; Mitchell et al., 2023). The principal objective in this case is to accelerate train- ing (Kaddour et al., 2023; Geiping & Goldstein, 2023). Such expansion operations have also been proposed for the Transformer architecture (Gesmundo & Maile, 2023; Chen et al., 2022) and have exhibited notable training speed-ups Gong et al. (2019); Yao et al. (2023); Wang et al. (2023); Lee et al. (2022); Shen et al. (2022); Li et al. (2022). Apart from determining how and where in the model this expansion should occur, a primary challenge is to resolve when to add new neurons. We advocate that an effective strategy for adjustments to the model shape should be informed by con- siderations of scaling laws and the performance gains achieved per additional unit of computational resources. Orthogonal to our approach, various techniques have been proposed to accelerate both inference and training, particularly in the context of Transformer models. These methods encompass a spectrum of strategies, including weight quantization (Dettmers et al., 2022; Frantar et al., 2022) and pruning weights and context (Frantar & Alistarh, 2023; Anagnostidis et al., 2023) among others. Specifically for ViTs, Bolya et al. (2022) propose to merge tokens at different layers in the architecture and De- hghani et al. (2023b) propose to pack sequences of tokens together to optimize hardware utilization. Additionally, d’Ascoli et al. (2021) proposes to initialize ViTs differently, making them look more like convolutions. Other methods have also been proposed to beat scaling laws, including data prun- ing (Sorscher et al., 2022) or shaping models (depth vs width) more optimally (Alabdulmohsin et al., 2023). These approaches are supplementary to our methodology and can be effectively employed in conjunction to further enhance the efficiency of the training process. 3 V ITS AND OPTIMAL PATCH SIZES We first focus the discussion on Vision Transformers — the de-facto dominant architecture for vision — and the choice of patch size to introduce the notion of an adaptive model. In Sec. 5 we will showcase how our strategy can also be leveraged to train language models with adaptive context lengths. ViTs process images x ∈ Rh×w×c where h and w are the height and width of the image in pixels and c is the number of channels. Images are ‘patchified’ into a sequence ofn tokens based on a specified patch sizep ∈ N, where n = ⌊w/p⌋×⌊h/p⌋, leading to a representationxpatched ∈ Rn×p2c. We illustrate the effect of different patch sizes in Fig. 1a. Each token is linearly embedded with learnable parameters Wemb ∈ Rp2c×d where d ∈ N is the embedding dimension or width of the ViT. These embeddings are further enhanced with learnable positional encodings Wpos ∈ Rn×d which enable a ViT to learn the spatial structure of the tokens. The resulting embeddings are then processed by L transformer blocks, consisting of a self-attention layer followed by an MLP that is shared across tokens. This specific structure of the architecture allows a ViT to generate predictions for token sequences of variable lengths, as is the case when dealing with images of different patch sizes. Fixed patch size training.Different patch sizes come at different computational costs; the number of tokens n scales with O(1/p2) and thus processing inputs scales with O(1/p4) due to quadratic dependence on the input sequence length of the self-attention operation 1. Consequently, a reduc- tion in the patch size results in a substantial increase in the computational requirements for a for- ward pass. We present our empirical analysis in Fig. 1b. Using smaller patch sizes is often de- sirable as it yields superior model performance when paired with enough compute. To explore 1The exact complexity isO(1/p4×d+1/p2×d2) for patch sizesd > nwhere, unless packing is performed, O(1/p2 × d2) is the dominant term. 30.0 0.2 0.4 0.6 0.8 1.0 FLOPs 1e17 0.00 0.05 0.10 0.15 0.20 0.25ImageNet 10-shot optimal smaller bs larger lr smaller lr larger bs (a) We optimize batch size, learning rate, and weight decay for each model config- uration by running a greed search, for a small compute budget. More details are presented in the Appendix. Name Width Depth Heads Param (M) GFLOPs X = 8 X = 24 V256-6/X 256 6 8 5 .1 1 .22 0 .120 V192-12/X 192 12 3 5 .6 1 .43 0 .136 V256-12/X 256 12 4 9 .9 2 .44 0 .240 V384-12/X 384 12 6 21 .8 5 .25 0 .538 V512-12/X 512 12 8 38 .6 9 .13 0 .953 V640-12/X 640 12 10 60 .0 14 .1 1 .49 V768-12/X 768 12 12 86 .2 20 .1 2 .14 (b) Details on the ViT models we are training. We use the standard s, S, Ti, B model sizes, as well as other intermediate model sizes. To simplify and unify notation, we adopt the naming convention Vd-L/X for a Vision Transformer of depth L and embedding dimensiond. Here X refers to the patch size. Figure 2: (Left) Hyperparameters are optimized across model classes. (Right) The ViT models used for this study. this trade-off, we pre-train Vision Transformers of different sizes (see Fig. 2(b) for a summary) on the public ImageNet-21k dataset (Ridnik et al., 2021), employing various patch sizes that remain fixed throughout training. To enhance computational efficiency and prevent bottlenecks caused by data transfer, we resize images to dimensions where h = w = 120, utilizing the FFCV data loader (Leclerc et al., 2023) 2. This approach enables the application of a variety of patch sizes p ∈ {120, 60, 30, 24, 20, 15, 12, 10, 8, 6, 4, 3, 2, 1}, each of which perfectly divides the input resolu- tion. During the training phase, we perform data augmentation, specifically random cropping, and horizontal flipping, and measure the 10-shot error (denoted asE) on the ImageNet-1k dataset (Deng et al., 2009). This is because upstream performance metrics may not reliably indicate model effec- tiveness (Tay et al., 2022; Zhai et al., 2022). While multiple epochs over the same dataset have been identified as less effective in language modeling tasks (Xue et al., 2023; Muennighoff et al., 2023), the use of augmentations in this work supports the feasibility of multi-epoch training without signif- icant degradation in performance. This observation holds for the data and computational scales (up to 10 EFLOPs) that we consider (Zhai et al., 2022). When calculating compute C, we exclude the computations associated with the ‘head’ of the net- work that maps the embedding dimension to the number of classes (Kaplan et al., 2020). Addition- ally, we adopt the approximation of previous work that the FLOPs required for the backward pass are approximately equivalent to twice the FLOPs incurred during the forward pass. Here, we are optimizing for FLOPs, and do not account for different types of hardware accelerators. For highly parallel neural architectures, FLOPs in general exhibit a strong correlation with accelerator time (see e.g. Fig. 4 (right) by Alabdulmohsin et al. (2023) for ViTs specifically and our results in App. F). In our study, we focus exclusively on Transformer models which are very hardware-efficient (Dosovit- skiy et al., 2020). More details regarding the experimental setup are provided in App. B. For a fixed model shape, we fit power laws for every patch size in terms of compute (which is proportional to the number of examples seen in this case). The power law takes the form3 EP = fP (C) =aP (C + dP )−bP + cP . (1) where the exponent bP dictates the decay speed and cP corresponds to the maximal reachable per- formance given infinite compute. After fitting the parameters aP , dP , bP , cP ∈ R+, we can predict downstream performance EP (ImageNet-1k 10-shot top-1 unless otherwise stated) as a function of compute C measured in FLOPs. We display the results for the V640-12 model in Fig. 4 and Fig. 5. 2We expect a small decrease in performance due to this decreased resolution. Our most compute-intensive models (ViT Base variant) achieve top-1 accuracy on ImageNet-1k 79.2 % when fine-tuned and 77.2 % when linear probed on top of the extracted embeddings. Steiner et al. (2021) report 80.42% fine-tuning performance for a ViT-B/16 model on 224 × 224 images trained for 30 epochs on ImageNet-21k, which already surpasses our maximum compute budget. 3As aforementioned, our models are bound by data rather than the number of parameters. 4C E = f(C) E C = f 1(E) E f 1(E) E Scaling law 0 Scaling law 1 Scaling law 2 Adaptive scaling law Optimality region for scaling law 0 Optimality region for scaling law 1 Optimality region for scaling law 2 Figure 3: (Left) Different scaling law curves (functionf in Eq. 1) corresponding to different training configurations. Black arrows indicate points of transition between scaling laws. (Middle) We illus- trate the inverse of the above functionf−1 for the same scaling law curves. (Right) We visualize the gradient of the inverse ∂f −1(E)/∂E for the same scaling laws. Taking the curve that maximizes the aforementioned gradient, leads to a partition of the space. From this partition, we can deduce a strategy determining which scaling law to ‘follow’ for each performance level. We provide analogous plots for all model sizes in App. C. From those scaling trends, it is evident that different patch sizes are optimal for different amounts of compute. In other words, given the same compute, different patch sizes yield different improvements at specific levels of performance. Given that insight, a very natural question emerges: Can we traverse between the scaling laws more efficiently by allowing for adaptive patch sizes? 4 A DAPTIVE PATCH SIZES AND TRAVERSING SCALING LAWS Adaptive patch size. To allow for a smooth traversal of different laws, we first need a mechanism that enables mapping a ViT fP with patch size P to a ViT fQ with patch size Q, while ideally not degrading performance, i.e. fP ≈ fQ. FlexiViT introduced by Beyer et al. (2023) achieves this. It redefines both the patch embedding Wemb and the positional encodings Wpos for a fixed base patch size. In every forward pass, depending on the patch size, the base embedding parameters Wemb are resized based on the pseudo inverse of the resizing matrix. Similarly, the base positional encodings Wpos are bi-linearly interpolated, enabling the model to change the patch size without a strong performance degradation. For further information, we direct readers to the work by Beyer et al. (2023). Traversing scaling laws. Let the set of scaling laws be {fP } with P denoting the patch size. Each law maps a given level of computeC to the predicted downstream performance EP = fP (C). Consider the inverted laws f−1 P (E) which predict for a given level of desired performance E, how much compute C needs to be invested. For a given error level E∗, we aim to reach a lower error E∗ − ϵ for ϵ >0, while spending the minimal amount of compute C to achieve this, i.e. we want the least change in f−1 P . To solve this problem, we simply compute the partial derivatives qP (E∗) :=∂f −1 P (E) ∂E \f\f\f E=E∗ ∀P. (2) Maximising qP over the patch size P, partitions the error space disjointly (e.g. if we assume E is the classification error taking values in [0, 1]), [0, 1] := [ P EP , where EP ⊂ [0, 1] denotes the set where the patch size P achieves the most efficient improvement. This partition naturally gives rise to a scheduler for the patch size, which empirically turns out to be monotonic (i.e. starting from the largest patch size for large classification error values and ending with the smallest for small classification errors), which is expected based on the observations in Fig. 4. We visualize the strategy in Fig. 3. 51016 1017 1018 1019 FLOPs 30 40 50 60 70 80 90ImageNet 10-shot error rate [%] Patch size: 60 40 30 24 20 15 12 10 8 Empirical point Fitted curveEmpirical point Fitted curve Figure 4: Downstream performance as a function of compute for the V640-12 model and different patch sizes. We use a log-log scale. -60% FLOPs 0 2 4 6 8 FLOPs 1e18 20 30 40 50 60 70 80 90ImageNet 10-shot error rate [%] Patch size: 60 40 30 24 20 15 12 10 8 Patch size scheduler Patch size decreasedPatch size scheduler Patch size decreased Figure 5: Downstream performance of theV640- 12 trained with our patch size scheduler, and its potential benefits. Scheduled training. We now test the devised strategy in a practical setting by pre-training variously-sized ViTs on ImageNet-21k using our patch size scheduler. We use the same train- ing setup as for the fixed patch size experiments and let the scheduler consider patch sizes P ∈ {60, 40, 30, 24, 20, 15, 12, 10, 8}. We display ImageNet-1k 10-shot error rate as a function of compute C for the model V640-12 in Fig. 5 and provide plots for all other models in the App. C. The crosses denote the points where the scheduler switches patch size. We observe a significant improvement in terms of compute efficiency, allowing for up to −60% FLOPs to achieve the same performance. While switching patch sizes may initially result in a slight decrease, attributed to changes in the entropy within the self-attention layers, this minor setback is rapidly offset as the image is parsed in a more fine-grained manner. Such degradation is thus not even visible in Fig. 54. To facilitate comparison across all model sizes at once, we further visualize the compute-optimal barrier for both fixed and scheduled training in Fig. 6. By compute-optimal, we refer to a model that optimally trades off model size, patch size, and number of samples seen for a given level of compute C, i.e. achieving the lowest error E. We observe that the optimally scheduled models significantly outperform the optimal static models, halving the required compute to optimally train a ViT-Base model (for our compute budget). 1015 1016 1017 1018 1019 FLOPs 30 40 50 60 70 80 90ImageNet 10-shot error rate [%] ViT BaseOur ViT -62% FLOPs Patch size: Scheduler 60 40 30 24 20 15 12 10 8Patch size: Scheduler 60 40 30 24 20 15 12 10 8 Figure 6: Compute-optimal static and scheduled models for various patch and model sizes. We plot using a log-log scale. 0.0 0.5 1.0 1.5 2.0 2.5 3.0 3.5 FLOPs 1e18 40 50 60 70 80ImageNet 10-shot error rate [%] Patch size scheduler FlexiViT Other SchedulersPatch size scheduler FlexiViT Other Schedulers Figure 7: We compare performance as a func- tion of training compute against the scheduler of FlexiViT and other schedulers. Irrespective of the current patch size in the scheduler, we use the smallest patch size (i.e. 8), when evaluating FlexiViT . 4Differences in the effective receptive field of each patch are typically mitigated by cropping as a component of the training procedure. 6Is the schedule optimal?While our scheduler improves over the individual models, it is not clear yet that it does so in an optimal sense, i.e. can other schedules achieve similar benefits? Beyer et al. (2023) also employ a patch size scheduler but use a uniformly random sampling of the patch size at every step. We compare against their modelFlexiViT in Fig. 7 and observe that our scheduler indeed outperforms FlexiViT as expected; FlexiViT targets a lower inference cost by making the model robust to many patch sizes (hence the random scheduler). Compute-optimality is not their objective. Additionally, we conduct comparisons with straightforward patch size scheduling strategies—both linear and logarithmic. Specifically, for a predetermined total computational budget, we distribute the transition points evenly or according to a logarithmic scale across the training process. This way, we assess whether simply any monotonic scheduler leads to the same improvements, or whether the position of the transition points matter. We display the results in Fig. 7. We again observe that our scheduler remains more efficient, carefully determining the transition points based on the scaling laws, thus indeed leading to a significant improvement. Smaller patch sizes. Undeniably, the choice of patch size affects the inductive bias of ViTs — in general, the mechanism of ‘tokenization’ in the input affects the inductive bias of any Transformer model — by controlling the amount of compute and the level of details we are interested in extract- ing from an image. The patch size also controls the overall sequence length n processed by the Transformer model, and therefore the degree of weight sharing between the parameters. Our pre- vious laws clearly show that smaller patch sizes lead to better performance in high-compute areas. But does this trend also extend to even smaller patch sizes? We explore this question empirically by using the same experimental setup and pre-training on even smaller patch sizes P ∈ {6, 4} in addition to the previous results. We display the results in Fig. 8. We observe that while some absolute gains in performance can still be achieved with patch size 6, the additional required amount of compute is extremely high. For the even smaller patch size 4, one actually starts to lose in performance, as can be seen from plotting the intercepts cP of the corresponding scaling laws. 1015 1016 1017 1018 1019 FLOPs 50 60 70 80 90ImageNet 10-shot error rate [%] Patch size: 60 40 30 24 20 15 12 10 8 6 4 Empirical point Fitted curveEmpirical point Fitted curve 60 40 30 24 20 15 12 10 8 6 4 Patch size 0.6 0.8 cP Figure 8: We train the V256-6 with smaller patch sizes. This does not lead to a monotonically better performance. The behavior of performance with respect to patch size is thus only monotonic up to a certain point, and performance may actually worsen beyond that. This is in contrast to other scal- ing parameters such as the number of samples or the model size that usually offer a monotonic behavior in performance when scaled appropri- ately. 5 A DAPTIVE CONTEXT SIZE OF AN LLM While the previous chapter focused on Vi- sion Transformers, the aforementioned intu- itions and results generalize also to other do- mains. In this section we present a compelling case for the training of a Transformer-based language model. A critical consideration in training such a model is determining the opti- mal context size, as most Transformer models do not inherently support context extrapolation out of the box (Kazemnejad et al., 2023). The context size determines the amount of information the model can use when making predictions, with an expanded context being essential for enhanced performance in certain downstream tasks (Dao et al., 2022; Tay et al., 2020). A larger context size, however, comes with increased computational requirements, similar to the patch size in ViTs. To mitigate the significant training overhead associated with long contexts — where the quadratic complexity of self-attention becomes a bottleneck — a common strategy involves initially train- ing with shorter contexts. Subsequently, the model is fine-tuned on longer contexts, employing a modest computational budget to balance performance and efficiency (Chen et al., 2023c). Here, we 7demonstrate how different context sizes lead to different learning improvements at different scales of compute, and navigating across the optimal scaling law can lead to substantial performance gains given the same amount of compute. We train Transformer models (Touvron et al., 2023a;b) on English Wikpedia and Books (more details in App. B.5). The findings illustrated in Fig. 9 align with our hypothesis: smaller context sizes prove to be more optimal at the beginning of the training process, while larger contexts yield greater efficiency as training progresses. Employing a similar approach to that used with patch sizes, we introduce a scheduler that is adjusted in accordance with scaling laws. This strategy, akin to our adjustments in patch size, results in substantial computational savings, enabling a reduction in FLOPs of up to 40%. -40% FLOPs 1017 1018 1019 FLOPs 2.1 2.3 2.5 2.7 2.9 3.1 3.3T est Loss Context size: 256 Context size: 1024 Context size: 2048 Context-size scheduler Figure 9: We compare model performance throughout training with dynamic (magenta) and fixed context sizes. 6 O THER SHAPE PARAMETERS To further verify the efficiency of our approach, we study different ‘shape’ parameters when training a Vision Transformer. 6.1 A DAPTING MODEL WIDTH Similarly to the patch size, we need a mechanism that maps a transformer of smaller width d1 to a transformer of larger width d2. This is a very well-studied problem, especially in the case of natural language processing, and many schemes have been proposed (Gesmundo & Maile, 2023; Chen et al., 2022; Gong et al., 2019; Yao et al., 2023; Wang et al., 2023; Lee et al., 2022; Shen et al., 2022; Li et al., 2022). Here, we focus on the simplest approach, where we expand the initial model d1 by adding randomly initialized weights (see App. C for details). Although our expansion is not function preserving — i.e. we can expect a small drop in performance immediately after adapting the model (see Fig. 11) — we found that the model quickly recovers, and hence conclude that while not ideal, this simple expansion mechanism suffices for our setting5. Scaling width. The role of the model width and its associated scaling properties have been long analysed in the literature (Zhai et al., 2022; Alabdulmohsin et al., 2023). We repeat the scaling study for our own experimental setup and pre-train Vision Transformers of various widths and training du- rations on ImageNet-21k, using the same experimental setup as detailed in Sec. 3. In Fig. 10 we 5In practice we found that proposed function preserving schemes that depend on zero-initializing weights in the network, e.g. by Gesmundo & Maile (2023), perform suboptimally. 81016 1017 1018 1019 FLOPs 40 50 60 70 80 90ImageNet 10-shot error rate [%] Model class: V256-6 V192-12 V256-12 V384-12 V512-12 V640-12 V768-12 Empirical point Fitted curveEmpirical point Fitted curve Figure 10: Downstream performance as a function of compute for ViT of different size, trained with a patch size of 20. We use a log-log scale. 1016 1017 1018 FLOPs 30 40 50 60 70 80 90ImageNet 10-shot error rate [%] Model class: V768-12 V384-12 V192-12 Model scheduler Transition pointsModel scheduler Transition points 1016 1017 1018 30 40 50 60 70 80 90 V192-12 -> V384-12 V384-12 -> V768-12 Predicted Model scheduler Actual Model schedulerPredicted Model scheduler Actual Model scheduler Figure 11: The theoretically predicted scheduled perfor- mance (left) and the empirical obtained (right) perfor- mance. While transitions are less smooth, the model based on the scheduler quickly recovers back to the predicted law. report 10-shot ImageNet-1k error as a function of compute for a fixed patch size P = 20 (more details and results for other patch sizes are provided in the App. C). We again observe that differ- ent model widths are optimal for different levels of compute, similarly offering the potential for computational speed-ups by adapting the shape throughout training. Scheduling width. It is worth noting that strategies for expanding models during training have been previously explored. However, the critical question of when this expansion should occur has largely remained unanswered. Our approach then offers a straightforward and principled solution. We consider three width settingsd ∈ {192, 384, 768} and devise our scheduler based on the scaling law as outlined in Sec. 4. We display the obtained optimal schedule and the actual resulting per- formance in Fig. 11. As remarked previously, changing the model width does lead to a momentary deterioration of the performance, but it smoothly recovers back to the predicted performance. We again observe that the scheduled model remains optimal throughout training when compared to the static models. Note that adapting multiple shape parameters — namely both patch size and model size — is possible and leads to further improvements, as we showcase in App. D. 6.2 A DAPTING THE TRAINING OBJECTIVE In previous experiments, we have fixed training hyperparameters and focused on adapting the model’s shape. Training hyperparameters can nonetheless also be altered optimally. Previous work has already established that bigger batch sizes are beneficial during later stages in training (Kaplan et al., 2020; Hoffmann et al., 2022; Zhai et al., 2023). Different training objectives are also known to contribute differently to downstream performance at distinct stages in training (Zhai et al., 2023; Singh et al., 2023). Previous work has relied on heuristics and brute force exploration to determine when these should change. Here, we demonstrate how scaling laws can help decide when to change these parameters and descend based on the optimal one. Batch size. We first focus on the batch size used during training. We train two Vision Transform- ers, one at a larger batch size and the other at a smaller batch size. We find that in terms of number of FLOPs, the smaller batch size initially dominates but again is surpassed at later stages in training by the large batch size run (Fig. 12 left). Our strategy allows us to maximally take advantage of this difference by optimally transitioning between the two batch sizes, leading to a more optimal model. Distillation. We additionally train ViT models by distilling from a powerful teacher, a bigger pre- trained ViT (Fig. 12 right). Distilled labels naturally come at an additional computational cost, due to queries to the teacher, and lead to slower convergence in terms of FLOPs initially in training (FLOPs here include the teacher compute). Distillation objectives, however, were found to lead to increases in performance (Hinton et al., 2015; Furlanello et al., 2018; Touvron et al., 2021). Thus, in later stages of training, such an objective will dominate the standard supervised loss. We again leverage this discrepancy and optimally switch from the standard to the distilled loss, allowing us to reach the same level of performance with fewer FLOPs. 91017 1018 FLOPs 40 50 60 70 80 90ImageNet 10-shot error rate [%] Smaller batch size Larger batch size Predicted batch size scheduler Actual batch size scheduler 1016 1017 1018 1019 FLOPs 40 50 60 70 80 90ImageNet 10-shot error rate [%] Classification Distillation Predicted training objective scheduler Actual training objective scheduler Figure 12: (Left) ViTs with small and big batch sizes and using (right) different upstream objectives. Different optimality regimes are observed for the different settings. Changing between them leads to optimal performance gain for a fixed compute budget. 7 C ONCLUSION We have explored strategies to train models with variable shape parameters throughout the train- ing process. By efficiently traversing scaling laws, we have illustrated the optimal scheduling of shape parameters — including patch size, context length, model width, and other hyperparameters — yielding notable computational efficiency for a given performance level. We further observe that models with dynamically scheduled shape parameters consistently outperform their static counter- parts in terms of computational efficiency during training. This underscores the effectiveness of our method. Our scheduling approach is highly adaptable and applies to any shape parameter that allows for a smooth transition between models of varying configurations. This opens up many opportunities for future research, applying our scheduling method to other shape parameters such as model depth, sparsity, or a combination of multiple parameters. Given the increased computational demand for deep learning, we believe our findings make a crucial contribution to the field. IMPACT STATEMENT Our method provides insights into the growing challenges associated with the exponential scaling of compute resources for the training of frontier models. By making the training of large models more accessible, our approach opens doors to a broader audience of researchers and practitioners, fostering innovation and breakthroughs in artificial intelligence. Importantly, this alternative strategy contributes to a reduction in environmental impact, showcasing a commitment to sustainable and responsible advancements in the field. In App. E we provide concrete insights on the expected reduction in CO2 emissions for our experimental setup. REFERENCES Ibrahim Alabdulmohsin, Xiaohua Zhai, Alexander Kolesnikov, and Lucas Beyer. Getting vit in shape: Scaling laws for compute-optimal model design. arXiv preprint arXiv:2305.13035, 2023. Sotiris Anagnostidis, Dario Pavllo, Luca Biggio, Lorenzo Noci, Aurelien Lucchi, and Thomas Hof- mann. Dynamic context pruning for efficient and interpretable autoregressive transformers, 2023. Timur Ash. Dynamic node creation in backpropagation networks. Connection science, 1(4):365– 375, 1989. Gregor Bachmann, Sotiris Anagnostidis, and Thomas Hofmann. Scaling mlps: A tale of inductive bias. arXiv preprint arXiv:2306.13575, 2023. Lucas Beyer, Pavel Izmailov, Alexander Kolesnikov, Mathilde Caron, Simon Kornblith, Xiaohua Zhai, Matthias Minderer, Michael Tschannen, Ibrahim Alabdulmohsin, and Filip Pavetic. Flex- 10ivit: One model for all patch sizes. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 14496–14506, 2023. Daniel Bolya, Cheng-Yang Fu, Xiaoliang Dai, Peizhao Zhang, Christoph Feichtenhofer, and Judy Hoffman. Token merging: Your vit but faster. arXiv preprint arXiv:2210.09461, 2022. Shouyuan Chen, Sherman Wong, Liangjian Chen, and Yuandong Tian. Extending context window of large language models via positional interpolation. arXiv preprint arXiv:2306.15595, 2023a. Wuyang Chen, Wei Huang, Xianzhi Du, Xiaodan Song, Zhangyang Wang, and Denny Zhou. Auto- scaling vision transformers without training. arXiv preprint arXiv:2202.11921, 2022. Xi Chen, Josip Djolonga, Piotr Padlewski, Basil Mustafa, Soravit Changpinyo, Jialin Wu, Car- los Riquelme Ruiz, Sebastian Goodman, Xiao Wang, Yi Tay, et al. Pali-x: On scaling up a multilingual vision and language model. arXiv preprint arXiv:2305.18565, 2023b. Yukang Chen, Shengju Qian, Haotian Tang, Xin Lai, Zhijian Liu, Song Han, and Jiaya Jia. Longlora: Efficient fine-tuning of long-context large language models. arXiv preprint arXiv:2309.12307 , 2023c. Corinna Cortes, Lawrence D Jackel, Sara Solla, Vladimir Vapnik, and John Denker. Learning curves: Asymptotic values and rate of convergence. Advances in neural information process- ing systems, 6, 1993. Tri Dao, Dan Fu, Stefano Ermon, Atri Rudra, and Christopher R´e. Flashattention: Fast and memory- efficient exact attention with io-awareness. Advances in Neural Information Processing Systems, 35:16344–16359, 2022. Mostafa Dehghani, Josip Djolonga, Basil Mustafa, Piotr Padlewski, Jonathan Heek, Justin Gilmer, Andreas Peter Steiner, Mathilde Caron, Robert Geirhos, Ibrahim Alabdulmohsin, et al. Scaling vision transformers to 22 billion parameters. In International Conference on Machine Learning, pp. 7480–7512. PMLR, 2023a. Mostafa Dehghani, Basil Mustafa, Josip Djolonga, Jonathan Heek, Matthias Minderer, Mathilde Caron, Andreas Steiner, Joan Puigcerver, Robert Geirhos, Ibrahim Alabdulmohsin, et al. Patch n’pack: Navit, a vision transformer for any aspect ratio and resolution. arXiv preprint arXiv:2307.06304, 2023b. Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li, and Li Fei-Fei. Imagenet: A large-scale hi- erarchical image database. In 2009 IEEE conference on computer vision and pattern recognition, pp. 248–255. Ieee, 2009. Tim Dettmers, Mike Lewis, Younes Belkada, and Luke Zettlemoyer. Llm. int8 (): 8-bit matrix multiplication for transformers at scale. arXiv preprint arXiv:2208.07339, 2022. Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner, Mostafa Dehghani, Matthias Minderer, Georg Heigold, Sylvain Gelly, et al. An image is worth 16x16 words: Transformers for image recognition at scale. arXiv preprint arXiv:2010.11929, 2020. St´ephane d’Ascoli, Hugo Touvron, Matthew L Leavitt, Ari S Morcos, Giulio Biroli, and Levent Sagun. Convit: Improving vision transformers with soft convolutional inductive biases. In Inter- national Conference on Machine Learning, pp. 2286–2296. PMLR, 2021. Thomas Elsken, Jan Hendrik Metzen, and Frank Hutter. Neural architecture search: A survey. The Journal of Machine Learning Research, 20(1):1997–2017, 2019. Talfan Evans, Shreya Pathak, Hamza Merzic, Jonathan Schwarz, Ryutaro Tanno, and Olivier J Henaff. Bad students make great teachers: Active learning accelerates large-scale visual un- derstanding. arXiv preprint arXiv:2312.05328, 2023. Elias Frantar and Dan Alistarh. Sparsegpt: Massive language models can be accurately pruned in one-shot, 2023. 11Elias Frantar, Saleh Ashkboos, Torsten Hoefler, and Dan Alistarh. Gptq: Accurate post-training quantization for generative pre-trained transformers. arXiv preprint arXiv:2210.17323, 2022. Tommaso Furlanello, Zachary Lipton, Michael Tschannen, Laurent Itti, and Anima Anandkumar. Born again neural networks. In Jennifer Dy and Andreas Krause (eds.), Proceedings of the 35th International Conference on Machine Learning, volume 80 of Proceedings of Machine Learning Research, pp. 1607–1616. PMLR, 10–15 Jul 2018. URL https://proceedings.mlr. press/v80/furlanello18a.html. Jonas Geiping and Tom Goldstein. Cramming: Training a language model on a single gpu in one day. In International Conference on Machine Learning, pp. 11117–11143. PMLR, 2023. Andrea Gesmundo and Kaitlin Maile. Composable function-preserving expansions for transformer architectures. arXiv preprint arXiv:2308.06103, 2023. Linyuan Gong, Di He, Zhuohan Li, Tao Qin, Liwei Wang, and Tieyan Liu. Efficient training of bert by progressively stacking. In International conference on machine learning, pp. 2337–2346. PMLR, 2019. Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Delving deep into rectifiers: Surpassing human-level performance on imagenet classification. In Proceedings of the IEEE international conference on computer vision, pp. 1026–1034, 2015. Tom Henighan, Jared Kaplan, Mor Katz, Mark Chen, Christopher Hesse, Jacob Jackson, Heewoo Jun, Tom B Brown, Prafulla Dhariwal, Scott Gray, et al. Scaling laws for autoregressive generative modeling. arXiv preprint arXiv:2010.14701, 2020. Danny Hernandez, Jared Kaplan, Tom Henighan, and Sam McCandlish. Scaling laws for transfer. arXiv preprint arXiv:2102.01293, 2021. Joel Hestness, Sharan Narang, Newsha Ardalani, Gregory Diamos, Heewoo Jun, Hassan Kianinejad, Md Mostofa Ali Patwary, Yang Yang, and Yanqi Zhou. Deep learning scaling is predictable, empirically. arXiv preprint arXiv:1712.00409, 2017. Geoffrey Hinton, Oriol Vinyals, and Jeff Dean. Distilling the knowledge in a neural network. arXiv preprint arXiv:1503.02531, 2015. Jordan Hoffmann, Sebastian Borgeaud, Arthur Mensch, Elena Buchatskaya, Trevor Cai, Eliza Rutherford, Diego de Las Casas, Lisa Anne Hendricks, Johannes Welbl, Aidan Clark, et al. Train- ing compute-optimal large language models. arXiv preprint arXiv:2203.15556, 2022. Max Jaderberg, Valentin Dalibard, Simon Osindero, Wojciech M Czarnecki, Jeff Donahue, Ali Razavi, Oriol Vinyals, Tim Green, Iain Dunning, Karen Simonyan, et al. Population based train- ing of neural networks. arXiv preprint arXiv:1711.09846, 2017. Jean Kaddour, Oscar Key, Piotr Nawrot, Pasquale Minervini, and Matt J Kusner. No train no gain: Revisiting efficient training algorithms for transformer-based language models. arXiv preprint arXiv:2307.06440, 2023. Jared Kaplan, Sam McCandlish, Tom Henighan, Tom B Brown, Benjamin Chess, Rewon Child, Scott Gray, Alec Radford, Jeffrey Wu, and Dario Amodei. Scaling laws for neural language models. arXiv preprint arXiv:2001.08361, 2020. Amirhossein Kazemnejad, Inkit Padhi, Karthikeyan Natesan Ramamurthy, Payel Das, and Siva Reddy. The impact of positional encoding on length generalization in transformers.arXiv preprint arXiv:2305.19466, 2023. Andreas K¨opf, Yannic Kilcher, Dimitri von R¨utte, Sotiris Anagnostidis, Zhi-Rui Tam, Keith Stevens, Abdullah Barhoum, Nguyen Minh Duc, Oliver Stanley, Rich ´ard Nagyfi, et al. Openassistant conversations–democratizing large language model alignment. arXiv preprint arXiv:2304.07327, 2023. Guillaume Leclerc, Andrew Ilyas, Logan Engstrom, Sung Min Park, Hadi Salman, and Aleksander Madry. Ffcv: Accelerating training by removing data bottlenecks, 2023. 12Yunsung Lee, Gyuseong Lee, Kwangrok Ryoo, Hyojun Go, Jihye Park, and Seungryong Kim. To- wards flexible inductive bias via progressive reparameterization scheduling. InEuropean Confer- ence on Computer Vision, pp. 706–720. Springer, 2022. Changlin Li, Bohan Zhuang, Guangrun Wang, Xiaodan Liang, Xiaojun Chang, and Yi Yang. Au- tomated progressive learning for efficient training of vision transformers. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 12486–12496, 2022. Rupert Mitchell, Martin Mundt, and Kristian Kersting. Self expanding neural networks. arXiv preprint arXiv:2307.04526, 2023. Niklas Muennighoff, Alexander M. Rush, Boaz Barak, Teven Le Scao, Aleksandra Piktus, Noua- mane Tazi, Sampo Pyysalo, Thomas Wolf, and Colin Raffel. Scaling data-constrained language models, 2023. Lorenzo Noci, Sotiris Anagnostidis, Luca Biggio, Antonio Orvieto, Sidak Pal Singh, and Aurelien Lucchi. Signal propagation in transformers: Theoretical perspectives and the role of rank collapse. Advances in Neural Information Processing Systems, 35:27198–27211, 2022. R OpenAI. Gpt-4 technical report. arXiv, pp. 2303–08774, 2023. Bowen Peng, Jeffrey Quesnelle, Honglu Fan, and Enrico Shippole. Yarn: Efficient context window extension of large language models. arXiv preprint arXiv:2309.00071, 2023. Ofir Press, Noah A Smith, and Mike Lewis. Train short, test long: Attention with linear biases enables input length extrapolation. arXiv preprint arXiv:2108.12409, 2021. Zhen Qin, Yiran Zhong, and Hui Deng. Exploring transformer extrapolation. arXiv preprint arXiv:2307.10156, 2023. Alec Radford, Jeff Wu, Rewon Child, David Luan, Dario Amodei, and Ilya Sutskever. Language models are unsupervised multitask learners. 2019. Tal Ridnik, Emanuel Ben-Baruch, Asaf Noy, and Lihi Zelnik-Manor. Imagenet-21k pretraining for the masses, 2021. Jonathan S Rosenfeld, Amir Rosenfeld, Yonatan Belinkov, and Nir Shavit. A constructive prediction of the generalization error across scales. arXiv preprint arXiv:1909.12673, 2019. Anian Ruoss, Gr ´egoire Del ´etang, Tim Genewein, Jordi Grau-Moya, R ´obert Csord ´as, Mehdi Ben- nani, Shane Legg, and Joel Veness. Randomized positional encodings boost length generalization of transformers. arXiv preprint arXiv:2305.16843, 2023. Sheng Shen, Pete Walsh, Kurt Keutzer, Jesse Dodge, Matthew Peters, and Iz Beltagy. Staged training for transformer language models. In International Conference on Machine Learning, pp. 19893– 19908. PMLR, 2022. Mannat Singh, Quentin Duval, Kalyan Vasudev Alwala, Haoqi Fan, Vaibhav Aggarwal, Aaron Ad- cock, Armand Joulin, Piotr Doll ´ar, Christoph Feichtenhofer, Ross Girshick, et al. The effec- tiveness of mae pre-pretraining for billion-scale pretraining. arXiv preprint arXiv:2303.13496 , 2023. Samuel L Smith, Andrew Brock, Leonard Berrada, and Soham De. Convnets match vision trans- formers at scale. arXiv preprint arXiv:2310.16764, 2023. Ben Sorscher, Robert Geirhos, Shashank Shekhar, Surya Ganguli, and Ari Morcos. Beyond neu- ral scaling laws: beating power law scaling via data pruning. Advances in Neural Information Processing Systems, 35:19523–19536, 2022. Petru Soviany, Radu Tudor Ionescu, Paolo Rota, and Nicu Sebe. Curriculum learning: A survey. International Journal of Computer Vision, 130(6):1526–1565, 2022. Andreas Steiner, Alexander Kolesnikov, Xiaohua Zhai, Ross Wightman, Jakob Uszkoreit, and Lucas Beyer. How to train your vit? data, augmentation, and regularization in vision transformers.arXiv preprint arXiv:2106.10270, 2021. 13Jianlin Su, Murtadha Ahmed, Yu Lu, Shengfeng Pan, Wen Bo, and Yunfeng Liu. Roformer: En- hanced transformer with rotary position embedding. Neurocomputing, 568:127063, 2024. Yi Tay, Mostafa Dehghani, Samira Abnar, Yikang Shen, Dara Bahri, Philip Pham, Jinfeng Rao, Liu Yang, Sebastian Ruder, and Donald Metzler. Long range arena: A benchmark for efficient transformers. arXiv preprint arXiv:2011.04006, 2020. Yi Tay, Mostafa Dehghani, Samira Abnar, Hyung Won Chung, William Fedus, Jinfeng Rao, Sharan Narang, Vinh Q Tran, Dani Yogatama, and Donald Metzler. Scaling laws vs model architectures: How does inductive bias influence scaling? arXiv preprint arXiv:2207.10551, 2022. Gemini Team, Rohan Anil, Sebastian Borgeaud, Yonghui Wu, Jean-Baptiste Alayrac, Jiahui Yu, Radu Soricut, Johan Schalkwyk, Andrew M Dai, Anja Hauth, et al. Gemini: a family of highly capable multimodal models. arXiv preprint arXiv:2312.11805, 2023. Hugo Touvron, Matthieu Cord, Matthijs Douze, Francisco Massa, Alexandre Sablayrolles, and Herv´e J ´egou. Training data-efficient image transformers & distillation through attention. In International conference on machine learning, pp. 10347–10357. PMLR, 2021. Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, Timoth ´ee Lacroix, Baptiste Rozi `ere, Naman Goyal, Eric Hambro, Faisal Azhar, Aurelien Rodriguez, Ar- mand Joulin, Edouard Grave, and Guillaume Lample. Llama: Open and efficient foundation language models, 2023a. Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei, Niko- lay Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, et al. Llama 2: Open founda- tion and fine-tuned chat models. arXiv preprint arXiv:2307.09288, 2023b. Szymon Tworkowski, Konrad Staniszewski, Mikołaj Pacek, Yuhuai Wu, Henryk Michalewski, and Piotr Miło ´s. Focused transformer: Contrastive training for context scaling. arXiv preprint arXiv:2307.03170, 2023. Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, Łukasz Kaiser, and Illia Polosukhin. Attention is all you need. Advances in neural informa- tion processing systems, 30, 2017. Pauli Virtanen, Ralf Gommers, Travis E Oliphant, Matt Haberland, Tyler Reddy, David Cournapeau, Evgeni Burovski, Pearu Peterson, Warren Weckesser, Jonathan Bright, et al. Scipy 1.0: funda- mental algorithms for scientific computing in python. Nature methods, 17(3):261–272, 2020. Peihao Wang, Rameswar Panda, Lucas Torroba Hennigen, Philip Greengard, Leonid Karlinsky, Rogerio Feris, David Daniel Cox, Zhangyang Wang, and Yoon Kim. Learning to grow pretrained models for efficient transformer training. arXiv preprint arXiv:2303.00980, 2023. Fuzhao Xue, Yao Fu, Wangchunshu Zhou, Zangwei Zheng, and Yang You. To repeat or not to repeat: Insights from scaling llm under token-crisis, 2023. Yiqun Yao, Zheng Zhang, Jing Li, and Yequan Wang. 2x faster language model pre-training via masked structural growth. arXiv preprint arXiv:2305.02869, 2023. Xiaohua Zhai, Alexander Kolesnikov, Neil Houlsby, and Lucas Beyer. Scaling vision transformers. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition , pp. 12104–12113, 2022. Xiaohua Zhai, Basil Mustafa, Alexander Kolesnikov, and Lucas Beyer. Sigmoid loss for language image pre-training. arXiv preprint arXiv:2303.15343, 2023. 14A L IMITATIONS We detail the limitations of our work to the best of our knowledge. • We used greedy search with a small compute budget to get optimal hyper-parameters per model class. In practice, optimal parameters can change if larger levels of compute are available, as also hinted in Sec. 6.2. • In order to determine the optimal scheduler for a given shape parameter, knowledge of its scaling behavior is needed, which comes at a high computational cost. On the other hand, the scaling behavior of many shape parameters has already been established (e.g. width, depth, MLP-dimension (Alabdulmohsin et al., 2023)) and can readily be used in our scheduler. • Accurately predicting compute-optimal models, requires one to accurately schedule the learning rate throughout training. As we are interested in what happens during training for low-budgets of computes we do not schedule the learning rate nor embark on a cooldown phase (Zhai et al., 2022), as this would constitute a large fraction of the overall training time. We expect learning rate schedulers may shift our conclusion but not the outcome and takeaway message. • While we observe that the scheduled models are compute-optimal throughout all of train- ing, we observe the largest gains earlier on throughout training. Indeed, we do not expect our scheduled models to reach better performance for an infinite amount of compute. B E XPERIMENTAL SETUP We provide more details on the basis on which the experiments were conducted. B.1 T RAINING DETAILS PARAMETER VALUE OPTIMIZER ADAM W BETAS (0.9, 0.999) LABEL SMOOTHING 0.2 WEIGHT -DECAY HEAD 0.01 WEIGHT -DECAY BODY 0.01 WARM -UP 1000 STEPS CLIP GRADIENTS ’ NORM 1.0 UNDERLYING PATCH -SIZE SHAPE 12 UNDERLYING POSEMB SHAPE 8 Table 1: Hyper-parameters during training. ‘Underlying patch-size’ and ‘Underlying posemb shape’ refer to the flexible modules when training under a flexible patch size scheduler. In Tab. 1 we showcase hyper-parameters used when training on ImageNet-21k. We optimized each of the parameters for the different model classes by training for different configurations for a fixed, small amount of compute, namely 4×1017 FLOPs. Some examples of such hyper-parameter search are illustrated in Fig 13. All experiments were conducted using bfloat16. B.2 F INETUNING DETAILS In Tab. 2 we showcase hyper-parameters used when finetuning on ImageNet-1k. For the few-shot results, we use the linear model.Ridge function from scikit-learn with a regularization parameter of 1e−2. 150.0 0.5 1.0 1.5 2.0 2.5 3.0 3.5 4.0 FLOPs 1e17 0.0 0.1 0.2 0.3 0.4 0.5 0.6 0.7ImageNet 10-shot optimal smaller lr + mixup + mixup - wd larger lr larger wd Figure 13: Hyper-parameter search for a fixed (and small) budget of compute. PARAMETER VALUE OPTIMIZER SGD LEARNING RATE 0.03 MOMENTUM 0.9 WEIGHT DECAY 0.0 NUMBER OF STEPS 20000 CLIP GRADIENTS ’ NORM 1.0 SCHEDULER COSINE Table 2: Hyper-parameters during fine-tuning on ImageNet-1k. B.3 D ATASET DESCRIPTION For the ViT experiments, we follow the protocol of Ridnik et al. (2021) to preprocessImageNet-21k. It consists of roughly 12 million images and 11 thousand different classes. This is still considerably lower than the ≥ 29, 593 classes in the JFT-3B dataset. We experimented using different weight decay values for the body and the head, as proposed in Zhai et al. (2022) but found no significant difference. We attribute this to the lower number of classes in the dataset we are training in and our value of label smoothing. B.4 S CALING LAWS We fit functions of the form E = a(C + d)−b + c. (3) Similar to previous work, we resample points to be almost equidistant in the log-domain, in terms of FLOPs. We minimize different initializations using the minimize function in scipy (Virtanen et al., 2020), and choose the one that leads to the smallest error. The function to minimize is based on the Huber loss with δ = 1e−3. B.5 S EC. 5 D ETAILS In Sec. 5, we train Llama (Touvron et al., 2023a;b) models, following the Transformer++ training recipe, as displayed in Tab. 3. Due to compute constraints, we train models with embedding dimen- sion 768 and depth 12. We train on a subset of the English Wikipedia 20220301.en and English 16bookcorpus datasets. As done for the ViT models, we select for each configuration the batch size that leads to the most efficient – in terms of FLOPs – convergence. We evaluate on a held-out validation set. Although the validation samples are the same across different runs, we truncate them to match the context size of the respectively trained model. Models with longer contexts are thus expected to achieve lower test loss due to the enhanced context size (conditioned on longer contexts, subsequent tokens are more predictable). As during inference we are interested in the validation cross-entropy loss over the longer contexts, we do not adjust for this discrepancy. PARAMETER VALUE OPTIMIZER ADAM W BETAS (0.9, 0.95) CLIP GRADIENT ’S NORM 1.0 WEIGHT DECAY 0.1 DROPOUT 0.0 WARMUP 1000 NORM RMSNorm BIAS NO PEAK LEARNING RATE GPT-3 VALUES Table 3: Hyper-parameters during training of the language models. Extrapolating to longer contexts is a very active area of research with exciting work published re- cently (Qin et al., 2023; Ruoss et al., 2023; Press et al., 2021; Peng et al., 2023). In our case, we are training using RoPE positional encodings (Su et al., 2024), which are known to extrapolate easier compared to other ones, such as absolute positional encodings. The exact number for the most compute-intensive point in Fig. 9 is 17825792000 tokens. The exact model size including the embedding parameters is 137841408 parameters. That leads to an approximate token per parameter value of 129.3, past the optimal ”Chinchilla” point, leading thus to model that are more inference efficient. B.6 S EC. 6.2 D ETAILS In Sec. 6.2, we train ViT models using the same hyperparameters found as described above. For the batch-size experiments, we train V384-20/12 models with batch size in {256, 2048} and navigate across the scaling laws corresponding to the same values. For the different training objective experi- ments, we train a V384-20/12 model using either supervised training or distillation from a powerful teacher. We use as a teacher a pre-trained V640-10/12 model and train using only distillation loss as in Hinton et al. (2015), with a temperature of T = 2. When calculating the FLOPs of the single step, we include the FLOPs of the teacher only for the forward pass. C A DDITIONAL EXPERIMENTS Patch size scheduler. We present additional experiments on patch size schedulers in Fig. 14. For FlexiViT – similar to the original paper – we sample uniformly at every step a patch size from the set {8, 10, 12, 15, 20, 24}. We did not use smaller patch sizes due to computational constraints. Note that our patch size scheduler leads to significantly faster convergence across the model classes we are analyzing. We also present in Fig. 15, the fitted scaling curves and the points where changing the patch size leads to the steepest descent for different scaling laws. Model width scheduler. Supplementary to the results in Sec. 6.1, we provide additional examples of width examples in Fig. 16. Note that we do not touch on the (1) where to add the new weights and (2) how to initialize these new weights. Our approach simly defines a strategy on the when to expand the model and can be used in conjunction with any related works that provide answers to the previous (1) and (2) questions. 170.0 0.5 1.0 1.5 2.0 FLOPs 1e18 40 50 60 70 80 90ImageNet 10-shot error rate [%] Patch size: 60 40 30 24 20 15 12 10 8 Patch size scheduler Patch size decreased FlexiViTPatch size scheduler Patch size decreased FlexiViT 0.00 0.25 0.50 0.75 1.00 1.25 1.50 1.75 FLOPs 1e18 30 40 50 59 69 79 89ImageNet 10-shot error rate [%] Patch size: 60 40 30 24 20 15 12 10 8 Patch size scheduler Patch size decreased FlexiViTPatch size scheduler Patch size decreased FlexiViT (a) Model V256-6. (b) Model V192-12. 0.0 0.5 1.0 1.5 2.0 2.5 FLOPs 1e18 30 40 50 59 69 79 89ImageNet 10-shot error rate [%] Patch size: 60 40 30 24 20 15 12 10 8 Patch size scheduler Patch size decreased FlexiViTPatch size scheduler Patch size decreased FlexiViT 0.0 0.5 1.0 1.5 2.0 2.5 3.0 3.5 FLOPs 1e18 20 30 40 50 60 70 80 90ImageNet 10-shot error rate [%] Patch size: 60 40 30 24 20 15 12 10 8 Patch size scheduler Patch size decreased FlexiViTPatch size scheduler Patch size decreased FlexiViT (c) Model V256-12. (d) Model V384-12. 0 1 2 3 4 5 FLOPs 1e18 20 30 40 50 60 70 80 90ImageNet 10-shot error rate [%] Patch size: 60 40 30 24 20 15 12 10 8 Patch size scheduler Patch size decreased FlexiViTPatch size scheduler Patch size decreased FlexiViT 0.0 0.2 0.4 0.6 0.8 1.0 FLOPs 1e19 20 30 40 50 60 70 80 90ImageNet 10-shot error rate [%] Patch size: 60 40 30 24 20 15 12 10 8 Patch size scheduler Patch size decreased FlexiViTPatch size scheduler Patch size decreased FlexiViT (e) Model V512-12. (f) Model V768-12. Figure 14: Patch size schedulers for all the remaining model classes analysed. 181014 1015 1016 1017 1018 1019 FLOPs 40 50 60 70 80 90ImageNet 10-shot error rate [%] Patch-size: 8 Patch-size: 10 Patch-size: 12 Patch-size: 15 Patch-size: 20 Patch-size: 24 Patch-size: 30 Patch-size: 60 Patch-size scheduler 1014 1015 1016 1017 1018 1019 FLOPs 40 50 60 70 80 90ImageNet 10-shot error rate [%] Patch-size: 8 Patch-size: 10 Patch-size: 12 Patch-size: 15 Patch-size: 20 Patch-size: 24 Patch-size: 30 Patch-size: 60 Patch-size scheduler (a) Model V256-6. (b) Model V192-12. 1014 1015 1016 1017 1018 1019 FLOPs 40 50 60 70 80 90ImageNet 10-shot error rate [%] Patch-size: 8 Patch-size: 10 Patch-size: 12 Patch-size: 15 Patch-size: 20 Patch-size: 24 Patch-size: 30 Patch-size: 60 Patch-size scheduler 1014 1015 1016 1017 1018 1019 FLOPs 40 50 60 70 80 90ImageNet 10-shot error rate [%] Patch-size: 8 Patch-size: 10 Patch-size: 12 Patch-size: 15 Patch-size: 20 Patch-size: 24 Patch-size: 30 Patch-size: 60 Patch-size scheduler (c) Model V256-12. (d) Model V384-12. 1014 1015 1016 1017 1018 1019 FLOPs 40 50 60 70 80 90ImageNet 10-shot error rate [%] Patch-size: 8 Patch-size: 10 Patch-size: 12 Patch-size: 15 Patch-size: 20 Patch-size: 24 Patch-size: 30 Patch-size: 60 Patch-size scheduler 1014 1015 1016 1017 1018 1019 FLOPs 40 50 60 70 80 90ImageNet 10-shot error rate [%] Patch-size: 8 Patch-size: 10 Patch-size: 12 Patch-size: 15 Patch-size: 20 Patch-size: 24 Patch-size: 30 Patch-size: 60 Patch-size scheduler (e) Model V512-12. (f) Model V640-12. Figure 15: Fitted scaling laws and the predicted transition points that lead to the steepest descent. 191016 1017 1018 FLOPs 30 40 50 60 70 80 90ImageNet 10-shot error rate [%] Model class: V768-12 V384-12 V192-12 Model scheduler Transition pointsModel scheduler Transition points 1016 1017 1018 30 40 50 60 70 80 90 V192-12 -> V384-12 V384-12 -> V768-12 Predicted Model scheduler Actual Model schedulerPredicted Model scheduler Actual Model scheduler (a) Patch size 15. 1016 1017 1018 FLOPs 30 40 50 60 70 80 90ImageNet 10-shot error rate [%] Model class: V768-12 V384-12 V192-12 Model scheduler Transition pointsModel scheduler Transition points 1016 1017 1018 30 40 50 60 70 80 90 V192-12 -> V384-12 V384-12 -> V768-12 Predicted Model scheduler Actual Model schedulerPredicted Model scheduler Actual Model scheduler (a) Patch size 24. 1016 1017 1018 FLOPs 30 40 50 60 70 80 90ImageNet 10-shot error rate [%] Model class: V768-12 V384-12 V192-12 Model scheduler Transition pointsModel scheduler Transition points 1016 1017 1018 30 40 50 60 70 80 90 V192-12 -> V384-12 V384-12 -> V768-12 Predicted Model scheduler Actual Model schedulerPredicted Model scheduler Actual Model scheduler (c) Patch size 30. Figure 16: Width scheduler for models trained with different patch sizes. We expand the model width twice, as done in Sec. 6.1. The transition points of the expansion are found through our maximum descent rule. Regarding (1), we focus on models with constant depth (remember we are using the established Ti, S, and B Vision Transformer sizes). Therefore, we do not add new layers but merely expand the weight matrices to the new embedding dimension. Our method is agnostic to where these weights are added, just on the final form of the scaling law. Note that there exist more optimal ways to expand the different components of a ViT model (Alabdulmohsin et al., 2023). 201016 1017 1018 FLOPs 30 40 50 60 70 80 90ImageNet 10-shot error rate [%] Model class: V768-12 V384-12 V192-12 Model scheduler Transition pointsModel scheduler Transition points 1016 1017 1018 30 40 50 60 70 80 90 V192-12 -> V384-12 V384-12 -> V768-12 Predicted Model scheduler Actual Model schedulerPredicted Model scheduler Actual Model scheduler Figure 17: Different initialization schemes when expanding the width of the model. In practice, we set the variance of the new weights to beγσ2, where σ2 is calculated from the pre-expanded weights W, for different values of γ ∈ {0.25, 0.5, 0.75, 1, 1.25, 1.5}. Regarding (2), there are numerous works on how to initialize the weights under a function preser- vation criterion. In our case, we found that zero-initializing weights, as commonly proposed, is significantly suboptimal. In practice, we expand the weights matrices by initializing the new entries in the weight matrices randomly based on the norm of the weights of the already learned weights. In more detail, linear layers are expanded as: W′ = \u0012 W W 1 W2 W3, \u0013 where W1, W2, W3 ∼ N(0, σ2I), and σ2 is calculated from W. This ensures better signal prop- agation in the network (He et al., 2015; Noci et al., 2022). The effect of this initialization can be important, but not detrimental, as illustrated in Fig. 17. When expanding the self-attention layers, we simply concatenate new heads, i.e. leave the heads that correspond to the previous embedding dimension unchanged. Again we stress that our method does not attempt to answer the question on how to initialize, and any method established in the literature can be used for this purpose. We also include additional commonly used downstream performance metrics. In particular, we report 5/10-shot results on ImageNet/Pets/Birdsas done in Zhai et al. (2022). Results can be seen in Fig. 18. D A DAPTING MULTIPLE MODEL SHAPE PARAMETERS CONCURRENTLY We first present more results on which model configuration (number of parameters or patch size) leads to the most efficient training for different levels of performance in Fig. 20 and 21. Motivated by these insights, we ask the question: Can we change both the model size and patch size during training, leading to even greater training compute savings? We present preliminary experiments here, and more specifically in Fig. 19. We compare results when changing only the model width, only the patch size, or both the model width and patch size si- multaneously. In every case, we find the transition points, when the model shape should be adapted, using our proposed methodology. Changing both patch size and model width leads to the most sig- nificant improvements. For simplicity and clarity, we here consider model sizes in the set{V192-12, V256-12, V384-12} and patch sizes in the set {10, 20, 30, 40}. We note that our method does not take into account momentary performance boost, when reducing the patch size and momentary performance deterioration when changing the model size, due to reasons highlighted in the main text. This justifies why changing only patch size can be better in some cases for the short term. As more compute is invested into the new model shape, these changes are counteracted. 211016 1017 1018 30 40 50 60 70 80 90ImageNet 5-shot error rate [%] 1016 1017 1018 30 40 50 60 70 80 90ImageNet 10-shot error rate [%] 1016 1017 1018 20 30 40 50 60 70 80 90Pets 5-shot error rate [%] 1016 1017 1018 20 30 40 50 60 70 80 90Pets 10-shot error rate [%] 1016 1017 1018 20 30 40 50 60 70 80 90Birds 5-shot error rate [%] 1016 1017 1018 20 30 40 50 60 70 80 90Birds 10-shot error rate [%] Patch size: 8 10 12 15 20 24 30 40 60 SchedulerPatch size: 8 10 12 15 20 24 30 40 60 Scheduler FLOPs    Figure 18: We include more downstream performance metrics for the V384-12 model. 1015 1016 1017 1018 FLOPs 40 50 60 70 80 90ImageNet 10-shot error rate [%] Patch sizes: 10 20 30 40 Model class: V192-12 V256-12 V384-12Model class: V192-12 V256-12 V384-12 1015 1016 1017 1018 FLOPs 40 50 60 70 80 90 Patch size changed Model size changed Model Scheduler Patch Size Scheduler Model and Patch Size SchedulerModel Scheduler Patch Size Scheduler Model and Patch Size Scheduler Figure 19: Changing both model width and patch size during training, further accelerates training. E E NVIRONMENTAL IMPACT To estimate the carbon footprint of our training, we follow the recipe detailed in Touvron et al. (2023a). Specifically, we approximate the Watt-hours (Wh) used as Wh = GPU-hours × GPU-power-consumption × PUE where PUE refers to Power Usage Effectiveness. Following Touvron et al. (2023a) we set this quantity to 1.1. In order to enable comparisons across different works, we use the national US average carbon intensity factor of 0.385 kg CO2eq/KW hand we thus estimate the amounts of carbon emissions as tCO2eq = MW h× 0.385. We compare our adaptively trained model against standard training of the compute-optimal model, in this case, the ViT Base model with patch size 8. The model requires ≈ 120 GPU-hours with an average consumption of ≈ 280W with the default training. Our adaptive training requires roughly 40% of GPU-hours, i.e. ≈ 48 GPU-hours while enjoying the same average consumption ≈ 280W. 228 10 12 15 20 24 30 40 60 40 50 60 70 80 90ImageNet 10-shot error rate [%] V192-12 8 10 12 15 20 24 30 40 60 V384-12 8 10 12 15 20 24 30 40 60 V512-12 8 10 12 15 20 24 30 40 60 V768-12 Patch Size 1e+00 3e+00 8e+00 2e+01 6e+01 2e+02 4e+02 1e+03 3e+03 8e+03 PFLOPs Figure 20: IsoFLOPs curves for different size ViTs trained with different constant patch sizes. Note how larger patch sizes are favored for smaller total FLOPs, while smaller patch sizes become more efficient as total FLOPs increase. Larger model sizes also become more favorable as total FLOPs increase. 8 10 12 15 20 24 30 40 60 V256-6 V192-12 V256-12 V384-12 V512-12 V640-12 V768-12 1.55 0.96 0.71 0.62 0.40 0.32 0.26 0.25 0.34 1.28 1.04 0.87 0.58 0.43 0.33 0.30 0.26 0.37 2.49 1.66 1.26 0.97 0.56 0.43 0.38 0.39 0.43 4.33 2.86 2.41 1.62 1.16 0.92 0.80 0.67 0.68 7.80 5.19 4.28 3.08 2.03 1.66 1.48 1.12 1.05 13.86 9.73 7.99 5.09 3.58 2.66 2.21 1.59 1.55 23.07 15.86 11.08 8.44 5.29 4.23 3.23 2.47 2.28 ImageNet 10-shot error rate: 90.0 % 8 10 12 15 20 24 30 40 60 0123456 4.14 2.85 2.28 1.86 1.67 1.62 2.53 11.97 3.26 2.39 2.07 1.61 1.45 1.38 1.83 5.02 5.14 3.49 2.79 2.13 1.56 1.38 1.70 2.93 174.99 8.39 5.70 4.59 3.34 2.59 2.20 2.34 3.11 14.13 13.48 9.23 7.48 5.65 3.99 3.57 3.50 4.09 11.96 22.34 15.94 12.81 8.78 6.66 5.39 5.06 5.56 12.07 34.10 24.06 17.34 13.44 9.28 7.86 7.03 7.18 13.94 ImageNet 10-shot error rate: 78.0 % 8 10 12 15 20 24 30 40 60 0123456 17.80 15.47 14.51 14.07 34.74 121.62 11.44 7.89 7.46 7.67 11.56 20.50 355.16 13.29 9.57 8.34 6.73 7.61 9.30 28.24 19.16 13.81 10.86 8.82 8.14 8.17 13.53 74.14 27.10 19.31 15.72 12.75 10.30 10.75 13.78 45.61 41.26 30.28 23.92 18.25 15.77 14.74 17.98 45.47 57.01 41.97 31.30 25.28 20.33 19.11 22.67 47.50 ImageNet 10-shot error rate: 66.0 % 8 10 12 15 20 24 30 40 60 0123456 217.89502.22902.79 63.64 52.02 64.75 148.4215124.44 45.86 38.91 40.00 44.50 168.341412.41 51.58 42.02 34.77 33.22 49.57 88.771419.53 64.46 48.80 42.26 38.71 42.89 62.75 287.10 89.34 69.07 54.75 49.17 55.36 69.95 194.29 111.88 89.05 68.07 60.97 64.27 77.69 184.58 ImageNet 10-shot error rate: 55.0 % 8 10 12 15 20 24 30 40 60 0123456 1110.4219092.43 289.57455.06819.49 180.53194.46227.99297.0014549.80 207.06174.91197.92244.521172.1841161.25 260.06226.36196.77247.52720.004996.30 309.20300.46221.40286.31722.166626.05 ImageNet 10-shot error rate: 45.0 % 1e-01 5e-01 3e+00 1e+01 7e+01 4e+02 2e+03 1e+04 PFLOPs Unattainable Patch size Figure 21: Values for −∂gP (E) ∂E in Eq. 2. Values indicate how many FLOPs are required for a proportionate increase in performance (i.e. drop in the error rate). This leads to ≈ 0.036MW hfor ViT-Base and ≈ 0.014MW hfor our adaptive training. Thus, the default training of the ViT Base model causes carbon emissions of 0.014tCO2eq and our training 0.006tCO2eq. F T IME MEASUREMENT Although we focused on FLOPS, a similar hardware-aware analysis can take place, where the de- sired quantity to minimize is time instead of FLOPs. We note that time and FLOPs are usually highly correlated (Alabdulmohsin et al., 2023). This relationship also depends on the type of hard- ware and the mode it is operating in, i.e. whether we are memory-bound, whether data loading is the bottleneck etc. As an additional result, we replicate Fig. 6 from the main text but with time on the x-axis. Results can be seen in Fig. 22. G D ISCUSSION Although our approach is not directly comparable or inspired by them, we discuss some further interesting connections. Neural Architecture Search. The discovery of optimal architectures has also been explored in the line of work of neural architecture search (Elsken et al., 2019). Neural architecture search explores a collection of techniques to automate the selection of an optimal architecture. We are interested in more efficient training for a fixed architecture, the Transformer, that has established itself across different modalities. Curriculum Learning. Curriculum learning argues that the order in which samples are presented plays a crucial role in the learning efficiency of a model (Soviany et al., 2022). The role of training 23104 105 Time (seconds) 30 40 50 60 70 80 90ImageNet 10-shot error rate [%] -60% Time Patch size: Scheduler 60 40 30 24 20 15 12 10 8Patch size: Scheduler 60 40 30 24 20 15 12 10 8 Figure 22: Same plot as Fig. 6 but with time instead of FLOPs in the x-axis. data undoubtedly played a crucial role in the convergence speed (Sorscher et al., 2022). Recently, other techniques for data selection have been proposed to accelerate large-scale pre-training (Evans et al., 2023). Our technique does not filter or select data, just chooses to invest different amounts of compute to different data, based on the current stage of training. Population based training is also a related area of work (Jaderberg et al., 2017). 24",
      "meta_data": {
        "arxiv_id": "2311.03233v3",
        "authors": [
          "Sotiris Anagnostidis",
          "Gregor Bachmann",
          "Imanol Schlag",
          "Thomas Hofmann"
        ],
        "published_date": "2023-11-06T16:20:28Z",
        "pdf_url": "https://arxiv.org/pdf/2311.03233v3.pdf"
      },
      "llm_extracted_info": {
        "main_contributions": "This research introduces adaptive model training, allowing models to dynamically change their 'shape' (e.g., patch size, context length, model width, batch size, training objective) during training. This adaptive approach optimizes resource allocation, enabling models to traverse between different scaling laws and significantly reduce the computational resources (FLOPs) required to achieve a target performance, often by more than 50% for Vision Transformers. The paper presents a simple and effective strategy to determine when to adapt a model, based on maximizing performance gain per unit of compute, and demonstrates its efficiency across different modalities and shape parameters.",
        "methodology": "The core methodology involves adapting model shape parameters based on neural scaling laws. For a set of scaling laws {fP} (where P is a shape parameter) that map compute C to performance E, the method calculates the inverse laws f−1P(E), which predict the compute needed for a given performance E. To determine the optimal transition point for adapting a shape parameter, the partial derivative qP(E*) = ∂f−1P(E)/∂E | E=E* is computed. The shape parameter P that maximizes this derivative (i.e., offers the steepest descent or maximal performance gain for minimal additional compute) is chosen. This creates a scheduler for shape parameter changes. For Vision Transformers (ViTs), FlexiViT's mechanism for resizing patch embeddings and bilinearly interpolating positional encodings is used to smoothly adapt patch sizes. For adaptive model width, a simple expansion with randomly initialized weights (based on the norm of existing weights) is used, although function-preserving methods are acknowledged as ideal. The same principle is extended to adapting batch size and switching between standard and distillation training objectives.",
        "experimental_setup": "Experiments were conducted on Vision Transformers (ViTs) and Language Models (LMs). ViTs of various sizes (e.g., V640-12, V256-6) were pre-trained on the ImageNet-21k dataset, with images resized to 120x120. Different fixed patch sizes (P ∈ {120, 60, ..., 1}) were explored, with data augmentation (random cropping, horizontal flipping). Performance was measured by 10-shot error on ImageNet-1k. Compute (FLOPs) excluded the network head and approximated backward pass as twice the forward pass. Hyperparameters were optimized using a greedy search for a fixed, small compute budget (4e17 FLOPs). LMs (Llama models with embedding dim 768, depth 12) were trained on subsets of English Wikipedia and Books datasets, evaluating on test loss. Comparisons were made against static models, FlexiViT's random patch size scheduler, and simple linear/logarithmic schedulers. Additional studies involved adapting model width, batch size ({256, 2048}), and training objectives (standard vs. distillation from a V640-10/12 teacher model). Environmental impact was estimated in tCO2eq based on GPU-hours and power consumption.",
        "limitations": "The hyperparameter optimization was conducted with a small compute budget, and optimal parameters may differ for larger budgets. Determining the optimal scheduler requires upfront computational cost to establish scaling behaviors, though this information is often pre-existing for common parameters. Learning rate scheduling was not performed for low-compute budgets, which might influence results but is not expected to change the main conclusion. While compute-optimal, the adaptive models show the largest gains earlier in training and are not expected to achieve universally better performance with infinite compute, but rather reach target performance more efficiently. The simple random initialization used for expanding model width caused momentary performance drops, suggesting more sophisticated function-preserving expansion techniques would be beneficial. Momentary performance changes (slight degradation or boost) during shape parameter transitions were observed but quickly recovered from.",
        "future_research_directions": "Future work could involve applying this adaptive scheduling method to other model shape parameters not explored in this paper, such as model depth or sparsity, or even to combinations of multiple parameters simultaneously for further efficiency gains. There is also scope for integrating this approach with existing techniques designed to improve scaling laws or training efficiency, such as data pruning or optimal model shaping (e.g., depth vs. width ratios), as these methods are complementary. Further research could also focus on developing more robust and function-preserving mechanisms for dynamically changing model architecture, particularly for width expansion, to minimize temporary performance degradation during transitions."
      }
    },
    {
      "title": " Bayesian filtering unifies adaptive and non-adaptive neural network optimization methods ",
      "abstract": "We formulate the problem of neural network optimization as Bayesian\nfiltering, where the observations are the backpropagated gradients. While\nneural network optimization has previously been studied using natural gradient\nmethods which are closely related to Bayesian inference, they were unable to\nrecover standard optimizers such as Adam and RMSprop with a root-mean-square\ngradient normalizer, instead getting a mean-square normalizer. To recover the\nroot-mean-square normalizer, we find it necessary to account for the temporal\ndynamics of all the other parameters as they are geing optimized. The resulting\noptimizer, AdaBayes, adaptively transitions between SGD-like and Adam-like\nbehaviour, automatically recovers AdamW, a state of the art variant of Adam\nwith decoupled weight decay, and has generalisation performance competitive\nwith SGD.",
      "full_text": "Bayesian ﬁltering uniﬁes adaptive and non-adaptive neural network optimization methods Laurence Aitchison Department of Computer Science University of Bristol Bristol, UK, BS8 1UB laurence.aitchison@bristol.ac.uk Abstract We formulate the problem of neural network optimization as Bayesian ﬁltering, where the observations are the backpropagated gradients. While neural network optimization has previously been studied using natural gradient methods which are closely related to Bayesian inference, they were unable to recover standard optimizers such as Adam and RMSprop with a root-mean-square gradient normal- izer, instead getting a mean-square normalizer. To recover the root-mean-square normalizer, we ﬁnd it necessary to account for the temporal dynamics of all the other parameters as they are geing optimized. The resulting optimizer, AdaBayes, adaptively transitions between SGD-like and Adam-like behaviour, automatically recovers AdamW, a state of the art variant of Adam with decoupled weight decay, and has generalisation performance competitive with SGD. 1 Introduction and Background The cannonical non-adaptive neural network optimization method is vanilla stochastic gradient descent (SGD) with momentum which updates parameters by multiplying the exponential moving average gradient, ⟨g(t)⟩, by a learning rate, ηSGD, ∆wSGD(t) = ηSGD ⟨g(t)⟩ minibatch size. (1) Here, we divide by the minibatch size because we deﬁne g(t) to be the gradient of the summed loss, whereas common practice is to use the gradient of the mean loss. Further, following the convention established by Adam (Kingma & Ba, 2015), ⟨g(t)⟩, is computed by debiasing a raw exponential moving average, m(t), m(t) = β1m(t−1) + (1−β1) g(t) ⟨g(t)⟩= m(t) 1 −βt 1 . (2) where g(t) is the raw minibatch gradient, and β1 is usually chosen to be 0.9. These methods typically give excellent generalisation performance, and as such are used to train many state-of-the-art networks (e.g. ResNet (He et al., 2016), DenseNet (Huang et al., 2017), ResNeXt (Xie et al., 2017)). Adaptive methods change the learning rates as a function of past gradients. These methods date back many years (e.g. vario-eta Neuneier & Zimmermann, 1998), and many variants have recently been developed, including AdaGrad (Duchi et al., 2011), RMSprop (Hinton et al., 2012) and Adam (Kingma & Ba, 2015). The cannonical adaptive method, Adam, normalises the exponential moving average gradient by the root mean square of past gradients, ∆wAdam(t) = ηAdam ⟨g(t)⟩√ ⟨g2(t)⟩ . (3) arXiv:1807.07540v5  [stat.ML]  16 Apr 2020where, v(t) = β2v(t−1) + (1−β2) g2(t) ⟨g2(t)⟩= v(t) 1 −βt 2 , (4) and where β2 is typically chosen to be 0.999. These methods are often observed to converge faster, and hence may be used on problems which are more difﬁcult to optimize (Graves, 2013), but can give worse generalisation performance than non-adaptive methods (Keskar & Socher, 2017; Loshchilov & Hutter, 2017; Wilson et al., 2017; Luo et al., 2019). Obtaining a principled theory of these types of method is important, as it should enable us to develop improved adaptive optimizers. As such, here we formulated Bayesian inference as an optimization problem (Puskorius & Feldkamp, 1991; Sha et al., 1992; Puskorius & Feldkamp, 1994, 2001; Feldkamp et al., 2003; Ollivier, 2017), and carefully considered how the dynamics of optimization of the other parameters inﬂuences any particular parameter. We were able to recover the standard root-mean-square normalizer for RMSprop and Adam. Critically, our approach was also able to recover a state-of-the-art variant of Adam with “decoupled” weight decay (Loshchilov & Hutter, 2017). As such, we hope that by pursuing our dynamical Bayesian approach further, it will be possible to develop improved adaptive optimization algorithms. 2 Related work Previous work has considered the relationships between adaptive stochastic gradient descent methods and variational online Newton (VON), which is very closely related to natural gradients (Khan & Lin, 2017; Khan et al., 2017, 2018) and Bayes (Ollivier, 2017). Critically, this work found that direct application of VON/Bayes gives a sum-squared normalizer, as opposed to a root-mean-squared normalizer as in Adam and RMSProp. In particular, see Eq. 7 in Khan et al. (2018), which gives the Variational-online Newton (VON) updates, and includes a mean-squared gradient normalizer. To provide a method that matches Adam and RMSProp more closely, they go on to provide an ad-hoc modiﬁcation of the VON updates, with a root-mean-square normalizer, saying “Using ... an additional modiﬁcation in the VON update, we can make the VON update very similar to RMSprop. Our modiﬁcation involves taking the square-root over s(t+ 1) in Eq. (7)”. In contrast, our approach gives the root-mean-square normalizer directly, without any additional modiﬁcations, and automatically recovers decoupled weight decay (Loshchilov & Hutter, 2017) which is not recovered by VON (again, see Eq. 7 in Khan et al., 2018). An alternative view on these results is given by considering equivalence of online natural gradients and Kalman ﬁltering (Ollivier, 2017). Through this equivalence, they have the same issues as in (Khan & Lin, 2017; Khan et al., 2017, 2018): having a mean-square rather than root-mean-square form for the gradient normalizer. Further, note that while they do consider a “fading memory” approach, they “multiply the log-likelihood of previous points by a forgetting factor(1 −λt) before each new observation. This is equivalent to an additional step Pt−1 →Pt−1/(1 −λt) in the Kalman ﬁlter, or to the addition of an artiﬁcial process noise Qt proportional to Pt−1”, where Pt−1 is their posterior covariance matrix. Critically, their “artiﬁcal process noise ... proportional to Pt−1” again gives a mean-square form for the gradient normalizer (see Appendix A for details). In contrast, we give an alternative motivation for the introduction of ﬁxed process noise, and show that ﬁxed process noise recovers the root-mean-square gradient normalizer in Adam. 3 Methods Here, we set up the problem of neural network optimization as Bayesian inference. Typically, when performing Bayesian inference, we would like to reason about correlations in the full posterior over all parameters jointly (Fig. 1A). However, neural networks have so many parameters that reasoning about correlations is intractable: instead, we are forced to work with factorised approximate posteriors. To understand the effects of factorised approximate posteriors, consider the ith parameter. The current estimate of the other parameters, µ−i(t) changes over time, t, as they are optimized. As there are correlations in the posterior, the optimal value for theith parameter, w∗ i(t), conditioned on the current setting of the other parameters, µ−i(t) also changes over time (Fig. 1B), w∗ i(t) = arg max wi L(wi,µ−i(t)) . (5) 2-2 0 2 -2 0 2 wj wi A -2 0 2 0 25 50 75 100 Iteration wi B -1.00 -0.75 -0.50 -0.25 0.00 Obj. Figure 1: A schematic ﬁgure showing correlation-induced dynamics. A The objective function (usually equivalent to a posterior over the parameters) induces correlations between the parameter of interest, wi, and other parameters (here represented by wj). The red line displays the optimal value for wi as a function of wj or time. B The other parameters (including wj) change over time as they are also being optimized, implying that the optimal value for wi changes over iterations. As such, to form optimal estimates, µi(t) of the ith parameter, we need to reason changes over time in the optimal setting for that parameter, w∗ i(t). If we knew the full, correlated posterior in Fig. 1A, then we could compute the change in the ith parameter from the change in all the other parameters. However, in our case, the correlations are unknown, so the best we can do is to say that the new optimal value for the ith parameter will be close to — but slightly different from — the current optimal value, and these changes in the optimal value (Fig. 1B) constitute stochastic-dynamics that are implicitly induced by our choice of a factorised approximate posterior. More formally, we can explicitly consider the stochastic dynamics of w∗ i(t) that emerge under a quadratic objective. We take the objective for a single datapoint (which, for a supervised learning problem, would be a single input, xα, output, yα, pair) to be quadratic, L(xα,yα; w) = Lα (w) = −1 2 wT Hw + ξT α w, (6) where we have assumed that the Hessian is the same across data points, but the location of the mode varies across datapoints. Using the Fisher Information identity (see Appendix B), we can identify the covariance of the datapoint-dependent noise term as equal to the Hessian, E[ξα] = 0 Cov [ξα] = H. (7) The gradient for a single datapoint is ∂ ∂wLα (w) = −Hw + ξα (8) Thus, the gradient with respect to the ith weight, when all the other parameters, w−i are set to the current estimate, µ−i(t), is ∂ ∂wi Lα (wi,µ−i(t)) = −Hiiwi −HT −i,iµ−i(t) + ξα,i (9) where HT −i,iis the ith column of the Hessian, omitting the iith element. The optimal value for the ith parameter can be found by solving for the value of wi for which the gradient of the average objective is zero, w∗ i(t) = − 1 Hii HT −i,iµ−i(t). (10) Thus, we can rewrite the gradients as a function only of the optimal weight, ∂ ∂wi Lα (wi,µ−i(t)) = Hii(w∗ i(t) −wi) + ξα,i. (11) The data we actually measure is the gradient of the objective evaluated at the current estimate of the parameter, wi = µi. Taking into account the stochasticity across datapoints given by ξα,i, the gradient has distribution, P (gi(t)|w∗ i(t)) = N(Hii(w∗ i(t) −µi(t)) ,Hii) . (12) 3w∗ i(t−1) w∗ i(t) w∗ i(t+ 1) w∗ i(t+ 2) gi(t−1) gi(t) gi(t+ 1) gi(t+ 2) Figure 2: Graphical model under which we perform inference. This expression is highly suggestive: we should be able to infer the optimal value for theith parameter, w∗ i(t), from the backpropagated gradients, gi(t). However, to do this inference correctly, we need to understand the stochastic dynamics of w∗ i(t). In particular, Eq. (10) shows us that the dynamics of w∗ i(t) are governed by the dynamics of our estimates, µ−i(t), as they are optimized, and this optimization is a complex stochastic process. As reasoning about the dynamics of µ−i(t) under optimization is intractable, we instead consider simpler, discretised Ornstein-Uhlenbeck dynamics for µ−i(t), P (µ−i(t+ 1)|µ−i(t)) = N (( 1 − η2 2σ2 ) µ−i(t),η2 µ ) . (13) Thus, the dynamics for the ith optimal weight become, P (w∗ i(t+ 1)|w∗ i(t)) = N (( 1 − η2 2σ2 ) w∗ i(t),η2 ) (14) where, η2 = η2 µ HT −i,iH−i,i. (15) Combined, Eq. (12) and Eq. (14) deﬁne a stochastic linear dynamical system where the optimal weight, w∗ i(t) is the latent variable, and the backpropagated gradients, gi(t) are the observations (Fig. 2). As such, we are able to use standard Kalman ﬁlter updates to infer a distribution over w∗(t) = w∗ i(t) (where we drop indices for brevity). As the dynamics and likelihood are Gaussian, the Kalman ﬁlter priors and posteriors are, P (w∗(t)|g(t−1),...,g (1)) = N ( µprior(t),σ2 prior(t) ) , (16a) P (w∗(t)|g(t),...,g (1)) = N ( µpost(t),σ2 post(t) ) , (16b) where we evaluate the gradient, g(t) = gi(t) at µi(t) = µprior(t), and where the updates for µprior(t) and σ2 prior(t) can be computed from Eq. (14), µprior(t) = ( 1 − η2 2σ2 ) µpost(t−1), (17a) σ2 prior(t) = ( 1 − η2 2σ2 )2 σ2 post(t−1) + η2. (17b) And the updates for µpost(t) and σ2 post(t) come from applying Bayes theorem (Appendix C), with the likelihood given by Eq. (12). Following the standard approach in this line of work (Khan & Lin, 2017; Zhang et al., 2017; Khan et al., 2017, 2018), we approximate Hii using the squared gradient (improving upon this approximation is an important avenue for future work, but not our focus here), σ2 post(t) = 1 1 σ2 prior(t) + Hii ≈ 1 1 σ2 prior(t) + g2(t), (18a) µpost(t) = µprior(t) + σ2 post(t)g(t). (18b) The full updates are now speciﬁed by iteratively applying Eq. (17) and Eq. (18). Next, we make two minor modiﬁcations to the updates for the mean, to match current best practice for optimizing neural networks. First, we allow more ﬂexibility in weight decay, by replacing the η2/(2σ2) term in Eq. (17a) with a new parameter, λ. Second, we incorporate momentum, by using an exponential moving average gradient, ⟨g(t)⟩, instead of the raw minibatch gradient in Eq. (18b). In combination, the updates for the mean become, µprior(t) = (1 −λ) µpost(t−1), (19a) µpost(t) = µprior(t) + σ2 post(t)⟨g(t)⟩. (19b) Our complete Bayesian updates are now given by using Eq. (19) to update µprior and µpost, and using Eq. (17b) and Eq. (18a) to update σ2 prior and σ2 post (see Algo. 1). 4Figure 3: The learning rate for AdaBayes (points) compared against the predicted ﬁxed-point value (green line), σ2 post. The plot displays the low-data limit (orange line), which is valid when the value on the x-axis, η/ √ ⟨g2⟩, is much greater than σ2 (purple line), and the high-data limit (blue line), which is valid when the value on the x-axis is much smaller than σ2 (purple line). 3.1 AdaBayes recovers SGD and Adam To understand how AdaBayes relates to previous algorihtms (SGD and Adam), we plotted the AdaBayes learning rate, σ2 post against the Adam learning rate, η/ √ ⟨g2⟩(Fig. 3, points) for the ResNet-34 considered later. We found that for high values of ⟨g2⟩, corresponding to large values of the Fisher Information, the AdaBayes learning rate closely matched the Adam learning rate (Fig. 3, blue line). In contrast, as the value of ⟨g2⟩decreased, corresponding to smaller values of the Fisher Information, we found that the AdaBayes learning rate became constant, mirroring standard SGD (Fig. 3, orange line). Thus, Fig. 3 empirically establishes that AdaBayes converges to SGD in the low data (Fisher-Information) limit, and Adam in the high data limit. Furthermore, the tight vertical spread of points in Fig. 3 indicates that, in practice, the AdaBayes value of σ2 post is largely determined by the Fisher-Information, ⟨g2⟩, thus raising the question of whether we can obtain better understanding of the relationship between ⟨g2⟩and σ2 post. Indeed, such an understanding is possible, if we consider the ﬁxed point of the σ2 post updates (Eq. 17b and 18a). To obtain the ﬁxed-point, we substitute the update for σ2 post (Eq. 18a) into the update for σ2 prior (Eq. 17b), and neglect small terms (see Appendix D), which tells us that the ﬁxed-point σ2 post is given by the solution of a quadratic equation, 0 ≈σ2 ( 1 σ2post )2 − 1 σ2post −⟨g2⟩σ2 η2 . (20) Solving for 1/σ2 post, we obtain, 1 σ2post ≈ 1 2σ2  1 + √1 + 4 ( σ2 η/√ ⟨g2⟩ )2  . (21) We conﬁrmed the ﬁxed-point indeed matches the empirically measured AdaBayes learning rates by plotting the ﬁxed-point predictions in Fig. 3 (green line). Importantly, the ﬁxed-point expression merely helps understand a result that we established empirically. Finally, this close match means that we can deﬁne another set of updates, AdaBayes-FP, where we set σ2 post directly to the ﬁxed-point value, using Eq. (21), rather than using the full AdaBayes updates given by Eq. (17b) and Eq. (18a). 3.1.1 Recovering SGD in the low-data limit In the low-data regime whereη/ √ ⟨g2⟩≫ σ2, the empirically measured AdaBayes learning rate,σ2 post, becomes constant (Fig. 3; orange line), so the AdaBayes updates (Eq. 19b) become approximately equivalent to vanilla SGD (Eq. 1). To understand this convergence, we can leverage the ﬁxed-point expression in Eq. (21) which accurately models empirically measured learning rates, lim ⟨g2⟩→0 σ2 post ≈σ2, (22) 5Algorithm 1 AdaBayes η ←ηAdam σ2 ←ηSGD/minibatch size σ2 prior ←σ2 while not converged do g ←∇Lt(µ) m ←β1m+ (1 −β1) g v ←β2v + (1 −β2) g2 ⟨g⟩ ←m/(1 −βt 1) ⟨g2⟩← v /(1 −βt 2) σ2 prior← ( 1 − η2 2σ2 )2 σ2 post + η2 σ2 post ← 1 σ−2 prior+g2 µ ←(1 −λ) µ+ σ2 post⟨g⟩ end while Algorithm 2 AdaBayes-FP 1: η ←ηAdam 2: σ2 ←ηSGD/minibatch size 3: σ2 prior ←σ2 4: while not converged do 5: g ←∇Lt(µ) 6: m ←β1m+ (1 −β1) g 7: v ←β2v + (1 −β2) g2 8: ⟨g⟩ ←m/(1 −βt 1) 9: ⟨g2⟩← v /(1 −βt 2) 10: σ2 ← 1 σ−2+g2 11: σ2 post ← 1 2σ2 + √ 1 4σ4 + ⟨g2⟩ η2 12: µ ←(1 −λ) µ+ σ2 post⟨g⟩ 13: end while We can leverage this equivalence to setσ2 using standard values of the SGD learning rate, σ2 = ηSGD minibatch size. (23) Setting σ2 in this way would suggest σ2 ∼0.0011, as ηSGD ∼0.1, and the minibatch size ∼100. It is important to sanity check that this value of σ2 corresponds to Bayesian ﬁltering in a sensible generative model. In particular, note that σ2 is the variance of the prior over wi, and as such σ2 should correspond to typical initialization schemes (e.g. He et al., 2015) which ensure that input and output activations have roughly the same scale. These schemes use σ2 ∼1/(number of inputs), and if we consider that there are typically ∼100 input channels, and we typically convolve over a 3 ×3 = 9 pixel patch, we obtain σ2 ∼0.001, matching the value we use. 3.1.2 Recovering Adam(W) in the high-data limit In the high-data regime where η/ √ ⟨g2⟩≪ σ2, the empirically measured AdaBayes learning rate, σ2 post, approaches the Adam learning rate (Fig. 3; blue line), so AdaBayes becomes approximately equivalent to Adam(W). To understand this convergence, we can leverage the ﬁxed-point expression in Eq. (21) which accurately models empirically measured learning rates, lim ⟨g2⟩→∞ σ2 post ≈ η√ ⟨g2⟩ (24) so the updates (Eq.19b) become equivalent to Adam updates if we take, η= ηAdam. (25) As such, we are able to use past experience with good values for the Adam learning rate ηAdam, to set η: in our case we use η= 0.001. Furthermore, when we consider the form of regularisation implied by our updates, we recover a state-of-the-art variant of Adam, known as AdamW (Loshchilov & Hutter, 2017). In standard Adam, weight-decay regularization is implemented by incorporating an L2 penalty on the weights in the loss function, so the gradient of the loss and regularizer are both normalized by the root-mean-square gradient. In contrast, AdamW “decouples” weight decay from the loss, such that the gradient of the loss is normalized by the root-mean-square gradients, but the weight decay is not. To see that our updates correspond to AdamW, we combine Eq. (19a) and Eq. (19b), and substitute for σ2 post (Eq. 24), µpost(t) ≈(λ−1) µpost(t−1) + η√ ⟨g2(t)⟩ ⟨g(t)⟩. (26) 1here we use x ∼ y as in Physics to denote “ x has the same order of magnitude as y”, see Acklam and Weisstein “Tilde” MathWorld. http://mathworld.wolfram.com/Tilde.html 6Table 1: A table displaying the minimal test error and test loss for a ResNet and DenseNet applied to CIFAR-10 and CIFAR-100 for different optimizers. The table displays the best adaptive algorithm (bold), which is always one of our methods: either AdaBayes or AdaBayes-FP. We also display the instances where SGD (gray) beats all adaptive methods (in which case we also embolden the SGD value). CIFAR-10 CIFAR-100 ResNet DenseNet ResNet DenseNet optimizer error (%) loss error (%) loss error (%) loss error (%) loss SGD 5.170 0.174 5.580 0.177 22.710 0.833 21.290 0.774 Adam 7.110 0.239 6.690 0.230 27.590 1.049 26.640 1.074 AdaGrad 6.840 0.307 7.490 0.338 30.350 1.347 30.110 1.319 AMSGrad 6.720 0.239 6.170 0.234 27.430 1.033 25.850 1.103 AdaBound 5.140 0.220 4.850 0.210 23.060 1.004 22.210 1.050 AMSBound 4.940 0.210 4.960 0.219 23.000 1.003 22.360 1.017 AdamW 5.080 0.239 5.190 0.214 24.850 1.142 23.480 1.043 AdaBayes-SS 5.230 0.187 4.910 0.176 23.120 0.935 22.600 0.934 AdaBayes 4.840 0.229 4.560 0.222 22.920 0.969 22.090 1.079 Indeed, the root-mean-square normalization applies only to the gradient of the loss, as in AdamW, and not to the weight decay term, as in standard Adam. Finally, note that AdaBayes-FP becomes exactly AdamW when we set σ2 →∞, lim σ2→∞ 1 σ2post = lim σ2→∞ ( 1 2σ2 + √ 1 4σ4 + ⟨g2⟩ η2 ) = η√ ⟨g2⟩ , (27) because we use the standard Adam(W) approach to computing unbiased estimates of ⟨g⟩and ⟨g2⟩ (see Algo. 2). 4 Experiments For our experiments, we have adapted the code and protocols from a recent paper (Luo et al., 2019) on alternative methods for combining non-adaptive and adaptive behaviour (AdaBound and AMSBound). They considered a 34-layer ResNet (He et al., 2016) and a 121-layer DenseNet on CIFAR-10 (Huang et al., 2017), trained for 200 epochs with learning rates that decreased by a factor of 10 at epoch 150. We used the exact same networks and protocol, except that we run for more epochs, we plot both classiﬁcation error and the loss, and we use both CIFAR-10 and CIFAR-100. We used their optimized hyperparameter settings for standard baselines (including SGD and Adam), and their choice of hyperparameters for their methods (AdaBound and AMSBound). For AdamW and AdaBayes, we used ηSGD = 0.1 and set σ2 using Eq. (23), and we used ηAdam = η= 0.001 (matched to the optimal learning rate for standard Adam). We used decoupled weight decay of 5 ×10−4 (from Luo et al., 2019), and we used the equivalence of SGD with weight decay and SGD with decoupled weight decay to set the decoupled weight decay coefﬁcient to λ= 5 ×10−5 for AdamW, AdaBayes and AdaBayes-FP. The results are given in Table 1 and Fig. 4. The best adaptive method is always one of our methods (AdaBayes or AdaBayes-FP), though SGD is frequently superior to all adaptive methods tested. To begin, we compare our methods (AdaBayes and AdaBayes-FP) to the canonical non-adaptive (SGD) and adaptive (Adam) method (see Fig. A1 for a cleaner ﬁgure, including other baselines). Note that AdaBayes and AdaBayes-FP improve their accuracy and loss more rapidly than baseline methods (i.e. SGD and Adam) during the initial part of learning. Our algorithms give better test error and loss than Adam, for all networks and datasets, they give better test error than SGD for CIFAR-10, and perform similarly to SGD in the other cases, with AdaBayes-FP often giving better performance than AdaBayes. Next, we see that AdaBayes-FP improves considerably over AdaBayes (see Fig. A2 for a cleaner ﬁgure), except in the case of CIFAR-10 classiﬁcation error, where the difference is minimal. 75.0 7.5 10.0 12.5 15.0 17.5 CIFAR-10 test error (%) ResNet SGD Adam AMSBound AdaBound AdamW AdaBayes-FP AdaBayes DenseNet 0 100 200 300 epoch 0.2 0.3 0.4 0.5 CIFAR-10 test loss 0 100 200 300 epoch Figure 4: Test loss and classiﬁcation error for CIFAR-10 and CIFAR-100 for a Resnet-34 and a DenseNet-121, for multiple update algorithms. Given the difﬁculties inherent in these types of comparison, we feel that only two conclusions can reasonably be drawn from these experiments. First, AdaBayes and AdaBayes-SS have comparable performance to other state-of-the-art adaptive methods, including AdamW, AdaBound and AMS- Bound. Second, and as expected, SGD frequently performs better than all adaptive methods, and the difference is especially dramatic if we focus on the test-loss for CIFAR-100. 5 Conclusions Our fundamental contribution is show that, if we seek to use Bayesian inference to perform stochastic optimization, we need a model describing the dynamics of all the other parameters as they are optimized. We found that even by assuming that the other parameters obey oversimpliﬁed OU dynamics, we recovered state-of-the-art adaptive optimizers (AdamW). In our experiments, either AdaBayes or AdaBayes-FP outperformed other adaptive methods, including AdamW (Loshchilov & Hutter, 2017), and Ada/AMSBound (Luo et al., 2019), though SGD frequently outperformed all adaptive methods. We hope that understanding optimization as inference, taking into account the dynamics in the other weights as they are optimized, will allow for the development of improved optimizers, for instance by exploiting Kronecker factorisation (Martens & Grosse, 2015; Grosse & Martens, 2016; Zhang et al., 2017). References Duchi, J., Hazan, E., and Singer, Y . Adaptive subgradient methods for online learning and stochastic optimization. Journal of Machine Learning Research, 12:2121–2159, 2011. Feldkamp, L. A., Prokhorov, D. V ., and Feldkamp, T. M. Simple and conditioned adaptive behavior from kalman ﬁlter trained recurrent networks. Neural Networks, 16(5-6):683–689, 2003. Graves, A. Generating sequences with recurrent neural networks. arXiv preprint arXiv:1308.0850, 2013. Grosse, R. and Martens, J. A kronecker-factored approximate ﬁsher matrix for convolution layers. In International Conference on Machine Learning, pp. 573–582, 2016. 8He, K., Zhang, X., Ren, S., and Sun, J. Delving deep into rectiﬁers: Surpassing human-level performance on imagenet classiﬁcation. In Proceedings of the IEEE international conference on computer vision, pp. 1026–1034, 2015. He, K., Zhang, X., Ren, S., and Sun, J. Deep residual learning for image recognition. In Proceedings of the IEEE conference on computer vision and pattern recognition, pp. 770–778, 2016. Hinton, G., Srivastava, N., and Swersky, K. Overview of mini-batch gradient descent. COURSERA: Neural Networks for Machine Learning: Lecture 6a, 2012. Huang, G., Liu, Z., Van Der Maaten, L., and Weinberger, K. Q. Densely connected convolutional networks. In Proceedings of the IEEE conference on computer vision and pattern recognition, pp. 4700–4708, 2017. Keskar, N. S. and Socher, R. Improving generalization performance by switching from adam to sgd. arXiv preprint arXiv:1712.07628, 2017. Khan, M. E. and Lin, W. Conjugate-computation variational inference: Converting variational infer- ence in non-conjugate models to inferences in conjugate models. arXiv preprint arXiv:1703.04265, 2017. Khan, M. E., Liu, Z., Tangkaratt, V ., and Gal, Y . Vprop: Variational inference using rmsprop.arXiv preprint arXiv:1712.01038, 2017. Khan, M. E., Nielsen, D., Tangkaratt, V ., Lin, W., Gal, Y ., and Srivastava, A. Fast and scalable bayesian deep learning by weight-perturbation in adam. arXiv preprint arXiv:1806.04854, 2018. Kingma, D. and Ba, J. Adam: A method for stochastic optimization. In International Conference on Learning Representations, 2015. Loshchilov, I. and Hutter, F. Fixing weight decay regularization in adam. arXiv preprint arXiv:1711.05101, 2017. Luo, L., Xiong, Y ., Liu, Y ., and Sun, X. Adaptive gradient methods with dynamic bound of learning rate. arXiv preprint arXiv:1902.09843, 2019. Martens, J. and Grosse, R. Optimizing neural networks with kronecker-factored approximate curvature. In International conference on machine learning, pp. 2408–2417, 2015. Neuneier, R. and Zimmermann, H. G. How to train neural networks. In Neural networks: tricks of the trade, pp. 373–423. Springer, 1998. Ollivier, Y . Online natural gradient as a kalman ﬁlter.arXiv preprint arXiv:1703.00209, 2017. Puskorius, G. V . and Feldkamp, L. A. Decoupled extended kalman ﬁlter training of feedforward layered networks. In Neural Networks, 1991., IJCNN-91-Seattle International Joint Conference on, volume 1, pp. 771–777. IEEE, 1991. Puskorius, G. V . and Feldkamp, L. A. Neurocontrol of nonlinear dynamical systems with kalman ﬁlter trained recurrent networks. IEEE Transactions on neural networks, 5(2):279–297, 1994. Puskorius, G. V . and Feldkamp, L. A. Parameter-based kalman ﬁlter training: theory and implemen- tation. In Kalman ﬁltering and neural networks. 2001. Reddi, S. J., Kale, S., and Kumar, S. On the convergence of adam and beyond. ICLR, 2018. Sha, S., Palmieri, F., and Datum, M. Optimal ﬁltering algorithms for fast learning in feedforward neual networks. Neural Networks, 1992. Wilson, A. C., Roelofs, R., Stern, M., Srebro, N., and Recht, B. The marginal value of adaptive gradient methods in machine learning. In Advances in Neural Information Processing Systems, pp. 4148–4158, 2017. Xie, S., Girshick, R., Dollár, P., Tu, Z., and He, K. Aggregated residual transformations for deep neural networks. In Proceedings of the IEEE conference on computer vision and pattern recognition, pp. 1492–1500, 2017. 9Zhang, G., Sun, S., Duvenaud, D., and Grosse, R. Noisy natural gradient as variational inference. arXiv preprint arXiv:1712.02390, 2017. 10A Mean square normalizer in Ollivier (2017) In our framework, we can encode the multiplication by the forgetting factor in the computation of σ2 prior(t+ 1) from σ2 post(t), 1 σ2 prior(t+ 1) = 1 −λ σ2post(t), (28) and the equivalent process noise is, η= λ 1 −λσ2 post(t). (29) To understand the typical learning rates in this model we perform a ﬁxed-point analysis by substituting Eq. (28) into Eq. (18a), 1 σ2post(t+ 1) = 1 −λ σ2post(t) + g2(t). (30) Solving for the ﬁxed point, σ2 post = σ2 post(t) = σ2 post(t+ 1), this choice of process noise gives a mean-squre normalizer, σ2 post = λ ⟨g2⟩. (31) B Fisher Information We assume that the objective for a single datapoint is a quadratic log-likelihood with a form given by Eq. (6), L(xα,yα; w) = log P (yα|xα,w) . (32) As such, if we evaluate the gradients of the objective at the correct value of w (without loss of generality, this is w = 0 in Eq. (6)), we have, g′= ξα. (33) And the Fisher Information identity tells us that at this location, E [ ξαξT α ] = E [ g′g′T] = H, (34) substituting the value for g′into this expression, we ﬁnd that covariance of ξα is equal to the Hessian, recovering Eq. (7). C Kalman ﬁlter The log-posterior (Eq. 16b) is the sum of the log-prior (Eq. 16a) and the log-likelihood (Eq. 12). As such, − 1 2σ2 post (w∗ i −µpost)2 = − 1 2σ2 prior (w∗ i −µprior)2 − 1 2Hii (gi −Hii(w∗ i −µprior))2 . (35) The quadratic terms allow us to identify σ2 post, − 1 2σ2 post w∗2 i = − 1 2σ2 prior w∗2 i −1 2 Hiiw∗2 i , (36) so, 1 σ2post = 1 σ2 prior + Hii. (37) 11or, σ2 post = 1 1 σ2 prior + Hii . (38) And the linear terms allow us to identify µpost, µpost σ2 post w∗ i = µprior σ2 prior w∗ i + giw∗ i + Hiiµpriorw∗ i (39) so, µpost = σ2 post (( 1 σ2 prior + Hii ) µprior + gi ) (40) identiﬁng 1/σ2 post, µpost = µprior + σ2 postgi. (41) Finally, as Hii is unknown, we use, Hii ≈g2 i(t), (42) as g2(t) is an unbiased estimator of Hii when µi(t) = µprior(t) is at the optimum (Eq. 34). D Fixed point variance For the ﬁxed-point covariance, it is slightly more convenient to work with the inverse variance, λpost = 1 σ2post , (43) though the same results can be obtained through either route. Substituting Eq. (17b) into Eq. (18a) and taking η2/σ2 ≪1, we obtain an update from λpost(t) to λpost(t+ 1), λpost(t+ 1) = 1 1−η2/σ2 λpost(t) + η2 + g2(t) (44) assuming λpost has reached ﬁxed-point, we have λpost = λpost(t) = λpost(t+ 1), λpost = 1 1−η2/σ2 λpost + η2 + ⟨g2⟩. (45) Rearranging, λpost = λpost 1 −η2/σ2 + η2λpost + ⟨g2⟩. (46) Assuming that the magnitude of the update to λpost is small, we can take a ﬁrst-order Taylor of the ﬁrst term, λpost ≈λpost ( 1 + η2 σ2 −η2λpost ) + ⟨g2⟩. (47) cancelling, 0 ≈η2λ2 post −η2 σ2 λpost −⟨g2⟩, (48) and rearranging, 0 ≈σ2λ2 post −λpost −⟨g2⟩σ2 η2 , (49) and ﬁnally substituting for λpost gives the expression in the main text. E Additional data ﬁgures Here, we replot Fig. 4 to clarify particular comparisons. In particular, we compare AdaBayes(-FP) with standard baselines (Fig. A1), Adam AdamW and AdaBayes-FP (Fig. A2), AdaBayes(-FP) and Ada/AMSBound (Fig. A3), and Ada/AMSBound and SGD (Fig. A4). Finally, we plot the training error and loss for all methods (Fig. A5; note the loss does not include the regularizer, so it may go up without this being evidence of overﬁtting). 125.0 7.5 10.0 12.5 15.0 17.5 CIFAR-10 test error (%) ResNet SGD Adam AdaGrad AMSGrad AdaBayes-FP AdaBayes DenseNet 0.2 0.3 0.4 0.5 CIFAR-10 test loss 25 30 35 40 45 CIFAR-100 test error (%) 0 100 200 300 epoch 0.8 1.0 1.2 1.4 1.6 1.8 2.0 CIFAR-100 test loss 0 100 200 300 epoch Figure A1: Test loss and classiﬁcation error for CIFAR-10 and CIFAR-100 for a Resnet-34 and a DenseNet-121, comparing our methods (AdaBayes and AdaBayes-FP) with standard baselines (SGD, Adam, AdaGrad and AMSGrad (Reddi et al., 2018)). 135.0 7.5 10.0 12.5 15.0 17.5 CIFAR-10 test error (%) ResNet Adam AdamW AdaBayes-FP DenseNet 0.2 0.3 0.4 0.5 CIFAR-10 test loss 25 30 35 40 45 CIFAR-100 test error (%) 0 100 200 300 epoch 0.8 1.0 1.2 1.4 1.6 1.8 2.0 CIFAR-100 test loss 0 100 200 300 epoch Figure A2: Test loss and classiﬁcation error for CIFAR-10 and CIFAR-100 for a Resnet-34 and a DenseNet-121, comparing Adam, AdamW and AdaBayes-FP. 145.0 7.5 10.0 12.5 15.0 17.5 CIFAR-10 test error (%) ResNet AdaBound AMSBound AdaBayes-FP AdaBayes DenseNet 0.2 0.3 0.4 0.5 CIFAR-10 test loss 25 30 35 40 45 CIFAR-100 test error (%) 0 100 200 300 epoch 0.8 1.0 1.2 1.4 1.6 1.8 2.0 CIFAR-100 test loss 0 100 200 300 epoch Figure A3: Test loss and classiﬁcation error for CIFAR-10 and CIFAR-100 for a Resnet-34 and a DenseNet-121, comparing our methods (AdaBayes and AdaBayes-FP) with AdaBound/AMSBound Luo et al. (2019). 155.0 7.5 10.0 12.5 15.0 17.5 CIFAR-10 test error (%) ResNet AdaBound AMSBound SGD DenseNet 0.2 0.3 0.4 0.5 CIFAR-10 test loss 25 30 35 40 45 CIFAR-100 test error (%) 0 100 200 300 epoch 0.8 1.0 1.2 1.4 1.6 1.8 2.0 CIFAR-100 test loss 0 100 200 300 epoch Figure A4: Test loss and classiﬁcation error for CIFAR-10 and CIFAR-100 for a Resnet-34 and a DenseNet-121, comparing AdaBound/AMSBound Luo et al. (2019) and SGD. 160.0 2.5 5.0 7.5 10.0 12.5 15.0 CIFAR-10 train error (%) ResNet SGD Adam AMSBound AdaBound AdamW AdaBayes-FP AdaBayes DenseNet 0.0 0.1 0.2 0.3 0.4 0.5 CIFAR-10 train loss 0 10 20 30 40 CIFAR-100 train error (%) 0 100 200 300 epoch 0.0 0.5 1.0 1.5 2.0 CIFAR-100 train loss 0 100 200 300 epoch Figure A5: Train loss and classiﬁcation error for CIFAR-10 and CIFAR-100 for a Resnet-34 and a DenseNet-121, for all methods in Fig. 4. 17",
      "meta_data": {
        "arxiv_id": "1807.07540v5",
        "authors": [
          "Laurence Aitchison"
        ],
        "published_date": "2018-07-19T17:12:48Z",
        "pdf_url": "https://arxiv.org/pdf/1807.07540v5.pdf"
      },
      "llm_extracted_info": {
        "main_contributions": "The paper formulates neural network optimization as a Bayesian filtering problem, where backpropagated gradients are observations. It introduces AdaBayes, an optimizer that recovers the root-mean-square gradient normalizer for Adam and RMSprop by accounting for the temporal dynamics of all other parameters. AdaBayes adaptively transitions between SGD-like and Adam-like behaviors, automatically recovers AdamW (a state-of-the-art variant of Adam with decoupled weight decay), and achieves generalization performance competitive with SGD.",
        "methodology": "Neural network optimization is framed as Bayesian inference using Kalman filtering. The optimal weight for a parameter is treated as a latent variable, and backpropagated gradients are observations. The stochastic dynamics of the optimal weight are modeled using discretised Ornstein-Uhlenbeck dynamics for the other parameters. The Hessian (Hii) is approximated using the squared gradient. The developed algorithms are AdaBayes, which uses iterative updates for mean and variance, and AdaBayes-FP, which directly sets the variance to its fixed-point value based on a quadratic equation solution.",
        "experimental_setup": "Experiments were conducted using a 34-layer ResNet and a 121-layer DenseNet on CIFAR-10 and CIFAR-100 datasets. Networks were trained for 200 epochs (or more), with learning rates decreasing by a factor of 10 at epoch 150. Baseline optimizers (SGD, Adam, AdaGrad, AMSGrad, AdaBound, AMSBound, AdamW) used optimized hyperparameters from previous work. For AdamW and AdaBayes, specific hyperparameters were set: \"etaSGD\"=0.1, \"sigma^2\"= \"etaSGD\"/minibatch size, \"etaAdam\"=\"eta\"=0.001, and a decoupled weight decay of 5e-4 (with \",lambda\"=5e-5). Performance was validated by comparing minimal test error and test loss.",
        "limitations": "The approach relies on approximating the Hessian (Hii) using the squared gradient, which is noted as an area for future improvement. The assumption that other parameters obey oversimplified Ornstein-Uhlenbeck dynamics is a simplification. Empirically, SGD frequently outperforms all tested adaptive methods, including AdaBayes, particularly concerning test-loss on CIFAR-100, suggesting that adaptive methods still have generalization limitations compared to SGD in certain scenarios.",
        "future_research_directions": "Future research directions include improving the approximation of the Hessian (Hii), further pursuing the dynamical Bayesian approach to develop improved optimizers, and exploring the exploitation of Kronecker factorisation for better optimization algorithms."
      }
    },
    {
      "title": "AdaTune: Adaptive Tensor Program Compilation Made Efficient"
    },
    {
      "title": "Gaussian Process Bandit Optimization of the Thermodynamic Variational Objective",
      "abstract": "Achieving the full promise of the Thermodynamic Variational Objective (TVO),\na recently proposed variational lower bound on the log evidence involving a\none-dimensional Riemann integral approximation, requires choosing a \"schedule\"\nof sorted discretization points. This paper introduces a bespoke Gaussian\nprocess bandit optimization method for automatically choosing these points. Our\napproach not only automates their one-time selection, but also dynamically\nadapts their positions over the course of optimization, leading to improved\nmodel learning and inference. We provide theoretical guarantees that our bandit\noptimization converges to the regret-minimizing choice of integration points.\nEmpirical validation of our algorithm is provided in terms of improved learning\nand inference in Variational Autoencoders and Sigmoid Belief Networks.",
      "full_text": "Gaussian Process Bandit Optimization of the Thermodynamic Variational Objective Vu Nguyen University of Oxford vu@robots.ox.ac.uk Vaden Masrani University of British Columbia vadmas@cs.ubc.ca Rob Brekelmans USC Information Sciences Institute brekelma@usc.edu Michael A. Osborne University of Oxford mosb@robots.ox.ac.uk Frank Wood University of British Columbia fwood@cs.ubc.ca Abstract Achieving the full promise of the Thermodynamic Variational Objective (TVO), a recently proposed variational lower bound on the log evidence involving a one- dimensional Riemann integral approximation, requires choosing a “schedule” of sorted discretization points. This paper introduces a bespoke Gaussian process bandit optimization method for automatically choosing these points. Our ap- proach not only automates their one-time selection, but also dynamically adapts their positions over the course of optimization, leading to improved model learn- ing and inference. We provide theoretical guarantees that our bandit optimization converges to the regret-minimizing choice of integration points. Empirical valida- tion of our algorithm is provided in terms of improved learning and inference in Variational Autoencoders and Sigmoid Belief Networks. 1 Introduction The Variational Autoencoder (VAE) framework has formed the basis for a number of recent advances in unsupervised representation learning [18, 36, 42]. Assuming a generative model involving latent variables, VAEs perform maximum likelihood parameter estimation by optimizing the tractable Evi- dence Lower Bound (ELBO ) on the logarithm of the model evidence. In doing so, theVAE framework introduces an inference network, which seeks to approximate the true posterior over latent variables. While the ELBO is a common choice of variational inference objective, recent work has sought to improve the model learning [7, 39, 31, 26] or inference aspects [35, 19, 8, 13] of this task. In this work, we build upon the recent Thermodynamic Variational Objective ( TVO), which frames log-likelihood estimation as a one-dimensional integral over the unit interval [27]. The integral is estimated using a Riemann sum approximation, as visualized in Figure 1, yielding a natural family of variational inference objectives which generalize and tighten the ELBO . The choice of a d-dimensional vector of points β = [β0,β1,...,β d−1]T at which to construct this numerical approximation is an important hyperparameter for the TVO, which we refer to as an “in- tegration schedule” throughout this work. Previous work [27] uses a static integration schedule, and requires grid search over the choice of initial β1. However, since the shape of the integrand reﬂects the quality of the inference network (§2), recent work [6] suggests that this scheduling procedure may be improved by dynamically choosing βover the course of training. Our proposed approach also allows the TVO to be adapted to different model architectures and schedule dimensionality without the need for grid search. 34th Conference on Neural Information Processing Systems (NeurIPS 2020), Vancouver, Canada. arXiv:2010.15750v3  [cs.LG]  20 Nov 20201 <latexit sha1_base64=\"f/ucaHSqP+D9Bp31BanY7u7K7A0=\">AAACS3icbVDLTgJBEJxFUcQX6NHLRmLiiewqRo9ELx4hkUcCGzI7NDgyu7OZ6VXIhi/wqp/lB/gd3owHh8fBBSvppFLVne4uPxJco+N8WpmNzezWdm4nv7u3f3BYKB41tYwVgwaTQqq2TzUIHkIDOQpoRwpo4Ato+aO7md96BqW5DB9wEoEX0GHIB5xRNFLd7RVKTtmZw14n7pKUyBK1XtGqdPuSxQGEyATVuuM6EXoJVciZgGm+G2uIKBvRIXQMDWkA2kvml07tM6P07YFUpkK05+rfiYQGWk8C33QGFB/1qjcT//M6MQ5uvISHUYwQssWiQSxslPbsbbvPFTAUE0MoU9zcarNHqihDE05qSxAL5Eq+TNMqHQEDIdKqL+UIqa/TX8M4kmoWSf8p1ujL8TSfNzm7q6muk+ZF2b0sX9UrpertMvEcOSGn5Jy45JpUyT2pkQZhBMgreSPv1of1ZX1bP4vWjLWcOSYpZLK/K6y0Og==</latexit> 0 <latexit sha1_base64=\"+bcvVsgReT4e+AN7PozBeVeZPe8=\">AAACS3icbVDLTgJBEJxFUcQX6NHLRmLiiewqRo9ELx4hkUcCGzI7NDgyu7OZ6VXIhi/wqp/lB/gd3owHh8fBBSvppFLVne4uPxJco+N8WpmNzezWdm4nv7u3f3BYKB41tYwVgwaTQqq2TzUIHkIDOQpoRwpo4Ato+aO7md96BqW5DB9wEoEX0GHIB5xRNFLd6RVKTtmZw14n7pKUyBK1XtGqdPuSxQGEyATVuuM6EXoJVciZgGm+G2uIKBvRIXQMDWkA2kvml07tM6P07YFUpkK05+rfiYQGWk8C33QGFB/1qjcT//M6MQ5uvISHUYwQssWiQSxslPbsbbvPFTAUE0MoU9zcarNHqihDE05qSxAL5Eq+TNMqHQEDIdKqL+UIqa/TX8M4kmoWSf8p1ujL8TSfNzm7q6muk+ZF2b0sX9UrpertMvEcOSGn5Jy45JpUyT2pkQZhBMgreSPv1of1ZX1bP4vWjLWcOSYpZLK/KcW0OQ==</latexit> log p ✓ ( x ) <latexit sha1_base64=\"EuQLouLJNOc/j7x87FGJ1WRCcbg=\">AAACZXicbVBNb9NAEN0YWkr6lQJCQhxYNUIql8imreAYwYVjkEhbKY6i2c042WbttXbHbSLL4tdwhd/DL+BvsE5zwC1PGunpvRnNzBO5Vo7C8HcrePR4a/vJztP27t7+wWHn6NmFM4WVOJRGG3slwKFWGQ5Jkcar3CKkQuOlWHyu/csbtE6Z7ButchynMMtUoiSQlyadV7E2M55PYpojwUmcAs1FUi6rd5NON+yFa/CHJNqQLttgMDlqncVTI4sUM5IanBtFYU7jEiwpqbFqx4XDHOQCZjjyNIMU3bhc/1Dxt16Z8sRYXxnxtfrvRAmpc6tU+M76Rnffq8X/eaOCko/jUmV5QZjJu0VJoTkZXgfCp8qiJL3yBKRV/lYu52BBko+tsSUtNClrbqumCguUqHVTFcYsCIRrfo3L3Ng6kul14UiYZdVu+5yj+6k+JBfve9Fp7/zrWbf/aZP4DnvNjtkJi9gH1mdf2IANmWTf2Q/2k/1q/Qn2gxfBy7vWoLWZec4aCN78BQtnu7s=</latexit> 2H#Q ( ✓ , \u0000 , x ) <latexit sha1_base64=\"YA9/3VggtBVRB7Ddblx9ChCZX2k=\">AAACc3icbVBNT9tAEN245aPho6E9clkRqoIEkV2oyhG1lx6p1ABSHEWzmzFZsvZau2NIZPkn8Gt6bX9If0jvXYccauiTdvX03oxm5olcK0dh+LsVvHi5srq2/qq9sbm1/bqz8+bSmcJK7Eujjb0W4FCrDPukSON1bhFSofFKTL/U/tUdWqdM9p3mOQ5TuMlUoiSQl0ad9zHhjJwsUQtTHcQ0QYIjHucT5f8UaCKSclYdjjrdsBcuwJ+TaEm6bImL0U7rNB4bWaSYkdTg3CAKcxqWYElJjVU7LhzmIKdwgwNPM0jRDcvFRRV/55UxT4z1LyO+UP/tKCF1bp4KX1nv6J56tfg/b1BQcjYsVZYXhJl8HJQUmpPhdTx8rCxK0nNPQFrld+VyAhYk+RAbU9JCk7LmvmqqMEWJWjdVYcyUQLjm1TjLja0jGd8WjoSZVe22zzl6mupzcvmhF530Pn477Z5/Xia+znbZHjtgEfvEztlXdsH6TLIH9oP9ZL9af4LdYC/YfywNWsuet6yB4PgvuanBgA==</latexit> hoP L ( ✓ , \u0000 , x ) <latexit sha1_base64=\"xphD6EiyjCYRWS8CbnGhtEVK3cc=\">AAACdHicbVBNb9NAEN24QEP4SuAIhxVRpSKhyKatyrEqFw5IFKlJKyVRNLsZN9usvdbuuE1k+S/wa7i2/4M/wpl1mgNOeNKu3ryZ0cw8kWnlKAx/N4KdR4+f7Daftp49f/HyVbvzeuBMbiX2pdHGXgpwqFWKfVKk8TKzCInQeCHmX6r8xQ1ap0x6TssMxwlcpSpWEshLk/b+iHBBThbng+/l5JsPZ0jwkY+ymfJ/AjQTcbEoP0za3bAXrsC3SbQmXbbG2aTTOBxNjcwTTElqcG4YhRmNC7CkpMayNcodZiDncIVDT1NI0I2L1Ukl3/PKlMfG+pcSX6n/dhSQOLdMhK+sdnSbuUr8X26YU/x5XKg0ywlT+TAozjUnwyt/+FRZlKSXnoC0yu/K5QwsSPIu1qYkuSZlzW1ZV2GOErWuq8KYOYFw9atxkRlbWTK9zh0JsyhbLe9ztOnqNhl86kUHvaMfh92T07XjTfaWvWf7LGLH7IR9ZWeszyT7yX6xO3bf+BO8C7rB3kNp0Fj3vGE1BL2/6AbBjA==</latexit> \u0000 1 <latexit sha1_base64=\"IPPimOeoNIu1qnbhzwPMBzSgA/8=\">AAACUXicbVBNSyNBEK0Zv2L8Xo9eBoPgKcxIQI+iF49Z2BghCaG6U9F2eqaH7ho1hPwIr7s/a0/7U7xtJ+bgqA8KHu9VUVVPFFo5juN/Qbiyura+Udusb23v7O7tH/y4daa0kjrSaGPvBDrSKqcOK9Z0V1jCTGjqivR67nefyDpl8l88KWiQ4X2uxkoie6nbF8Q4TIb7jbgZLxB9JcmSNGCJ9vAgaPVHRpYZ5Sw1OtdL4oIHU7SspKZZvV86KlCmeE89T3PMyA2mi3tn0YlXRtHYWF85Rwv148QUM+cmmfCdGfKD++zNxe+8Xsnji8FU5UXJlMv3ReNSR2yi+fPRSFmSrCeeoLTK3xrJB7Qo2UdU2ZKVmpU1z7OqiilJ0rqqCmNSRuGqX9NLYew8ktFj6ViYl1m97nNOPqf6ldyeNZNWs/Wz1bi8WiZegyM4hlNI4Bwu4Qba0AEJKbzCb/gT/A3eQgjD99YwWM4cQgXh1n9Dq7TP</latexit> \u0000 2 <latexit sha1_base64=\"MCOu5VyeyW1R9PBQAXGx7bnt6Hs=\">AAACUXicbVBNSyNBEK2Z/VCTddX16GXYIHgKMxLQo+jFowvGBJIQqjsVbadneuiuUUPIj/CqP8vT/pS9bSfm4CQ+KHi8V0VVPVFo5TiO/wbhl6/fvm9sbtXqP7Z/7uzu/bpxprSS2tJoY7sCHWmVU5sVa+oWljATmjoivZj7nQeyTpn8micFDTK8zdVYSWQvdfqCGIfHw91G3IwXiNZJsiQNWOJquBe0+iMjy4xylhqd6yVxwYMpWlZS06zWLx0VKFO8pZ6nOWbkBtPFvbPo0CujaGysr5yjhfpxYoqZc5NM+M4M+c6tenPxM69X8vh0MFV5UTLl8n3RuNQRm2j+fDRSliTriScorfK3RvIOLUr2EVW2ZKVmZc3jrKpiSpK0rqrCmJRRuOrX9FQYO49kdF86FuZpVqv5nJPVVNfJzXEzaTVbf1qNs/Nl4ptwAL/hCBI4gTO4hCtog4QUnuEFXoO34F8IYfjeGgbLmX2oIKz/B0WStNA=</latexit> E ⇡ \u0000 [log p ✓ ( x , z ) q \u0000 ( z | x ) ] <latexit sha1_base64=\"2gwDVx1FkoExCqGSX0IfpeQ76OY=\">AAACsHicbZFdixMxFIbT8WutH9vVS2+CReiClJmloJeLIni5gt0tdIYhyWTabDOTmJzR1pg/6D/wX3irV2bayjq7Hgi8POcN5+QN1VJYiOMfvejW7Tt37x3c7z94+Ojx4eDoyblVjWF8ypRUZkaJ5VLUfAoCJJ9pw0lFJb+gq7dt/+IzN1ao+iNsNM8qsqhFKRiBgPJBkVYElpS6dz53qRZ5SjkQP0+lWuC0NIQ5naewDHC0s5Zu7V/iv/qrx8fefcpTvRSjK/jtynvss3wwjMfxtvBNkezFEO3rLD/qTdJCsabiNTBJrJ0nsYbMEQOCSe77aWO5JmxFFnweZE0qbjO3jcPjF4EUuFQmnBrwlv57w5HK2k1Fg7Pd0l7vtfB/vXkD5evMiVo3wGu2G1Q2EoPCbba4EIYzkJsgCDMi7IrZkoQMIfxAZ0rVSBBGffFdSlaccSm7lCq1AkJt99V8rZVpIykuGwtUrX2/H3JOrqd6U5yfjJPJePJhMjx9s0/8AD1Dz9EIJegVOkXv0RmaIoa+o5/oF/odnUSzKI/Izhr19neeok5Fl38AGajbmA==</latexit> \u0000 j <latexit sha1_base64=\"BnOLj3SGTHxGpIWQCNZ9bh3eJJA=\">AAACUXicbVBNT9tAEB0bSiFpIdAjF4uoUk+RXUUCbqhcOKZSQ5CSKJrdTGDx2mvtjluiKD+CK/wsTvwUbt18HOrQJ4309N6MZuaJQivHcfwahFvbH3Y+7u7V6p8+7x80Do+unSmtpK402tgbgY60yqnLijXdFJYwE5p6Ir1c+L3fZJ0y+S+eFjTM8DZXEyWRvdQbCGIc3Y8azbgVLxG9J8maNGGNzugwaA/GRpYZ5Sw1OtdP4oKHM7SspKZ5bVA6KlCmeEt9T3PMyA1ny3vn0VevjKOJsb5yjpbqvxMzzJybZsJ3Zsh3btNbiP/z+iVPzoYzlRclUy5XiyaljthEi+ejsbIkWU89QWmVvzWSd2hRso+osiUrNStr/syrKqYkSeuqKoxJGYWrfk0PhbGLSMb3pWNhHua1ms852Uz1Pbn+3krarfOf7ebFj3Xiu3AMJ/ANEjiFC7iCDnRBQgqP8ATPwUvwFkIYrlrDYD3zBSoI638BsbS1DQ==</latexit> E ⇡ \u0000  log p ✓ ( x , z ) q \u0000 ( z | x ) \u0000 <latexit sha1_base64=\"QH/zXBxYwJYi5Iqr+nZPBRpxCOg=\">AAAHq3ichVXbbhs3EGUujdz0Eid57AtRw4BjqIaVOEiCvBi5oC3aomkbJ0a8isDlzq4Ic7kUl5IlMftx+Yx+QV/bP+hwJUXSUm4X0Gp2zpkLZzhkrKUo7eHhn1euXrv+2Y3W1uc3v/jyq69vbd++86YshobDCS9kYU5jVoIUCk6ssBJOtQGWxxLexufPPf52BKYUhXptJxq6OcuUSAVnFlW97bMoZ7Yfx+5l1XORFviKwbKqimKRZWc0kkVGo9Qw7jRitu/BvWg0btNoNL1XuYE36wuvm35A/b2K0tq2S3vbO4cHh/VDQ6EzF3aOt0j9vOrdvvE4Sgo+zEFZLllZnt0/0rYNKsNK9LuOGSu4hOpmNCxBM37OMjhDUbEcyq6rq1HRXdQkNC0M/pSltXbVwrG8LCd5jEy/9rKJeeUnbHctlE0fd51QemhB8VmkdCipLaivLU2EAW7lBAXGjcBkKe8zrJ3FDqwl/brTdT4772YNkNgX1VkJspZdzGKQ1VpSTo+9oxKJCaS4C+r1umHJewaSyv3+/bPKdR4+aj84aj98gCwFF7zIc6YS3+rKvzKhHAwUM4ZNKjpXMCkyhR4aJuBNAKUFTuuvpXkQQqkgyP7/R6mtLo+zHwQajSvnZrs5peOmw9HFCnoRoJMVdBKg0xV0GqDpCpo20XOJ6Iue++nnqgklwBFbThX1Y/WhnqpmNZRnLiaNLketwctAhR7ng9qgSpmE1ICl/cr13iYHerxmv9F84GO4wQLcrdeQsEvp05VVbgwqBsj4seddnj59F9Zp8KkVswNtEBR9lK10KwuqwjyKpxeeewxPHWgSzJJgNhLqauiaxAydF6c5EinocpkGJsUMaoQsVJAvSKmNyMHTUX6Pxak/g3YIDSb3LC02YAsXWlzqQRUCz028cRaZcSaxiHuzdINaJznzOyDpefI48CZEUoo6n8JfP2AdanCivdJzXwAe8AZ+wTi/YubMFmbfRcxktVv8j9pe+i+iUAsiSnjXdJo3Syi8uX/QOTp48tvRzvGz2aVDtsg35FuyRzrkETkmP5BX5IRw8pH8Rf4m/7S+a/3ReteKZtSrV+Y2d8na04J/ARXC2SE=</latexit> Figure 1: The TVO objective frames log likelihood estimation as a Riemann sum approximation to a 1-d integral, with the ELBO as a special case for a single β0 = 0. The TVO (area in blue) bounds the integral more tightly than the ELBO (area within dotted lines). Our primary contribution is to automate the choice of integration schedules using a Gaussian process bandit optimization. We ﬁrst demonstrate that maximizing the TVO objective as a function of βis equivalent to a regret-minimization problem, where the black-box reward function reﬂects improve- ment in the objective for a given choice of schedule. We model this reward function over the course of training epochs using a time-varying Gaussian process (GP). Our entire procedure amounts to 1) choosing β to maximize an acquisition function in our surrogate GP model, 2) observing the reward function as the improvement in the TVO objective over one or more epochs of training with the chosen schedule, and 3) using these observations to update the GP model and select a new β. Our bandit algorithm is optimal in the sense of converging to a global regret-minimizing solution, as in the time-varying GP bandit optimization approach [5]. By choosing βto maximize an acqui- sition function that balances exploration and performance, our algorithm achieves global guarantees despite the non-convexity of the reward function. Further, our approach is directly aligned with the goal of improved model learning and inference, as the bandit reward function tracks the variational objective over the course of training. We review the TVO framework in §2, before presenting our bandit optimization approach in §3. We provide details of our time-varying Gaussian process model and discuss its convergence properties in §4. Finally, we demonstrate that our method can improve both model learning and inference in Variational Autoencoders and Sigmoid Belief Networks, in §5. 2 The Thermodynamic Variational Objective (TVO) Assuming a generative model pθ(x,z), we are interested in maximizing the log-likelihood log pθ(x) = log ∫ pθ(x,z)dz over parameters θ, given the empirical data x. However, this is in- tractable due to the integral over the latent variables z. Variational inference methods [4] often seek to maximize the tractable ELBO instead, obtained by introducing an approximate posterior qφ(z |x) and optimizing the objective ELBO (θ,φ, x) = log pθ(x) −DKL[qφ(z |x) ||pθ(z |x)] = Eqφ(z |x) [ log pθ(x,z) qφ(z |x) ] . (1) Thermodynamic Integration (TI) [33, 10, 11] is a common technique for estimating (ratios of) parti- tion functions in statistical physics, which instead frames estimatinglog pθ(x) as a one-dimensional integral over a geometric mixture curve parameterized by β.1 In particular, for the TVO, this curve interpolates between the approximate posterior qφ(z |x) and true posterior pθ(z |x). Following [6] 1Here β is a scalar to be consistent with notation in [27] In the remainder of the paper, we let β hold the sorted vector of discretization points β = [β0,β1,...,β d−1]T, so that each βj speciﬁes a πβj(z | x) in (2). 2this mixture curve can be interpreted as an exponential family of distributions over z given x πβ(z |x) = qφ(z |x) exp{β·log pθ(x,z) qφ(z |x) −log Zβ(x)} (2) where Zβ(x) = ∫ qφ(z |x)1−βpθ(x,z)βdz . Noting that log Z0(x) = 0 and log Z1(x) = log pθ(x), TI now applies the fundamental theorem of calculus to write the model evidence as an integral log pθ(x) −0 = ∫ 1 0 ∂ ∂β log Zβ(x)dβ = ∫ 1 0 Eπβ [ log pθ(x,z) qφ(z |x) ] dβ, (3) where we have used the known property of exponential families [44] that the derivative oflog Zβ(x) with respect to β matches the expected sufﬁcient statistics [27, 6]. Masrani et al. [27] use self- normalized importance sampling ( SNIS ) to estimate each term in the integrand, with S importance samples and qφ(z |x) as the proposal for each β Eπβ[·] ≈ S∑ ℓ=1 wβ ℓ∑ ℓwβ ℓ [·], w ℓ = pθ(x,zℓ) qφ(zℓ|x), zℓ ∼qφ(z |x). (4) Since log Zβ(x) is convex [44], we know that the integrand in (3) is an increasing function of β. Thus, we can obtain lower and upper bounds using left- and right-Riemann sums, respectively, over a discrete partition βof the unit interval. The left-Riemann sum then deﬁnes the TVO lower bound TVO(θ,φ, β,x) := d−1∑ j=0 (βj+1 −βj) Eπβj [ log pθ(x,z) qφ(z |x) ] , (5) where β = [ βj]d−1 j=0 with β0 = 0 and βj < βj+1. Note that the single-term left-Riemann sum with β = β0 = 0 matches the ELBO in (1), since π0(z |x) = qφ(z |x). However, how to choose intermediate βj for d >1 remains an interesting question, which we proceed to frame as a bandit problem. 3 From Evidence Maximization to Regret Minimization We view the vector β ∈[0,1]d as an arm [1] to be pulled in a continuous space, given a ﬁxed resource of T training epochs. After each round, we receive an estimate of the log evidenceL, from which we will construct a reward function. An important feature of our problem is that the integrand in Figure 1 changes between rounds as training progresses. Thus, our multi-armed bandit problem is said to be time-varying, in that the optimal arm and reward function depend on round t. More formally, we deﬁne the time-varying reward function ft : [0,1]d →R which takes an input βt and produces reward ft(βt). At each round we get access to a noisy reward yt = ft(βt) + ϵt where we assume Gaussian noise ϵt ∼ N(0,σ2 f). We aim to maximize the cumulative reward∑T/w t=1 ft(βt) across T/w rounds, where wis a divisor of T and will later control the ratio of bandit rounds tto training epochs i. Maximizing the cumulative reward is equivalent to minimizing thecumulative regret RT/w := T/w∑ t=1 ft(β∗ t) −ft(βt), (6) where rt := ft(β∗ t) −ft(βt) is the instantaneous regret deﬁned by the difference between the received reward ft(βt) and maximum reward attainable ft(β∗ t) at round t. The regret, which is non-negative and monotonic, is more convenient to work with than the cumulative reward and will allow us to derive upper bounds in §4.3. In order to translate the problem of maximizing the log evidence as a function of βinto the bandits framework, we deﬁne a time-varying reward functionft(βt). We construct this reward in such a way 3Algorithm 1GP-bandit for TVO (high level) Input: schedule dimension d, reward function ft(βt) where βt ∈[0,1]d , update frequency w 1: for t= 1....T do 2: Train θ,φ for one epoch using TVO and previously βt−1, evaluate Lt from Eq. (5) 3: if mod(t,w) = 0 : time to update βt then 4: Estimate the utilityyt = Lt−Lt−w and augment (βt−1,t,y t) into training dataDt 5: Fit a time-varying, permutation invariant GP to Dt 6: Estimate GP predictive mean µt(β) and uncertainty σt(β) from Eqs. (15,16) 7: Select βt = arg maxµt(β) + √κtσt(β) where κt is from Theorem 1 8: end if 9: end for that minimizing the cumulative regret is equivalent to maximizing the ﬁnal log evidence estimate LT := log pθT(x), i.e., such that min RT/w = max LT. Such a reward function can be deﬁned by partitioning the T training epochs into windows of equal length w, and deﬁning the reward for each window t∈{0,1,...,T/w −1} ft(βt) := Lw(t+1) −Lwt (7) as the difference between the TVO log evidence estimate one window-length in the future Lw(t+1) and the present estimate Lwt. Then, the cumulative reward is given by a telescoping sum over windows T/w −1∑ t=0 ft(βt) = ( Lw −L0 ) + ( L2w −Lw ) + ...+ ( L(T/w)w −L(T/w −1)w ) (8) = LT −L0, (9) where L0 is the initial (i.e. untrained) loss. Recalling the deﬁnition of cumulative regret in Eq. (6), min RT/w = min   T/w −1∑ t=0 ft(β∗ t)  −   T/w −1∑ t=0 ft(βt)   (10) = min ( (L∗ T −L0) −(LT −L0) ) (11) = min (const −LT) (12) = max LT. (13) Therefore minimizing the cumulative regret for the reward function deﬁned by Eq. (7) is equivalent to maximizing the log evidence on the ﬁnal epoch. Next, we describe how to design an optimal decision mechanism to minimize the cumulative regret RT/w using Gaussian processes. 4 Minimizing Regret with Gaussian Processes There are two unresolved problems with the reward function deﬁned in Eq. (7) which still must be addressed. The ﬁrst is that it is not in fact computable, due to its use of future observations. The second is that it ignores the ordering constraint required for βto be a valid Riemann partition. We can handle both by problems by using a permutation-invariant Gaussian process to form a surro- gate for the reward function ft(βt). The surrogate model will be updated by past rewards, and used in place of ft(βt) to select the next schedule at the current round, as described in Algorithm 1. In §4.1 we formally deﬁne how to use (time-varying) Gaussian processes in bandit optimization, before describing how our permutation-invariant kernel can be used to solve the problem of order- ing constraints on β in §4.2. Finally in §4.3 we provide a theoretical guarantee that our bandit optimization will converge to the regret-minimizing choice of β. 44.1 Time-varying Gaussian processes for Bandit Optimization A popular design in handling time-varying functions [21, 41, 20, 32] such as ft(βt) is to jointly model the spatial and temporal dimensions using a product of covariance functions k= kβ ⊗kT, where kβ : [0,1]d ×[0,1]d →R is a spatial covariance function over actions, kT : N ×N →R is a temporal covariance function, and k: Rd+1 + ×Rd+1 + →R. Under this joint modeling framework, the GP is deﬁned as follows. At round twe have the history of rewards yt = [y0,...,y t]T and sample points Xt = {x0,..., xt}, where we deﬁne xt ∈Rd+1 to be the concatenation of βt and timestep t, i.e xt := [βt,t]T. Then the time-varying reward function is GP-distributed according to ft ∼GP ( 0,k(x,x′) ) where k(x,x′) := kβ(β,β′) ×kT(t,t′), (14) where we have assumed zero prior mean for simplicity. For theoretical convenience we follow [5] and choose kT(t,t′) = (1 −ω) |t−t′| 2 , where ω is a “forgetting-remembering” trade-off parameter learned from data. We describe kβ(β,β′) in §4.2. Using standard Gaussian identities [3, 34], the posterior predictive is also GP distributed, with mean and variance given by µt(β∗) = kt(β∗)T ( Kt + σ2 fI )−1 yt (15) σ2 t(β∗) = kt(β∗,β∗) −kt(β∗)T ( Kt + σ2 fI )−1 kt(β∗) (16) where kt(β) = [ k(x0,x),...,k (xt,x)]T and Kt = [ k(x,x′)]x,x′∈Xt. Using this permutation- invariant, time-varying GP we can select βt+1 by maximizing a linear combination of the GP pos- terior mean and variance w.r.tβt βt+1 = arg max βt µt(βt) + √κtσt(βt), (17) where Eq. (17) is referred to as anacquisition function and κtis its exploration-exploitation trade-off parameter. We note that there are other acquisition functions available [16, 14, 15]. Our acquisi- tion function, Eq. (17), is the time-varying version of GP-UCB [40, 5], which allows us to obtain convergence results in §4.3 and set κt in Theorem 1. 4.2 Ordering Constraints and Permutation Invariance Recall that the vector β= [β0,...,β d−1]T holds the locations of the left Riemann integral approxi- mation in Eq. (5). In order for the left Riemann approximation to the TVO to be sensible, there must be an ordering constraint imposed on βsuch that 0 < β1 < ... < βd−1 <1. We model this in our GP using a projection operator Φ which imposes the constraint by sorting the vector β. Applying Φ within the spatial kernel, we obtain kβ ( β,β′) := kS ( Φ(β),Φ(β′) ) . (18) This projection does not change the value of our acquisition function, and maintains the positive deﬁnite for any covariance function for the spatial kS, e.g., Matern, Polynomial. We then optimize the acquisition function via a projected-gradient approach. If a βt iterate leaves the feasible set after taking a gradient step, we project it back into the feasible set using Φ and continue. We note that existing work in the GP literature has considered such projection operations in various contexts [38, 43]. 4.3 Convergence Analysis In Eq. (10), we showed that maximizing the TVO objective function LT as a function of βt is equivalent to minimizing the cumulative regret RT/w by sequential optimization within the bandit framework. Here, the subscript T/w refers to the number of bandit updates given the maximum epochs T and the update frequency wwhere w≪T. We now derive an upper bound on the cumulative regret, and show that it asymptotically goes to zero as T increases, i.e., limT→∞ RT/w T = 0. Thus, our bandit will converge to choosing βT which yields the optimal value of the TVO objective L∗ T for model parameters at step T. 50.0 0.2 0.4 0.6 0.8 1.0 3 2 1 0 1 2 3 4 Standardized f( ) Time-Varying Reward Function t=[1-2k] GP mean GP var t=1000 t=1200 t=1400 t=1600 t=1800 t=2000 0.0 0.2 0.4 0.6 0.8 1.0 3 2 1 0 1 2 3 4 Standardized f( ) Time-Varying Reward Function t=[2-3k] GP mean GP var t=2000 t=2200 t=2400 t=2600 t=2800 t=3000 0.0 0.2 0.4 0.6 0.8 1.0 3 2 1 0 1 2 3 4 Standardized f( ) Time-Varying Reward Function t=[9-10k] GP mean GP var t=9000 t=9200 t=9400 t=9600 t=9800 Figure 2: Time-varying reward function ft(βt) after 2k, 3k, and 10k training epochs, with bandit choices of a single intermediate β1 (i.e. β = [0,β1]) colored by timestep. Scattered β1 in neigh- boring epochs indicate ‘exploration’, while similarly colored values of β1 in regions where the GP mean, or predicted reward, is high indicate ‘exploitation’. We present the main theoretical result in Theorem 1. OurTVO framework mirrors the standard time- varying GP bandit optimization, and thus inherits convergence guarantees from Bogunovic et al. [5]. However, as discussed in Appendix §C, we provide a tighter bound on the mutual information gain γT/w which may be of wider interest. Theorem 1. Let the domain D ⊂[0,1]d be compact and convex. Let Lt ≥0 be the Lipschitz constant for the reward function at time t. Assume that the covariance function k is almost surely continuously differentiable, with f ∼GP(0,k). Further, for t≤T and j ≤d, we assume Pr ( sup ⏐⏐⏐∂ft(βt)/∂β(j) t ⏐⏐⏐≥Lt ) ≤ae−(Lt/b)2 for appropriate choice of aand bcorresponding to Lt. For δ∈(0,1), we write κT/w = 2 log π2T2 2δw2 + 2dlog dbT2 w2 √ log daπ2T2 2δw2 and C1 = 8/log(1 + σ2 f). Then, after T/w time steps, our algorithm satisﬁes RT/w = T/w∑ t=1 ft(β∗ t) −ft(βt) ≤ √ γT/w ·C1 ·κT/w ·T/w + 2 with probability at least 1 −δ, where γT/w is the maximum information gain for the time-varying covariance function (see below). In the above theorem, the quantity γT/w measures the maximum information gain obtained about the reward function after pulling T/w arms [40, 5]. In the Appendix §C, we show that γT/w ≤( 1 + T/ [ w˜N ])( γβ ˜N + σ−2 f ˜N5/2ω ) , where ˜N ∈ {1,...,T/w }denotes a time-varying block length, and γβ ˜N is deﬁned with respect to the covariance kernel for β. For our particular choice of exponentiated-quadratic kernel, the maximum information gain scales as γβ ˜N ≤O(log ˜Nd+1) [40]. Compared with [5], our proof tightens the upper bound on γT/w from O( ˜N3) to O( ˜N5/2). Combining these terms, we can then write the bound as RT/w ≲ O( √ ([ log ˜Nd+1 + σ−2 f ˜N5/2ω ] T/w ) , which is sublinear in T when the function f be- comes time-invariant, i.e., ω →0. In contrast, the sublinear guarantee does not hold when the time-varying function is non-correlated, i.e., ω = 1 , in which case the time covariance matrix becomes identity matrix. The bound is tighter for lower schedule dimension d. 5 Experiments We demonstrate the effectiveness of our method for training VAEs [18] on MNIST and Fashion MNIST, and a Sigmoid Belief Network [28] on binarized MNIST and binarized Omniglot, using the TVO objective. In Appendix D, we explore learning and inference in a discrete probabilistic context- free grammar [24], showing that theTVO objective and our bandit optimization can translate to other 6learning settings. In addition, we run ablation studies using random choices of βand a GP without permutation invariance, and compare the runtime and performance of our method with grid search. Our code is available at http://github.com/ntienvu/tvo_gp_bandit. 0 2000 4000 6000 8000 10000 Epochs 0.0 0.2 0.4 0.6 0.8 1.0 Selecting t by GP Bandit 1 <latexit sha1_base64=\"f/ucaHSqP+D9Bp31BanY7u7K7A0=\">AAACS3icbVDLTgJBEJxFUcQX6NHLRmLiiewqRo9ELx4hkUcCGzI7NDgyu7OZ6VXIhi/wqp/lB/gd3owHh8fBBSvppFLVne4uPxJco+N8WpmNzezWdm4nv7u3f3BYKB41tYwVgwaTQqq2TzUIHkIDOQpoRwpo4Ato+aO7md96BqW5DB9wEoEX0GHIB5xRNFLd7RVKTtmZw14n7pKUyBK1XtGqdPuSxQGEyATVuuM6EXoJVciZgGm+G2uIKBvRIXQMDWkA2kvml07tM6P07YFUpkK05+rfiYQGWk8C33QGFB/1qjcT//M6MQ5uvISHUYwQssWiQSxslPbsbbvPFTAUE0MoU9zcarNHqihDE05qSxAL5Eq+TNMqHQEDIdKqL+UIqa/TX8M4kmoWSf8p1ujL8TSfNzm7q6muk+ZF2b0sX9UrpertMvEcOSGn5Jy45JpUyT2pkQZhBMgreSPv1of1ZX1bP4vWjLWcOSYpZLK/K6y0Og==</latexit> 0 <latexit sha1_base64=\"+bcvVsgReT4e+AN7PozBeVeZPe8=\">AAACS3icbVDLTgJBEJxFUcQX6NHLRmLiiewqRo9ELx4hkUcCGzI7NDgyu7OZ6VXIhi/wqp/lB/gd3owHh8fBBSvppFLVne4uPxJco+N8WpmNzezWdm4nv7u3f3BYKB41tYwVgwaTQqq2TzUIHkIDOQpoRwpo4Ato+aO7md96BqW5DB9wEoEX0GHIB5xRNFLd6RVKTtmZw14n7pKUyBK1XtGqdPuSxQGEyATVuuM6EXoJVciZgGm+G2uIKBvRIXQMDWkA2kvml07tM6P07YFUpkK05+rfiYQGWk8C33QGFB/1qjcT//M6MQ5uvISHUYwQssWiQSxslPbsbbvPFTAUE0MoU9zcarNHqihDE05qSxAL5Eq+TNMqHQEDIdKqL+UIqa/TX8M4kmoWSf8p1ujL8TSfNzm7q6muk+ZF2b0sX9UrpertMvEcOSGn5Jy45JpUyT2pkQZhBMgreSPv1of1ZX1bP4vWjLWcOSYpZLK/KcW0OQ==</latexit> 1 <latexit sha1_base64=\"f/ucaHSqP+D9Bp31BanY7u7K7A0=\">AAACS3icbVDLTgJBEJxFUcQX6NHLRmLiiewqRo9ELx4hkUcCGzI7NDgyu7OZ6VXIhi/wqp/lB/gd3owHh8fBBSvppFLVne4uPxJco+N8WpmNzezWdm4nv7u3f3BYKB41tYwVgwaTQqq2TzUIHkIDOQpoRwpo4Ato+aO7md96BqW5DB9wEoEX0GHIB5xRNFLd7RVKTtmZw14n7pKUyBK1XtGqdPuSxQGEyATVuuM6EXoJVciZgGm+G2uIKBvRIXQMDWkA2kvml07tM6P07YFUpkK05+rfiYQGWk8C33QGFB/1qjcT//M6MQ5uvISHUYwQssWiQSxslPbsbbvPFTAUE0MoU9zcarNHqihDE05qSxAL5Eq+TNMqHQEDIdKqL+UIqa/TX8M4kmoWSf8p1ujL8TSfNzm7q6muk+ZF2b0sX9UrpertMvEcOSGn5Jy45JpUyT2pkQZhBMgreSPv1of1ZX1bP4vWjLWcOSYpZLK/K6y0Og==</latexit> 0 <latexit sha1_base64=\"+bcvVsgReT4e+AN7PozBeVeZPe8=\">AAACS3icbVDLTgJBEJxFUcQX6NHLRmLiiewqRo9ELx4hkUcCGzI7NDgyu7OZ6VXIhi/wqp/lB/gd3owHh8fBBSvppFLVne4uPxJco+N8WpmNzezWdm4nv7u3f3BYKB41tYwVgwaTQqq2TzUIHkIDOQpoRwpo4Ato+aO7md96BqW5DB9wEoEX0GHIB5xRNFLd6RVKTtmZw14n7pKUyBK1XtGqdPuSxQGEyATVuuM6EXoJVciZgGm+G2uIKBvRIXQMDWkA2kvml07tM6P07YFUpkK05+rfiYQGWk8C33QGFB/1qjcT//M6MQ5uvISHUYwQssWiQSxslPbsbbvPFTAUE0MoU9zcarNHqihDE05qSxAL5Eq+TNMqHQEDIdKqL+UIqa/TX8M4kmoWSf8p1ujL8TSfNzm7q6muk+ZF2b0sX9UrpertMvEcOSGn5Jy45JpUyT2pkQZhBMgreSPv1of1ZX1bP4vWjLWcOSYpZLK/KcW0OQ==</latexit> E ⇡ \u0000 [ · ] <latexit sha1_base64=\"tTJPeHdmduE7mFmZ2r5wwU1BpfQ=\">AAAHb3ichVXbbhMxEDW3ptxbeOABCVZUlUoVVU0vahEvFRcBAsRF9CKaEHm9sxurXq/jddqkZh/5Gl7hY/gM/oDxpqFJnIKlbGbnnJk5Hq/tUAmem+XlX+fOX7h4aaoyffnK1WvXb9ycmb21k2cdzWCbZSLTeyHNQXAJ24YbAXtKA01DAbvhwVOH7x6CznkmP5megkZKE8ljzqhBV3Pmfj2lphWG9nnRtHXF8RGCoUWxX2dRZhrNmbnlpeVyBL5ROzHmtqZJOd43Z6c261HGOilIwwTN8/2VNWWqIBOcTathqTacCSiu1Ds5KMoOaAL7aEqaQt6w5YyKYB49URBnGn/SBKV3OMLSNM97aYhMpz8fx5zzLzY/UsrEmw3LpeoYkKxfKe6IwGSB608QcQ3MiB4alGmOYgPWopoyg10cEf2p1rBOnUszAgjsrawNFRlRF9IQRDEiyqquS5QjMYIYV7Kcr+3krKkhKuzHF08KW1vfqK6uVddXkSXhiGVpSmXklqtwj4RLC21Jtaa9IjhxUMETiRnGQsCFAFoDPCjfTsO9ElJ6RRb/X6WMOrvOolfosFtY2/8i46A7nvDwaAg98tDeENrz0OMh9NhD4yE0HkcPBKLPmvb1m2IcioAhpnDXmJbbNsECzuErVnvodUM6ZtttshYvecfI63q8BKSfsRpMSilE5FM9lnIzVwuTEqjuSPzE8LarYdsDcL6cQ0TPpB8PzXJiUd5GxqumS7n3+LPfp/bfpegfSm2v6YfJ0GolXleoQ0OeBGjiqQPjBH1K0BMJZTdUSaI6OGnO+JaIQeWnMlAU1ejhIpOeXhBCaZ6Co6P9BZtTvnrLwRXo1LEUn4ANUih+ZgaZcTw38dYYKGNUYBMX+nK9XkcpdV9A1HTkrpeN8yjnpZ7MXSFgLHpwRzun4z4DPOA1vMU671A5NZletHWqkzIt/terzvoXkcsBES28a2rjN4tv7Kws1daWHn1Ym9t60r90yDS5Sx6QBVIjG2SLvCTvyTZh5Bv5Tn6Qn1O/K3cq9ypBn3r+3EnMbTIyKg//ALs2wYo=</latexit> \u0000 j <latexit sha1_base64=\"6rw73jSUGkY1BBMam5J8+ICCs6I=\">AAAHVXichVXbbhMxEHVLSUu5lMsjLyuqSqWKqqYtooiXqhQBAsRFvYkmRF7v7MbU63W8TpvU7EfwCh+G+BgkxtuGJnEKKyWZnXNm5ngmtkMleG5WVn5NTF6ZulqZnrk2e/3GzVtzt+/c3cuzjmawyzKR6YOQ5iC4hF3DjYADpYGmoYD98OiZw/ePQec8kzump6CR0kTymDNq0LVfD8HQ5pfm7fmV5ZXyCXyjdm7Mb86Q8nnfvFPZqEcZ66QgDRM0zw9X15WpgkxQc6thqTacCShm650cFGVHNIFDNCVNIW/YUncRLKAnCuJM40eaoPQORlia5nkvDZGZUtPKRzHn/IstDJUy8UbDcqk6BiQ7qxR3RGCywHUhiLgGZkQPDco0R7EBa1FNmcFeDYneqTWsU+fSDAECOyhrA0WG1IU0BFEMibKq6xLlSIwgxnmV67WdnDU1RIX9+GKrsLVHj6tr69VHa8iScMKyNKUysjinwn0lXFpoS6o17RXBuYMKnkjMMBICLgTQ6uNB+XYR7pWQ0iuy9P8qZdTldZa8Qsfdwtq6m1oYB93RhMcnA+iJh/YG0J6Hng6gpx4aD6DxKHokEN1u2tdvilEoAoaYatq6aeF+KYJFXMNXrPbQ64Z0zDYyVYuXvFPkdT1eAtLPWA3GpRQi8qkeS7mVq8VxCVR3KH5seNvVsO0+uFCuIaKX0k8HVjm2KG8j41XTpTx4+snvU/vvKEL7vHA8b1rJwLQSryvUoSFPAjTx1IFRgr4g6LGEshuqJFEdnDdndEvEoPILGSiKavRwkUlPLwihNE/B0dH+jM0pX71xcAU6dSzFx2D9FIpfmkFmHM9NvBv6yhgV2MTFM7ler6OUun9A1HTkrpeN8yjnpZ7MXRRgLHpwRzun424DHvAa3mKdd6icmkwv2TrVSZkWf+tVZ/2LyGWfiBbeNbXRm8U39laXa+vLTz6sz29unV06ZIbcJw/IIqmRx2STvCTvyS5h5Ih8I9/Jj8rPyu/pqenKGXVy4jzmHhl6puf+AMIMt4k=</latexit> \u0000 j <latexit sha1_base64=\"6rw73jSUGkY1BBMam5J8+ICCs6I=\">AAAHVXichVXbbhMxEHVLSUu5lMsjLyuqSqWKqqYtooiXqhQBAsRFvYkmRF7v7MbU63W8TpvU7EfwCh+G+BgkxtuGJnEKKyWZnXNm5ngmtkMleG5WVn5NTF6ZulqZnrk2e/3GzVtzt+/c3cuzjmawyzKR6YOQ5iC4hF3DjYADpYGmoYD98OiZw/ePQec8kzump6CR0kTymDNq0LVfD8HQ5pfm7fmV5ZXyCXyjdm7Mb86Q8nnfvFPZqEcZ66QgDRM0zw9X15WpgkxQc6thqTacCShm650cFGVHNIFDNCVNIW/YUncRLKAnCuJM40eaoPQORlia5nkvDZGZUtPKRzHn/IstDJUy8UbDcqk6BiQ7qxR3RGCywHUhiLgGZkQPDco0R7EBa1FNmcFeDYneqTWsU+fSDAECOyhrA0WG1IU0BFEMibKq6xLlSIwgxnmV67WdnDU1RIX9+GKrsLVHj6tr69VHa8iScMKyNKUysjinwn0lXFpoS6o17RXBuYMKnkjMMBICLgTQ6uNB+XYR7pWQ0iuy9P8qZdTldZa8Qsfdwtq6m1oYB93RhMcnA+iJh/YG0J6Hng6gpx4aD6DxKHokEN1u2tdvilEoAoaYatq6aeF+KYJFXMNXrPbQ64Z0zDYyVYuXvFPkdT1eAtLPWA3GpRQi8qkeS7mVq8VxCVR3KH5seNvVsO0+uFCuIaKX0k8HVjm2KG8j41XTpTx4+snvU/vvKEL7vHA8b1rJwLQSryvUoSFPAjTx1IFRgr4g6LGEshuqJFEdnDdndEvEoPILGSiKavRwkUlPLwihNE/B0dH+jM0pX71xcAU6dSzFx2D9FIpfmkFmHM9NvBv6yhgV2MTFM7ler6OUun9A1HTkrpeN8yjnpZ7MXRRgLHpwRzun424DHvAa3mKdd6icmkwv2TrVSZkWf+tVZ/2LyGWfiBbeNbXRm8U39laXa+vLTz6sz29unV06ZIbcJw/IIqmRx2STvCTvyS5h5Ih8I9/Jj8rPyu/pqenKGXVy4jzmHhl6puf+AMIMt4k=</latexit> Figure 3: Bandit-chosen βover time on MNIST using d = 5 . We can interpret the β selection process in 3 phases: (blue) random selection in initial epochs; (orange) focusing on small values βi < 0.5 as training progresses; (green) moving toward βi = 1 as learning approaches conver- gence. The bottom panel illustrates a hypotheti- cal integrand curve and βselections at interme- diate (left) and later (right) epochs. Experimental Setup: We evaluate our GP- bandit for S ∈{10,50}and d ∈{2,5,10,15} and, for each conﬁguration, train until conver- gence using 5 random seeds. Note that, for each setting of d, we implicitly include β0 = 0 and append 1 to the vector βto perform the integra- tion in Eq. (5). For each S,d conﬁguration, we compare against three baseline integration schedules: log- uniform spacing in the interval [β1,1], linear- uniform spacing in the interval [0,1], and the moments schedule of [6, 12], which corre- sponds to uniform spacing along the y-axis. For log/linear-uniform spacing, we set β1 = 0 .025 for all experiments, reﬂecting the results of grid search in [27]. We use a ﬁxed model architec- ture for all experiments, which we describe in Appendix A. To obtain the bandit feedback in Eq. (7), we use a ﬁxed, linear schedule with d = 50 for calculating Lt with Eq. (5). This yields a tighter log pθ(x) bound, decouples reward func- tion evaluation from model training and sched- ule selection in each round, and is still efﬁcient using SNIS in Eq. (4). We limit the value of d for TVO training following observations of dete- riorating performance in [27]. GP Implementation: For GP modeling, we use an exponentiated quadratic covariance function for kβ and estimate hyperparameters via type II maximum likelihood estimation [34]. We use multi-start BFGS [9] to optimize the acquisition function in Eq. (17). We set the update frequency w = 6 initially and increment w by one after every 10 bandit iterations to account for smaller objective changes later in training, and update early if Lt ≤−0.05. We found that selecting βj too close to either 0 or 1 could negatively affect per- formance, and thus restrict β ∈[0.05,0.95]d in all experiments. We follow a common practice to standardize with the running average the utility score y∼N(0,1) for robustness. 5.1 Scheduling Behaviour We ﬁrst investigate the behaviour of our time-varying reward function and bandit scheduling. These experiments highlight the adaptive nature of our algorithm, as we inspect the choice of integration schedule across training epochs for both d= 2 and d= 5. Time-varying Reward Function: In Figure 2, we visualize the mean and variance of our time- varying estimate of the utility function yt = ft(βt) + ϵt after 2000, 3000, and 10,000 epochs, respectively. We illustrate the choice of βfor d = 2, so that β0 = 0 is ﬁxed and we can write the reward as ft(β1). Colored dots indicate values of β1 selected by our bandit algorithm in each round, with the vertical axis reﬂecting the observed reward ft(β1) as the change in model evidence L. In the ﬁrst two panels, we observe instances where our bandit prioritizes exploitation, choosing similar, high-reward β1 values in neighboring rounds with the same color. However, note that these β1 may not match the highest GP predictive mean for ft(β), since the blue line is shown at the ﬁnal training epoch in a window. In the ﬁnal panel, we observe that our time-varying reward function has adapted to have very low variance, since the TVO objective changes only slightly near convergence and the choice of β1 has little impact. 72 5 10 15 Number of Partitions 92 91 90 89 88 87 test logp (x) S=10 gp_bandit (best: -88.51±0.06) log (best: -88.58±0.10) linear (best: -89.11±0.11) moments (best: -88.65±0.09) 2 5 10 15 Number of Partitions S=50 gp_bandit (best: -87.49±0.11) log (best: -87.52±0.06) linear (best: -88.43±0.04) moments (best: -87.51±0.09) MNIST 2 5 10 15 Number of Partitions 235 234 233 232 test logp (x) S=10 gp_bandit (best: -232.34±0.12) log (best: -232.40±0.18) linear (best: -232.74±0.16) moments (best: -232.52±0.23) 2 5 10 15 Number of Partitions S=50 gp_bandit (best: -231.60±0.07) log (best: -231.64±0.08) linear (best: -232.07±0.09) moments (best: -231.60±0.05) Fashion MNIST 2 5 10 15 Number of Partitions 3 4 5 6 7 8 9 10 KL[q ||p ] 4.9 6.2 6.8 6.9 4.1 6.5 7.1 7.4 5.2 6.4 7.1 7.6 S: 10 gp_bandit log moments 2 5 10 15 Number of Partitions KL[q ||p ] 5.0 7.1 8.4 8.8 3.8 7.8 8.7 9.0 5.6 7.9 8.9 9.0 S: 50 gp_bandit log moments MNIST 2 5 10 15 Number of Partitions 1 2 3 4 5 6 7 KL[q ||p ] 2.7 3.5 4.2 4.5 2.0 4.0 4.6 5.0 2.8 3.9 4.6 5.0 S: 10 gp_bandit log moments 2 5 10 15 Number of Partitions KL[q ||p ] 2.7 3.7 5.4 6.0 1.9 4.7 5.9 6.2 2.9 4.7 5.5 6.0 S: 50 gp_bandit log moments Fashion MNIST Figure 4: Performance comparison for continuous V AE on MNIST and Fashion MNIST. Top: we compare model learning performance using test likelihood (higher is better). Bottom: We compare posterior inference as measured by the test KL divergence (lower is better) against the log and moments baselines. Although, in general, we ﬁnd that models with worse log p(x) tend to have lower DKL, our GP-bandit schedule provides improvements in both learning and inference. 2 5 10 15 Number of Partitions 124 122 120 118 test logp (x) S=10 gp_bandit (best: -118.88±0.41) log (best: -118.94±0.39) linear (best: -119.82±0.56) moments (best: -118.85±0.33) 2 5 10 15 Number of Partitions S=50 gp_bandit (best: -117.80±0.27) log (best: -117.76±0.15) linear (best: -119.41±0.56) moments (best: -117.64±0.12) Binarized MNIST 2 5 10 15 Number of Partitions 2 4 6 8 10 12 KL[q ||p ] 4.2 5.5 6.0 6.1 3.5 6.0 6.3 6.2 4.8 5.9 6.2 6.2 S: 10 gp_bandit log moments 2 5 10 15 Number of Partitions KL[q ||p ] 5.2 7.6 9.7 9.4 3.5 9.0 10.0 9.5 5.6 10.2 11.5 11.9 S: 50 gp_bandit log moments Binarized MNIST Figure 5: Performance comparison in discrete latent variable model using a Sigmoid Belief Network on the binarized MNIST dataset. Our GP-bandit achieves comparable results to the log and moments schedule in terms model learning (higher log p(x)), with better posterior inference (lower DKL). βAcross Training: In Figure 3, we visualize bandit choices of βwith d = 5. In initial epochs (blue), the GP-bandit algorithm prioritizes exploration before focusing on βj < 0.5 in the second phase (orange). As the VAE converges, our algorithm begins to exploreβj further from zero (green). Beyond avoiding the need for an expensive grid search, a primary motivation for our bandit approach is a lack of knowledge about the shape of the integrand. Using the intuition thatβj choices should be concentrated in regions where the integrand is changing quickly in order to obtain accurate Riemann approximations, we can still translate the observed bandit choices of βinto example integrands in the middle (orange) and late (green) stages of training in the bottom panel of Fig. 3. An integrand that rises steeply away from β = 0 indicates that qφ(z |x) is mismatched to pθ(z |x), and the TVO might be improved by choosing small βj. As the curve begins to smooth later in training, with a higher proportion of importance samples yielding high likelihood under the generative model, our bandit begins to explore βj closer to 1. 5.2 Model Learning and Inference Continuous V AE: We present results of training a continuous VAE on the MNIST and Fashion MNIST dataset in Figure 4. We measure model learning performance using the test log evidence, as estimated by the IWAE bound [7] with 5000 samples per data point. We also compare inference performance using DKL[qφ(z |x) ||pθ(z |x)], which we calculate by subtracting the test ELBO from our estimate of log pθ(x). For most scenarios in Figure 4, our GP bandit optimization outperforms baselines with respect to both model learning and inference. In general, we observe that models with lower model evidence attain lower test KL divergence. Thus, in comparing inference performance in the bottom panel of Figure 4, we compare against the log and moment schedules, baselines with comparable test log 8likelihoods. It is notable that our approach often achieves better results for both learning (higher log pθ(x)) and inference (lower DKL). We obtain the highest log evidence with d= 10 for MNIST and d= 15 for Fashion MNIST. Sigmoid Belief Network: We present similar results for learning discrete latent variable mod- els using a Sigmoid Belief Network [28]. We show results on binarized MNIST in Figure 5, with binarized Omniglot in Figure 7 in Appendix D.2. Our GP bandit optimization achieves competi- tive model learning performance with the log-uniform and moment schedules and better posterior inference across models with comparable log p(x), indicating our GP-bandit schedule can ﬂexibly optimize the TVO for various model types. 6 Conclusion We have presented a new approach for automated selection of the integration schedule for the Ther- modynamic Variational Objective. Our bandit framework optimizes a reward function that is directly linked to improvements in the generative model evidence over the course of training the model pa- rameters. We show theoretically that this procedure asymptotically minimizes the regret as a func- tion of the choice of schedule. Finally, we demonstrated that the proposed approach empirically outperforms existing schedules in both model learning and inference for discrete and continuous generative models. Our GP bandit optimization offers a general solution to choosing the integration schedule in the TVO. However, our algorithm, as well as all other existing schedules, still rely on the number of partitions d as a hyperparameter which is ﬁxed over the course of the training. Incorporating the adaptive selection of dinto our bandit optimization remains an interesting direction for future work. 7 Broader Impact Our research can be widely applied for variational inference in deep generative models, including variational autoencoders with autoregressive decoders and normalizing ﬂows. Variational inference, and Bayesian methods more generally, have broad applications spanning science and engineering, from epidemiology [45] to particle physics [2]. Our methodological contributions for variational in- ference may ﬁnd broader impact through improved modelling in these disparate domains. However, our method is general in nature, so domain-speciﬁc applications should further consider implications for deployment in the real-world. 8 Acknowledgements VM acknowledges the support of the Natural Sciences and Engineering Research Council of Canada (NSERC) under award number PGSD3-535575-2019 and the British Columbia Graduate Scholar- ship, award number 6768. VM/FW acknowledge the support of the Natural Sciences and Engi- neering Research Council of Canada (NSERC), the Canada CIFAR AI Chairs Program, and the Intel Parallel Computing Centers program. RB acknowledges support from the Defense Advanced Research Projects Agency (DARPA) under award FA8750-17-C-0106. This material is based upon work supported by the United States Air Force Research Laboratory (AFRL) under the Defense Advanced Research Projects Agency (DARPA) Data Driven Discovery Models (D3M) program (Contract No. FA8750-19-2-0222) and Learning with Less Labels (LwLL) program (Contract No.FA875019C0515). Additional support was provided by UBC’s Composites Research Network (CRN), Data Science Institute (DSI) and Support for Teams to Advance Interdis- ciplinary Research (STAIR) Grants. This research was enabled in part by technical support and com- putational resources provided by WestGrid (https://www.westgrid.ca/) and Compute Canada (www.computecanada.ca). References [1] Peter Auer, Nicolo Cesa-Bianchi, Yoav Freund, and Robert E Schapire. The nonstochastic multiarmed bandit problem. SIAM journal on computing, 32(1):48–77, 2002. 9[2] Atilim Gunes Baydin, Lei Shao, Wahid Bhimji, Lukas Heinrich, Saeid Naderiparizi, Andreas Munk, Jialin Liu, Bradley Gram-Hansen, Gilles Louppe, Lawrence Meadows, et al. Efﬁcient probabilistic inference in the quest for physics beyond the standard model. In Advances in Neural Information Processing Systems, pages 5460–5473, 2019. [3] Christopher M Bishop. Pattern recognition and machine learning. springer New York, 2006. [4] David M Blei, Alp Kucukelbir, and Jon D McAuliffe. Variational inference: A review for statisticians. Journal of the American statistical Association, 112(518):859–877, 2017. [5] Ilija Bogunovic, Jonathan Scarlett, and V olkan Cevher. Time-varying Gaussian process bandit optimization. In Artiﬁcial Intelligence and Statistics, pages 314–323, 2016. [6] Rob Brekelmans, Vaden Masrani, Frank Wood, Greg Ver Steeg, and Aram Galstyan. All in the exponential family: Bregman duality in thermodynamic variational inference. In International Conference on Machine Learning, 2020. [7] Yuri Burda, Roger Grosse, and Ruslan Salakhutdinov. Importance weighted autoencoders. In International Conference on Representation Learning, 2016. [8] Chris Cremer, Xuechen Li, and David Duvenaud. Inference suboptimality in variational au- toencoders. In International Conference on Machine Learning, pages 1078–1086, 2018. [9] Roger Fletcher. Practical methods of optimization. John Wiley & Sons, 2013. [10] Daan Frenkel and Berend Smit. Understanding molecular simulation: from algorithms to applications, volume 1. Elsevier, 2001. [11] Andrew Gelman and Xiao-Li Meng. Simulating normalizing constants: From importance sampling to bridge sampling to path sampling. Statistical science, pages 163–185, 1998. [12] Roger B Grosse, Chris J Maddison, and Russ R Salakhutdinov. Annealing between distribu- tions by averaging moments. In Advances in Neural Information Processing Systems , pages 2769–2777, 2013. [13] Junxian He, Daniel Spokoyny, Graham Neubig, and Taylor Berg-Kirkpatrick. Lagging infer- ence networks and posterior collapse in variational autoencoders. In International Conference on Representation Learning, 2019. [14] Philipp Hennig and Christian J Schuler. Entropy search for information-efﬁcient global opti- mization. Journal of Machine Learning Research, 13:1809–1837, 2012. [15] José Miguel Hernández-Lobato, Matthew W Hoffman, and Zoubin Ghahramani. Predictive entropy search for efﬁcient global optimization of black-box functions. In Advances in Neural Information Processing Systems, pages 918–926, 2014. [16] Donald R Jones, Matthias Schonlau, and William J Welch. Efﬁcient global optimization of expensive black-box functions. Journal of Global optimization, 13(4):455–492, 1998. [17] Diederik P Kingma and Jimmy Ba. Adam: A method for stochastic optimization. International Conference on Learning Representations, 2014. [18] Diederik P. Kingma and Max Welling. Auto-encoding variational bayes. In International Conference on Learning Representations, 2014. [19] Durk P Kingma, Tim Salimans, Rafal Jozefowicz, Xi Chen, Ilya Sutskever, and Max Welling. Improved variational inference with inverse autoregressive ﬂow. In Advances in Neural Infor- mation Processing Systems, pages 4743–4751, 2016. [20] Aaron Klein, Stefan Falkner, Simon Bartels, Philipp Hennig, and Frank Hutter. Fast Bayesian optimization of machine learning hyperparameters on large datasets. In Artiﬁcial Intelligence and Statistics, pages 528–536, 2017. [21] Andreas Krause and Cheng S Ong. Contextual Gaussian process bandit optimization. In Advances in Neural Information Processing Systems, pages 2447–2455, 2011. 10[22] Brenden M Lake, Russ R Salakhutdinov, and Josh Tenenbaum. One-shot learning by inverting a compositional causal process. In Advances in Neural Information Processing Systems, pages 2526–2534, 2013. [23] Brenden M Lake, Ruslan Salakhutdinov, and Joshua B Tenenbaum. Human-level concept learning through probabilistic program induction. Science, 350(6266):1332–1338, 2015. [24] Tuan Anh Le, Adam R Kosiorek, N Siddharth, Yee Whye Teh, and Frank Wood. Revisiting reweighted wake-sleep for models with stochastic control ﬂow. In Uncertainty in Artiﬁcial Intelligence, pages 1039–1049. PMLR, 2020. [25] Yann LeCun, Léon Bottou, Yoshua Bengio, and Patrick Haffner. Gradient-based learning applied to document recognition. Proceedings of the IEEE, 86(11):2278–2324, 1998. [26] Yucen Luo, Alex Beatson, Mohammad Norouzi, Jun Zhu, David Duvenaud, Ryan P Adams, and Ricky TQ Chen. Sumo: Unbiased estimation of log marginal probability for latent variable models. arXiv preprint arXiv:2004.00353, 2020. [27] Vaden Masrani, Tuan Anh Le, and Frank Wood. The thermodynamic variational objective. In Advances in Neural Information Processing Systems, pages 11521–11530, 2019. [28] Andriy Mnih and Karol Gregor. Neural variational inference and learning in belief networks. In International Conference on Machine Learning, pages 1791–1799, 2014. [29] Andriy Mnih and Danilo J Rezende. Variational inference for monte carlo objectives. In International Conference on Machine Learning, pages 2188–2196, 2016. [30] Vu Nguyen, Sebastian Schulze, and Michael A Osborne. Bayesian optimization for iterative learning. In Advances in Neural Information Processing Systems, 2020. [31] Sebastian Nowozin. Debiasing evidence approximations: On importance-weighted autoen- coders and jackknife variational inference. International Conference on Learning Representa- tions, 2018. [32] Favour Mandanji Nyikosa. Adaptive Bayesian optimization for dynamic problems. PhD thesis, University of Oxford, 2018. [33] Yosihiko Ogata. A monte carlo method for high dimensional integration. Numerische Mathe- matik, 55(2):137–157, 1989. [34] Carl Edward Rasmussen. Gaussian processes for machine learning. 2006. [35] Danilo Rezende and Shakir Mohamed. Variational inference with normalizing ﬂows. In Inter- national Conference on Machine Learning, pages 1530–1538, 2015. [36] Danilo Jimenez Rezende, Shakir Mohamed, and Daan Wierstra. Stochastic backpropagation and approximate inference in deep generative models. InInternational Conference on Machine Learning, pages 1278–1286, 2014. [37] Ruslan Salakhutdinov and Iain Murray. On the quantitative analysis of deep belief networks. In International Conference on Machine Learning, pages 872–879, 2008. [38] Jasper Snoek, Kevin Swersky, Rich Zemel, and Ryan Adams. Input warping for Bayesian optimization of non-stationary functions. In International Conference on Machine Learning , pages 1674–1682, 2014. [39] Casper Kaae Sønderby, Tapani Raiko, Lars Maaløe, Søren Kaae Sønderby, and Ole Winther. Ladder variational autoencoders. In Advances in Neural Information Processing Systems 29 , pages 3738–3746. 2016. [40] Niranjan Srinivas, Andreas Krause, Sham Kakade, and Matthias Seeger. Gaussian process optimization in the bandit setting: No regret and experimental design. In International Con- ference on Machine Learning, pages 1015–1022, 2010. 11[41] Kevin Swersky, Jasper Snoek, and Ryan P Adams. Multi-task Bayesian optimization. In Advances in Neural Information Processing Systems, pages 2004–2012, 2013. [42] Michael Tschannen, Olivier Bachem, and Mario Lucic. Recent advances in autoencoder-based representation learning. arXiv preprint arXiv:1812.05069, 2018. [43] Mark van der Wilk, Matthias Bauer, ST John, and James Hensman. Learning invariances using the marginal likelihood. In Advances in Neural Information Processing Systems, pages 9938–9948, 2018. [44] Martin J Wainwright and Michael I Jordan. Graphical models, exponential families, and vari- ational inference. Foundations and Trends® in Machine Learning, 1(1-2):1–305, 2008. [45] Frank Wood, Andrew Warrington, Saeid Naderiparizi, Christian Weilbach, Vaden Masrani, William Harvey, Adam Scibior, Boyan Beronov, and Ali Nasseri. Planning as inference in epidemiological models. arXiv preprint arXiv:2003.13221, 2020. [46] Han Xiao, Kashif Rasul, and Roland V ollgraf. Fashion-mnist: a novel image dataset for bench- marking machine learning algorithms. arXiv preprint arXiv:1708.07747, 2017. 12A Experimental Setup Dataset Description The discrete and continuous V AE literature use slightly different training procedures. For continuous V AEs, we follow the sampling procedure described in footnote 2, page 6 of Burda et al. [7], and sample binary-valued pixels with expectation equal to the original gray scale 28×28 image. We split MNIST [25] and Fashion MNIST [46] into 60k training examples and 10k testing examples across 10 classes. For Sigmoid Belief Networks, we follow the procedure described by Mnih and Rezende [29] and use 50k training examples and 10k testing examples (the remaining 10k validation examples are not used) with the binarized MNIST [37] dataset. Training Procedure All models are written in PyTorch and trained on GPUs. For each sched- uler, we train for 10,000 epochs using the Adam optimizer [17] with a learning rate of 10−3, and minibatch size of 1000. All weights are initialized with PyTorch’s default initializer. For the neural network architecture, we use two hidden layers of [100,25] nodes. Reward Evaluation To obtain the bandit feedback in Eq. (7), we use a ﬁxed, linear schedule with d = 50 for calculating Lt with Eq. (5). This yields a tighter log pθ(x) bound, decouples reward function evaluation from model training and schedule selection in each round, and is still efﬁcient using SNIS in Eq. (4). We limit the value of dfor TVO training following observations of deteriorating performance in [27]. B GP kernels and treatment of GP hyperparameters We present the GP kernels and treatment of GP hyperparameters for the black-box function f. We use the exponentiated quadratic (or squared exponential) covariance function for input hyper- parameter kβ(β,β′) = exp ( −||β−β′||2 2σ2 β ) and a time kernel kT(t,t′) = (1 −ω) |t−t′| 2 where the observation βand tare normalized to [0,1]d and the outcome y is standardized y ∼N (0,1) for robustness. As a result, our product kernel becomes k([β,t],[β′,t′]) = k(β,β′) ×k(t,t′) = exp ( −||β−β′||2 2σ2 β ) (1 −ω) |t−t′| 2 . The length-scales σβ is estimated from the data indicating the variability of the function with regards to the hyperparameter input x and number of training iterations t. Estimating appropriate values for them is critical as this represents the GP’s prior regarding the sensitivity of performance w.r.t. changes in the number of training iterations and hyperparameters. We note that previous works have also utilized the above product of spatial and temporal covariance functions for different settings [21, 5, 30]. We ﬁt the GP hyperparameters by maximizing their posterior probability (MAP), p(σl,ω |β,t,y) ∝ p(σl,ω, β,t,y), which, thanks to the Gaussian likelihood, is available in closed form as [34] ln p(y,β,t,σl,ω) =1 2yT ( K+ σ2IN )−1 y −1 2 ln ⏐⏐K+ σ2IN ⏐⏐+ lnphyp (σx,ω) + const (19) where IN is the identity matrix in dimension N (the number of points in the training set), and phyp(σl,ω) is the prior over hyperparameters, described in the following. We maximize the marginal likelihood in Eq. (19) to select the suitable lengthscale parameter σl, remembering-forgetting trade-off ω, and noise variance σ2 f. Optimizing Eq. (19) involves taking the derivative w.r.t. each variable, such as ∂ln p(y,β,t,σl,ω) ∂ω = ∂ln p(y,β,t,σl,ω) ∂K × ∂K ∂k(t,t′) ×∂k(t,t′) ∂ω . While the derivatives of σl and σ2 f are standard and can be found in [34], we present the derivative w.r.t.ωas follows ∂k(t,t′) ∂ω = −v(1 −ω)v−1 where v= |t−t′|/2. (20) 13We optimize Eq. (19) with a gradient-based optimizer, providing the analytical gradient to the algo- rithm. We start the optimization from the previous hyperparameter values θprev. If the optimization fails due to numerical issues, we keep the previous value of the hyperparameters. C Proof of Theorem 1 Our use of the TVGP within the TVO setting requires no problem speciﬁc modiﬁcations compared to the general formulation in Bogunovic. As such, the proof of Theorem 1 closely follows the proof of Theorem 4.3 in Bogunovic et al. [5] App. C. with time kernel kT (i,j) = (1 −ω) |i−j| 2 . At a high level, their proof proceeds by partitioning theT random functions into blocks of length ˜N, and bounding each using Mirsky’s theorem. Referring to Table 1 for notation, this results in a bound on the maximum mutual information ˜γ˜N ≤ (T ˜N + 1 )( γ˜N + ˜N3ω ) , (21) which leads directly to their bound on the cumulative regret (cf. App C.2 in [5]). Our contribution is to recognize we can achieve a tighter bound on the maximum mutual information with an application Cauchy Schwarz and Jensen’s inequality. Proof. Beginning from Bogunovic et al. [5] Eq. (58), we have ˜γ˜N ≤γ˜N + ˜N∑ i=1 log (1 + ∆i) (22) ˜γ˜N ≤γ˜N + ˜Nlog  1 + 1 ˜N ˜N∑ i=1 ∆i   Jensen’s inequality (23) ˜γ˜N ≤γ˜N + ˜Nlog  1 + 1√ ˜N √ ˜N∑ i=1 ∆2 i   Cauchy-Schwartz (24) ˜γ˜N ≤γ˜N + ˜Nlog ( 1 + ˜N3/2ω ) ˜N∑ i=1 ∆2 i ≤ ˜N4ω2 (25) ˜γ˜N ≤γ˜N + ˜N5/2ω log(1 + x) ≤x (26) This bound is tighter than [5] Eq. (60) ( ˜N5/2 ≤ ˜N3), where the latter was achieved via a simple constrained optimization argument. Using (26), Theorem 1 follows using identical arguments as in [5]. D Additional Experiments and Ablation Studies We present additional experiments on a Probabilistic Context Free Grammar (PCFG) model and Sigmoid Belief Networks in §D.1 and §D.2, a wall-clock time benchmark in §D.3, ablation studies in §D.4, and additional training curves in §D.5. D.1 Training Probabilistic Context Free Grammar In order to evaluate our method outside of the Variational Autoencoder framework, we consider model learning and amortized inference in the probabilistic context-free grammar setting described in Section 4.1 of Le et al. [24]. Here pθ(x,z) = p(x |z)pθ(z), where pθ(z) is a prior over parse trees z, p(x |z) is a soft relaxation of the {0,1}likelihood which indicates if sentence x matches the set of terminals (i.e the sentence) produced by z, and θ is the set of probabilities associated with each production rule in the grammar. The inference network φis a recurrent neural network which outputs qφ(z |x), the conditional distribution of a parse tree given an input sentence. We use 14Table 1: Supporting notations in regret analysis. We use notation to similar to Appendix C of Bogunovic et al. [5] when possible. Parameter Domain Meaning ω scalar, (0,1) Remembering-forgetting trade-off parameter (ϵin [5]) fT vector, RT Vector of T function evaluations from f, fT := [f(x1),...,f (xT)]T. ˜fT vector, RT (time-varying case) Vector of T function evaluations from f1:T, ˜fT := [f1(x1),...,f T(xT)]T I(yT; fT) scalar, R+ The mutual information in fT after revealing yT = fT + ϵ. For a GP with covariance function KT, I(yT; fT) = 1 2 log |I + σ−2KT| ˜I(yT;˜fT) scalar, R+ (time-varying case) Mutual information ˜I(yT;˜fT) = 1 2 log |I + σ−2 ˜KT|, where ˜KT is a covariance function that incorporates time kernel kT(i,j) γT scalar, R+ The maximum information gain γT := max x1,...,xT I(yT; fT) after T rounds ˜γT scalar, R+ (time-varying case) The analogous time-varying maximum information gain ˜γT := max x1,...,xT ˜I(yT;˜fT) ˜N scalar, R+ An artifact of the proof technique used by [5]. The T time steps are partitioned into blocks of length ˜N the Astronomers PCFG considered by Le et al. [24], and therefore have access to the ground truth production probabilities θtrue, which we will use to evaluate the quality of our learned model θ. We compare the TVO with GP-bandit and log schedules against REINFORCE , WAKE -WAKE, and WAKE -SLEEP , where WAKE -WAKE and WAKE -SLEEP use data from the true model x ∼pθtrue (x) and learned model x ∼pθ(x) respectively. For each run, we use a batch size of2 and train for 2000 epochs with Adam using default parameters. For all KL divergences (see caption in Figure 6), we compute the median over 20 seeds and then plot the average over the last 100 epochs. As observed by Le et al. [24], sleep- φ updates avoid the deleterious effects of the SNIS bias and is therefore preferable to wake- φupdates in this context. Therefore for all runs we use the TVO to update θ, and use sleep-φto update φ. Sleep- φis a special case of the TVO (cf. Masrani et al. [27] Appendix G.1). In Figure 6 we see that both GP-bandits and log schedules have comparable performance in this setting, with TVO-log, S = 20 achieving the lowest KL [pθ||pθtrue ] and KL [qφ(z |x)||pθtrue (z |x)] across all trials. KL [qφ||pθtrue ] is a preferable metric to KL [qφ||pθ] because the former does not depend on the quality of the learned model. We also note that GP-bandits appears to be less sensitive to the number of partitions than the log schedule. D.2 Training Sigmoid Belief Network on Binarized Omniglot We train the Sigmoid Belief Network described in §5.2 on the binarized Omniglot dataset. Omniglot [22] has 1623 handwritten characters across 50 alphabets. We manually binarize the omniglot[23] dataset by sampling once according to procedure described in [7], and split the binarized omniglot dataset into 23,000 training and 8,070 test examples. Results are shown in Figure 7. At S = 50, GP-bandit achieves similar model learning but better inference compared to log scheduling. D.3 Wall-clock time Comparison We benchmark the wall-clock time of our GP-bandit schedule against the cumulative wall-clock time of the grid-search log schedule. For both schedules we train a V AE on the Omniglot dataset for S = 10 and 5000 epochs. For the log schedule, we run the sweep ran by Masrani et al. [27] (cf. section 7.2), i.e. 20 β1 linearly spaced between [10−2,0.9] for d = 2,..., 6, for a total of 100 runs. For a fair comparison against the log schedule, we loop over d = 2,..., 6 for our GP bandits because dis unlearned, for a total of ﬁve runs. We note that each run of the GP bandits schedule 152 5 10 15 0.02 0.04 0.06 0.08 0.10 0.12KL[ | true] S=2 tvo (log) (0.078) tvo (gp_bandits) (0.091) wake-sleep (0.102) wake-wake (0.112) reinforce (0.072) 2 5 10 15 S=5 tvo (log) (0.103) tvo (gp_bandits) (0.097) wake-sleep (0.102) wake-wake (0.146) reinforce (0.082) 2 5 10 15 S=10 tvo (log) (0.065) tvo (gp_bandits) (0.076) wake-sleep (0.074) wake-wake (0.139) reinforce (0.084) 2 5 10 15 S=20 tvo (log) (0.059) tvo (gp_bandits) (0.064) wake-sleep (0.072) wake-wake (0.106) reinforce (0.09) PCFG 2 5 10 15 12 14 16 18 20 22 24KL[ | true] tvo (log) (20.651) tvo (gp_bandits) (20.707) wake-sleep (21.602) wake-wake (36.737) reinforce (28.341) 2 5 10 15 tvo (log) (20.261) tvo (gp_bandits) (20.038) wake-sleep (20.01) wake-wake (30.627) reinforce (25.779) 2 5 10 15 tvo (log) (17.969) tvo (gp_bandits) (18.704) wake-sleep (18.302) wake-wake (27.85) reinforce (36.469) 2 5 10 15 tvo (log) (17.021) tvo (gp_bandits) (17.383) wake-sleep (17.641) wake-wake (25.727) reinforce (33.281) 2 5 10 15 Number of Partitions 4 6 8 10 12 14KL[ | ] tvo (log) (10.465) tvo (gp_bandits) (11.117) wake-sleep (11.364) wake-wake (12.711) reinforce (10.92) 2 5 10 15 Number of Partitions tvo (log) (10.286) tvo (gp_bandits) (10.639) wake-sleep (11.103) wake-wake (13.184) reinforce (9.288) 2 5 10 15 Number of Partitions tvo (log) (10.41) tvo (gp_bandits) (10.381) wake-sleep (10.74) wake-wake (11.744) reinforce (13.099) 2 5 10 15 Number of Partitions tvo (log) (9.625) tvo (gp_bandits) (9.827) wake-sleep (10.288) wake-wake (12.637) reinforce (11.466) Figure 6: Evaluation of model learning in a PCFG, whereθis the set of probabilities associated with each production rule in the grammar, and φis an RNN which generates the conditional probability of a parse tree given a sentence. GP-bandits (ours) is comparable to the baseline log schedule and less sensitive to number of partitions, as evaluated by the KL divergence between learned and true model parameters (top row). Inference network learning is evaluated by the KL divergence between qφ(z |x) and pθtrue (z |x) (middle row) andpθ(z |x) (bottom row). We compare againstREINFORCE , WAKE -WAKE, and WAKE -SLEEP , where some baselines aren’t shown due to being out of range. At S = 20, TVO with log and GP-bandits schedule outperforms REINFORCE , WAKE -WAKE, and WAKE -SLEEP both in terms of the quality of the generative model (top row, right) and inference network (middle row, right) for all S ∈{2,5,10,20}. 2 5 10 15 Number of Partitions 133 132 131 test logp (x) S=10 gp_bandit (best: -131.50±0.13) log (best: -131.49±0.16) linear (best: -131.64±0.10) moments (best: -131.55±0.15) 2 5 10 15 Number of Partitions S=50 gp_bandit (best: -131.31±0.12) log (best: -131.24±0.10) linear (best: -131.51±0.16) moments (best: -131.19±0.05) Binarized Omniglot 2 5 10 15 Number of Partitions 0 1 2 3 4 5 6 7 8 KL[q ||p ] 1.9 2.4 2.8 2.9 1.4 2.7 3.1 3.2 2.0 2.8 2.9 3.0 S: 10 gp_bandit log moments 2 5 10 15 Number of Partitions KL[q ||p ] 2.1 3.1 4.2 4.6 1.5 3.3 5.1 5.1 2.2 3.7 4.6 5.5 S: 50 gp_bandit log moments Binarized Omniglot Figure 7: Performance of the Sigmoid Belief Network described in §5.2 on the binarized omniglot dataset. The GP-bandit schedule at d= 15,S = 50 outperforms all baselines in terms of model and inference network learning. includes learning the GP hyperparameters as described in Appendix B. For both schedules, we take the best log p(x) and corresponding KL divergence, and plot the cumulative run time across all runs. The results in Table 2 show that the GP-bandits schedule does comparable to the grid searched log schedule (log likelihood: −110.72 vs −110.99) while requiring signiﬁcantly less cumulative wall-clock time (10 hrs vs 178 hrs). 162 5 10 15 d 90.5 90.0 89.5 89.0 88.5 88.0 87.5 87.0 test logp(x) S: 10 rand (best: -89.053)  gp-bandit (best: -88.504)  2 5 10 15 d test logp(x) S: 50 rand (best: -88.244)  gp-bandit (best: -87.579)  mnist 2 5 10 15 d 234.0 233.5 233.0 232.5 232.0 231.5 test logp(x) S: 10 rand (best: -232.462)  gp-bandit (best: -232.014)  2 5 10 15 d test logp(x) S: 50 rand (best: -231.961)  gp-bandit (best: -231.643)  fashion_mnist Figure 8: We compare the performance between our GP-bandit against the Random search (Rand) baseline which uniformly generates the integration schedules βt. The GP-bandit schedule outper- forms the random counterpart by using information obtained from previous choices, as described in Algorithm 1. Table 2: Wallclock time of the GP-bandit schedule compared to the grid-search of [27] for the log schedule. GP-bandit approach achieves a competitive test log likelihood and lower KL divergence compared with the grid-searched log schedule, but requires signiﬁcantly lower cumulative run-time. best log p(x) best kl number of runs cumulative run time (hrs) GP bandit (ours) -110.995 7.655 5 10.99 grid-searched log -110.722 8.389 100 177.01 D.4 Ablation studies Ablation study between GP-bandit and random search.To demonstrate that our model can leverage useful information from past data, we compare against the Random Search picks the inte- gration schedule uniformly at random. We present the results in Figure 8 using MNIST (left) and Fashion MNIST (right). We observe that our GP-bandit clearly outperforms the Random baseline. The gap is generally increasing with larger dimension d, e.g., d= 15 as the search space grows exponentially with the dimension. Ablation study between permutation invariant GPs.We compare our GP-bandit model using two versions of (1) non-permutation invariant GP and (2) permutation invariant GP in Table 3. Our permutation invariant GP does not need to add all permuted observations into the model, but is still capable of generalizing. The result in Table 3 conﬁrms that if we have more samples to learn the GP, such as using larger epochs budget T, the two versions will result in the same performance. On the other hand, if we have limited number of training budgets, e.g., using lower number of epochs, the permutation invariant GP will be more favorable and outperforms the non-permutation invariant. In addition, the result suggests that for higher dimension d = 15 (number of partitions) our permutation invariant GP performs consistently better than the counterpart. D.5 Training Curves We show example training curves for S = 10,d ∈{5,15}obtained using the linear, log, moment, and GP-bandit schedules in Figure 9. We can see sudden drops in the GP-bandit training curves indicating our model is exploring alternate schedules during training (cf. Section 4.1). 17Table 3: Comparison between permutation invariant and non-permutation invariant in MNIST dataset using S= 10 (top) and S= 50 (bottom). The best scores are in bold. Given T used epochs, the number of bandit update and thus the number of sample for GP is T/w where w = 10 is the frequency update. The permutation invariant will be more favorable when we have less samples for ﬁtting the GP, as indicated in less number of used epochs T = 1000 ,2000. The performance is comparable when we collect sufﬁciently large number of samples, e.g., when T/w = 1000. S=10 Used Epoch T/ Bandit Iteration 1000/100 2000 /200 5000 /500 10000 /1000 d=5 Perm Invariant −91.488 −90.129 −89.130 −88.651 Non Perm Invariant −91.554 −90.206 −89.262 −88.552 d=10 Perm Invariant −91.430 −90.219 −89.159 −88.603 Non Perm Invariant −91.553 −90.249 −89.110 −88.466 d=15 Perm Invariant −91.386 −90.059 −88.957 −88.504 Non Perm Invariant −91.550 −90.224 −89.215 −88.564 S=50 Used Epoch T/ Bandit Iteration 1000/100 2000 /200 5000 /500 10000 /1000 d=5 Permutation Invariant −90.071 −89.068 −88.163 −87.979 Non Permutation Invariant −90.119 −89.142 −88.215 −87.860 d=10 Perm Invariant −90.125 −89.115 −88.187 −87.859 Non Permutation Invariant −90.212 −89.225 −88.231 −87.702 d=15 Permutation Invariant −90.029 −89.082 −88.102 −87.579 Non Permutation Invariant −90.157 −89.247 −88.173 −87.631 180 2000 4000 6000 8000 10000 epoch 94 93 92 91 90 89 train logp (x) mnist (d=5) gp_bandit log linear moments 0 2000 4000 6000 8000 10000 epoch 94 93 92 91 90 89 train logp (x) mnist (d=15) gp_bandit log linear moments 0 2000 4000 6000 8000 10000 epoch 234.0 233.5 233.0 232.5 232.0 231.5 231.0 230.5 230.0 train logp (x) fashion mnist (d=5) gp_bandit log linear moments 0 2000 4000 6000 8000 10000 epoch 234.0 233.5 233.0 232.5 232.0 231.5 231.0 230.5 230.0 train logp (x) fashion mnist (d=15) gp_bandit log linear moments 0 2000 4000 6000 8000 10000 epoch 134.0 133.5 133.0 132.5 132.0 131.5 131.0 130.5 130.0 train logp (x) binarized omniglot (d=5) gp_bandit log linear moments 0 2000 4000 6000 8000 10000 epoch 134.0 133.5 133.0 132.5 132.0 131.5 131.0 130.5 130.0 train logp (x) binarized omniglot (d=15) gp_bandit log linear moments 0 2000 4000 6000 8000 10000 epoch 125 124 123 122 121 120 119 118 train logp (x) binarized mnist (d=5) gp_bandit log linear moments 0 2000 4000 6000 8000 10000 epoch 125 124 123 122 121 120 119 118 train logp (x) binarized mnist (d=15) gp_bandit log linear moments Figure 9: We plot log p(x) on the training set throughoutS = 10, d∈{5,15}for each dataset using the experimental setup described in Appendix A. The ﬁnal training log likelihoods are consistent with the test log likelihoods. Small drops in the GP-bandit training curves indicate the algorithm exploring the reward landscape (see Section 4.1). 19",
      "meta_data": {
        "arxiv_id": "2010.15750v3",
        "authors": [
          "Vu Nguyen",
          "Vaden Masrani",
          "Rob Brekelmans",
          "Michael A. Osborne",
          "Frank Wood"
        ],
        "published_date": "2020-10-29T16:57:27Z",
        "pdf_url": "https://arxiv.org/pdf/2010.15750v3.pdf"
      },
      "llm_extracted_info": {
        "main_contributions": "This paper addresses the challenge of automatically choosing and dynamically adapting the 'integration schedule' (sorted discretization points) required for the Thermodynamic Variational Objective (TVO), a recently proposed variational lower bound. The key contribution is the introduction of a bespoke Gaussian process bandit optimization method for this purpose. The approach automates the selection of these points and adapts their positions during optimization, leading to improved model learning and inference. Theoretical guarantees are provided, demonstrating that the bandit optimization converges to a regret-minimizing choice of integration points. Empirical validation shows improved learning and inference in Variational Autoencoders (VAEs) and Sigmoid Belief Networks (SBNs).",
        "methodology": "The methodology frames the choice of integration points (β, a d-dimensional vector) for the TVO as a time-varying Gaussian process (GP) bandit optimization problem. A time-varying reward function is defined as the difference in TVO log evidence estimates between training windows, which is equivalent to maximizing the final log evidence. A time-varying, permutation-invariant GP model is constructed by combining spatial and temporal covariance functions. Specifically, k(x,x') = kβ(β,β') × kT(t,t'), where kT is a temporal covariance function and kβ uses a projection operator Φ to ensure ordering constraints (0 < β1 < ... < βd-1 < 1) by sorting the β vector. At each round, the algorithm selects the next integration schedule βt by maximizing an acquisition function, a linear combination of the GP posterior mean and variance (GP-UCB variant), balancing exploration and exploitation. The GP model is then updated with the observed reward.",
        "experimental_setup": "The method was empirically validated on Variational Autoencoders (VAEs) using MNIST and Fashion MNIST datasets, and on Sigmoid Belief Networks (SBNs) with binarized MNIST and binarized Omniglot. Additionally, it was applied to a Probabilistic Context Free Grammar (PCFG) model using the Astronomers PCFG dataset. Experiments were conducted with varying numbers of importance samples (S ∈ {10, 50}) and schedule dimensions (d ∈ {2, 5, 10, 15}), each with 5 random seeds. Model learning was measured by test log evidence (using IWAE bound with 5000 samples) and KL divergence to true parameters (for PCFG), while inference performance was measured by DKL[qφ(z|x) || pθ(z|x)]. Baselines included log-uniform, linear-uniform, and moments schedules. The GP implementation used an exponentiated quadratic kernel, type II maximum likelihood for hyperparameter estimation, multi-start BFGS for acquisition function optimization, and a dynamic update frequency (w). Ablation studies compared the GP-bandit against random search and examined permutation-invariant versus non-permutation-invariant GPs. A wall-clock time benchmark was performed against grid search.",
        "limitations": "The primary limitation noted is that the proposed GP bandit optimization, while automating the selection of integration points, still relies on the number of partitions (d) as a fixed hyperparameter throughout the training process. Additionally, the restriction of β values to the range [0.05, 0.95]d was found necessary to avoid negative performance impacts, implying a constraint on the search space.",
        "future_research_directions": "A clear direction for future research is to incorporate the adaptive selection of the number of partitions (d) directly into the bandit optimization framework. This would further automate the integration schedule selection beyond just the point locations, addressing the current limitation of d being a fixed hyperparameter."
      }
    },
    {
      "title": "Active Statistical Inference",
      "abstract": "Inspired by the concept of active learning, we propose active\ninference$\\unicode{x2013}$a methodology for statistical inference with\nmachine-learning-assisted data collection. Assuming a budget on the number of\nlabels that can be collected, the methodology uses a machine learning model to\nidentify which data points would be most beneficial to label, thus effectively\nutilizing the budget. It operates on a simple yet powerful intuition:\nprioritize the collection of labels for data points where the model exhibits\nuncertainty, and rely on the model's predictions where it is confident. Active\ninference constructs provably valid confidence intervals and hypothesis tests\nwhile leveraging any black-box machine learning model and handling any data\ndistribution. The key point is that it achieves the same level of accuracy with\nfar fewer samples than existing baselines relying on non-adaptively-collected\ndata. This means that for the same number of collected samples, active\ninference enables smaller confidence intervals and more powerful p-values. We\nevaluate active inference on datasets from public opinion research, census\nanalysis, and proteomics.",
      "full_text": "Active Statistical Inference Tijana Zrnic∗ Emmanuel J. Cand` es† {tijana.zrnic,candes}@stanford.edu ∗Department of Statistics and Stanford Data Science †Department of Statistics and Department of Mathematics Stanford University Abstract Inspired by the concept of active learning, we propose active inference—a methodology for statistical inference with machine-learning-assisted data collection. Assuming a budget on the number of labels that can be collected, the methodology uses a machine learning model to identify which data points would be most beneficial to label, thus effectively utilizing the budget. It operates on a simple yet powerful intuition: prioritize the collection of labels for data points where the model exhibits uncertainty, and rely on the model’s predictions where it is confident. Active inference constructs provably valid confidence intervals and hypothesis tests while leveraging any black-box machine learning model and handling any data distribution. The key point is that it achieves the same level of accuracy with far fewer samples than existing baselines relying on non-adaptively-collected data. This means that for the same number of collected samples, active inference enables smaller confidence intervals and more powerful p-values. We evaluate active inference on datasets from public opinion research, census analysis, and proteomics. 1 Introduction In the realm of data-driven research, collecting high-quality labeled data is a continuing impediment. The impediment is particularly acute when operating under stringent labeling budgets, where the cost and effort of obtaining each label can be substantial. Recognizing these limitations, many have turned to machine learning as a pragmatic solution, leveraging it to predict unobserved labels across various fields. In remote sensing, machine learning assists in annotating and interpreting satellite imagery [24, 43, 55]; in proteomics, tools like AlphaFold [26] are revolutionizing our understanding of protein structures; even in the realm of elections—including most major US elections—technologies combining scanners and predictive models are used as efficient tools for vote counting [56]. These applications reflect a growing reliance on machine learning for extracting information and knowledge from unlabeled datasets. However, this reliance on machine learning is not without its pitfalls. The core issue lies in the inherent biases of these models. No matter how sophisticated, predictions lead to dubious conclusions; as such, predic- tions cannot fully substitute for traditional data sources such as gold-standard experimental measurements, high-quality surveys, and expert annotations. This begs the question: is there a way to effectively leverage the predictive power of machine learning while still ensuring the integrity of our inferences? Drawing inspiration from the concept of active learning, we propose active inference—a novel methodol- ogy for statistical inference that harnesses machine learning not as a replacement for data collection but as a strategic guide to it. The methodology uses a machine learning model to identify which data points would be most beneficial to label, thus effectively utilizing the labeling budget. It operates on a simple yet pow- erful intuition: prioritize the collection of labels for data points where the model exhibits uncertainty, and rely on the model’s predictions where it is confident. Active inference constructs provably valid confidence intervals and hypothesis tests for any black-box machine learning model and any data distribution. The key takeaway is that it achieves the same level of accuracy with far fewer samples than existing baselines 1 arXiv:2403.03208v2  [stat.ML]  29 May 2024relying on non-adaptively-collected data. Put differently, this means that for the same number of collected samples, active inference enables smaller confidence intervals and more powerful p-values. We will show in our experiments that active inference can save over 80% of the sample budget required by classical inference methods (see Figure 4). Although quite different in scope, our work is inspired by the recent framework of prediction-powered inference (PPI) [1]. PPI assumes access to a small labeled dataset and a large unlabeled dataset, drawn i.i.d. from the population of interest. It then asks how one can use machine learning and the unlabeled dataset to sharpen inference about population parameters depending on the distribution of labels. Our objective in this paper is different since the core of our contribution is (1) designing strategic data collection approaches that enable more powerful inferences than collecting labels in an i.i.d. manner, and (2) showing how to perform inference with such strategically collected data. That said, we will see that prediction- powered inference can be seen as a special case of our methodology: while PPI ignores the issue of strategic data collection and instead uses a trivial, uniform data collection strategy, it leverages machine learning to enhance inference in a similar way to our method. We provide a further discussion of prior work in Section 3. 2 Problem description We introduce the formal problem setting. We observe unlabeled instances X1, . . . , Xn, drawn i.i.d. from a distribution PX. The labels Yi are unobserved, and we shall use ( X, Y) ∼ P = PX × PY |X to denote a generic feature–label pair drawn from the underlying data distribution. We are interested in performing inference—conducting a hypothesis test or forming a confidence interval—for a parameterθ∗ that depends on the distribution of the unobserved labels; that is, the parameter is a functional of PX × PY |X. For example, we might be interested in forming a confidence interval for the mean label, θ∗ = E[Yi], where Yi is the label corresponding to Xi. Although we will primarily focus on forming confidence intervals, the standard duality between confidence intervals and hypothesis tests makes our results directly applicable to testing as well. We have no collected labels a priori. Rather, the goal is to efficiently and strategically acquire labels for certain points Xi, so that inference is as powerful as possible for a given collection budget—more so than if labels were collected uniformly at random—while also remaining valid. We denote by nlab the number of collected labels. We assume that we are constrained to collect, on average, E[nlab] ≤ nb labels, for some budget nb. (For simplicity we bound E[nlab], however the budget can be met with high probability, since nlab concentrates around nb at a fast rate.) Typically, nb ≪ n. To guide the choice of which instances to label, we will make use of a predictive model f. Typically this will be a black-box machine learning model, but it could also be a hand-designed decision rule based on expert knowledge. This is the key component that will enable us to get a significant boost in power. We do not assume any knowledge of the predictive performance of f, or any parametric form for it. Our key takeaway is that, if we have a reasonably good model for predicting the labels Yi based on Xi, then we can achieve a significant boost in power compared to labeling a uniformly-at-random chosen set of instances. We will consider two settings, depending on whether or not we update the predictive model f as we gather more labels. • The first is a batch setting, where we simultaneously make decisions of whether or not to collect the corresponding label for all unlabeled points at once. In this setting, the model f is pre-trained and remains fixed during the label collection. The batch setting is simpler and arguably more practical if we already have a good off-the-shelf predictor. • The second setting is sequential: we go through the unlabeled points one by one and update the predictive model as we collect more data. The benefit of the second approach is that it is applicable even when we do not have access to a pre-trained model, but we have to train a model from scratch. Our proposed active inference strategy will be applicable to all convex M-estimation problems. This 2means that it handles all targets of inference θ∗ that can be written as: θ∗ = arg min θ E[ℓθ(X, Y)], where (X, Y) ∼ P, for a loss function ℓθ that is convex in θ. We denote L(θ) = E[ℓθ(X, Y)] for brevity. M-estimation captures many relevant targets, such as the mean label, quantiles of the label, and regression coefficients. Example 1 (Mean label). If ℓθ(x, y) = 1 2 (y − θ)2, then the target is the mean label, θ∗ = E[Y ]. Note that this loss has no dependence on the features. Example 2 (Linear regression). If ℓθ(x, y) = 1 2 (y − x⊤θ)2, then θ∗ is the vector of linear regression coeffi- cients obtained by regressing y on x, that is, the “effect” of x on y. Example 3 (Label quantile). For a given q ∈ (0, 1), let ℓθ(x, y) = q(y −θ)1{y > θ}+ (1−q)(θ −y)1{y ≤ θ} be the “pinball” loss. Then, θ∗ is equal to the q-quantile of the label distribution: θ∗ = inf{θ : P(Y ≤ θ) ≥ q}. 3 Related work Our work is most closely related to prediction-powered inference (PPI) and other recent works on inference with machine learning predictions [1, 3, 19, 33, 34, 61]. This recent literature in turn relates to classical work on inference with missing data and semiparametric statistics [13, 41, 42, 44, 45, 46], as well as semi-supervised inference [5, 57, 60]. We consider the same set of inferential targets as in [1, 3, 61], building on classical M-estimation theory [52] to enable inference. While PPI assumes access to a small labeled dataset and a large unlabeled dataset, which are drawn i.i.d., our work is different in that it leverages machine learning in order to design adaptive label collection strategies, which breaks the i.i.d. structure between the labeled and the unlabeled data. That said, we shall however see that our active inference estimator will reduce to the prediction-powered estimator when we apply a trivial, uniform label collection strategy. We will demonstrate empirically that the adaptivity in label collection enables significant improvements in statistical power. There is a growing literature on inference from adaptively collected data [14, 29, 59], often focusing on data collected via a bandit algorithm. These papers typically focus on average treatment effect estimation. In contrast to our work, these works generally do not focus on how to set the data-collection policy as to achieve good statistical power, but their main focus is on providing valid inferences given a fixed data- collection policy. Notably, Zhang et al. [59] study inference for M-estimators from bandit data. However, their estimators do not leverage machine learning, which is central to our work. A substantial line of work studies adaptive experiment design [8, 10, 20, 21, 23, 28, 31, 32, 40], often with the goal of maximizing welfare during the experiment or identifying the best treatment. Most related to our motivation, a subset of these works [10, 21, 32] study adaptive design with the goal of efficiently estimating average treatment effects. While our motivation is not necessarily treatment effect estimation, we continue in a similar vein—collecting data adaptively with the goal of improved efficiency—with a focus on using modern, black-box machine learning to produce uncertainty estimates that can be turned into efficient label collection methods. Related variance-reduction ideas appear in stratified survey sampling [27, 30, 35, 48]. Our proposal can be seen as stratifying the population of interest based on the certainty of a black-box machine learning model. Finally, our work draws inspiration from active learning, which is a subarea of machine learning centered around the observation that a machine learning model can enhance its predictive capabilities if it is allowed to choose the data points from which it learns. In particular, our setup is analogous to pool-based active learning [50]. Sampling according to a measure of predictive uncertainty is a central idea in active learning [4, 6, 18, 22, 25, 39, 49, 51]. Since our goal is statistical inference, rather than training a good predictor, our sampling rules are different and adapt to the inferential question at hand. More generally, we note that there is a large body of work studying ways to efficiently collect gold-standard labels, often under a budget constraint [12, 53, 58]. 34 Warm-up: active inference for mean estimation We first focus on the special case of estimating the mean label, θ∗ = E[Y ], in the batch setting. The intuition derived from this example carries over to all other problems. Recall the setup: we observe n i.i.d. unlabeled instances X1, . . . , Xn, and we can collect labels for at most nb of them (on average). Consider first a “classical” solution, which does not leverage machine learning. Given a budget nb, we can simply label any arbitrarily chosen nb points. Since the instances are i.i.d., without loss of generality we can choose to label instances {1, . . . , nb} and compute: ˆθnoML = 1 nb nbX i=1 Yi. The estimator ˆθnoML is clearly unbiased. It is an average over nb terms, and thus its variance is equal to Var(ˆθnoML) = 1 nb Var(Y ). Now, suppose that we are given a machine learning model f(X), which predicts the label Y ∈ R from observed covariates X ∈ X.1 The idea behind our active inference strategy is to increase the effective sample size by using the model’s predictions on points X where the model is confident and focusing the labeling budget on the points X where the model is uncertain. To implement this idea, we design a sampling rule π : X →[0, 1] and collect label Yi with probability π(Xi). The sampling rule is derived from f, by appropriately measuring its uncertainty. The hope is that π(x) ≈ 1 signals that the model f is very uncertain about instance x, whereas π(x) ≈ 0 indicates that the model f should be very certain about instance x. Let ξi ∼ Bern(π(Xi)) denote the indicator of whether we collect the label for point i. By definition, nlab =Pn i=1 ξi. The rule π will be carefully rescaled to meet the budget constraint: E[nlab] = E[π(X)] · n ≤ nb. Our active estimator of the mean θ∗ is given by: ˆθπ = 1 n nX i=1 \u0012 f(Xi) + (Yi − f(Xi)) ξi π(Xi) \u0013 . (1) This is essentially the augmented inverse propensity weighting (AIPW) estimator [42], with a particular choice of propensities π(Xi) based on the certainty of the machine learning model that predicts the missing labels. When the sampling rule is uniform, i.e. π(x) = nb/n for all x, ˆθπ is equal to the prediction-powered mean estimator [1]. It is not hard to see that ˆθπ is unbiased: E[ˆθπ] = θ∗. A short calculation shows that its variance equals Var(ˆθπ) = 1 n \u0012 Var(Y ) + E \u0014 (Y − f(X))2 \u0012 1 π(X) − 1 \u0013\u0015\u0013 . (2) If the model is highly accurate for all x, i.e. f(X) ≈ Y , then Var(ˆθπ) ≈ 1 n Var(Y ), which is far smaller than Var(ˆθnoML) since nb ≪ n. Of course, f will never be perfect and accurate for all instances x. For this reason, we will aim to choose π such that π is small when f(X) ≈ Y and large otherwise, so that the relevant term (Y − f(X))2 \u0000 π−1(X) − 1 \u0001 is always small (of course, subject to the sampling budget constraint). For example, for instances for which the predictor is correct, i.e. f(X) = Y , we would ideally like to setπ(X) = 0 as this incurs no additional variance. We note that the variance reduction of active inference compared to the “classical” solution also implies that the resulting confidence intervals get smaller. This follows because interval width scales with the standard deviation for most standard intervals (e.g., those derived from the central limit theorem). Finally, we explain how to set the sampling rule π. The rule will be derived from a measure of model uncertainty u(x) and we shall provide different choices of u(x) in the following paragraphs. At a high level, 1X is the set of values the covariates can take on, e.g.Rd. 4one can think of u(Xi) as the model’s best guess of |Yi − f(Xi)|. We will choose π(x) proportional to u(x), that is, π(x) ∝ u(x), normalized to meet the budget constraint. Intuitively, this means that we want to focus our data collection budget on parts of the covariate space where the model is expected to make the largest errors. Roughly speaking, we will set π(x) = u(x) E[u(X)] · nb n ; this implies E[nlab] = E[π(X)] ·n ≤ nb. (This is an idealized form of π(x) because E[u(X)] cannot be known exactly, though it can be estimated very accurately from the unlabeled data; we will formalize this in the following section.) We will take two different approaches for choosing the uncertainty u(x), depending on whether we are in a regression or a classification setting. Regression uncertainty In regression, we explicitly train a model u(x) to predict |f(Xi) − Yi| from Xi. We note that we aim to predict only the magnitude of the error and not the directionality. In the batch setting, we typically have historical data of ( X, Y) pairs that are used to train the model f. We thus train u(x) on this historical data, by setting |f(X) −Y | as the target label for instance X. The data used to train u should ideally be disjoint from the data used to train f to avoid overoptimistic estimates of uncertainty. We will typically use data splitting to avoid this issue, though there are more data efficient solutions such as cross-fitting. Notice that access to historical data will only be important in the batch setting, as assumed in this section. In the sequential setting we will be able to train u(x) gradually on the collected data. Classification uncertainty Next we look at classification, where Y is supported on a discrete set of values. Our main focus will be on binary classification, where Y ∈ {0, 1}. In such cases, our target is θ∗ = P(Y = 1). We might care about E[Y ] more generally when Y takes on K distinct values (e.g., K distinct ratings in a survey, K distinct qualification levels, etc). In classification, f(x) is usually obtained as the “most likely” class. If K is the number of classes, we have f(x) = arg maxi∈[K] pi(x), for some probabilistic output p(x) = ( p1(x), . . . , pK(x)) which satisfiesPK i=1 pi(x) = 1. For example, p(x) could be the softmax output of a neural network given input x. We will measure the uncertainty as: u(x) = K K − 1 · \u0012 1 − max i∈[K] pi(x) \u0013 . (3) In binary classification, this reduces to u(x) = 2 min{p(x), 1 − p(x)}, where we use p(x) to denote the raw classifier output in [0 , 1]. Therefore, u(x) is large when p(x) is close to uniform, i.e. max i pi(x) ≈ 1/K. On the other hand, if the model is confident, i.e. max i pi(x) ≈ 1, the uncertainty is close to zero. 5 Batch active inference Building on the discussion from Section 4, we provide formal results for active inference in the batch setting. Recall that in the batch setting we observe i.i.d. unlabeled points X1, . . . , Xn, all at once. We consider a family of sampling rules πη(x) = η u(x), where u(x) is the chosen uncertainty measure and η ∈ H ⊆R+ is a tuning parameter. We will discuss ways of choosing u(x) in Section 7. The role of the tuning parameter is to scale the sampling rule to the sampling budget. We choose ˆη = max ( η ∈ H: η nX i=1 u(Xi) ≤ nb ) , (4) and deploy πˆη as the sampling rule. With this choice, we have E[nlab] = E \" nX i=1 ˆη u(Xi) # ≤ nb; therefore, πˆη meets the label collection budget. We denote ˆθη ≡ ˆθπη . 5Mean estimation We first explain how to perform inference for mean estimation in Proposition 1. Recall the active mean estimator: ˆθˆη = 1 n nX i=1 \u0012 f(Xi) + (Yi − f(Xi)) ξi πˆη(Xi) \u0013 , (5) where ξi ∼ Bern(πˆη(Xi)). Following standard notation, zq below denotes the qth quantile of the standard normal distribution. Proposition 1. Suppose that there exists η∗ ∈ Hsuch that P(ˆη ̸= η∗) → 0. Then √n(ˆθˆη − θ∗) d → N(0, σ2 ∗), where σ2 ∗ = Var(f(X) + (Y − f(X)) ξη∗ πη∗ (X) ) and ξη∗ ∼ Bern(πη∗ (X)). Consequently, for any ˆσ2 p → σ2 ∗, Cα = (ˆθˆη ± z1−α/2 ˆσ√n ) is a valid (1 − α)-confidence interval: lim n→∞ P(θ∗ ∈ Cα) = 1 − α. A few remarks about Proposition 1 are in order: first, the consistency condition P(ˆη ̸= η∗) → 0 is easily ensured if nb/n has a limit p ∈ (0, 1), that is, if nb is asymptotically proportional to n. Then, as long as the space of tuning parameters H is discrete and there is no η ∈ Hsuch that η E[u(X)] = p exactly, the consistency condition is met. Second, obtaining a consistent variance estimate ˆσ2 is straightforward, as one can simply take the empirical variance of the increments in the estimator (5). We note that, while our main results will all focus on asymptotic confidence intervals, some of our results have direct non-asymptotic and time-uniform analogues; see Section C. General M-estimation Next, we turn to general convex M-estimation. Recall this means that we can write θ∗ = arg minθ L(θ) = arg min θ E[ℓθ(X, Y)], for a convex loss ℓθ. To simplify notation, let ℓθ,i = ℓθ(Xi, Yi), ℓf θ,i = ℓθ(Xi, f(Xi)). We similarly use ∇ℓθ,i and ∇ℓf θ,i. For a general sampling rule π, our active estimator is defined as ˆθπ = arg min θ Lπ(θ), where Lπ(θ) = 1 n nX i=1 \u0012 ℓf θ,i + (ℓθ,i − ℓf θ,i) ξi π(Xi) \u0013 . (6) As before, ξi ∼ Bern(π(Xi)). When π is the uniform rule, π(x) = nb/n, the estimator (6) equals the general prediction-powered estimator from [3]. Notice that the loss estimate Lπ(θ) is unbiased: E[Lπ(θ)] = L(θ). We again scale the sampling rule πη(x) = η u(x) according to the sampling budget, as in Eq. (4). We next show asymptotic normality of ˆθˆη for general targets θ∗ which, in turn, enables inference. The result essentially follows from the usual asymptotic normality for M-estimators [52, Ch. 5], with some nec- essary modifications to account for the data-driven selection of ˆ η. We require standard, mild smoothness assumptions on the loss ℓθ, formally stated in Ass. 1 in the Appendix. Theorem 1 (CLT for batch active inference) . Assume the loss is smooth (Ass. 1) and define the Hessian Hθ∗ = ∇2E[ℓθ∗ (X, Y)]. Suppose that there exists η∗ ∈ Hsuch that P(ˆη ̸= η∗) → 0. Then, if ˆθη∗ p → θ∗, we have √n(ˆθˆη − θ∗) d → N(0, Σ∗), where Σ∗ = H−1 θ∗ Var \u0010 ∇ℓf θ∗,i + \u0010 ∇ℓθ∗,i − ∇ℓf θ∗,i \u0011 ξη∗ πη∗ (X) \u0011 H−1 θ∗ . Consequently, for any ˆΣ p → Σ∗, Cα = ( ˆθˆη j ± z1−α/2 q ˆΣjj n ) is a valid (1 − α)-confidence interval for θ∗ j : lim n→∞ P(θ∗ j ∈ Cα) = 1 − α. 6The remarks following Proposition 1 again apply: the consistency condition on ˆ η is easily ensured if nb/n has a limit, and ˆΣ admits a simple plug-in estimate by replacing all quantities with their empirical counterparts. The consistency condition on ˆθη∗ is a standard requirement for analyzing M-estimators [see 52, Ch. 5]; it is studied and justified at length in the literature and we shall therefore not discuss it in close detail. We however remark that it can be deduced if the empirical loss Lπ(θ) is almost surely convex or if the parameter space is compact. The empirical loss Lπ(θ) is convex in a number of cases of interest, including means and generalized linear models; for the proof, see [3]. 6 Sequential active inference In the batch setting we observe all data points X1, . . . , Xn at once and fix a predictive model f and sampling rule π that guide our choice of which labels to collect. An arguably more natural data collection strategy would operate in an online manner: as we collect more labels, we iteratively update the model and our strategy for which labels to collect next. This allows for further efficiency gains over using a fixed model throughout, as the latter ignores knowledge acquired during the data collection. For example, if we are conducting a survey and we collect responses from members of a certain demographic group, it is only natural that we update our sampling rule to reflect the fact that we have more knowledge and certainty about that demographic group. Formally, instead of having a fixed model f and rule π, we go through our data sequentially. At step t ∈ {1, . . . , n}, we observe data point Xt and collect its label with probability πt(Xt), where πt(·) is based on the uncertainty of model ft. The model ft can be fine-tuned on all information observed up to time t; formally, we require thatft, πt ∈ Ft−1, where Ft is the σ-algebra generated by the firstt points Xs, 1 ≤ s ≤ t, their labeling decisions ξs, and their labels Ys, if observed: Ft = σ((X1, Y1ξ1, ξ1), . . . ,(Xt, Ytξt, ξt)). (Note that Ytξt = Yt if and only if ξt = 1; otherwise, Ytξt = ξt = 0.) We will again calibrate our decisions of whether to collect a label according to a budget on the sample size nb. We denote by nlab,t the number of labels collected up to time t. Inference in the sequential setting is more challenging than batch inference because the data points (Xt, Yt, ξt), t∈ [n], are dependent; indeed, the purpose of the sequential setting is to leverage previous obser- vations when deciding on future labeling decisions. We will construct estimators that respect a martingale structure, which will enable tractable inference via the martingale central limit theorem [17]. This resembles the approach taken by Zhang et al. [59] (though our estimators are quite different due to the use of machine learning predictions). Mean estimation We begin by focusing on the mean. If we take ℓθ to be the squared loss as in Example 1, we obtain the sequential active mean estimator: ˆθ # »π = 1 n nX t=1 ∆t, ∆t = ft(Xt) + (Yt − ft(Xt)) ξt πt(Xt). We note that ∆ t are martingale increments ; they share a common conditional mean E[∆t|Ft−1] = θ∗, and they are Ft-measurable, ∆t ∈ Ft. We let σ2 t = V (ft, πt) = Var(∆t|ft, πt) denote the conditional variance of the increments. To show asymptotic normality of ˆθ # »π , we shall require the Lindeberg condition, whose statement we defer to the Appendix. It is a standard assumption for proving central limit theorems when the increments are not i.i.d.. Roughly speaking, the Lindeberg condition requires that the increments do not have very heavy tails; it prevents any increment from having a disproportionately large contribution to the overall variance. Proposition 2. Suppose 1 n Pn t=1 σ2 t p → σ2 ∗ = V (f∗, π∗), for some fixed model–rule pair (f∗, π∗), and that the increments ∆t satisfy the Lindeberg condition (Ass. 2). Then √n(ˆθ # »π − θ∗) d → N(0, σ2 ∗). 7Consequently, for any ˆσ2 p → σ2 ∗, Cα = (ˆθ # »π ± z1−α/2 ˆσ√n ) is a valid (1 − α)-confidence interval: lim n→∞ P(θ∗ ∈ Cα) = 1 − α. Intuitively, Proposition 2 requires that the model ft and sampling rule πt converge. For example, a sufficient condition for 1 n Pn t=1 σ2 t p → σ2 ∗ is V (fn, πn) L1 → V (f∗, π∗). Since the sampling rule is typically based on the model, it makes sense that it would converge if ft converges. At the same time, it makes sense for ft to gradually stop updating after sufficient accuracy is achieved. General M-estimation We generalize Proposition 2 to all convex M-estimation problems. The general version of our sequential active estimator takes the form ˆθ # »π = arg min θ L # »π (θ), where L # »π (θ) = 1 n nX t=1 Lt(θ), L t(θ) = ℓft θ,t + (ℓθ,t − ℓft θ,t) ξt πt(Xt). (7) Let Vθ,t = Vθ(ft, πt) = Var (∇Lt(θ)|ft, πt). We will again require that ( ft, πt) converge in an appropriate sense. Theorem 2 (CLT for sequential active inference). Assume the loss is smooth (Ass. 1) and define the Hessian Hθ∗ = ∇2E[ℓθ∗ (X, Y)]. Suppose also that 1 n Pn t=1 Vθ∗,t p → V∗ = Vθ∗ (f∗, π∗) entry-wise for some fixed model– rule pair (f∗, π∗), and that the increments Lt(θ) satisfy the Lindeberg condition (Ass. 3). Then, if ˆθ # »π p → θ∗, we have √n(ˆθ # »π − θ∗) d → N(0, Σ∗), where Σ∗ = H−1 θ∗ V∗H−1 θ∗ . Consequently, for any ˆΣ p → Σ∗, Cα = ( ˆθ # »π j ± z1−α/2 q ˆΣjj n ) is a valid (1 − α)- confidence interval for θ∗ j : lim n→∞ P(θ∗ j ∈ Cα) = 1 − α. The conditions of Theorem 2 are largely the same as in Theorem 1; the main difference is the requirement of convergence of the model–sampling rule pairs, which is similar to the analogous condition of Proposition 2. Proposition 2 and Theorem 2 apply to any sampling rule πt, as long as the variance convergence require- ment is met. We discuss ways to set πt so that the sampling budget nb is met. Our default will be to “spread out” the budget nb over the n observations. We will do so by having an “imaginary” budget for the expected number of collected labels by step t, equal to nb,t = tnb/n. Let n∆,t = nb,t − nlab,t−1 denote the remaining budget at step t. We derive a measure of uncertainty ut from model ft, as before, and let πt(x) = min {ηt ut(x), n∆,t}[0,1] , (8) where ηt normalizes ut(x) and the subscript [0, 1] denotes clipping to [0, 1]. The normalizing constant ηt can be arbitrary, but we find it helpful to set it roughly as ηt = nb/(n E[ut(X)]) and this is what we do in our experiments, with the proviso that we substitute E[ut(X)] with its empirical approximation. In words, the sampling probability is high if the uncertainty is high and we have not used up too much of the sampling budget thus far. Of course, if the model consistently estimates low uncertainty ut(x) throughout, the budget will be underutilized. For this reason, to make sure we use up the budget in practice, we occasionally set πt(x) = ( n∆,t)[0,1] regardless of the reported uncertainty. This periodic deviation from the rule (8) is consistent with the variance convergence conditions required for Proposition 2 and Theorem 2 to hold. 7 Choosing the sampling rule We have seen how to perform inference given an abstract sampling rule, and argued that, intuitively, the sampling rule should be calibrated to the uncertainty of the model’s predictions. Here we argue that this 8is in fact the optimal strategy. In particular, we derive an “oracle” rule, which optimally sets the sampling probabilities so that the variance of ˆθπ is minimized. While the oracle rule cannot be implemented since it depends on unobserved information, it provides an ideal that our algorithms will try to approximate. We discuss ways of tuning the approximations to make them practical and powerful. 7.1 Oracle sampling rules We begin with the optimal sampling rule for mean estimation. We then state the optimal rule for general M-estimation and instantiate it for generalized linear models. Mean estimation Recall the expression for Var( ˆθπ) (2). Given that E \u0002 π−1(X)(Y − f(X))2\u0003 is the only term that depends on π, we define the oracle rule as the solution to: min π E \u0014 1 π(X)(Y − f(X))2 \u0015 s.t. E[π(X)] ≤ nb n . (9) The optimization problem (9) appears in a number of other topics, including importance sampling [37, Ch. 9], constrained utility optimization [7], and, relatedly to our work, survey sampling [47]. The optimality conditions of (9) show that its solution πopt satisfies: πopt(X) ∝ p E[(Y − f(X))2|X], where ∝ ignores the normalizing constant required to make E[πopt(X)] ≤ nb/n. Therefore, the optimal sampling rule is one that samples data points according to the expected magnitude of the model error: the larger the model error, the higher the probability of sampling should be. Of course, E[(Y −f(X))2|X] cannot be known since the label distribution is unknown, and that is why we call πopt an oracle. To develop intuition, it is instructive to consider an even more powerful oracle ˜πopt(X, Y) that is allowed to depend on Y . To be clear, we would commit to the same functional form as in (1) and would seek to minimize Var(ˆθπ) while allowing the sampling probabilities to depend on both X and Y . In this case, by the same argument we conclude that ˜πopt(X, Y) ∝ |Y − f(X)|. (10) The perspective of allowing the oracle to depend on both X and Y is directly prescriptive: a natural way to approximate the rule ˜πopt is to train an arbitrary black-box model u on historical (X, Y) pairs to predict |Y − f(X)| from X. We provide further practical guidelines for sampling rules at the end of this section. General M-estimation In the case of general M-estimation, we cannot hope to minimize the variance of ˆθπ at a fixed sample size n since the finite-sample distribution of ˆθπ is not tractable. However, we can find a sampling rule that minimizes the asymptotic variance of ˆθπ. Since the estimator is potentially multi- dimensional, to make the problem well-posed we assume that we want to minimize the asymptotic variance of a single coordinate ˆθπ j (for example, one coefficient in a multi-dimensional regression). Recall the expression for the asympotic covariance Σ ∗ from Theorem 1. A short derivation shows that Σ∗,jj = E \"\u0012\u0010 ∇ℓθ∗,i − ∇ℓf θ∗,i \u0011⊤ h(j) \u00132 · 1 π(X) # + C, where h(j) is the j-th column of H−1 θ∗ and C is a term that has no dependence on the sampling rule π. Therefore, by the same theory as for mean estimation, the ideal rule πopt(X) would be πopt(X) ∝ q E[((∇ℓθ∗ (X, Y) − ∇ℓθ∗ (X, f(X)))⊤ h(j))2|X]. This recovers πopt for the mean, since ∇ℓθ∗ (x, y) = θ∗ − y and h(j) = 1 for the squared loss. Our measure of uncertainty u(x) should therefore approximate the errors of the predicted gradients along the h(j) direction. 9Generalized linear models (GLMs) We simplify the general solution πopt in the case of generalized linear models (GLMs). We define GLMs as M-estimators whose loss function takes the form ℓθ(x, y) = −log pθ(y|x) = −yx⊤θ + ψ(x⊤θ), for some convex log-partition function ψ. This definition recovers linear regression by taking ψ(s) = 1 2 s2 and logistic regression by taking ψ(s) = log(1 + es). By the definition of the GLM loss, we have ∇ℓθ∗ (x, y) − ∇ℓθ∗ (x, f(x)) = (f(x) − y)x and, therefore, πopt(X) ∝ p E[(f(X) − Y )2|X] · |X⊤h(j)|, where the Hessian is equal to Hθ∗ = E[ψ′′(X⊤θ∗)XX ⊤] and h(j) is the j-th column of H−1 θ∗ . In linear regression, for instance, Hθ∗ = E[XX ⊤]. Again, we see that the model errors play a role in determining the optimal sampling. In particular, again considering the more powerful oracle ˜ πopt(X, Y) that is allowed to set the sampling probabilities according to both X and Y , we get ˜πopt(X, Y) ∝ |f(X) − Y | · |X⊤h(j)|. (11) Therefore, as in the case of the mean, our measure of uncertainty will aim to predict |f(X) − Y | from X and plug those predictions into the above rule. 7.2 Practical sampling rules As explained in Section 5 and Section 6, our sampling rule π(x) will be derived from a measure of un- certainty u(x). As clear from the preceding discussion, the right notion of uncertainty should measure a notion of error dependent on the estimation problem at hand. In particular, we hope to have u(X) ≈ |(∇ℓθ∗ (X, Y) − ∇ℓθ∗ (X, f(X)))⊤ h(j)|. For GLMs and means, in light of Eq. (10) and Eq. (11), this often boils down to training a predictor of |f(X) − Y | from X and, in the case of GLMs, using a plug-in estimate of the Hessian. This is what we do in our experiments (except in the case of binary classification where we simply use the uncertainty from Eq. (3)). Of course, the learned predictor of model errors cannot be perfect; as a result, π(x) ∝ u(x) cannot naively be treated as the oracle rule πopt. For example, the model might mistakenly estimate (near-)zero uncertainty (u(X) ≈ 0) when |f(X) −Y | is large, which would blow up the estimator variance. To fix this issue, we find that it helps to stabilize the rule π(x) ∝ u(x) by mixing it with a uniform rule. Denote the uniform rule by πunif(x) = nb/n. Clearly the uniform rule meets the budget constraint, since n E[πunif(X)] = nb. For a fixed τ ∈ [0, 1] and π(x) ∝ u(x), we define the τ-mixed rule as π(τ)(x) = (1 − τ) · π(x) + τ · πunif(x). Any positive value of τ ensures that π(τ)(x) > 0 for all x, avoiding instability due to small uncertainty estimates u(x). When historical data is available, one can tune τ by optimizing the empirical estimate of the (asymptotic) variance of ˆθπ(τ) given by Theorem 1. For example, in the case of mean estimation, this would correspond to solving: ˆτ = arg min τ∈[0,1] nhX i=1 1 π(τ)(Xh i )(Y h i − f(Xh i ))2, (12) where ( Xh i , Yh i ), . . . ,(Xh nn, Yh nh) are the historical data points. Otherwise, one can set τ to be any user- specified constant. In our experiments, in the batch setting we tune τ on historical data when such data is available. In the sequential setting we simply set τ = 0.5 as the default. 8 Experiments We evaluate active inference on several problems and compare it to two baselines. 10Algorithm 1 Batch active inference Input: unlabeled data X1, . . . , Xn, sampling budget nb, predictive model f, error level α ∈ (0, 1) 1: Choose uncertainty measure u(x) based on f 2: Let π(x) = ˆη u(x), where ˆη = nb nˆE[u(X)] ; let πunif = nb n 3: Select τ ∈ (0, 1) and choose sampling rule π(τ)(x) = (1 − τ) · π(x) + τ · πunif 4: Sample labeling decisions ξi ∼ Bern(π(τ)(Xi)), i∈ [n] 5: Collect labels {Yi : ξi = 1} 6: Compute batch active estimator ˆθπ(τ) (Eq. (6)) Algorithm 2 Sequential active inference Input: unlabeled data X1, . . . , Xn, sampling budget nb, initial predictive model f1, error level α ∈ (0, 1), fine-tuning batch size B 1: Set Dtune ← ∅ 2: for t = 1, . . . , ndo 3: Choose uncertainty measure ut(x) for ft 4: Set πt(x) as in Eq. (8) with ηt = nb nˆE[ut(X)] ; let πunif = nb n 5: Select τ ∈ (0, 1) and choose sampling rule π(τ) t (x) = (1 − τ) · πt(x) + τ · πunif 6: Sample labeling decision ξt ∼ Bern(π(τ) t (Xt)) 7: if ξt = 1 then 8: Collect label Yt 9: Dtune ← Dtune ∪ {(Xt, Yt)} 10: if |Dtune| = B then 11: Fine-tune model on Dtune: ft+1 = finetune(ft, Dtune) 12: Set Dtune ← ∅ 13: else 14: ft+1 ← ft 15: else 16: ft+1 ← ft 17: Compute sequential active estimator ˆθ # »π (τ) (Eq. (7)) The first baseline replaces active sampling with the uniformly random sampling rule πunif. Importantly, this baseline still uses machine learning predictions f(Xi) and corresponds to prediction-powered inference (PPI) [1]. Formally, the prediction-powered estimator is given by ˆθPPI = arg min θ LPPI(θ), where LPPI(θ) = 1 n nX i=1 ℓθ(Xi, f(Xi)) + 1 nb nX i=1 (ℓθ(Xi, Yi) − ℓθ(Xi, f(Xi))) ξi, where ξi ∼ Bern(nb n ). This estimator can be recovered as a special case of estimator (6). The purpose of this comparison is to quantify the benefits of machine-learning-driven data collection. In the rest of this section we refer to this baseline as the “uniform” baseline because the only difference from our estimator is that it replaces active sampling with uniform sampling. The second baseline removes machine learning altogether and computes the “classical” estimate based on uniformly random sampling, ˆθnoML = arg min θ 1 nb nX i=1 ℓθ(Xi, Yi)ξi, where ξi ∼ Bern(nb n ). This baseline serves to evaluate the cumulative benefits of machine learning for data collection and inference combined. We refer to this baseline as the “classical” baseline, or classical inference, in the rest of this section. For all methods we compute standard confidence intervals based on asymptotic normality. The target 110.5 0.6 0.7 0.8 approval rate 220 304 418 576 792 nb 0.04 0.05 0.08 0.12 0.18interval width 221 364 507 650 793 nb 0.6 0.7 0.8 0.9 1.0coverage active uniform classical 0.2 0.3 0.4 approval rate 222 305 420 577 795 nb 0.04 0.05 0.07 0.09 0.12interval width 222 365 508 651 795 nb 0.6 0.7 0.8 0.9 1.0coverage active uniform classical Figure 1: Post-election survey research. Example intervals in five randomly chosen trials (left), average confidence interval width (middle), and coverage (right) for the average approval of Joe Biden’s (top) and Donald Trump’s (bottom) political messaging to the country following the 2020 US presidential election. error level is α = 0.1 throughout. We report the average interval width and coverage for varying sample sizes nb, averaged over 1000 and 100 trials for the batch and sequential settings, respectively. We plot the interval width on a log–log scale. We also report the percentage of budget saved by active inference relative to the baselines when the methods are matched to be equally accurate. More precisely, for varying nb we compute the average interval width achieved by the uniform and classical baselines; then, we look for the budget size nactive b for which active inference achieves the same average interval width, and report (nb −nactive b )/nb ·100% as the percentage of budget saved. The batch and sequential active inference methods used in our experiments are outlined in Algorithm 1 and Algorithm 2, respectively. Each application will specify the general parameters from the algorithm state- ments. We defer some experimental details, such as the choices of τ, to Appendix B. Code for reproducing the experiments is available at https://github.com/tijana-zrnic/active-inference. 8.1 Post-election survey research We apply active inference to survey data collected by the Pew Research Center following the 2020 United States presidential election [38]. We focus on one specific question in the survey, aimed at gauging people’s approval of the presidential candidates’ political messaging following the election. The target of inference is the average approval rate of Joe Biden’s (Donald Trump’s, respectively) political messaging. Approval is encoded as a binary response, Yi ∈ {0, 1}. The respondents—a nationally representative pool of US adults—provide background information such as age, gender, education, political affiliation, etc. We show that, by training a machine learning model to predict people’s approval from their background information and measuring the model’s uncertainty, we can allocate the per-question budget in a way that achieves higher statistical power than uniform allocation. Careful budget allocation is important, because Pew pays each respondent proportionally to the number of questions they answer. We use half of all available data for the analysis; for the purpose of evaluating coverage, we take the average approval on all available data as the ground truth θ∗. To obtain the predictive model f, we train an XGBoost model [11] on the half of the data not used for the analysis. Since approval is encoded as a binary response, we use the measure of uncertainty from Eq. (3). 12800 1000 1200 1400 regression coefficient 189 444 1040 2435 5701 nb 69 127 236 437 809interval width 190 1567 2945 4323 5701 nb 0.6 0.7 0.8 0.9 1.0coverage active uniform classical Figure 2: Census data analysis. Example intervals in five randomly chosen trials (left), average confidence interval width (middle), and coverage (right) for the linear regression coefficient quantifying the relationship between age and income, controlling for sex, in US Census data. 2 4 6 odds ratio 108 228 482 1021 2159 nb 0.57 1.09 2.08 3.99 7.64interval width 108 621 1134 1647 2160 nb 0.6 0.7 0.8 0.9 1.0coverage active uniform classical Figure 3: AlphaFold-assisted proteomics research. Example intervals in five randomly chosen trials (left), average confidence interval width (middle), and coverage (right) for the odds ratio between phospho- rylation and being part of an IDR. In Figure 1 we compare active inference to the uniform (PPI) and classical baselines. All methods meet the coverage requirement. Across different values of the budget nb, active sampling reduces the confidence interval width of the uniform baseline (PPI) by a significant margin (at least ∼ 10%). Classical inference is highly suboptimal compared to both alternatives. In Figure 4 we report the percentage of budget saved due to active sampling. For estimating Biden’s approval, we observe an over 85% save in budget over classical inference and around 25% save over the uniform baseline. For estimating Trump’s approval, we observe an over 70% save in budget over classical inference and around 25% save over the uniform baseline. 8.2 Census data analysis Next, we study the American Community Survey (ACS) Public Use Microdata Sample (PUMS) collected by the US Census Bureau. ACS PUMS is an annual survey that collects information about citizenship, education, income, employment, and other factors previously contained only in the long form of the decennial census. We use the Folktables [15] interface to download the data. We investigate the relationship between age and income in survey data collected in California in 2019, controlling for sex. Specifically, we target the linear regression coefficient when regressing income on age and sex (that is, its age coordinate). Analogously to the previous application, we use half of all available data for the analysis and train an XGBoost model [11] of a person’s income from the available demographic covariates on the other half. As the ground-truth value of the target θ∗, we take the corresponding linear regression coefficient computed on all available data. To quantify the model’s uncertainty, we use the strategy described in Section 4, training a separate XGBoost model e(·) to predict |f(X)−Y | from X. Then, we set the uncertainty u(x) as prescribed in Eq. (11), replacing |f(X) − Y | by e(X). The interval widths and coverage are shown in Figure 2. As in the previous application, all methods approximately achieve the target coverage, however this time we observe more extreme gains over the uniform baseline (PPI): the interval widths almost double when going from active sampling to uniform sampling. Of 13250 500 750 1000 1250 nb 0 25 50 75 100 budget save  over classical (%) Post-election research Biden Trump 250 500 750 1000 1250 nb 0 25 50 75 100 budget save  over uniform (%) Biden Trump 2000 3000 4000 5000 nb 0 25 50 75 100  Census analysis 2000 3000 4000 5000 nb 0 25 50 75 100 500 1000 1500 2000 nb 0 25 50 75 100  AlphaFold 500 1000 1500 2000 nb 0 25 50 75 100 Figure 4: Save in sample budget due to active inference. Reduction in sample size required to achieve the same confidence interval width with active inference and (top) classical inference and (bottom) uniform sampling, respectively, across the applications shown in Figures 1-3. course, the improvement of active inference over classical inference is even more substantial. The large gains of active sampling can also be seen in Figure 4: we save around 80% of the budget over classical inference and over 60% over the uniform baseline. 8.3 AlphaFold-assisted proteomics research Inspired by the findings of Bludau et al. [9] and the subsequent analysis of Angelopoulos et al. [1], we study the odds ratio of a protein being phosphorylated, a functional property of a protein, and being part of an intrinsically disordered region (IDR), a structural property. The latter can only be obtained from knowledge about the protein structure, which can in turn be measured to a high accuracy only via expensive experimental techniques. To overcome this challenge, Bludau et al. used AlphaFold predictions [26] to estimate the odds ratio. AlphaFold is a machine learning model that predicts a protein’s structure from its amino acid sequence. Angelopoulos et al. [1] showed that forming a classical confidence interval around the odds ratio based on AlphaFold predictions is not valid given that the predictions are imperfect. They provide a valid alternative assuming access to a small subset of proteins with true structure measurements, uniformly sampled from the larger population of proteins of interest. We show that, by strategically choosing which protein structures to experimentally measure, active infer- ence allows for intervals that retain validity and are tighter than intervals based on uniform sampling. Natu- rally, for the purpose of evaluating validity, we restrict the analysis to proteins where we have gold-standard structure measurements; we use the post-processed AlphaFold outputs made available by Angelopoulos et al. [1], which predict the IDR property based on the raw AlphaFold output. We leverage the predictions to guide the choice of which structures to experimentally derive, subject to a budget constraint. The odds ratio we aim to estimate is defined as: θ∗ = µ1/(1 − µ1) µ0/(1 − µ0), where µ1 = P(Y = 1|Xph = 1) and µ0 = P(Y = 1|Xph = 0); Y is a binary indicator of disorder and Xph is a binary indicator of phosphorylation. While the odds ratio is not a solution to an M-estimation problem, it is a function of two means, µ1 and µ0 (see also [1, 3]). Confidence intervals can thus be computed by applying the delta method to the asymptotic normality result for the mean. Since Y is binary, we use the measure of uncertainty from Eq. (3) to estimate µ1 and µ0. For the purpose of evaluating coverage, we take the empirical odds ratio computed on the whole dataset as the ground-truth value of θ∗. Figure 3 shows the interval widths and coverage for the three methods, and Figure 4 shows the percentage of budget saved due to adaptive data collection. The gains are substantial: over 75% of the budget is saved in comparison to classical inference, and around 20 − 25% is saved in comparison to the uniform baseline 140.68 0.70 0.72 approval rate 1855 2370 3029 3871 4946 nb 0.019 0.022 0.026 0.031 0.038interval width 1855 2628 3401 4174 4947 nb 0.6 0.7 0.8 0.9 1.0coverage active (w/ fine-tuning) active (no fine-tuning) uniform 0.28 0.30 0.32 approval rate 1860 2376 3037 3881 4961 nb 0.018 0.022 0.027 0.032 0.039interval width 1860 2635 3410 4185 4961 nb 0.6 0.7 0.8 0.9 1.0coverage active (w/ fine-tuning) active (no fine-tuning) uniform Figure 5: Post-election survey research with fine-tuning. Example intervals in five randomly chosen trials (left), average confidence interval width (middle), and coverage (right) for the average approval of Joe Biden’s (top) and Donald Trump’s (bottom) political messaging to the country following the 2020 US presidential election. Active inference with no fine-tuning and inference with uniformly sampled data use the same model. (PPI). Given the cost of experimental measurement techniques in proteomics, this save in sample size would imply a massive save in cost. 8.4 Post-election survey research with fine-tuning We return to the example from Section 8.1, this time evaluating the benefits of sequential fine-tuning. We compare active inference, with and without fine-tuning, and PPI, which relies on uniform sampling. We show that active inference with no fine-tuning can hurt compared to PPI if the former uses a poorly trained model; fine-tuning, on the other hand, remedies this issue. The predictive model may be poorly trained due to little or no historical data; sequential fine-tuning is necessary in such cases. We train an XGBoost model on only 10 labeled examples and use this model for active inference with no fine-tuning and PPI. The latter is similar to the former in the sense that it only replaces active with uniform sampling. Active inference with fine-tuning continues to fine-tune the model with every B = 100 new survey responses, also updating the sampling rule via update (8). The uncertainty measure ut(x) is given by Eq. (3), as before. As discussed in Section 6, we also periodically use up the remaining budget regardless of the computed uncertainty in order to avoid underutilizing the budget (in particular, every 100n/nb steps). We fine-tune the model using the training continuation feature of XGBoost. The interval widths and coverage are reported in Figure 5. We find that fine-tuning substantially improves inferential power and retains correct coverage. In Figure 7 we show the save in sample size budget over active inference with no fine-tuning and inference based on uniform sampling, i.e. PPI. For estimating Biden’s approval, we observe a gain of around 40% and 30% relative to active inference without fine-tuning and PPI, respectively. For Trump’s approval, we observe even larger gains around 45% and 35%, respectively. 8.5 Census data analysis with fine-tuning We similarly evaluate the benefits of sequential fine-tuning in the problem setting from Section 8.2. We again compare active inference, with and without fine-tuning, and PPI, i.e., active inference with a trivial, uniform sampling rule. Recall that in Section 8.2 we trained a separate model e to predict the prediction errors, which we in turn used to form the uncertainty u(x) according to Eq. (11). This time we fine-tune both the prediction model, ft, and the error model, et. 15900 1000 1100 1200 regression coefficient 3799 4999 6580 8660 11399 nb 70 88 110 138 173interval width 3799 5699 7599 9499 11399 nb 0.6 0.7 0.8 0.9 1.0coverage active (w/ fine-tuning) active (no fine-tuning) uniform Figure 6: Census data analysis with fine-tuning. Example intervals in five randomly chosen trials (left), average confidence interval width (middle), and coverage (right) for the linear regression coefficient quantifying the relationship between age and income, controlling for sex, in US Census data. Active inference with no fine-tuning and inference with uniformly sampled data use the same model. 4000 4500 5000 nb 0 25 50 75 100 budget save over  no fine-tuning (%) Post-election research Biden Trump 3500 4000 4500 5000 nb 0 25 50 75 100 budget save  over uniform (%) Biden Trump 8000 9000 10000 11000 nb 0 25 50 75 100  Census analysis 7000 8000 9000 10000 11000 nb 0 25 50 75 100 Figure 7: Save in sample size budget due to fine-tuning. Reduction in sample size required to achieve the same confidence interval width with active inference with fine-tuning and (top) active inference with no fine-tuning and (bottom) the uniform baseline (PPI), respectively, in the applications shown in Figure 5 and Figure 6. We train initial XGBoost models f1 and e1 on 100 labeled examples. We use f1 for PPI and both f1 and e1 for active inference with no fine-tuning. Active inference with fine-tuning continues to fine-tune the two models with every B = 1000 new survey responses, also updating the model uncertainty via update (8). We fine-tune the models using the training continuation feature of XGBoost. We compute ut from et based on Eq. (11). As discussed earlier, we also periodically use up the remaining budget regardless of the computed uncertainty in order to avoid underutilizing the budget (in particular, every 500 n/nb steps). We show the interval widths and coverage in Figure 6. We see that the gains of fine-tuning are significant and increase as nb increases. In Figure 7 we show the save in sample size budget. Fine-tuning saves around 32 − 40% over the baseline with no fine-tuning and around 20 − 30% over the uniform baseline. Moreover, the save increases as the sample budget grows because the prediction problem is difficult and the model’s performance keeps improving even after 10000 training examples. Acknowledgements We thank Lihua Lei, Jann Spiess, and Stefan Wager for many insightful comments and pointers to relevant work. T.Z. was supported by Stanford Data Science through the Fellowship program. E.J.C. was supported by the Office of Naval Research grant N00014-20-1-2157, the National Science Foundation grant DMS- 2032014, the Simons Foundation under award 814641, and the ARO grant 2003514594. 16References [1] Anastasios N Angelopoulos, Stephen Bates, Clara Fannjiang, Michael I Jordan, and Tijana Zrnic. Prediction-powered inference. Science, 382(6671):669–674, 2023. [2] Anastasios N Angelopoulos, Stephen Bates, Clara Fannjiang, Michael I Jordan, and Tijana Zrnic. Prediction-powered inference: Data sets, 2023. URL https://doi.org/10.5281/zenodo.8397451. [3] Anastasios N Angelopoulos, John C Duchi, and Tijana Zrnic. PPI++: Efficient prediction-powered inference. arXiv preprint arXiv:2311.01453 , 2023. [4] Jordan T Ash, Chicheng Zhang, Akshay Krishnamurthy, John Langford, and Alekh Agarwal. Deep batch active learning by diverse, uncertain gradient lower bounds. arXiv preprint arXiv:1906.03671 , 2019. [5] David Azriel, Lawrence D Brown, Michael Sklar, Richard Berk, Andreas Buja, and Linda Zhao. Semi- supervised linear regression. Journal of the American Statistical Association, 117(540):2238–2251, 2022. [6] Maria-Florina Balcan, Alina Beygelzimer, and John Langford. Agnostic active learning. In Proceedings of the 23rd international conference on Machine learning , pages 65–72, 2006. [7] Maria-Florina Balcan, Amit Daniely, Ruta Mehta, Ruth Urner, and Vijay V Vazirani. Learning economic parameters from revealed preferences. In Web and Internet Economics: 10th International Conference, WINE 2014, Beijing, China, December 14-17, 2014. Proceedings 10 , pages 338–353. Springer, 2014. [8] Debopam Bhattacharya and Pascaline Dupas. Inferring welfare maximizing treatment assignment under budget constraints. Journal of Econometrics , 167(1):168–196, 2012. [9] Isabell Bludau, Sander Willems, Wen-Feng Zeng, Maximilian T Strauss, Fynn M Hansen, Maria C Tanzer, Ozge Karayel, Brenda A Schulman, and Matthias Mann. The structural context of posttrans- lational modifications at a proteome-wide scale. PLoS biology, 20(5):e3001636, 2022. [10] Yash Chandak, Shiv Shankar, Vasilis Syrgkanis, and Emma Brunskill. Adaptive instrument design for indirect experiments. arXiv preprint arXiv:2312.02438 , 2023. [11] Tianqi Chen and Carlos Guestrin. Xgboost: A scalable tree boosting system. In Proceedings of the 22nd acm sigkdd international conference on knowledge discovery and data mining , pages 785–794, 2016. [12] Chen Cheng, Hilal Asi, and John Duchi. How many labelers do you have? a closer look at gold-standard labels. arXiv preprint arXiv:2206.12041 , 2022. [13] Victor Chernozhukov, Denis Chetverikov, Mert Demirer, Esther Duflo, Christian Hansen, Whitney Newey, and James Robins. Double/debiased machine learning for treatment and structural parameters, 2018. [14] Thomas Cook, Alan Mishler, and Aaditya Ramdas. Semiparametric efficient inference in adaptive experiments. arXiv preprint arXiv:2311.18274 , 2023. [15] Frances Ding, Moritz Hardt, John Miller, and Ludwig Schmidt. Retiring adult: New datasets for fair machine learning. Advances in neural information processing systems , 34:6478–6490, 2021. [16] Rick Durrett. Probability: theory and examples , volume 49. Cambridge university press, 2019. [17] Aryeh Dvoretzky. Asymptotic normality for sums of dependent random variables. In Proceedings of the Sixth Berkeley Symposium on Mathematical Statistics and Probability, Volume 2: Probability Theory , volume 6, pages 513–536. University of California Press, 1972. [18] Yarin Gal, Riashat Islam, and Zoubin Ghahramani. Deep Bayesian active learning with image data. In International conference on machine learning , pages 1183–1192. PMLR, 2017. 17[19] Feng Gan and Wanfeng Liang. Prediction de-correlated inference. arXiv preprint arXiv:2312.06478 , 2023. [20] Vitor Hadad, David A Hirshberg, Ruohan Zhan, Stefan Wager, and Susan Athey. Confidence intervals for policy evaluation in adaptive experiments. Proceedings of the national academy of sciences, 118(15): e2014602118, 2021. [21] Jinyong Hahn, Keisuke Hirano, and Dean Karlan. Adaptive experimental design using the propensity score. Journal of Business & Economic Statistics , 29(1):96–108, 2011. [22] Steve Hanneke et al. Theory of disagreement-based active learning. Foundations and Trends ® in Machine Learning, 7(2-3):131–309, 2014. [23] Feifang Hu and William F Rosenberger. The theory of response-adaptive randomization in clinical trials. John Wiley & Sons, 2006. [24] Neal Jean, Marshall Burke, Michael Xie, W Matthew Davis, David B Lobell, and Stefano Ermon. Combining satellite imagery and machine learning to predict poverty. Science, 353(6301):790–794, 2016. [25] Ajay J Joshi, Fatih Porikli, and Nikolaos Papanikolopoulos. Multi-class active learning for image clas- sification. In 2009 ieee conference on computer vision and pattern recognition , pages 2372–2379. IEEE, 2009. [26] John Jumper, Richard Evans, Alexander Pritzel, Tim Green, Michael Figurnov, Olaf Ronneberger, Kathryn Tunyasuvunakool, Russ Bates, AugustinˇZ´ ıdek, Anna Potapenko, et al. Highly accurate protein structure prediction with alphafold. Nature, 596(7873):583–589, 2021. [27] Graham Kalton. Introduction to survey sampling . Number 35. Sage Publications, 2020. [28] Maximilian Kasy and Anja Sautmann. Adaptive treatment assignment in experiments for policy choice. Econometrica, 89(1):113–132, 2021. [29] Masahiro Kato, Takuya Ishihara, Junya Honda, and Yusuke Narita. Efficient adaptive experimental design for average treatment effect estimation. arXiv preprint arXiv:2002.05308 , 2020. [30] Mohammad GM Khan, Karuna G Reddy, and Dinesh K Rao. Designing stratified sampling in economic and business surveys. Journal of applied statistics , 42(10):2080–2099, 2015. [31] Tze Leung Lai and Herbert Robbins. Asymptotically efficient adaptive allocation rules. Advances in applied mathematics, 6(1):4–22, 1985. [32] John A List, Sally Sadoff, and Mathis Wagner. So you want to run an experiment, now what? some simple rules of thumb for optimal experimental design. Experimental Economics, 14:439–457, 2011. [33] Jiacheng Miao, Xinran Miao, Yixuan Wu, Jiwei Zhao, and Qiongshi Lu. Assumption-lean and data- adaptive post-prediction inference. arXiv preprint arXiv:2311.14220 , 2023. [34] Keshav Motwani and Daniela Witten. Valid inference after prediction. arXiv preprint arXiv:2306.13746, 2023. [35] Dankit K Nassiuma. Survey sampling: Theory and methods, 2001. [36] Francesco Orabona and Kwang-Sung Jun. Tight concentrations and confidence sequences from the regret of universal portfolio. IEEE Transactions on Information Theory , 2023. [37] Art B. Owen. Monte Carlo theory, methods and examples . https://artowen.su.domains/mc/, 2013. [38] Pew. American trends panel (ATP) wave 79, 2020. URL https://www.pewresearch.org/science/ dataset/american-trends-panel-wave-79/ . 18[39] Pengzhen Ren, Yun Xiao, Xiaojun Chang, Po-Yao Huang, Zhihui Li, Brij B Gupta, Xiaojiang Chen, and Xin Wang. A survey of deep active learning. ACM computing surveys (CSUR) , 54(9):1–40, 2021. [40] Herbert Robbins. Some aspects of the sequential design of experiments. 1952. [41] James M Robins and Andrea Rotnitzky. Semiparametric efficiency in multivariate regression models with missing data. Journal of the American Statistical Association , 90(429):122–129, 1995. [42] James M Robins, Andrea Rotnitzky, and Lue Ping Zhao. Estimation of regression coefficients when some regressors are not always observed. Journal of the American statistical Association , 89(427):846–866, 1994. [43] Esther Rolf, Jonathan Proctor, Tamma Carleton, Ian Bolliger, Vaishaal Shankar, Miyabi Ishihara, Benjamin Recht, and Solomon Hsiang. A generalizable and accessible approach to machine learning with global satellite imagery. Nature communications, 12(1):4392, 2021. [44] D Rubin. Multiple imputation for nonresponse in surveys. Wiley Series in Probability and Statistics , page 1, 1987. [45] Donald B Rubin. Inference and missing data. Biometrika, 63(3):581–592, 1976. [46] Donald B Rubin. Multiple imputation after 18+ years. Journal of the American statistical Association , 91(434):473–489, 1996. [47] Carl Erik S¨ arndal. Onπ-inverse weighting versus best linear unbiased weighting in probability sampling. Biometrika, 67(3):639–650, 1980. [48] Carl-Erik S¨ arndal, Bengt Swensson, and Jan Wretman.Model assisted survey sampling. Springer Science & Business Media, 2003. [49] Greg Schohn and David Cohn. Less is more: Active learning with support vector machines. In ICML, volume 2, page 6, 2000. [50] Burr Settles. Active learning literature survey. Department of Computer Sciences, University of Wisconsin-Madison, 2009. [51] Simon Tong and Daphne Koller. Support vector machine active learning with applications to text classification. Journal of machine learning research , 2(Nov):45–66, 2001. [52] Aad W Van der Vaart. Asymptotic statistics, volume 3. Cambridge university press, 2000. [53] Harit Vishwakarma, Heguang Lin, Frederic Sala, and Ramya Korlakai Vinayak. Promises and pitfalls of threshold-based auto-labeling. Advances in Neural Information Processing Systems , 36, 2023. [54] Ian Waudby-Smith and Aaditya Ramdas. Estimating means of bounded random variables by betting. Journal of the Royal Statistical Society Series B: Statistical Methodology , 86(1):1–27, 2024. [55] Michael Xie, Neal Jean, Marshall Burke, David Lobell, and Stefano Ermon. Transfer learning from deep features for remote sensing and poverty mapping. In Proceedings of the AAAI conference on artificial intelligence, volume 30, 2016. [56] Matt Zdun. Machine politics: How America casts and counts its votes. Reuters, 2022. [57] Anru Zhang, Lawrence D Brown, and T Tony Cai. Semi-supervised inference: General theory and estimation of means. Annals of Statistics , 47(5):2538–2566, 2019. [58] Jiaqi Zhang, Louis Cammarata, Chandler Squires, Themistoklis P Sapsis, and Caroline Uhler. Active learning for optimal intervention design in causal models. Nature Machine Intelligence , pages 1–10, 2023. [59] Kelly Zhang, Lucas Janson, and Susan Murphy. Statistical inference with M-estimators on adaptively collected data. Advances in neural information processing systems , 34:7460–7471, 2021. 19[60] Yuqian Zhang and Jelena Bradic. High-dimensional semi-supervised learning: in search of optimal inference of the mean. Biometrika, 109(2):387–403, 2022. [61] Tijana Zrnic and Emmanuel J Cand` es. Cross-prediction-powered inference.Proceedings of the National Academy of Sciences, 121(15):e2322083121, 2024. 20A Proofs A.1 Proof of Proposition 1 Recall that ξi ∼ Bern(πˆη(Xi)). For any η ∈ H, we define ξη i = 1{πη(Xi) ≤ πˆη(Xi)}ξi(1 − ξ≤ i ) + 1{πη(Xi) > πˆη(Xi)}(ξi + (1 − ξi)ξ> i ), (13) where ξ≤ i ∼ Bern(πˆη(Xi)−πη(Xi) πˆη(Xi) ) and ξ> i ∼ Bern(πη(Xi)−πˆη(Xi) 1−πˆη(Xi) ) are drawn independently of ξi. This defini- tion couples ξη∗ i with ξi, while ensuring that ξη∗ i ∼ Bern(πη∗ (Xi)). Let ˆθη∗ = 1 n nX i=1   f(Xi) + (Yi − f(Xi)) ξη∗ i πη∗ (Xi) ! . By the central limit theorem, we know that √n(ˆθη∗ − θ∗) d → N(0, σ2 ∗), (14) where σ2 ∗ = Var \u0010 f(X) + (Y − f(X)) ξη∗ πη∗ (X) \u0011 . On the other hand, we have √n(ˆθˆη − θ∗) = √n(ˆθη∗ − θ∗) + √n(ˆθˆη − ˆθη∗ ). For any ϵ >0, we have P(|√n(ˆθˆη − ˆθη∗ )| ≥ϵ) ≤ P(ˆη ̸= η∗) → 0; therefore, √n(ˆθˆη − ˆθη∗ ) p → 0. Putting this fact together with Eq. (14), we conclude that √n(ˆθˆη − θ∗) d → N(0, σ2 ∗) by Slutsky’s theorem. A.2 Proof of Theorem 1 The proof follows a similar argument as the classical proof of asymptotic normality for M-estimation; see [52, Thm. 5.23]. A similar proof is also given for the prediction-powered estimator [3], which is closely related to our active inference estimator. The main difference between our proof and the classical proof is that ˆ η is tuned in a data-adaptive fashion, so the increments in the empirical loss Lπˆη (θ) are not independent. We begin by formally stating the required smoothness assumption. Assumption 1 (Smoothness). The loss ℓ is smooth if: • ℓθ(x, y) is differentiable at θ∗ for all (x, y); • ℓθ is locally Lipschitz around θ∗: there is a neighborhood of θ∗ such that ℓθ(x, y) is C(x, y)-Lipschitz and ℓθ(x, f(x)) is C(x)-Lipschitz in θ, where E[C(X, Y)2] < ∞, E[C(X)2] < ∞; • L(θ) = E[ℓθ(X, Y)] and Lf (θ) = E[ℓθ(X, f(X))] have Hessians, and Hθ∗ = ∇2L(θ∗) ≻ 0. Using the same definition ofξη i as in Eq. (13), let Lη θ,i = ℓθ(Xi, f(Xi))+(ℓθ(Xi, Yi) − ℓθ(Xi, f(Xi))) ξη i πη(Xi) . We define ∇Lη θ,i analogously, replacing the losses with their gradients. Given a function g, let Gn[g(Lη θ)] := 1√n nX i=1 \u0010 g(Lη θ,i) − E[g(Lη θ,i)] \u0011 ; En[g(Lη θ)] := 1 n nX i=1 g(Lη θ,i). We similarly use Gn[g(∇Lη θ)], En[g(∇Lη θ)], etc. Notice that En[Lˆη θ] = Lπˆη (θ). By the differentiability and local Lipschitzness of the loss, for any hn = OP (1) we have Gn[√n(Lη∗ θ∗+hn/√n − Lη∗ θ∗ ) − h⊤ n ∇Lη∗ θ∗ ] p → 0. 21By definition, this is equivalent to nEn[Lη∗ θ∗+hn/√n − Lη∗ θ∗ ] = n(L(θ∗ + hn/√n) − L(θ∗)) + h⊤ n Gn[∇Lη∗ θ∗ ] + oP (1), where L(θ) = E[ℓθ(X, Y)] is the population loss. A second-order Taylor expansion now implies nEn[Lη∗ θ∗+hn/√n − Lη∗ θ∗ ] = 1 2h⊤ n Hθ∗ hn + h⊤ n Gn[∇Lη∗ θ∗ ] + oP (1). At the same time, since P(ˆη ̸= η∗) → 0, we have nEn[Lˆη θ∗+hn/√n − Lˆη θ∗ ] = nEn[Lη∗ θ∗+hn/√n − Lη∗ θ∗ ] + oP (1). Putting everything together, we have shown nEn[Lˆη θ∗+hn/√n − Lˆη θ∗ ] = 1 2h⊤ n Hθ∗ hn + h⊤ n Gn[∇Lη∗ θ∗ ] + oP (1). The rest of the proof is standard. We apply the previous display with hn = ˆhn := √n(ˆθˆη − θ∗) (which is OP (1) by the consistency of ˆθη∗ ; see [52, Thm. 5.23]) and hn = ˜hn := −H−1 θ∗ Gn[∇Lη∗ θ∗ ]: nEn[Lˆη ˆθˆη − Lˆη θ∗ ] = 1 2 ˆh⊤ n Hθ∗ ˆhn + ˆh⊤ n Gn[∇Lη∗ θ∗ ] + oP (1); nEn[Lˆη θ∗+˜hn/√n − Lˆη θ∗ ] = 1 2 ˜h⊤ n Hθ∗ ˜hn + ˜h⊤ n Gn[∇Lη∗ θ∗ ] + oP (1). By the definition of ˆθˆη, the left-hand side of the first equation is smaller than the left-hand side of the second equation. Therefore, the same must be true of the right-hand sides of the equations. If we take the difference between the equations and complete the square, we get 1 2 \u0010√n(ˆθˆη − θ∗) − ˜hn \u0011⊤ Hθ∗ \u0010√n(ˆθˆη − θ∗) − ˜hn \u0011 + oP (1) ≤ 0. Since the Hessian Hθ∗ is positive-definite, it must be the case that √n(ˆθˆη − θ∗) − ˜hn p → 0. By the central limit theorem, ˜hn = −H−1 θ∗ Gn[∇Lη∗ θ∗ ] converges to N(0, Σ∗) in distribution, where Σ∗ = H−1 θ∗ Var \u0012 ∇ℓθ∗ (X, f(X)) + (∇ℓθ∗ (X, Y) − ∇ℓθ∗ (X, f(X))) ξη∗ πη∗ (X) \u0013 H−1 θ∗ . The final statement thus follows by Slutsky’s theorem. A.3 Proof of Proposition 2 We prove the result by an application of the martingale central limit theorem (see Theorem 8.2.4. in [16]). Let ¯∆t denote the increments ∆ t with their mean subtracted out, i.e. ¯∆t = ∆ t − θ∗. To apply the theorem, we first need to verify that the increments ¯∆t = ∆t − θ∗ are martingale increments; this follows because E[ ¯∆t|Ft−1] = E[ ¯∆t|ft, πt] = E[ft(Xt)|ft, πt] + E[Yt − ft(Xt)|ft, πt]E \u0014 ξt πt(Xt)|ft, πt \u0015 − θ∗ = 0, together with the fact that ¯∆t ∈ Ft. The martingale central limit theorem is now applicable given two regularity conditions. The first is that 1 n Pn t=1 σ2 t converges in probability, which holds by assumption. The second condition is the so-called Lindeberg condition, stated below. 22Assumption 2. Let ¯∆t = ∆t − θ∗. We say that ∆t satisfy the Lindeberg condition if for all ϵ >0, 1 n nX t=1 E[ ¯∆2 t 1{|¯∆t| > ϵ√n}|Ft−1] p → 0. Since this condition holds by assumption, we can apply the central limit theorem to conclude √n(ˆθ # »π −θ∗) = 1√n Pn t=1 ¯∆t d → N(0, σ2 ∗). A.4 Proof of Theorem 2 We follow a similar approach as in the proof of Theorem 1, which is in turn similar to the classical argument for M-estimation [52, Thm. 5.23]. In this case, the main difference to the classical proof is that the empirical loss L # »π (θ) comprises martingale, rather than i.i.d. increments. We explain the differences relative to the proof of Theorem 1. We define Lθ,i = ℓθ(Xi, fi(Xi)) + (ℓθ(Xi, Yi) − ℓθ(Xi, fi(Xi))) ξi πi(Xi) , and ∇Lθ,i is defined analogously. We again use the notation Gn[g(Lθ)], En[g(Lθ)], Gn[g(∇Lθ)], En[g(∇Lθ)], etc. As in the classical argument, for any hn = OP (1) we have Gn[√n(Lθ∗+hn/√n − Lθ∗ ) − h⊤ n ∇Lθ∗ ] p → 0. This can be concluded from the martingale central limit theorem, since the variance of the increments tends to zero. Specifically, define the triangular array Ln,i = √n(Lθ∗+hn/√n,i − Lθ∗,i) − h⊤ n ∇Lθ∗,i, and let Vn,i = Var(Ln,i|Fi−1). We have | 1√n Pn i=1 Ln,i| ≤maxi p Vn,i| 1√n Pn i=1 Ln,i√ Vn,i |. By the martingale central limit theorem, 1√n Pn i=1 Ln,i√ Vn,i d → N(0, 1) and, since max i p Vn,i p → 0, we conclude by Slutsky’s theorem that Gn[√n(Lθ∗+hn/√n − Lθ∗ ) − h⊤ n ∇Lθ∗ ] p → 0. The following steps are the same as in the proof of Theorem 1; we conclude that √n(ˆθ # »π − θ∗) − ˜hn p → 0, where ˜hn = −H−1 θ∗ Gn[∇Lθ∗ ]. Finally, we argue that ˜hn converges to N(0, Σ∗) in distribution. To see this, first note that all one-dimensional projections v⊤˜hn converge to v⊤Z, Z ∼ N(0, Σ∗), by the martingale central limit theorem, which is applicable because the Lindeberg condition holds by assumption (see below for statement) and the variance process Vθ∗,n converges to V∗. Once we have the convergence of all one- dimensional projections, convergence of ˜hn follows by the Cram´ er-Wold theorem. Assumption 3. We say that the increments satisfy the Lindeberg condition if, for all v ∈ Sd−1 and ϵ >0, 1 n nX t=1 E[(v⊤∇Lθ∗,t)21{|v⊤∇Lθ∗,t| > ϵ√n}|Ft−1] p → 0. B Experimental details In all our experiments, we have a labeled dataset of n examples. We treat the solution on the full dataset as the ground-truth θ∗ for the purpose of evaluating coverage. In each trial, the underlying data points (Xi, Yi) are fixed and the randomness comes from the labeling decisions ξi. In the sequential experiments, we additionally randomly permute the data points at the beginning of each trial. The experiments in the batch setting average the results over 1000 trials and the experiments in the sequential setting average the results over 100 trials. The Pew dataset is available at [38]; the census dataset is available through Folktables [15]; the Alphafold dataset is available at [2]. As discussed in Section 7.2, to avoid values of π(x) that are close to zero we mix the “standard” sampling rule based on the uncertainty u(x) with a uniform rule πunif = nb n according to a parameter τ ∈ (0, 1). In post-election survey research, we have training data for the prediction model and we use the same data to select τ so as to minimize an empirical approximation of the variance Var( ˆθπ(τ) ), as in Eq. (12). In the 23AlphaFold example and both problems with model fine-tuning we set τ = 0.5 for simplicity. In the census example, the trained predictor of model error e(x) rarely gives very small values, and so we set τ = 0.001. In each experiment, we vary nb over a grid of uniformly spaced values. We take 20 grid values for the batch experiments and 10 grid values for the sequential experiments. The plots of interval width and coverage linearly interpolate between the respective values obtained at the grid points. There linearly interpolated values are used to produce the plots of budget save: for all values of nb from the grid, we look for n′ b such that the (linearly interpolated) width of active inference at sample size n′ b matches the interval width of classical (resp. uniform) inference at sample size nb. For the leftmost plot in Figures 1-3 and Figures 5-6, we uniformly sample five trials for a fixed nb and show the intervals for all methods in those same five trials. We arbitrarily select nb to be the fourth largest value in the grid of budget sizes for all experiments. C Non-asymptotic results While our results focus on asymptotic confidence intervals based on the central limit theorem, some of them—in particular, those for mean estimation—have direct non-asymptotic and time-uniform analogues. We explain this extension for the sequential algorithm, as it subsumes the extension for the batch setting. Let ∆t = ft(Xt) + (Yt − ft(Xt)) ξt πt(Xt) . As explained in Section 6, ∆ t have a common conditional mean: E[∆t|∆1, . . . ,∆t−1] = θ∗. Moreover, if Yt and ft(Xt) are almost surely bounded, and πt(Xt) is almost surely bounded from below, then ∆ t are bounded as well. (Given that we construct πt by “τ-mixing” it with a uniform rule, as explained in Section 7.2, in our applications πt(Xt) is always bounded from below since πt(x) ≥ τ nb n .) Therefore, given that we have bounded observations (with a known bound) having a common conditional mean, we can apply the recent betting-based methods [36, 54] for constructing non-asymptotic confidence intervals and time-uniform confidence sequences satisfying P(θ∗ ∈ Ct, ∀t) ≥ 1 − α. We demonstrate the non-asymptotic extension in the problem of post-election survey analysis from Sec- tion 8.1. Figure 8 provides a non-asymptotic analogue of the corresponding batch results from Figure 1, applying the method from Theorem 3 of Waudby-Smith and Ramdas [54] to form a non-asymptotic con- fidence interval. Qualitatively we observe a similar comparison as before—active inference outperforms both uniform sampling and classical inference—though the methods naturally overcover as a result of using non-asymptotic intervals that do not have exact coverage. 0.6 0.7 0.8 Biden's approval rate 247 331 445 598 803 nb 0.04 0.06 0.09 0.14 0.21interval width 247 386 525 664 804 nb 0.6 0.7 0.8 0.9 1.0coverage active uniform classical 0.25 0.30 0.35 0.40 Trump's approval rate 248 332 447 600 806 nb 0.04 0.06 0.08 0.10 0.14interval width 248 387 527 666 806 nb 0.6 0.7 0.8 0.9 1.0coverage active uniform classical Figure 8: Non-asymptotic experiments. Example intervals in five randomly chosen trials (left), average confidence interval width (middle), and coverage (right) in post-election survey research with non-asymptotic confidence intervals. 24",
      "meta_data": {
        "arxiv_id": "2403.03208v2",
        "authors": [
          "Tijana Zrnic",
          "Emmanuel J. Candès"
        ],
        "published_date": "2024-03-05T18:46:50Z",
        "pdf_url": "https://arxiv.org/pdf/2403.03208v2.pdf"
      },
      "llm_extracted_info": {
        "main_contributions": "The paper introduces 'active inference,' a novel methodology for statistical inference that strategically guides data collection using machine learning models to identify which data points are most beneficial to label. It addresses the challenge of collecting high-quality labeled data under stringent budgets. The key contributions include constructing provably valid confidence intervals and hypothesis tests, leveraging any black-box machine learning model and handling any data distribution. Active inference achieves the same level of accuracy with significantly fewer samples than existing baselines, saving over 80% of the sample budget compared to classical inference and between 20-60% compared to uniform prediction-powered inference, resulting in smaller confidence intervals and more powerful p-values. It is applicable to all convex M-estimation problems and supports both batch and sequential data collection settings.",
        "methodology": "Active inference operates on the intuition of prioritizing labels for data points where the machine learning model is uncertain and relying on model predictions where it is confident. The core mechanism is based on Augmented Inverse Propensity Weighting (AIPW) estimators for mean estimation and a general M-estimation framework for other targets. A key component is the sampling rule, π(x), which determines the probability of labeling an instance x. This rule is derived from a measure of model uncertainty, u(x). For regression, u(x) is trained to predict the magnitude of the model error, |f(X)-Y|, while for classification (e.g., binary), it's based on predictive probabilities (e.g., 2 * min{p(x), 1-p(x)}). The sampling rule is calibrated to meet a given budget `nb` by scaling `π(x) = u(x) * nb / (n * E[u(X)])`. In the batch setting, a pre-trained model and fixed sampling rule are used. In the sequential setting, the model and sampling rule are iteratively updated as new labels are collected, respecting a martingale structure for tractable inference. Practical sampling rules stabilize the `u(x)`-based rule by mixing it with a uniform sampling rule (`πunif = nb/n`) using a parameter `τ` (e.g., `π(τ)(x) = (1-τ)π(x) + τπunif(x)`) to prevent instability from near-zero uncertainty estimates.",
        "experimental_setup": "The methodology was evaluated across three distinct domains: 1. Post-election survey research: Analyzing average approval rates of political messaging from the Pew Research Center 2020 US presidential election data (binary classification). 2. Census data analysis: Investigating the linear regression coefficient quantifying the relationship between age and income from American Community Survey (ACS) Public Use Microdata Sample (PUMS) data. 3. AlphaFold-assisted proteomics research: Estimating the odds ratio between protein phosphorylation and being part of an intrinsically disordered region (IDR). For all experiments, XGBoost models were used as black-box predictors (f) and for uncertainty estimation (u or e). Baselines included a 'uniform' baseline (Prediction-Powered Inference, PPI) which uses ML predictions but uniform sampling, and a 'classical' baseline which uses no ML and uniform sampling. Evaluation metrics were average confidence interval width, coverage (target α=0.1), and percentage of budget saved. Experiments involved 1000 trials for batch settings and 100 for sequential settings. Sequential fine-tuning was also evaluated by initializing models with limited data and updating them with collected labels.",
        "limitations": "The primary results focus on asymptotic confidence intervals, although non-asymptotic and time-uniform analogues exist for mean estimation (which naturally overcover). The effectiveness of active inference relies on having a 'reasonably good' machine learning model for predicting labels. The exact expected value of the uncertainty measure, `E[u(X)]`, required for budget normalization, cannot be known precisely and must be empirically estimated. Practical sampling rules need stabilization (e.g., mixing with a uniform rule) to prevent variance blow-up if the model mistakenly estimates near-zero uncertainty when errors are large. In the sequential setting, there's a risk of underutilizing the budget if the model consistently estimates low uncertainty, which is addressed by periodically sampling uniformly. The consistency condition for the batch setting requires `nb/n` to have a limit and a discrete space of tuning parameters. The sequential setting requires the Lindeberg condition (increments without very heavy tails) and convergence of the model-sampling rule pairs.",
        "future_research_directions": "Not mentioned"
      }
    },
    {
      "title": "Active Statistical Inference",
      "abstract": "Inspired by the concept of active learning, we propose active\ninference$\\unicode{x2013}$a methodology for statistical inference with\nmachine-learning-assisted data collection. Assuming a budget on the number of\nlabels that can be collected, the methodology uses a machine learning model to\nidentify which data points would be most beneficial to label, thus effectively\nutilizing the budget. It operates on a simple yet powerful intuition:\nprioritize the collection of labels for data points where the model exhibits\nuncertainty, and rely on the model's predictions where it is confident. Active\ninference constructs provably valid confidence intervals and hypothesis tests\nwhile leveraging any black-box machine learning model and handling any data\ndistribution. The key point is that it achieves the same level of accuracy with\nfar fewer samples than existing baselines relying on non-adaptively-collected\ndata. This means that for the same number of collected samples, active\ninference enables smaller confidence intervals and more powerful p-values. We\nevaluate active inference on datasets from public opinion research, census\nanalysis, and proteomics.",
      "full_text": "Active Statistical Inference Tijana Zrnic∗ Emmanuel J. Cand` es† {tijana.zrnic,candes}@stanford.edu ∗Department of Statistics and Stanford Data Science †Department of Statistics and Department of Mathematics Stanford University Abstract Inspired by the concept of active learning, we propose active inference—a methodology for statistical inference with machine-learning-assisted data collection. Assuming a budget on the number of labels that can be collected, the methodology uses a machine learning model to identify which data points would be most beneficial to label, thus effectively utilizing the budget. It operates on a simple yet powerful intuition: prioritize the collection of labels for data points where the model exhibits uncertainty, and rely on the model’s predictions where it is confident. Active inference constructs provably valid confidence intervals and hypothesis tests while leveraging any black-box machine learning model and handling any data distribution. The key point is that it achieves the same level of accuracy with far fewer samples than existing baselines relying on non-adaptively-collected data. This means that for the same number of collected samples, active inference enables smaller confidence intervals and more powerful p-values. We evaluate active inference on datasets from public opinion research, census analysis, and proteomics. 1 Introduction In the realm of data-driven research, collecting high-quality labeled data is a continuing impediment. The impediment is particularly acute when operating under stringent labeling budgets, where the cost and effort of obtaining each label can be substantial. Recognizing these limitations, many have turned to machine learning as a pragmatic solution, leveraging it to predict unobserved labels across various fields. In remote sensing, machine learning assists in annotating and interpreting satellite imagery [24, 43, 55]; in proteomics, tools like AlphaFold [26] are revolutionizing our understanding of protein structures; even in the realm of elections—including most major US elections—technologies combining scanners and predictive models are used as efficient tools for vote counting [56]. These applications reflect a growing reliance on machine learning for extracting information and knowledge from unlabeled datasets. However, this reliance on machine learning is not without its pitfalls. The core issue lies in the inherent biases of these models. No matter how sophisticated, predictions lead to dubious conclusions; as such, predic- tions cannot fully substitute for traditional data sources such as gold-standard experimental measurements, high-quality surveys, and expert annotations. This begs the question: is there a way to effectively leverage the predictive power of machine learning while still ensuring the integrity of our inferences? Drawing inspiration from the concept of active learning, we propose active inference—a novel methodol- ogy for statistical inference that harnesses machine learning not as a replacement for data collection but as a strategic guide to it. The methodology uses a machine learning model to identify which data points would be most beneficial to label, thus effectively utilizing the labeling budget. It operates on a simple yet pow- erful intuition: prioritize the collection of labels for data points where the model exhibits uncertainty, and rely on the model’s predictions where it is confident. Active inference constructs provably valid confidence intervals and hypothesis tests for any black-box machine learning model and any data distribution. The key takeaway is that it achieves the same level of accuracy with far fewer samples than existing baselines 1 arXiv:2403.03208v2  [stat.ML]  29 May 2024relying on non-adaptively-collected data. Put differently, this means that for the same number of collected samples, active inference enables smaller confidence intervals and more powerful p-values. We will show in our experiments that active inference can save over 80% of the sample budget required by classical inference methods (see Figure 4). Although quite different in scope, our work is inspired by the recent framework of prediction-powered inference (PPI) [1]. PPI assumes access to a small labeled dataset and a large unlabeled dataset, drawn i.i.d. from the population of interest. It then asks how one can use machine learning and the unlabeled dataset to sharpen inference about population parameters depending on the distribution of labels. Our objective in this paper is different since the core of our contribution is (1) designing strategic data collection approaches that enable more powerful inferences than collecting labels in an i.i.d. manner, and (2) showing how to perform inference with such strategically collected data. That said, we will see that prediction- powered inference can be seen as a special case of our methodology: while PPI ignores the issue of strategic data collection and instead uses a trivial, uniform data collection strategy, it leverages machine learning to enhance inference in a similar way to our method. We provide a further discussion of prior work in Section 3. 2 Problem description We introduce the formal problem setting. We observe unlabeled instances X1, . . . , Xn, drawn i.i.d. from a distribution PX. The labels Yi are unobserved, and we shall use ( X, Y) ∼ P = PX × PY |X to denote a generic feature–label pair drawn from the underlying data distribution. We are interested in performing inference—conducting a hypothesis test or forming a confidence interval—for a parameterθ∗ that depends on the distribution of the unobserved labels; that is, the parameter is a functional of PX × PY |X. For example, we might be interested in forming a confidence interval for the mean label, θ∗ = E[Yi], where Yi is the label corresponding to Xi. Although we will primarily focus on forming confidence intervals, the standard duality between confidence intervals and hypothesis tests makes our results directly applicable to testing as well. We have no collected labels a priori. Rather, the goal is to efficiently and strategically acquire labels for certain points Xi, so that inference is as powerful as possible for a given collection budget—more so than if labels were collected uniformly at random—while also remaining valid. We denote by nlab the number of collected labels. We assume that we are constrained to collect, on average, E[nlab] ≤ nb labels, for some budget nb. (For simplicity we bound E[nlab], however the budget can be met with high probability, since nlab concentrates around nb at a fast rate.) Typically, nb ≪ n. To guide the choice of which instances to label, we will make use of a predictive model f. Typically this will be a black-box machine learning model, but it could also be a hand-designed decision rule based on expert knowledge. This is the key component that will enable us to get a significant boost in power. We do not assume any knowledge of the predictive performance of f, or any parametric form for it. Our key takeaway is that, if we have a reasonably good model for predicting the labels Yi based on Xi, then we can achieve a significant boost in power compared to labeling a uniformly-at-random chosen set of instances. We will consider two settings, depending on whether or not we update the predictive model f as we gather more labels. • The first is a batch setting, where we simultaneously make decisions of whether or not to collect the corresponding label for all unlabeled points at once. In this setting, the model f is pre-trained and remains fixed during the label collection. The batch setting is simpler and arguably more practical if we already have a good off-the-shelf predictor. • The second setting is sequential: we go through the unlabeled points one by one and update the predictive model as we collect more data. The benefit of the second approach is that it is applicable even when we do not have access to a pre-trained model, but we have to train a model from scratch. Our proposed active inference strategy will be applicable to all convex M-estimation problems. This 2means that it handles all targets of inference θ∗ that can be written as: θ∗ = arg min θ E[ℓθ(X, Y)], where (X, Y) ∼ P, for a loss function ℓθ that is convex in θ. We denote L(θ) = E[ℓθ(X, Y)] for brevity. M-estimation captures many relevant targets, such as the mean label, quantiles of the label, and regression coefficients. Example 1 (Mean label). If ℓθ(x, y) = 1 2 (y − θ)2, then the target is the mean label, θ∗ = E[Y ]. Note that this loss has no dependence on the features. Example 2 (Linear regression). If ℓθ(x, y) = 1 2 (y − x⊤θ)2, then θ∗ is the vector of linear regression coeffi- cients obtained by regressing y on x, that is, the “effect” of x on y. Example 3 (Label quantile). For a given q ∈ (0, 1), let ℓθ(x, y) = q(y −θ)1{y > θ}+ (1−q)(θ −y)1{y ≤ θ} be the “pinball” loss. Then, θ∗ is equal to the q-quantile of the label distribution: θ∗ = inf{θ : P(Y ≤ θ) ≥ q}. 3 Related work Our work is most closely related to prediction-powered inference (PPI) and other recent works on inference with machine learning predictions [1, 3, 19, 33, 34, 61]. This recent literature in turn relates to classical work on inference with missing data and semiparametric statistics [13, 41, 42, 44, 45, 46], as well as semi-supervised inference [5, 57, 60]. We consider the same set of inferential targets as in [1, 3, 61], building on classical M-estimation theory [52] to enable inference. While PPI assumes access to a small labeled dataset and a large unlabeled dataset, which are drawn i.i.d., our work is different in that it leverages machine learning in order to design adaptive label collection strategies, which breaks the i.i.d. structure between the labeled and the unlabeled data. That said, we shall however see that our active inference estimator will reduce to the prediction-powered estimator when we apply a trivial, uniform label collection strategy. We will demonstrate empirically that the adaptivity in label collection enables significant improvements in statistical power. There is a growing literature on inference from adaptively collected data [14, 29, 59], often focusing on data collected via a bandit algorithm. These papers typically focus on average treatment effect estimation. In contrast to our work, these works generally do not focus on how to set the data-collection policy as to achieve good statistical power, but their main focus is on providing valid inferences given a fixed data- collection policy. Notably, Zhang et al. [59] study inference for M-estimators from bandit data. However, their estimators do not leverage machine learning, which is central to our work. A substantial line of work studies adaptive experiment design [8, 10, 20, 21, 23, 28, 31, 32, 40], often with the goal of maximizing welfare during the experiment or identifying the best treatment. Most related to our motivation, a subset of these works [10, 21, 32] study adaptive design with the goal of efficiently estimating average treatment effects. While our motivation is not necessarily treatment effect estimation, we continue in a similar vein—collecting data adaptively with the goal of improved efficiency—with a focus on using modern, black-box machine learning to produce uncertainty estimates that can be turned into efficient label collection methods. Related variance-reduction ideas appear in stratified survey sampling [27, 30, 35, 48]. Our proposal can be seen as stratifying the population of interest based on the certainty of a black-box machine learning model. Finally, our work draws inspiration from active learning, which is a subarea of machine learning centered around the observation that a machine learning model can enhance its predictive capabilities if it is allowed to choose the data points from which it learns. In particular, our setup is analogous to pool-based active learning [50]. Sampling according to a measure of predictive uncertainty is a central idea in active learning [4, 6, 18, 22, 25, 39, 49, 51]. Since our goal is statistical inference, rather than training a good predictor, our sampling rules are different and adapt to the inferential question at hand. More generally, we note that there is a large body of work studying ways to efficiently collect gold-standard labels, often under a budget constraint [12, 53, 58]. 34 Warm-up: active inference for mean estimation We first focus on the special case of estimating the mean label, θ∗ = E[Y ], in the batch setting. The intuition derived from this example carries over to all other problems. Recall the setup: we observe n i.i.d. unlabeled instances X1, . . . , Xn, and we can collect labels for at most nb of them (on average). Consider first a “classical” solution, which does not leverage machine learning. Given a budget nb, we can simply label any arbitrarily chosen nb points. Since the instances are i.i.d., without loss of generality we can choose to label instances {1, . . . , nb} and compute: ˆθnoML = 1 nb nbX i=1 Yi. The estimator ˆθnoML is clearly unbiased. It is an average over nb terms, and thus its variance is equal to Var(ˆθnoML) = 1 nb Var(Y ). Now, suppose that we are given a machine learning model f(X), which predicts the label Y ∈ R from observed covariates X ∈ X.1 The idea behind our active inference strategy is to increase the effective sample size by using the model’s predictions on points X where the model is confident and focusing the labeling budget on the points X where the model is uncertain. To implement this idea, we design a sampling rule π : X →[0, 1] and collect label Yi with probability π(Xi). The sampling rule is derived from f, by appropriately measuring its uncertainty. The hope is that π(x) ≈ 1 signals that the model f is very uncertain about instance x, whereas π(x) ≈ 0 indicates that the model f should be very certain about instance x. Let ξi ∼ Bern(π(Xi)) denote the indicator of whether we collect the label for point i. By definition, nlab =Pn i=1 ξi. The rule π will be carefully rescaled to meet the budget constraint: E[nlab] = E[π(X)] · n ≤ nb. Our active estimator of the mean θ∗ is given by: ˆθπ = 1 n nX i=1 \u0012 f(Xi) + (Yi − f(Xi)) ξi π(Xi) \u0013 . (1) This is essentially the augmented inverse propensity weighting (AIPW) estimator [42], with a particular choice of propensities π(Xi) based on the certainty of the machine learning model that predicts the missing labels. When the sampling rule is uniform, i.e. π(x) = nb/n for all x, ˆθπ is equal to the prediction-powered mean estimator [1]. It is not hard to see that ˆθπ is unbiased: E[ˆθπ] = θ∗. A short calculation shows that its variance equals Var(ˆθπ) = 1 n \u0012 Var(Y ) + E \u0014 (Y − f(X))2 \u0012 1 π(X) − 1 \u0013\u0015\u0013 . (2) If the model is highly accurate for all x, i.e. f(X) ≈ Y , then Var(ˆθπ) ≈ 1 n Var(Y ), which is far smaller than Var(ˆθnoML) since nb ≪ n. Of course, f will never be perfect and accurate for all instances x. For this reason, we will aim to choose π such that π is small when f(X) ≈ Y and large otherwise, so that the relevant term (Y − f(X))2 \u0000 π−1(X) − 1 \u0001 is always small (of course, subject to the sampling budget constraint). For example, for instances for which the predictor is correct, i.e. f(X) = Y , we would ideally like to setπ(X) = 0 as this incurs no additional variance. We note that the variance reduction of active inference compared to the “classical” solution also implies that the resulting confidence intervals get smaller. This follows because interval width scales with the standard deviation for most standard intervals (e.g., those derived from the central limit theorem). Finally, we explain how to set the sampling rule π. The rule will be derived from a measure of model uncertainty u(x) and we shall provide different choices of u(x) in the following paragraphs. At a high level, 1X is the set of values the covariates can take on, e.g.Rd. 4one can think of u(Xi) as the model’s best guess of |Yi − f(Xi)|. We will choose π(x) proportional to u(x), that is, π(x) ∝ u(x), normalized to meet the budget constraint. Intuitively, this means that we want to focus our data collection budget on parts of the covariate space where the model is expected to make the largest errors. Roughly speaking, we will set π(x) = u(x) E[u(X)] · nb n ; this implies E[nlab] = E[π(X)] ·n ≤ nb. (This is an idealized form of π(x) because E[u(X)] cannot be known exactly, though it can be estimated very accurately from the unlabeled data; we will formalize this in the following section.) We will take two different approaches for choosing the uncertainty u(x), depending on whether we are in a regression or a classification setting. Regression uncertainty In regression, we explicitly train a model u(x) to predict |f(Xi) − Yi| from Xi. We note that we aim to predict only the magnitude of the error and not the directionality. In the batch setting, we typically have historical data of ( X, Y) pairs that are used to train the model f. We thus train u(x) on this historical data, by setting |f(X) −Y | as the target label for instance X. The data used to train u should ideally be disjoint from the data used to train f to avoid overoptimistic estimates of uncertainty. We will typically use data splitting to avoid this issue, though there are more data efficient solutions such as cross-fitting. Notice that access to historical data will only be important in the batch setting, as assumed in this section. In the sequential setting we will be able to train u(x) gradually on the collected data. Classification uncertainty Next we look at classification, where Y is supported on a discrete set of values. Our main focus will be on binary classification, where Y ∈ {0, 1}. In such cases, our target is θ∗ = P(Y = 1). We might care about E[Y ] more generally when Y takes on K distinct values (e.g., K distinct ratings in a survey, K distinct qualification levels, etc). In classification, f(x) is usually obtained as the “most likely” class. If K is the number of classes, we have f(x) = arg maxi∈[K] pi(x), for some probabilistic output p(x) = ( p1(x), . . . , pK(x)) which satisfiesPK i=1 pi(x) = 1. For example, p(x) could be the softmax output of a neural network given input x. We will measure the uncertainty as: u(x) = K K − 1 · \u0012 1 − max i∈[K] pi(x) \u0013 . (3) In binary classification, this reduces to u(x) = 2 min{p(x), 1 − p(x)}, where we use p(x) to denote the raw classifier output in [0 , 1]. Therefore, u(x) is large when p(x) is close to uniform, i.e. max i pi(x) ≈ 1/K. On the other hand, if the model is confident, i.e. max i pi(x) ≈ 1, the uncertainty is close to zero. 5 Batch active inference Building on the discussion from Section 4, we provide formal results for active inference in the batch setting. Recall that in the batch setting we observe i.i.d. unlabeled points X1, . . . , Xn, all at once. We consider a family of sampling rules πη(x) = η u(x), where u(x) is the chosen uncertainty measure and η ∈ H ⊆R+ is a tuning parameter. We will discuss ways of choosing u(x) in Section 7. The role of the tuning parameter is to scale the sampling rule to the sampling budget. We choose ˆη = max ( η ∈ H: η nX i=1 u(Xi) ≤ nb ) , (4) and deploy πˆη as the sampling rule. With this choice, we have E[nlab] = E \" nX i=1 ˆη u(Xi) # ≤ nb; therefore, πˆη meets the label collection budget. We denote ˆθη ≡ ˆθπη . 5Mean estimation We first explain how to perform inference for mean estimation in Proposition 1. Recall the active mean estimator: ˆθˆη = 1 n nX i=1 \u0012 f(Xi) + (Yi − f(Xi)) ξi πˆη(Xi) \u0013 , (5) where ξi ∼ Bern(πˆη(Xi)). Following standard notation, zq below denotes the qth quantile of the standard normal distribution. Proposition 1. Suppose that there exists η∗ ∈ Hsuch that P(ˆη ̸= η∗) → 0. Then √n(ˆθˆη − θ∗) d → N(0, σ2 ∗), where σ2 ∗ = Var(f(X) + (Y − f(X)) ξη∗ πη∗ (X) ) and ξη∗ ∼ Bern(πη∗ (X)). Consequently, for any ˆσ2 p → σ2 ∗, Cα = (ˆθˆη ± z1−α/2 ˆσ√n ) is a valid (1 − α)-confidence interval: lim n→∞ P(θ∗ ∈ Cα) = 1 − α. A few remarks about Proposition 1 are in order: first, the consistency condition P(ˆη ̸= η∗) → 0 is easily ensured if nb/n has a limit p ∈ (0, 1), that is, if nb is asymptotically proportional to n. Then, as long as the space of tuning parameters H is discrete and there is no η ∈ Hsuch that η E[u(X)] = p exactly, the consistency condition is met. Second, obtaining a consistent variance estimate ˆσ2 is straightforward, as one can simply take the empirical variance of the increments in the estimator (5). We note that, while our main results will all focus on asymptotic confidence intervals, some of our results have direct non-asymptotic and time-uniform analogues; see Section C. General M-estimation Next, we turn to general convex M-estimation. Recall this means that we can write θ∗ = arg minθ L(θ) = arg min θ E[ℓθ(X, Y)], for a convex loss ℓθ. To simplify notation, let ℓθ,i = ℓθ(Xi, Yi), ℓf θ,i = ℓθ(Xi, f(Xi)). We similarly use ∇ℓθ,i and ∇ℓf θ,i. For a general sampling rule π, our active estimator is defined as ˆθπ = arg min θ Lπ(θ), where Lπ(θ) = 1 n nX i=1 \u0012 ℓf θ,i + (ℓθ,i − ℓf θ,i) ξi π(Xi) \u0013 . (6) As before, ξi ∼ Bern(π(Xi)). When π is the uniform rule, π(x) = nb/n, the estimator (6) equals the general prediction-powered estimator from [3]. Notice that the loss estimate Lπ(θ) is unbiased: E[Lπ(θ)] = L(θ). We again scale the sampling rule πη(x) = η u(x) according to the sampling budget, as in Eq. (4). We next show asymptotic normality of ˆθˆη for general targets θ∗ which, in turn, enables inference. The result essentially follows from the usual asymptotic normality for M-estimators [52, Ch. 5], with some nec- essary modifications to account for the data-driven selection of ˆ η. We require standard, mild smoothness assumptions on the loss ℓθ, formally stated in Ass. 1 in the Appendix. Theorem 1 (CLT for batch active inference) . Assume the loss is smooth (Ass. 1) and define the Hessian Hθ∗ = ∇2E[ℓθ∗ (X, Y)]. Suppose that there exists η∗ ∈ Hsuch that P(ˆη ̸= η∗) → 0. Then, if ˆθη∗ p → θ∗, we have √n(ˆθˆη − θ∗) d → N(0, Σ∗), where Σ∗ = H−1 θ∗ Var \u0010 ∇ℓf θ∗,i + \u0010 ∇ℓθ∗,i − ∇ℓf θ∗,i \u0011 ξη∗ πη∗ (X) \u0011 H−1 θ∗ . Consequently, for any ˆΣ p → Σ∗, Cα = ( ˆθˆη j ± z1−α/2 q ˆΣjj n ) is a valid (1 − α)-confidence interval for θ∗ j : lim n→∞ P(θ∗ j ∈ Cα) = 1 − α. 6The remarks following Proposition 1 again apply: the consistency condition on ˆ η is easily ensured if nb/n has a limit, and ˆΣ admits a simple plug-in estimate by replacing all quantities with their empirical counterparts. The consistency condition on ˆθη∗ is a standard requirement for analyzing M-estimators [see 52, Ch. 5]; it is studied and justified at length in the literature and we shall therefore not discuss it in close detail. We however remark that it can be deduced if the empirical loss Lπ(θ) is almost surely convex or if the parameter space is compact. The empirical loss Lπ(θ) is convex in a number of cases of interest, including means and generalized linear models; for the proof, see [3]. 6 Sequential active inference In the batch setting we observe all data points X1, . . . , Xn at once and fix a predictive model f and sampling rule π that guide our choice of which labels to collect. An arguably more natural data collection strategy would operate in an online manner: as we collect more labels, we iteratively update the model and our strategy for which labels to collect next. This allows for further efficiency gains over using a fixed model throughout, as the latter ignores knowledge acquired during the data collection. For example, if we are conducting a survey and we collect responses from members of a certain demographic group, it is only natural that we update our sampling rule to reflect the fact that we have more knowledge and certainty about that demographic group. Formally, instead of having a fixed model f and rule π, we go through our data sequentially. At step t ∈ {1, . . . , n}, we observe data point Xt and collect its label with probability πt(Xt), where πt(·) is based on the uncertainty of model ft. The model ft can be fine-tuned on all information observed up to time t; formally, we require thatft, πt ∈ Ft−1, where Ft is the σ-algebra generated by the firstt points Xs, 1 ≤ s ≤ t, their labeling decisions ξs, and their labels Ys, if observed: Ft = σ((X1, Y1ξ1, ξ1), . . . ,(Xt, Ytξt, ξt)). (Note that Ytξt = Yt if and only if ξt = 1; otherwise, Ytξt = ξt = 0.) We will again calibrate our decisions of whether to collect a label according to a budget on the sample size nb. We denote by nlab,t the number of labels collected up to time t. Inference in the sequential setting is more challenging than batch inference because the data points (Xt, Yt, ξt), t∈ [n], are dependent; indeed, the purpose of the sequential setting is to leverage previous obser- vations when deciding on future labeling decisions. We will construct estimators that respect a martingale structure, which will enable tractable inference via the martingale central limit theorem [17]. This resembles the approach taken by Zhang et al. [59] (though our estimators are quite different due to the use of machine learning predictions). Mean estimation We begin by focusing on the mean. If we take ℓθ to be the squared loss as in Example 1, we obtain the sequential active mean estimator: ˆθ # »π = 1 n nX t=1 ∆t, ∆t = ft(Xt) + (Yt − ft(Xt)) ξt πt(Xt). We note that ∆ t are martingale increments ; they share a common conditional mean E[∆t|Ft−1] = θ∗, and they are Ft-measurable, ∆t ∈ Ft. We let σ2 t = V (ft, πt) = Var(∆t|ft, πt) denote the conditional variance of the increments. To show asymptotic normality of ˆθ # »π , we shall require the Lindeberg condition, whose statement we defer to the Appendix. It is a standard assumption for proving central limit theorems when the increments are not i.i.d.. Roughly speaking, the Lindeberg condition requires that the increments do not have very heavy tails; it prevents any increment from having a disproportionately large contribution to the overall variance. Proposition 2. Suppose 1 n Pn t=1 σ2 t p → σ2 ∗ = V (f∗, π∗), for some fixed model–rule pair (f∗, π∗), and that the increments ∆t satisfy the Lindeberg condition (Ass. 2). Then √n(ˆθ # »π − θ∗) d → N(0, σ2 ∗). 7Consequently, for any ˆσ2 p → σ2 ∗, Cα = (ˆθ # »π ± z1−α/2 ˆσ√n ) is a valid (1 − α)-confidence interval: lim n→∞ P(θ∗ ∈ Cα) = 1 − α. Intuitively, Proposition 2 requires that the model ft and sampling rule πt converge. For example, a sufficient condition for 1 n Pn t=1 σ2 t p → σ2 ∗ is V (fn, πn) L1 → V (f∗, π∗). Since the sampling rule is typically based on the model, it makes sense that it would converge if ft converges. At the same time, it makes sense for ft to gradually stop updating after sufficient accuracy is achieved. General M-estimation We generalize Proposition 2 to all convex M-estimation problems. The general version of our sequential active estimator takes the form ˆθ # »π = arg min θ L # »π (θ), where L # »π (θ) = 1 n nX t=1 Lt(θ), L t(θ) = ℓft θ,t + (ℓθ,t − ℓft θ,t) ξt πt(Xt). (7) Let Vθ,t = Vθ(ft, πt) = Var (∇Lt(θ)|ft, πt). We will again require that ( ft, πt) converge in an appropriate sense. Theorem 2 (CLT for sequential active inference). Assume the loss is smooth (Ass. 1) and define the Hessian Hθ∗ = ∇2E[ℓθ∗ (X, Y)]. Suppose also that 1 n Pn t=1 Vθ∗,t p → V∗ = Vθ∗ (f∗, π∗) entry-wise for some fixed model– rule pair (f∗, π∗), and that the increments Lt(θ) satisfy the Lindeberg condition (Ass. 3). Then, if ˆθ # »π p → θ∗, we have √n(ˆθ # »π − θ∗) d → N(0, Σ∗), where Σ∗ = H−1 θ∗ V∗H−1 θ∗ . Consequently, for any ˆΣ p → Σ∗, Cα = ( ˆθ # »π j ± z1−α/2 q ˆΣjj n ) is a valid (1 − α)- confidence interval for θ∗ j : lim n→∞ P(θ∗ j ∈ Cα) = 1 − α. The conditions of Theorem 2 are largely the same as in Theorem 1; the main difference is the requirement of convergence of the model–sampling rule pairs, which is similar to the analogous condition of Proposition 2. Proposition 2 and Theorem 2 apply to any sampling rule πt, as long as the variance convergence require- ment is met. We discuss ways to set πt so that the sampling budget nb is met. Our default will be to “spread out” the budget nb over the n observations. We will do so by having an “imaginary” budget for the expected number of collected labels by step t, equal to nb,t = tnb/n. Let n∆,t = nb,t − nlab,t−1 denote the remaining budget at step t. We derive a measure of uncertainty ut from model ft, as before, and let πt(x) = min {ηt ut(x), n∆,t}[0,1] , (8) where ηt normalizes ut(x) and the subscript [0, 1] denotes clipping to [0, 1]. The normalizing constant ηt can be arbitrary, but we find it helpful to set it roughly as ηt = nb/(n E[ut(X)]) and this is what we do in our experiments, with the proviso that we substitute E[ut(X)] with its empirical approximation. In words, the sampling probability is high if the uncertainty is high and we have not used up too much of the sampling budget thus far. Of course, if the model consistently estimates low uncertainty ut(x) throughout, the budget will be underutilized. For this reason, to make sure we use up the budget in practice, we occasionally set πt(x) = ( n∆,t)[0,1] regardless of the reported uncertainty. This periodic deviation from the rule (8) is consistent with the variance convergence conditions required for Proposition 2 and Theorem 2 to hold. 7 Choosing the sampling rule We have seen how to perform inference given an abstract sampling rule, and argued that, intuitively, the sampling rule should be calibrated to the uncertainty of the model’s predictions. Here we argue that this 8is in fact the optimal strategy. In particular, we derive an “oracle” rule, which optimally sets the sampling probabilities so that the variance of ˆθπ is minimized. While the oracle rule cannot be implemented since it depends on unobserved information, it provides an ideal that our algorithms will try to approximate. We discuss ways of tuning the approximations to make them practical and powerful. 7.1 Oracle sampling rules We begin with the optimal sampling rule for mean estimation. We then state the optimal rule for general M-estimation and instantiate it for generalized linear models. Mean estimation Recall the expression for Var( ˆθπ) (2). Given that E \u0002 π−1(X)(Y − f(X))2\u0003 is the only term that depends on π, we define the oracle rule as the solution to: min π E \u0014 1 π(X)(Y − f(X))2 \u0015 s.t. E[π(X)] ≤ nb n . (9) The optimization problem (9) appears in a number of other topics, including importance sampling [37, Ch. 9], constrained utility optimization [7], and, relatedly to our work, survey sampling [47]. The optimality conditions of (9) show that its solution πopt satisfies: πopt(X) ∝ p E[(Y − f(X))2|X], where ∝ ignores the normalizing constant required to make E[πopt(X)] ≤ nb/n. Therefore, the optimal sampling rule is one that samples data points according to the expected magnitude of the model error: the larger the model error, the higher the probability of sampling should be. Of course, E[(Y −f(X))2|X] cannot be known since the label distribution is unknown, and that is why we call πopt an oracle. To develop intuition, it is instructive to consider an even more powerful oracle ˜πopt(X, Y) that is allowed to depend on Y . To be clear, we would commit to the same functional form as in (1) and would seek to minimize Var(ˆθπ) while allowing the sampling probabilities to depend on both X and Y . In this case, by the same argument we conclude that ˜πopt(X, Y) ∝ |Y − f(X)|. (10) The perspective of allowing the oracle to depend on both X and Y is directly prescriptive: a natural way to approximate the rule ˜πopt is to train an arbitrary black-box model u on historical (X, Y) pairs to predict |Y − f(X)| from X. We provide further practical guidelines for sampling rules at the end of this section. General M-estimation In the case of general M-estimation, we cannot hope to minimize the variance of ˆθπ at a fixed sample size n since the finite-sample distribution of ˆθπ is not tractable. However, we can find a sampling rule that minimizes the asymptotic variance of ˆθπ. Since the estimator is potentially multi- dimensional, to make the problem well-posed we assume that we want to minimize the asymptotic variance of a single coordinate ˆθπ j (for example, one coefficient in a multi-dimensional regression). Recall the expression for the asympotic covariance Σ ∗ from Theorem 1. A short derivation shows that Σ∗,jj = E \"\u0012\u0010 ∇ℓθ∗,i − ∇ℓf θ∗,i \u0011⊤ h(j) \u00132 · 1 π(X) # + C, where h(j) is the j-th column of H−1 θ∗ and C is a term that has no dependence on the sampling rule π. Therefore, by the same theory as for mean estimation, the ideal rule πopt(X) would be πopt(X) ∝ q E[((∇ℓθ∗ (X, Y) − ∇ℓθ∗ (X, f(X)))⊤ h(j))2|X]. This recovers πopt for the mean, since ∇ℓθ∗ (x, y) = θ∗ − y and h(j) = 1 for the squared loss. Our measure of uncertainty u(x) should therefore approximate the errors of the predicted gradients along the h(j) direction. 9Generalized linear models (GLMs) We simplify the general solution πopt in the case of generalized linear models (GLMs). We define GLMs as M-estimators whose loss function takes the form ℓθ(x, y) = −log pθ(y|x) = −yx⊤θ + ψ(x⊤θ), for some convex log-partition function ψ. This definition recovers linear regression by taking ψ(s) = 1 2 s2 and logistic regression by taking ψ(s) = log(1 + es). By the definition of the GLM loss, we have ∇ℓθ∗ (x, y) − ∇ℓθ∗ (x, f(x)) = (f(x) − y)x and, therefore, πopt(X) ∝ p E[(f(X) − Y )2|X] · |X⊤h(j)|, where the Hessian is equal to Hθ∗ = E[ψ′′(X⊤θ∗)XX ⊤] and h(j) is the j-th column of H−1 θ∗ . In linear regression, for instance, Hθ∗ = E[XX ⊤]. Again, we see that the model errors play a role in determining the optimal sampling. In particular, again considering the more powerful oracle ˜ πopt(X, Y) that is allowed to set the sampling probabilities according to both X and Y , we get ˜πopt(X, Y) ∝ |f(X) − Y | · |X⊤h(j)|. (11) Therefore, as in the case of the mean, our measure of uncertainty will aim to predict |f(X) − Y | from X and plug those predictions into the above rule. 7.2 Practical sampling rules As explained in Section 5 and Section 6, our sampling rule π(x) will be derived from a measure of un- certainty u(x). As clear from the preceding discussion, the right notion of uncertainty should measure a notion of error dependent on the estimation problem at hand. In particular, we hope to have u(X) ≈ |(∇ℓθ∗ (X, Y) − ∇ℓθ∗ (X, f(X)))⊤ h(j)|. For GLMs and means, in light of Eq. (10) and Eq. (11), this often boils down to training a predictor of |f(X) − Y | from X and, in the case of GLMs, using a plug-in estimate of the Hessian. This is what we do in our experiments (except in the case of binary classification where we simply use the uncertainty from Eq. (3)). Of course, the learned predictor of model errors cannot be perfect; as a result, π(x) ∝ u(x) cannot naively be treated as the oracle rule πopt. For example, the model might mistakenly estimate (near-)zero uncertainty (u(X) ≈ 0) when |f(X) −Y | is large, which would blow up the estimator variance. To fix this issue, we find that it helps to stabilize the rule π(x) ∝ u(x) by mixing it with a uniform rule. Denote the uniform rule by πunif(x) = nb/n. Clearly the uniform rule meets the budget constraint, since n E[πunif(X)] = nb. For a fixed τ ∈ [0, 1] and π(x) ∝ u(x), we define the τ-mixed rule as π(τ)(x) = (1 − τ) · π(x) + τ · πunif(x). Any positive value of τ ensures that π(τ)(x) > 0 for all x, avoiding instability due to small uncertainty estimates u(x). When historical data is available, one can tune τ by optimizing the empirical estimate of the (asymptotic) variance of ˆθπ(τ) given by Theorem 1. For example, in the case of mean estimation, this would correspond to solving: ˆτ = arg min τ∈[0,1] nhX i=1 1 π(τ)(Xh i )(Y h i − f(Xh i ))2, (12) where ( Xh i , Yh i ), . . . ,(Xh nn, Yh nh) are the historical data points. Otherwise, one can set τ to be any user- specified constant. In our experiments, in the batch setting we tune τ on historical data when such data is available. In the sequential setting we simply set τ = 0.5 as the default. 8 Experiments We evaluate active inference on several problems and compare it to two baselines. 10Algorithm 1 Batch active inference Input: unlabeled data X1, . . . , Xn, sampling budget nb, predictive model f, error level α ∈ (0, 1) 1: Choose uncertainty measure u(x) based on f 2: Let π(x) = ˆη u(x), where ˆη = nb nˆE[u(X)] ; let πunif = nb n 3: Select τ ∈ (0, 1) and choose sampling rule π(τ)(x) = (1 − τ) · π(x) + τ · πunif 4: Sample labeling decisions ξi ∼ Bern(π(τ)(Xi)), i∈ [n] 5: Collect labels {Yi : ξi = 1} 6: Compute batch active estimator ˆθπ(τ) (Eq. (6)) Algorithm 2 Sequential active inference Input: unlabeled data X1, . . . , Xn, sampling budget nb, initial predictive model f1, error level α ∈ (0, 1), fine-tuning batch size B 1: Set Dtune ← ∅ 2: for t = 1, . . . , ndo 3: Choose uncertainty measure ut(x) for ft 4: Set πt(x) as in Eq. (8) with ηt = nb nˆE[ut(X)] ; let πunif = nb n 5: Select τ ∈ (0, 1) and choose sampling rule π(τ) t (x) = (1 − τ) · πt(x) + τ · πunif 6: Sample labeling decision ξt ∼ Bern(π(τ) t (Xt)) 7: if ξt = 1 then 8: Collect label Yt 9: Dtune ← Dtune ∪ {(Xt, Yt)} 10: if |Dtune| = B then 11: Fine-tune model on Dtune: ft+1 = finetune(ft, Dtune) 12: Set Dtune ← ∅ 13: else 14: ft+1 ← ft 15: else 16: ft+1 ← ft 17: Compute sequential active estimator ˆθ # »π (τ) (Eq. (7)) The first baseline replaces active sampling with the uniformly random sampling rule πunif. Importantly, this baseline still uses machine learning predictions f(Xi) and corresponds to prediction-powered inference (PPI) [1]. Formally, the prediction-powered estimator is given by ˆθPPI = arg min θ LPPI(θ), where LPPI(θ) = 1 n nX i=1 ℓθ(Xi, f(Xi)) + 1 nb nX i=1 (ℓθ(Xi, Yi) − ℓθ(Xi, f(Xi))) ξi, where ξi ∼ Bern(nb n ). This estimator can be recovered as a special case of estimator (6). The purpose of this comparison is to quantify the benefits of machine-learning-driven data collection. In the rest of this section we refer to this baseline as the “uniform” baseline because the only difference from our estimator is that it replaces active sampling with uniform sampling. The second baseline removes machine learning altogether and computes the “classical” estimate based on uniformly random sampling, ˆθnoML = arg min θ 1 nb nX i=1 ℓθ(Xi, Yi)ξi, where ξi ∼ Bern(nb n ). This baseline serves to evaluate the cumulative benefits of machine learning for data collection and inference combined. We refer to this baseline as the “classical” baseline, or classical inference, in the rest of this section. For all methods we compute standard confidence intervals based on asymptotic normality. The target 110.5 0.6 0.7 0.8 approval rate 220 304 418 576 792 nb 0.04 0.05 0.08 0.12 0.18interval width 221 364 507 650 793 nb 0.6 0.7 0.8 0.9 1.0coverage active uniform classical 0.2 0.3 0.4 approval rate 222 305 420 577 795 nb 0.04 0.05 0.07 0.09 0.12interval width 222 365 508 651 795 nb 0.6 0.7 0.8 0.9 1.0coverage active uniform classical Figure 1: Post-election survey research. Example intervals in five randomly chosen trials (left), average confidence interval width (middle), and coverage (right) for the average approval of Joe Biden’s (top) and Donald Trump’s (bottom) political messaging to the country following the 2020 US presidential election. error level is α = 0.1 throughout. We report the average interval width and coverage for varying sample sizes nb, averaged over 1000 and 100 trials for the batch and sequential settings, respectively. We plot the interval width on a log–log scale. We also report the percentage of budget saved by active inference relative to the baselines when the methods are matched to be equally accurate. More precisely, for varying nb we compute the average interval width achieved by the uniform and classical baselines; then, we look for the budget size nactive b for which active inference achieves the same average interval width, and report (nb −nactive b )/nb ·100% as the percentage of budget saved. The batch and sequential active inference methods used in our experiments are outlined in Algorithm 1 and Algorithm 2, respectively. Each application will specify the general parameters from the algorithm state- ments. We defer some experimental details, such as the choices of τ, to Appendix B. Code for reproducing the experiments is available at https://github.com/tijana-zrnic/active-inference. 8.1 Post-election survey research We apply active inference to survey data collected by the Pew Research Center following the 2020 United States presidential election [38]. We focus on one specific question in the survey, aimed at gauging people’s approval of the presidential candidates’ political messaging following the election. The target of inference is the average approval rate of Joe Biden’s (Donald Trump’s, respectively) political messaging. Approval is encoded as a binary response, Yi ∈ {0, 1}. The respondents—a nationally representative pool of US adults—provide background information such as age, gender, education, political affiliation, etc. We show that, by training a machine learning model to predict people’s approval from their background information and measuring the model’s uncertainty, we can allocate the per-question budget in a way that achieves higher statistical power than uniform allocation. Careful budget allocation is important, because Pew pays each respondent proportionally to the number of questions they answer. We use half of all available data for the analysis; for the purpose of evaluating coverage, we take the average approval on all available data as the ground truth θ∗. To obtain the predictive model f, we train an XGBoost model [11] on the half of the data not used for the analysis. Since approval is encoded as a binary response, we use the measure of uncertainty from Eq. (3). 12800 1000 1200 1400 regression coefficient 189 444 1040 2435 5701 nb 69 127 236 437 809interval width 190 1567 2945 4323 5701 nb 0.6 0.7 0.8 0.9 1.0coverage active uniform classical Figure 2: Census data analysis. Example intervals in five randomly chosen trials (left), average confidence interval width (middle), and coverage (right) for the linear regression coefficient quantifying the relationship between age and income, controlling for sex, in US Census data. 2 4 6 odds ratio 108 228 482 1021 2159 nb 0.57 1.09 2.08 3.99 7.64interval width 108 621 1134 1647 2160 nb 0.6 0.7 0.8 0.9 1.0coverage active uniform classical Figure 3: AlphaFold-assisted proteomics research. Example intervals in five randomly chosen trials (left), average confidence interval width (middle), and coverage (right) for the odds ratio between phospho- rylation and being part of an IDR. In Figure 1 we compare active inference to the uniform (PPI) and classical baselines. All methods meet the coverage requirement. Across different values of the budget nb, active sampling reduces the confidence interval width of the uniform baseline (PPI) by a significant margin (at least ∼ 10%). Classical inference is highly suboptimal compared to both alternatives. In Figure 4 we report the percentage of budget saved due to active sampling. For estimating Biden’s approval, we observe an over 85% save in budget over classical inference and around 25% save over the uniform baseline. For estimating Trump’s approval, we observe an over 70% save in budget over classical inference and around 25% save over the uniform baseline. 8.2 Census data analysis Next, we study the American Community Survey (ACS) Public Use Microdata Sample (PUMS) collected by the US Census Bureau. ACS PUMS is an annual survey that collects information about citizenship, education, income, employment, and other factors previously contained only in the long form of the decennial census. We use the Folktables [15] interface to download the data. We investigate the relationship between age and income in survey data collected in California in 2019, controlling for sex. Specifically, we target the linear regression coefficient when regressing income on age and sex (that is, its age coordinate). Analogously to the previous application, we use half of all available data for the analysis and train an XGBoost model [11] of a person’s income from the available demographic covariates on the other half. As the ground-truth value of the target θ∗, we take the corresponding linear regression coefficient computed on all available data. To quantify the model’s uncertainty, we use the strategy described in Section 4, training a separate XGBoost model e(·) to predict |f(X)−Y | from X. Then, we set the uncertainty u(x) as prescribed in Eq. (11), replacing |f(X) − Y | by e(X). The interval widths and coverage are shown in Figure 2. As in the previous application, all methods approximately achieve the target coverage, however this time we observe more extreme gains over the uniform baseline (PPI): the interval widths almost double when going from active sampling to uniform sampling. Of 13250 500 750 1000 1250 nb 0 25 50 75 100 budget save  over classical (%) Post-election research Biden Trump 250 500 750 1000 1250 nb 0 25 50 75 100 budget save  over uniform (%) Biden Trump 2000 3000 4000 5000 nb 0 25 50 75 100  Census analysis 2000 3000 4000 5000 nb 0 25 50 75 100 500 1000 1500 2000 nb 0 25 50 75 100  AlphaFold 500 1000 1500 2000 nb 0 25 50 75 100 Figure 4: Save in sample budget due to active inference. Reduction in sample size required to achieve the same confidence interval width with active inference and (top) classical inference and (bottom) uniform sampling, respectively, across the applications shown in Figures 1-3. course, the improvement of active inference over classical inference is even more substantial. The large gains of active sampling can also be seen in Figure 4: we save around 80% of the budget over classical inference and over 60% over the uniform baseline. 8.3 AlphaFold-assisted proteomics research Inspired by the findings of Bludau et al. [9] and the subsequent analysis of Angelopoulos et al. [1], we study the odds ratio of a protein being phosphorylated, a functional property of a protein, and being part of an intrinsically disordered region (IDR), a structural property. The latter can only be obtained from knowledge about the protein structure, which can in turn be measured to a high accuracy only via expensive experimental techniques. To overcome this challenge, Bludau et al. used AlphaFold predictions [26] to estimate the odds ratio. AlphaFold is a machine learning model that predicts a protein’s structure from its amino acid sequence. Angelopoulos et al. [1] showed that forming a classical confidence interval around the odds ratio based on AlphaFold predictions is not valid given that the predictions are imperfect. They provide a valid alternative assuming access to a small subset of proteins with true structure measurements, uniformly sampled from the larger population of proteins of interest. We show that, by strategically choosing which protein structures to experimentally measure, active infer- ence allows for intervals that retain validity and are tighter than intervals based on uniform sampling. Natu- rally, for the purpose of evaluating validity, we restrict the analysis to proteins where we have gold-standard structure measurements; we use the post-processed AlphaFold outputs made available by Angelopoulos et al. [1], which predict the IDR property based on the raw AlphaFold output. We leverage the predictions to guide the choice of which structures to experimentally derive, subject to a budget constraint. The odds ratio we aim to estimate is defined as: θ∗ = µ1/(1 − µ1) µ0/(1 − µ0), where µ1 = P(Y = 1|Xph = 1) and µ0 = P(Y = 1|Xph = 0); Y is a binary indicator of disorder and Xph is a binary indicator of phosphorylation. While the odds ratio is not a solution to an M-estimation problem, it is a function of two means, µ1 and µ0 (see also [1, 3]). Confidence intervals can thus be computed by applying the delta method to the asymptotic normality result for the mean. Since Y is binary, we use the measure of uncertainty from Eq. (3) to estimate µ1 and µ0. For the purpose of evaluating coverage, we take the empirical odds ratio computed on the whole dataset as the ground-truth value of θ∗. Figure 3 shows the interval widths and coverage for the three methods, and Figure 4 shows the percentage of budget saved due to adaptive data collection. The gains are substantial: over 75% of the budget is saved in comparison to classical inference, and around 20 − 25% is saved in comparison to the uniform baseline 140.68 0.70 0.72 approval rate 1855 2370 3029 3871 4946 nb 0.019 0.022 0.026 0.031 0.038interval width 1855 2628 3401 4174 4947 nb 0.6 0.7 0.8 0.9 1.0coverage active (w/ fine-tuning) active (no fine-tuning) uniform 0.28 0.30 0.32 approval rate 1860 2376 3037 3881 4961 nb 0.018 0.022 0.027 0.032 0.039interval width 1860 2635 3410 4185 4961 nb 0.6 0.7 0.8 0.9 1.0coverage active (w/ fine-tuning) active (no fine-tuning) uniform Figure 5: Post-election survey research with fine-tuning. Example intervals in five randomly chosen trials (left), average confidence interval width (middle), and coverage (right) for the average approval of Joe Biden’s (top) and Donald Trump’s (bottom) political messaging to the country following the 2020 US presidential election. Active inference with no fine-tuning and inference with uniformly sampled data use the same model. (PPI). Given the cost of experimental measurement techniques in proteomics, this save in sample size would imply a massive save in cost. 8.4 Post-election survey research with fine-tuning We return to the example from Section 8.1, this time evaluating the benefits of sequential fine-tuning. We compare active inference, with and without fine-tuning, and PPI, which relies on uniform sampling. We show that active inference with no fine-tuning can hurt compared to PPI if the former uses a poorly trained model; fine-tuning, on the other hand, remedies this issue. The predictive model may be poorly trained due to little or no historical data; sequential fine-tuning is necessary in such cases. We train an XGBoost model on only 10 labeled examples and use this model for active inference with no fine-tuning and PPI. The latter is similar to the former in the sense that it only replaces active with uniform sampling. Active inference with fine-tuning continues to fine-tune the model with every B = 100 new survey responses, also updating the sampling rule via update (8). The uncertainty measure ut(x) is given by Eq. (3), as before. As discussed in Section 6, we also periodically use up the remaining budget regardless of the computed uncertainty in order to avoid underutilizing the budget (in particular, every 100n/nb steps). We fine-tune the model using the training continuation feature of XGBoost. The interval widths and coverage are reported in Figure 5. We find that fine-tuning substantially improves inferential power and retains correct coverage. In Figure 7 we show the save in sample size budget over active inference with no fine-tuning and inference based on uniform sampling, i.e. PPI. For estimating Biden’s approval, we observe a gain of around 40% and 30% relative to active inference without fine-tuning and PPI, respectively. For Trump’s approval, we observe even larger gains around 45% and 35%, respectively. 8.5 Census data analysis with fine-tuning We similarly evaluate the benefits of sequential fine-tuning in the problem setting from Section 8.2. We again compare active inference, with and without fine-tuning, and PPI, i.e., active inference with a trivial, uniform sampling rule. Recall that in Section 8.2 we trained a separate model e to predict the prediction errors, which we in turn used to form the uncertainty u(x) according to Eq. (11). This time we fine-tune both the prediction model, ft, and the error model, et. 15900 1000 1100 1200 regression coefficient 3799 4999 6580 8660 11399 nb 70 88 110 138 173interval width 3799 5699 7599 9499 11399 nb 0.6 0.7 0.8 0.9 1.0coverage active (w/ fine-tuning) active (no fine-tuning) uniform Figure 6: Census data analysis with fine-tuning. Example intervals in five randomly chosen trials (left), average confidence interval width (middle), and coverage (right) for the linear regression coefficient quantifying the relationship between age and income, controlling for sex, in US Census data. Active inference with no fine-tuning and inference with uniformly sampled data use the same model. 4000 4500 5000 nb 0 25 50 75 100 budget save over  no fine-tuning (%) Post-election research Biden Trump 3500 4000 4500 5000 nb 0 25 50 75 100 budget save  over uniform (%) Biden Trump 8000 9000 10000 11000 nb 0 25 50 75 100  Census analysis 7000 8000 9000 10000 11000 nb 0 25 50 75 100 Figure 7: Save in sample size budget due to fine-tuning. Reduction in sample size required to achieve the same confidence interval width with active inference with fine-tuning and (top) active inference with no fine-tuning and (bottom) the uniform baseline (PPI), respectively, in the applications shown in Figure 5 and Figure 6. We train initial XGBoost models f1 and e1 on 100 labeled examples. We use f1 for PPI and both f1 and e1 for active inference with no fine-tuning. Active inference with fine-tuning continues to fine-tune the two models with every B = 1000 new survey responses, also updating the model uncertainty via update (8). We fine-tune the models using the training continuation feature of XGBoost. We compute ut from et based on Eq. (11). As discussed earlier, we also periodically use up the remaining budget regardless of the computed uncertainty in order to avoid underutilizing the budget (in particular, every 500 n/nb steps). We show the interval widths and coverage in Figure 6. We see that the gains of fine-tuning are significant and increase as nb increases. In Figure 7 we show the save in sample size budget. Fine-tuning saves around 32 − 40% over the baseline with no fine-tuning and around 20 − 30% over the uniform baseline. Moreover, the save increases as the sample budget grows because the prediction problem is difficult and the model’s performance keeps improving even after 10000 training examples. Acknowledgements We thank Lihua Lei, Jann Spiess, and Stefan Wager for many insightful comments and pointers to relevant work. T.Z. was supported by Stanford Data Science through the Fellowship program. E.J.C. was supported by the Office of Naval Research grant N00014-20-1-2157, the National Science Foundation grant DMS- 2032014, the Simons Foundation under award 814641, and the ARO grant 2003514594. 16References [1] Anastasios N Angelopoulos, Stephen Bates, Clara Fannjiang, Michael I Jordan, and Tijana Zrnic. Prediction-powered inference. Science, 382(6671):669–674, 2023. [2] Anastasios N Angelopoulos, Stephen Bates, Clara Fannjiang, Michael I Jordan, and Tijana Zrnic. Prediction-powered inference: Data sets, 2023. URL https://doi.org/10.5281/zenodo.8397451. [3] Anastasios N Angelopoulos, John C Duchi, and Tijana Zrnic. PPI++: Efficient prediction-powered inference. arXiv preprint arXiv:2311.01453 , 2023. [4] Jordan T Ash, Chicheng Zhang, Akshay Krishnamurthy, John Langford, and Alekh Agarwal. Deep batch active learning by diverse, uncertain gradient lower bounds. arXiv preprint arXiv:1906.03671 , 2019. [5] David Azriel, Lawrence D Brown, Michael Sklar, Richard Berk, Andreas Buja, and Linda Zhao. Semi- supervised linear regression. Journal of the American Statistical Association, 117(540):2238–2251, 2022. [6] Maria-Florina Balcan, Alina Beygelzimer, and John Langford. Agnostic active learning. In Proceedings of the 23rd international conference on Machine learning , pages 65–72, 2006. [7] Maria-Florina Balcan, Amit Daniely, Ruta Mehta, Ruth Urner, and Vijay V Vazirani. Learning economic parameters from revealed preferences. In Web and Internet Economics: 10th International Conference, WINE 2014, Beijing, China, December 14-17, 2014. Proceedings 10 , pages 338–353. Springer, 2014. [8] Debopam Bhattacharya and Pascaline Dupas. Inferring welfare maximizing treatment assignment under budget constraints. Journal of Econometrics , 167(1):168–196, 2012. [9] Isabell Bludau, Sander Willems, Wen-Feng Zeng, Maximilian T Strauss, Fynn M Hansen, Maria C Tanzer, Ozge Karayel, Brenda A Schulman, and Matthias Mann. The structural context of posttrans- lational modifications at a proteome-wide scale. PLoS biology, 20(5):e3001636, 2022. [10] Yash Chandak, Shiv Shankar, Vasilis Syrgkanis, and Emma Brunskill. Adaptive instrument design for indirect experiments. arXiv preprint arXiv:2312.02438 , 2023. [11] Tianqi Chen and Carlos Guestrin. Xgboost: A scalable tree boosting system. In Proceedings of the 22nd acm sigkdd international conference on knowledge discovery and data mining , pages 785–794, 2016. [12] Chen Cheng, Hilal Asi, and John Duchi. How many labelers do you have? a closer look at gold-standard labels. arXiv preprint arXiv:2206.12041 , 2022. [13] Victor Chernozhukov, Denis Chetverikov, Mert Demirer, Esther Duflo, Christian Hansen, Whitney Newey, and James Robins. Double/debiased machine learning for treatment and structural parameters, 2018. [14] Thomas Cook, Alan Mishler, and Aaditya Ramdas. Semiparametric efficient inference in adaptive experiments. arXiv preprint arXiv:2311.18274 , 2023. [15] Frances Ding, Moritz Hardt, John Miller, and Ludwig Schmidt. Retiring adult: New datasets for fair machine learning. Advances in neural information processing systems , 34:6478–6490, 2021. [16] Rick Durrett. Probability: theory and examples , volume 49. Cambridge university press, 2019. [17] Aryeh Dvoretzky. Asymptotic normality for sums of dependent random variables. In Proceedings of the Sixth Berkeley Symposium on Mathematical Statistics and Probability, Volume 2: Probability Theory , volume 6, pages 513–536. University of California Press, 1972. [18] Yarin Gal, Riashat Islam, and Zoubin Ghahramani. Deep Bayesian active learning with image data. In International conference on machine learning , pages 1183–1192. PMLR, 2017. 17[19] Feng Gan and Wanfeng Liang. Prediction de-correlated inference. arXiv preprint arXiv:2312.06478 , 2023. [20] Vitor Hadad, David A Hirshberg, Ruohan Zhan, Stefan Wager, and Susan Athey. Confidence intervals for policy evaluation in adaptive experiments. Proceedings of the national academy of sciences, 118(15): e2014602118, 2021. [21] Jinyong Hahn, Keisuke Hirano, and Dean Karlan. Adaptive experimental design using the propensity score. Journal of Business & Economic Statistics , 29(1):96–108, 2011. [22] Steve Hanneke et al. Theory of disagreement-based active learning. Foundations and Trends ® in Machine Learning, 7(2-3):131–309, 2014. [23] Feifang Hu and William F Rosenberger. The theory of response-adaptive randomization in clinical trials. John Wiley & Sons, 2006. [24] Neal Jean, Marshall Burke, Michael Xie, W Matthew Davis, David B Lobell, and Stefano Ermon. Combining satellite imagery and machine learning to predict poverty. Science, 353(6301):790–794, 2016. [25] Ajay J Joshi, Fatih Porikli, and Nikolaos Papanikolopoulos. Multi-class active learning for image clas- sification. In 2009 ieee conference on computer vision and pattern recognition , pages 2372–2379. IEEE, 2009. [26] John Jumper, Richard Evans, Alexander Pritzel, Tim Green, Michael Figurnov, Olaf Ronneberger, Kathryn Tunyasuvunakool, Russ Bates, AugustinˇZ´ ıdek, Anna Potapenko, et al. Highly accurate protein structure prediction with alphafold. Nature, 596(7873):583–589, 2021. [27] Graham Kalton. Introduction to survey sampling . Number 35. Sage Publications, 2020. [28] Maximilian Kasy and Anja Sautmann. Adaptive treatment assignment in experiments for policy choice. Econometrica, 89(1):113–132, 2021. [29] Masahiro Kato, Takuya Ishihara, Junya Honda, and Yusuke Narita. Efficient adaptive experimental design for average treatment effect estimation. arXiv preprint arXiv:2002.05308 , 2020. [30] Mohammad GM Khan, Karuna G Reddy, and Dinesh K Rao. Designing stratified sampling in economic and business surveys. Journal of applied statistics , 42(10):2080–2099, 2015. [31] Tze Leung Lai and Herbert Robbins. Asymptotically efficient adaptive allocation rules. Advances in applied mathematics, 6(1):4–22, 1985. [32] John A List, Sally Sadoff, and Mathis Wagner. So you want to run an experiment, now what? some simple rules of thumb for optimal experimental design. Experimental Economics, 14:439–457, 2011. [33] Jiacheng Miao, Xinran Miao, Yixuan Wu, Jiwei Zhao, and Qiongshi Lu. Assumption-lean and data- adaptive post-prediction inference. arXiv preprint arXiv:2311.14220 , 2023. [34] Keshav Motwani and Daniela Witten. Valid inference after prediction. arXiv preprint arXiv:2306.13746, 2023. [35] Dankit K Nassiuma. Survey sampling: Theory and methods, 2001. [36] Francesco Orabona and Kwang-Sung Jun. Tight concentrations and confidence sequences from the regret of universal portfolio. IEEE Transactions on Information Theory , 2023. [37] Art B. Owen. Monte Carlo theory, methods and examples . https://artowen.su.domains/mc/, 2013. [38] Pew. American trends panel (ATP) wave 79, 2020. URL https://www.pewresearch.org/science/ dataset/american-trends-panel-wave-79/ . 18[39] Pengzhen Ren, Yun Xiao, Xiaojun Chang, Po-Yao Huang, Zhihui Li, Brij B Gupta, Xiaojiang Chen, and Xin Wang. A survey of deep active learning. ACM computing surveys (CSUR) , 54(9):1–40, 2021. [40] Herbert Robbins. Some aspects of the sequential design of experiments. 1952. [41] James M Robins and Andrea Rotnitzky. Semiparametric efficiency in multivariate regression models with missing data. Journal of the American Statistical Association , 90(429):122–129, 1995. [42] James M Robins, Andrea Rotnitzky, and Lue Ping Zhao. Estimation of regression coefficients when some regressors are not always observed. Journal of the American statistical Association , 89(427):846–866, 1994. [43] Esther Rolf, Jonathan Proctor, Tamma Carleton, Ian Bolliger, Vaishaal Shankar, Miyabi Ishihara, Benjamin Recht, and Solomon Hsiang. A generalizable and accessible approach to machine learning with global satellite imagery. Nature communications, 12(1):4392, 2021. [44] D Rubin. Multiple imputation for nonresponse in surveys. Wiley Series in Probability and Statistics , page 1, 1987. [45] Donald B Rubin. Inference and missing data. Biometrika, 63(3):581–592, 1976. [46] Donald B Rubin. Multiple imputation after 18+ years. Journal of the American statistical Association , 91(434):473–489, 1996. [47] Carl Erik S¨ arndal. Onπ-inverse weighting versus best linear unbiased weighting in probability sampling. Biometrika, 67(3):639–650, 1980. [48] Carl-Erik S¨ arndal, Bengt Swensson, and Jan Wretman.Model assisted survey sampling. Springer Science & Business Media, 2003. [49] Greg Schohn and David Cohn. Less is more: Active learning with support vector machines. In ICML, volume 2, page 6, 2000. [50] Burr Settles. Active learning literature survey. Department of Computer Sciences, University of Wisconsin-Madison, 2009. [51] Simon Tong and Daphne Koller. Support vector machine active learning with applications to text classification. Journal of machine learning research , 2(Nov):45–66, 2001. [52] Aad W Van der Vaart. Asymptotic statistics, volume 3. Cambridge university press, 2000. [53] Harit Vishwakarma, Heguang Lin, Frederic Sala, and Ramya Korlakai Vinayak. Promises and pitfalls of threshold-based auto-labeling. Advances in Neural Information Processing Systems , 36, 2023. [54] Ian Waudby-Smith and Aaditya Ramdas. Estimating means of bounded random variables by betting. Journal of the Royal Statistical Society Series B: Statistical Methodology , 86(1):1–27, 2024. [55] Michael Xie, Neal Jean, Marshall Burke, David Lobell, and Stefano Ermon. Transfer learning from deep features for remote sensing and poverty mapping. In Proceedings of the AAAI conference on artificial intelligence, volume 30, 2016. [56] Matt Zdun. Machine politics: How America casts and counts its votes. Reuters, 2022. [57] Anru Zhang, Lawrence D Brown, and T Tony Cai. Semi-supervised inference: General theory and estimation of means. Annals of Statistics , 47(5):2538–2566, 2019. [58] Jiaqi Zhang, Louis Cammarata, Chandler Squires, Themistoklis P Sapsis, and Caroline Uhler. Active learning for optimal intervention design in causal models. Nature Machine Intelligence , pages 1–10, 2023. [59] Kelly Zhang, Lucas Janson, and Susan Murphy. Statistical inference with M-estimators on adaptively collected data. Advances in neural information processing systems , 34:7460–7471, 2021. 19[60] Yuqian Zhang and Jelena Bradic. High-dimensional semi-supervised learning: in search of optimal inference of the mean. Biometrika, 109(2):387–403, 2022. [61] Tijana Zrnic and Emmanuel J Cand` es. Cross-prediction-powered inference.Proceedings of the National Academy of Sciences, 121(15):e2322083121, 2024. 20A Proofs A.1 Proof of Proposition 1 Recall that ξi ∼ Bern(πˆη(Xi)). For any η ∈ H, we define ξη i = 1{πη(Xi) ≤ πˆη(Xi)}ξi(1 − ξ≤ i ) + 1{πη(Xi) > πˆη(Xi)}(ξi + (1 − ξi)ξ> i ), (13) where ξ≤ i ∼ Bern(πˆη(Xi)−πη(Xi) πˆη(Xi) ) and ξ> i ∼ Bern(πη(Xi)−πˆη(Xi) 1−πˆη(Xi) ) are drawn independently of ξi. This defini- tion couples ξη∗ i with ξi, while ensuring that ξη∗ i ∼ Bern(πη∗ (Xi)). Let ˆθη∗ = 1 n nX i=1   f(Xi) + (Yi − f(Xi)) ξη∗ i πη∗ (Xi) ! . By the central limit theorem, we know that √n(ˆθη∗ − θ∗) d → N(0, σ2 ∗), (14) where σ2 ∗ = Var \u0010 f(X) + (Y − f(X)) ξη∗ πη∗ (X) \u0011 . On the other hand, we have √n(ˆθˆη − θ∗) = √n(ˆθη∗ − θ∗) + √n(ˆθˆη − ˆθη∗ ). For any ϵ >0, we have P(|√n(ˆθˆη − ˆθη∗ )| ≥ϵ) ≤ P(ˆη ̸= η∗) → 0; therefore, √n(ˆθˆη − ˆθη∗ ) p → 0. Putting this fact together with Eq. (14), we conclude that √n(ˆθˆη − θ∗) d → N(0, σ2 ∗) by Slutsky’s theorem. A.2 Proof of Theorem 1 The proof follows a similar argument as the classical proof of asymptotic normality for M-estimation; see [52, Thm. 5.23]. A similar proof is also given for the prediction-powered estimator [3], which is closely related to our active inference estimator. The main difference between our proof and the classical proof is that ˆ η is tuned in a data-adaptive fashion, so the increments in the empirical loss Lπˆη (θ) are not independent. We begin by formally stating the required smoothness assumption. Assumption 1 (Smoothness). The loss ℓ is smooth if: • ℓθ(x, y) is differentiable at θ∗ for all (x, y); • ℓθ is locally Lipschitz around θ∗: there is a neighborhood of θ∗ such that ℓθ(x, y) is C(x, y)-Lipschitz and ℓθ(x, f(x)) is C(x)-Lipschitz in θ, where E[C(X, Y)2] < ∞, E[C(X)2] < ∞; • L(θ) = E[ℓθ(X, Y)] and Lf (θ) = E[ℓθ(X, f(X))] have Hessians, and Hθ∗ = ∇2L(θ∗) ≻ 0. Using the same definition ofξη i as in Eq. (13), let Lη θ,i = ℓθ(Xi, f(Xi))+(ℓθ(Xi, Yi) − ℓθ(Xi, f(Xi))) ξη i πη(Xi) . We define ∇Lη θ,i analogously, replacing the losses with their gradients. Given a function g, let Gn[g(Lη θ)] := 1√n nX i=1 \u0010 g(Lη θ,i) − E[g(Lη θ,i)] \u0011 ; En[g(Lη θ)] := 1 n nX i=1 g(Lη θ,i). We similarly use Gn[g(∇Lη θ)], En[g(∇Lη θ)], etc. Notice that En[Lˆη θ] = Lπˆη (θ). By the differentiability and local Lipschitzness of the loss, for any hn = OP (1) we have Gn[√n(Lη∗ θ∗+hn/√n − Lη∗ θ∗ ) − h⊤ n ∇Lη∗ θ∗ ] p → 0. 21By definition, this is equivalent to nEn[Lη∗ θ∗+hn/√n − Lη∗ θ∗ ] = n(L(θ∗ + hn/√n) − L(θ∗)) + h⊤ n Gn[∇Lη∗ θ∗ ] + oP (1), where L(θ) = E[ℓθ(X, Y)] is the population loss. A second-order Taylor expansion now implies nEn[Lη∗ θ∗+hn/√n − Lη∗ θ∗ ] = 1 2h⊤ n Hθ∗ hn + h⊤ n Gn[∇Lη∗ θ∗ ] + oP (1). At the same time, since P(ˆη ̸= η∗) → 0, we have nEn[Lˆη θ∗+hn/√n − Lˆη θ∗ ] = nEn[Lη∗ θ∗+hn/√n − Lη∗ θ∗ ] + oP (1). Putting everything together, we have shown nEn[Lˆη θ∗+hn/√n − Lˆη θ∗ ] = 1 2h⊤ n Hθ∗ hn + h⊤ n Gn[∇Lη∗ θ∗ ] + oP (1). The rest of the proof is standard. We apply the previous display with hn = ˆhn := √n(ˆθˆη − θ∗) (which is OP (1) by the consistency of ˆθη∗ ; see [52, Thm. 5.23]) and hn = ˜hn := −H−1 θ∗ Gn[∇Lη∗ θ∗ ]: nEn[Lˆη ˆθˆη − Lˆη θ∗ ] = 1 2 ˆh⊤ n Hθ∗ ˆhn + ˆh⊤ n Gn[∇Lη∗ θ∗ ] + oP (1); nEn[Lˆη θ∗+˜hn/√n − Lˆη θ∗ ] = 1 2 ˜h⊤ n Hθ∗ ˜hn + ˜h⊤ n Gn[∇Lη∗ θ∗ ] + oP (1). By the definition of ˆθˆη, the left-hand side of the first equation is smaller than the left-hand side of the second equation. Therefore, the same must be true of the right-hand sides of the equations. If we take the difference between the equations and complete the square, we get 1 2 \u0010√n(ˆθˆη − θ∗) − ˜hn \u0011⊤ Hθ∗ \u0010√n(ˆθˆη − θ∗) − ˜hn \u0011 + oP (1) ≤ 0. Since the Hessian Hθ∗ is positive-definite, it must be the case that √n(ˆθˆη − θ∗) − ˜hn p → 0. By the central limit theorem, ˜hn = −H−1 θ∗ Gn[∇Lη∗ θ∗ ] converges to N(0, Σ∗) in distribution, where Σ∗ = H−1 θ∗ Var \u0012 ∇ℓθ∗ (X, f(X)) + (∇ℓθ∗ (X, Y) − ∇ℓθ∗ (X, f(X))) ξη∗ πη∗ (X) \u0013 H−1 θ∗ . The final statement thus follows by Slutsky’s theorem. A.3 Proof of Proposition 2 We prove the result by an application of the martingale central limit theorem (see Theorem 8.2.4. in [16]). Let ¯∆t denote the increments ∆ t with their mean subtracted out, i.e. ¯∆t = ∆ t − θ∗. To apply the theorem, we first need to verify that the increments ¯∆t = ∆t − θ∗ are martingale increments; this follows because E[ ¯∆t|Ft−1] = E[ ¯∆t|ft, πt] = E[ft(Xt)|ft, πt] + E[Yt − ft(Xt)|ft, πt]E \u0014 ξt πt(Xt)|ft, πt \u0015 − θ∗ = 0, together with the fact that ¯∆t ∈ Ft. The martingale central limit theorem is now applicable given two regularity conditions. The first is that 1 n Pn t=1 σ2 t converges in probability, which holds by assumption. The second condition is the so-called Lindeberg condition, stated below. 22Assumption 2. Let ¯∆t = ∆t − θ∗. We say that ∆t satisfy the Lindeberg condition if for all ϵ >0, 1 n nX t=1 E[ ¯∆2 t 1{|¯∆t| > ϵ√n}|Ft−1] p → 0. Since this condition holds by assumption, we can apply the central limit theorem to conclude √n(ˆθ # »π −θ∗) = 1√n Pn t=1 ¯∆t d → N(0, σ2 ∗). A.4 Proof of Theorem 2 We follow a similar approach as in the proof of Theorem 1, which is in turn similar to the classical argument for M-estimation [52, Thm. 5.23]. In this case, the main difference to the classical proof is that the empirical loss L # »π (θ) comprises martingale, rather than i.i.d. increments. We explain the differences relative to the proof of Theorem 1. We define Lθ,i = ℓθ(Xi, fi(Xi)) + (ℓθ(Xi, Yi) − ℓθ(Xi, fi(Xi))) ξi πi(Xi) , and ∇Lθ,i is defined analogously. We again use the notation Gn[g(Lθ)], En[g(Lθ)], Gn[g(∇Lθ)], En[g(∇Lθ)], etc. As in the classical argument, for any hn = OP (1) we have Gn[√n(Lθ∗+hn/√n − Lθ∗ ) − h⊤ n ∇Lθ∗ ] p → 0. This can be concluded from the martingale central limit theorem, since the variance of the increments tends to zero. Specifically, define the triangular array Ln,i = √n(Lθ∗+hn/√n,i − Lθ∗,i) − h⊤ n ∇Lθ∗,i, and let Vn,i = Var(Ln,i|Fi−1). We have | 1√n Pn i=1 Ln,i| ≤maxi p Vn,i| 1√n Pn i=1 Ln,i√ Vn,i |. By the martingale central limit theorem, 1√n Pn i=1 Ln,i√ Vn,i d → N(0, 1) and, since max i p Vn,i p → 0, we conclude by Slutsky’s theorem that Gn[√n(Lθ∗+hn/√n − Lθ∗ ) − h⊤ n ∇Lθ∗ ] p → 0. The following steps are the same as in the proof of Theorem 1; we conclude that √n(ˆθ # »π − θ∗) − ˜hn p → 0, where ˜hn = −H−1 θ∗ Gn[∇Lθ∗ ]. Finally, we argue that ˜hn converges to N(0, Σ∗) in distribution. To see this, first note that all one-dimensional projections v⊤˜hn converge to v⊤Z, Z ∼ N(0, Σ∗), by the martingale central limit theorem, which is applicable because the Lindeberg condition holds by assumption (see below for statement) and the variance process Vθ∗,n converges to V∗. Once we have the convergence of all one- dimensional projections, convergence of ˜hn follows by the Cram´ er-Wold theorem. Assumption 3. We say that the increments satisfy the Lindeberg condition if, for all v ∈ Sd−1 and ϵ >0, 1 n nX t=1 E[(v⊤∇Lθ∗,t)21{|v⊤∇Lθ∗,t| > ϵ√n}|Ft−1] p → 0. B Experimental details In all our experiments, we have a labeled dataset of n examples. We treat the solution on the full dataset as the ground-truth θ∗ for the purpose of evaluating coverage. In each trial, the underlying data points (Xi, Yi) are fixed and the randomness comes from the labeling decisions ξi. In the sequential experiments, we additionally randomly permute the data points at the beginning of each trial. The experiments in the batch setting average the results over 1000 trials and the experiments in the sequential setting average the results over 100 trials. The Pew dataset is available at [38]; the census dataset is available through Folktables [15]; the Alphafold dataset is available at [2]. As discussed in Section 7.2, to avoid values of π(x) that are close to zero we mix the “standard” sampling rule based on the uncertainty u(x) with a uniform rule πunif = nb n according to a parameter τ ∈ (0, 1). In post-election survey research, we have training data for the prediction model and we use the same data to select τ so as to minimize an empirical approximation of the variance Var( ˆθπ(τ) ), as in Eq. (12). In the 23AlphaFold example and both problems with model fine-tuning we set τ = 0.5 for simplicity. In the census example, the trained predictor of model error e(x) rarely gives very small values, and so we set τ = 0.001. In each experiment, we vary nb over a grid of uniformly spaced values. We take 20 grid values for the batch experiments and 10 grid values for the sequential experiments. The plots of interval width and coverage linearly interpolate between the respective values obtained at the grid points. There linearly interpolated values are used to produce the plots of budget save: for all values of nb from the grid, we look for n′ b such that the (linearly interpolated) width of active inference at sample size n′ b matches the interval width of classical (resp. uniform) inference at sample size nb. For the leftmost plot in Figures 1-3 and Figures 5-6, we uniformly sample five trials for a fixed nb and show the intervals for all methods in those same five trials. We arbitrarily select nb to be the fourth largest value in the grid of budget sizes for all experiments. C Non-asymptotic results While our results focus on asymptotic confidence intervals based on the central limit theorem, some of them—in particular, those for mean estimation—have direct non-asymptotic and time-uniform analogues. We explain this extension for the sequential algorithm, as it subsumes the extension for the batch setting. Let ∆t = ft(Xt) + (Yt − ft(Xt)) ξt πt(Xt) . As explained in Section 6, ∆ t have a common conditional mean: E[∆t|∆1, . . . ,∆t−1] = θ∗. Moreover, if Yt and ft(Xt) are almost surely bounded, and πt(Xt) is almost surely bounded from below, then ∆ t are bounded as well. (Given that we construct πt by “τ-mixing” it with a uniform rule, as explained in Section 7.2, in our applications πt(Xt) is always bounded from below since πt(x) ≥ τ nb n .) Therefore, given that we have bounded observations (with a known bound) having a common conditional mean, we can apply the recent betting-based methods [36, 54] for constructing non-asymptotic confidence intervals and time-uniform confidence sequences satisfying P(θ∗ ∈ Ct, ∀t) ≥ 1 − α. We demonstrate the non-asymptotic extension in the problem of post-election survey analysis from Sec- tion 8.1. Figure 8 provides a non-asymptotic analogue of the corresponding batch results from Figure 1, applying the method from Theorem 3 of Waudby-Smith and Ramdas [54] to form a non-asymptotic con- fidence interval. Qualitatively we observe a similar comparison as before—active inference outperforms both uniform sampling and classical inference—though the methods naturally overcover as a result of using non-asymptotic intervals that do not have exact coverage. 0.6 0.7 0.8 Biden's approval rate 247 331 445 598 803 nb 0.04 0.06 0.09 0.14 0.21interval width 247 386 525 664 804 nb 0.6 0.7 0.8 0.9 1.0coverage active uniform classical 0.25 0.30 0.35 0.40 Trump's approval rate 248 332 447 600 806 nb 0.04 0.06 0.08 0.10 0.14interval width 248 387 527 666 806 nb 0.6 0.7 0.8 0.9 1.0coverage active uniform classical Figure 8: Non-asymptotic experiments. Example intervals in five randomly chosen trials (left), average confidence interval width (middle), and coverage (right) in post-election survey research with non-asymptotic confidence intervals. 24",
      "meta_data": {
        "arxiv_id": "2403.03208v2",
        "authors": [
          "Tijana Zrnic",
          "Emmanuel J. Candès"
        ],
        "published_date": "2024-03-05T18:46:50Z",
        "pdf_url": "https://arxiv.org/pdf/2403.03208v2.pdf"
      },
      "llm_extracted_info": {
        "main_contributions": "The paper introduces 'active inference,' a novel methodology for statistical inference that strategically guides data collection using machine learning. The core problem addressed is the high cost and bias associated with labeled data collection, and the pitfall of solely relying on machine learning predictions for inference. The main contributions are: 1) It leverages a black-box machine learning model to identify which data points are most beneficial to label by prioritizing those where the model is uncertain and relying on predictions where it is confident. 2) It constructs provably valid confidence intervals and hypothesis tests for any black-box machine learning model and data distribution. 3) It significantly reduces the number of samples needed to achieve the same level of accuracy as non-adaptively collected data, enabling smaller confidence intervals and more powerful p-values. Experiments demonstrate over 80% budget savings compared to classical inference and substantial savings over prediction-powered inference (PPI). 4) The methodology is applicable to all convex M-estimation problems and supports both batch and sequential data collection settings.",
        "methodology": "Active inference operates on the principle of sampling data points based on a machine learning model's uncertainty. A sampling rule π(x) is derived from an uncertainty measure u(x) (e.g., predicting error magnitude in regression, or using probability distribution entropy in classification), with π(x) being proportional to u(x) and normalized to meet a predefined labeling budget. In the batch setting, a pre-trained model f is fixed, and labels are collected simultaneously. The estimator used is an augmented inverse propensity weighting (AIPW) type for mean estimation, and a modified M-estimator for general convex M-estimation, where the loss function incorporates predictions and inverse propensity weights. In the sequential setting, the model ft and sampling rule πt are iteratively updated as new labels are collected, leveraging a martingale structure for tractable inference via the martingale central limit theorem. An 'oracle' sampling rule is derived to minimize variance, which samples proportionally to the expected magnitude of model error. Practically, the sampling rule π(x) ∝ u(x) is stabilized by mixing it with a uniform sampling rule (πunif = nb/n) using a tuning parameter τ, defined as π(τ)(x) = (1 - τ) * π(x) + τ * πunif(x). τ can be optimized on historical data or set as a constant.",
        "experimental_setup": "The active inference methodology is evaluated against two baselines: the 'uniform' baseline (prediction-powered inference, PPI), which uses ML predictions but employs uniform random sampling, and the 'classical' baseline, which removes ML entirely and relies on uniform random sampling. Confidence intervals are computed based on asymptotic normality with an error level α=0.1. Performance is measured by average interval width, coverage, and percentage of budget saved, averaged over 1000 trials for batch settings and 100 trials for sequential settings. The ground-truth parameter is derived from the full dataset. Three real-world applications are used: 1) Post-election survey research (Pew Research Center 2020 US election data) for estimating average approval rates (binary classification) using an XGBoost model. 2) Census data analysis (American Community Survey PUMS from California 2019) for estimating linear regression coefficients (regression) using XGBoost for both the prediction model and a separate error prediction model. 3) AlphaFold-assisted proteomics research for estimating odds ratios related to protein phosphorylation and intrinsically disordered regions (binary outcome), leveraging AlphaFold predictions. Additionally, fine-tuning experiments for survey and census data are conducted, starting with poorly trained models and iteratively updating them as data is collected.",
        "limitations": "The primary theoretical guarantees are asymptotic, meaning the validity of confidence intervals holds as the sample size approaches infinity, though non-asymptotic analogues are discussed for mean estimation. The consistency conditions for asymptotic normality in the batch setting rely on assumptions about the scaling of the budget (nb/n) and the discreteness of the tuning parameter space. For general M-estimation, the consistency of the estimator itself (ˆθη* p → θ*) is assumed. In the sequential setting, convergence of the model-sampling rule pairs and the Lindeberg condition for martingale increments are required assumptions. The 'oracle' sampling rules derived for optimal variance reduction are not directly implementable because they depend on unobserved information. Practical sampling rules approximate this optimum, and issues like a model mistakenly estimating near-zero uncertainty for large errors can arise, potentially inflating estimator variance. This is mitigated by mixing with a uniform rule. In sequential settings, there's a risk of budget underutilization if the model consistently estimates low uncertainty, necessitating periodic deviations from the sampling rule to ensure budget consumption.",
        "future_research_directions": "Not mentioned"
      }
    },
    {
      "title": "Convergence Rates of Variational Inference in Sparse Deep Learning",
      "abstract": "Variational inference is becoming more and more popular for approximating\nintractable posterior distributions in Bayesian statistics and machine\nlearning. Meanwhile, a few recent works have provided theoretical justification\nand new insights on deep neural networks for estimating smooth functions in\nusual settings such as nonparametric regression. In this paper, we show that\nvariational inference for sparse deep learning retains the same generalization\nproperties than exact Bayesian inference. In particular, we highlight the\nconnection between estimation and approximation theories via the classical\nbias-variance trade-off and show that it leads to near-minimax rates of\nconvergence for H\\\"older smooth functions. Additionally, we show that the model\nselection framework over the neural network architecture via ELBO maximization\ndoes not overfit and adaptively achieves the optimal rate of convergence.",
      "full_text": "arXiv:1908.04847v2  [math.ST]  5 Sep 2019 Convergence Rates of V ariational Inference in Sparse Deep Learning Convergence Rates of V ariational Inference in Sparse Deep Learning Badr-Eddine Chérief-Abdellatif badr.eddine.cherief.abdellatif@ensae.fr CREST, ENSAE, Institut Polytechnique de Paris Editor: Abstract V ariational inference is becoming more and more popular for approximating intractable pos- terior distributions in Bayesian statistics and machine le arning. Meanwhile, a few recent works have provided theoretical justiﬁcation and new insig hts on deep neural networks for estimating smooth functions in usual settings such as no nparametric regression. In this paper, we show that variational inference for sparse de ep learning retains the same generalization properties than exact Bayesian inference. In particular, we highlight the connection between estimation and approximation theories via the classical bias-variance trade-oﬀ and show that it leads to near-minimax rates of conv ergence for Hölder smooth functions. Additionally , we show that the model selection f ramework over the neural net- work architecture via ELBO maximization does not overﬁt and adaptively achieves the optimal rate of convergence. Keywords:V ariational Inference, Neural Networks, Deep Learning, Ge neralization 1. Introduction Deep learning (DL) is a ﬁeld of machine learning that aims to model data using complex ar- chitectures combining several nonlinear transformations with hundreds of parameters called Deep Neural Networks (DNN) ( LeCun et al. , 2015; Goodfellow et al. , 2016). Although gen- eralization theory that explains why DL generalizes so well is still an open problem, it is widely acknowledged that it mainly takes advantage of large datasets containing millions of samples and a huge computing power coming from clusters of gr aphics processing units. V ery popular architectures for deep neural networks such as the m ultilayer perceptron, the convo- lutional neural network ( Lecun et al. , 1998), the recurrent neural network ( Rumelhart et al. , 1986) or the generative adversarial network ( Goodfellow et al. , 2014) have shown impressive results and have enabled to perform better than humans in var ious important areas in ar- tiﬁcial intelligence such as image recognition, game playi ng, machine translation, computer vision or natural language processing, to name a few promine nt examples. An outstanding example is AlphaGo ( Silver et al. , 2017), an artiﬁcial intelligence developed by Google that learned to play the game of Go using deep learning techniques and even defeated the world champion in 2016. The Bayesian approach, leading to popular methods such as Hid den Markov Models (Baum and Petrie , 1966) and Particle Filtering ( Doucet and Johansen , 2009), provides a natural way to model uncertainty . Some prior distribution i s put over the space of parameters and represents the prior belief as to which parameters are li kely to have generated the data 1Chérief-Abdellatif before any datapoint is observed. Then this prior distribut ion is updated using the Bayes rule when new data arrive in order to capture the more likely param eters given the observations. Unfortunately , exact Bayesian inference is computationall y challenging for complex models as the normalizing constant of the posterior distribution i s often intractable. In such cases, approximate inference methods such as variational inferen ce (VI) ( Jordan et al. , 1999) and expectation propagation ( Minka, 2001) are popular to overcome intractability in Bayesian modeling. The idea of VI is to minimize the Kullback-Leibler (KL) divergence with respect to the posterior given a set of tractable distributions, whi ch is also equivalent to maximizing a numerical criterion called the Evidence Lower Bound (ELBO). Recent advances of VI have shown great performance in practice and have been appli ed to many machine learning problems ( Hoﬀman et al. , 2013; Kingma and W elling , 2013). The Bayesian approach to learning in neural networks has a lon g history . Bayesian Neural Networks (BNN) have been ﬁrst proposed in the 90s and wi dely studied since then (MacKay, 1992a; Neal, 1995). They oﬀer a probabilistic interpretation and a measure of uncertainty for DL models. They are more robust to overﬁttin g than classical neural net- works and still achieve great performance even on small data sets. A prior distribution is put on the parameters of the network, namely the weight matri ces and the bias vectors, for instance a Gaussian or a uniform distribution, and Bayesian i nference is done through the likelihood speciﬁcation. Nevertheless, state-of-the-ar t neural networks may contain millions of parameters and the form of a neural network is not adapted t o exact integration, which makes the posterior distribution be intractable in practic e. Modern approximate inference mainly relies on VI, with sometimes a ﬂavor of sampling techn iques. A lot of recent pa- pers have investigated variational inference for DNNs ( Hinton and van Camp , 1993; Graves, 2011; Blundell et al. , 2015) to ﬁt an approximate posterior that maximizes the evidence lower bound. F or instance, Blundell et al. (2015) introduced Bayes by Backprop, one of the most famous techniques of VI applied to neural networks, which de rives a fully factorized Gaussian approximation to the posterior: using the reparameterizat ion trick ( Opper and Archambeau , 2008), the gradients of ELBO towards parameters of the Gaussian ap proximation can be computed by backpropagation, and then be used for updates. A nother point of interest in DNNs is the choice of the prior. Blundell et al. (2015) introduced a mixture of Gaus- sians prior on the weights, with one mixture tightly concent rated around zero, imitating the sparsity-inducing spike-and-slab prior. This oﬀers a Ba yesian alternative to the dropout regularization procedure ( Srivastava et al. , 2014) which injects sparsity in the network by switching oﬀ randomly some of the weights of the network. Thi s idea goes back to David MacKay who discussed in his thesis the possibility of choosi ng a spike-and-slab prior over the weights of the neural network ( MacKay, 1992b). More recently , Rockova and Polson (2018) introduced Spike-and-Slab Deep Learning (SS-DL), a fully Bayesian alternative to dropout for improving generalizability of deep ReLU networ ks. 1.1 Related work Although deep learning is extremely popular, the study of ge neralization properties of DNNs is still an open problem. Some works have been conducted in or der to investigate the theoret- ical properties of neural networks from diﬀerent points of v iew. The literature developed in the past decades can be shared in three parts. First, the appr oximation theory wonders how 2Convergence Rates of V ariational Inference in Sparse Deep Learning well a function can be approximated by neural networks. The ﬁ rst studies were mostly con- ducted to obtain approximation guarantees for shallow neur al nets with a single hidden layer (Cybenko, 1989; Barron, 1993). Since then, modern research has focused on the expressive power of depth and extended the previous results to deep neur al networks with a larger num- ber of layers ( Bengio and Delalleau , 2011; Y arotsky, 2016; Petersen and V oigtländer , 2017; Grohs et al. , 2019). Indeed, even though the universal approximation theorem (Cybenko, 1989) states that a shallow neural network containing a ﬁnite num ber of neurons can approx- imate any continuous function on compact sets under mild ass umptions on the activation function, recent advances showed that a shallow network req uires exponentially many neu- rons in terms of the dimension to represent a monomial functi on, whereas linearly many neurons are suﬃcient for a deep network ( Rolnick and T egmark , 2018). Second, as the ob- jective function in deep learning is known to be nonconvex, t he optimization community has discussed the landscape of the objective as well as the dy namics of some learning algo- rithms such as Stochastic Gradient Descent (SGD) ( Baldi and Hornik , 1989; Stanford et al. , 2000; Soudry and Carmon , 2016; Kawaguchi, 2016; Kawaguchi et al. , 2019; Nguyen et al. , 2019; Allen-Zhu et al. , 2019; Du et al. , 2019). Finally , the statistical learning community has investigated generalization properties of DNNs, see Barron (1994); Zhang et al. (2017); Schmidt-Hieber (2017); Suzuki (2018); Imaizumi and F ukumizu (2019); Suzuki (2019). In particular, Schmidt-Hieber (2017) and Suzuki (2019) showed that estimators in nonparamet- ric regression based on sparsely connected DNNs with ReLU ac tivation function and wisely chosen architecture achieve the minimax estimation rates ( up to logarithmic factors) under classical smoothness assumptions on the regression functi on. In the same time, Bartlett et al. (2017) and Neyshabur et al. (2018) respectively used Rademacher complexity and covering number, and P AC-Bayes theory to get spectrally-normalized m argin bounds for deep ReLU networks. More recently , Imaizumi and F ukumizu (2019) and Hayakawa and Suzuki (2019) showed the superiority of DNNs over linear operators in some situations when DNNs achieve the minimax rate of convergence while alternative methods f ail. F rom a Bayesian point of view, Rockova and Polson (2018) and Suzuki (2018) studied the concentration of the pos- terior distribution while Vladimirova et al. (2019) investigated the regularization eﬀect of prior distributions at the level of the units. Such as for generalization properties of DNNs, only little a ttention has been put in the literature towards the theoretical properties of VI until r ecently . Alquier et al. (2016) stud- ied generalization properties of variational approximati ons of Gibbs distributions in machine learning for bounded loss functions. Alquier and Ridgway (2017); Zhang and Gao (2017); Sheth and Khardon (2017); Bhattacharya et al. (2018); Chérief-Abdellatif and Alquier (2018); Cherief-Abdellatif (2019); Jaiswal et al. (2019b) extended the previous guarantees to more general statistical models and studied the concentration o f variational approximations of the posterior distribution, while W ang and Blei (2018) provided Bernstein-von-Mises’ theorems for variational approximations in parametric models. Huggins et al. (2018); Campbell and Li (2019); Jaiswal et al. (2019a) discussed theoretical properties of variational inferen ce algo- rithms based on various divergences (respectively W assers tein and Hellinger distances, and Rényi divergence). More recently , Chérief-Abdellatif et al. (2019) presented generalization bounds for online variational inference. All these works sh ow that under mild conditions, the variational approximation is consistent and achieves t he same rate of convergence than the Bayesian posterior distribution it approximates. Note t hat Alquier and Ridgway (2017); 3Chérief-Abdellatif Bhattacharya et al. (2018); Chérief-Abdellatif and Alquier (2018); Cherief-Abdellatif (2019) restricted their studies to tempered versions of the poster ior distribution where the likelihood is raised to an α-power ( α < 1) as it is known to require less stringent assumptions to obta in consistency and to be robust to misspeciﬁcation, see respec tively Bhattacharya et al. (2016) and Grünwald and V an Ommen (2017). Nevertheless, some questions remain unanswered, as the theoretical study of generalization of variational i nference for deep neural networks. 1.2 Contributions This paper aims at ﬁlling the gap between theory and practice when using variational ap- proximations for tempered Bayesian Deep Neural Networks. T o the best of our knowledge, this is the ﬁrst paper to present theoretical generalizatio n error bounds of variational infer- ence for Bayesian deep learning. Inspired by the related lite rature, our work is motivated by the following questions: • Do consistency of Bayesian DNNs still hold when an approximat ion is used instead of the exact posterior distribution, and can we obtain the sa me rates of convergence than those obtained for the regular posterior distribution and frequentist estimators ? • Is it possible to obtain a nonasymptotic generalization err or bound that holds for (almost) any generating distribution function and that giv es a general formula ? • What about the consistency of numerical algorithms used to c ompute these variational approximations ? • Can we obtain new insights on the structure of the networks ? The main contribution of this paper, a nonasymptotic genera lization error bound for vari- ational inference in sparse DL in the nonparametric regress ion framework, answers the ﬁrst two questions. This generalization result is similar to the oretical inequalities in the seminal works of Suzuki (2018); Imaizumi and F ukumizu (2019); Rockova and Polson (2018) on gen- eralization properties of deep neural networks, and is insp ired by the general literature on the consistency of variational approximations ( Alquier and Ridgway , 2017; Bhattacharya et al. , 2018). In particular, it states that under the same conditions, s parse variational approxima- tions of posterior distributions of deep neural networks ar e consistent at the same rate of convergence than the exact posterior. It also raises the question of ﬁnding a relevant general deﬁn ition of consistency that can be used to provide theoretical properties for the exact Bayes ian DNNs distribution and their variational approximations. Indeed, a classical criterio n used to assess frequentist guarantees for Bayesian estimators is the concentration of the posterio r (to the true distribution) which is deﬁned as the asymptotic concentration of the Bayesian est imator to the true distribution (Ghosal et al. , 2000). Nevertheless, posterior concentration to the true distr ibution only applies when the model is well speciﬁed, or at least when the m odel contains distributions in the neighborhood of the true distribution, which is probl ematic for misspeciﬁed models e.g. when the neural network does not suﬃciently approximate the generating distribution. And although the posterior distribution may concentrate to the best approximation of the true distribution in KL divergence in such misspeciﬁed models, t here exists pathological cases 4Convergence Rates of V ariational Inference in Sparse Deep Learning where the regular Bayesian posterior is not consistent at all , see Grünwald and V an Ommen (2017). This is the reason why we focus here on tempered posteriors which are robust to such misspeciﬁcation. Therefore, we introduce in Section 2 a notion of consistency of a Bayesian estimator which is closely related to the notion of c oncentration - even stronger - and which enables a more robust formulation of generalizati on error bounds for variational approximations. See Appendix A for more details on the connection between the notions of consistency and concentration. Then we focus on optimization aspects. W e no longer assume an ideal optimization, as done for instance in Schmidt-Hieber (2017); Imaizumi and F ukumizu (2019). W e address in this paper the question of the consistency of numerical algo rithms used to compute our ideal approximations. W e consider an optimization error given by any algorithm and independent to the statistical error, and we show how it aﬀects our genera lization result. Our upper bound highlights the connection between the consistency of the va riational approximation and the convergence of the ELBO. W e also provide insights on the structure of the network whic h leads to optimal rates of convergence, i.e. its depth, its width and its sparsity . I ndeed, in our ﬁrst generalization error bound, the structure of the network is ideally tuned fo r some choice of the generating function, and we show how to choose such a structure. Neverth eless, the characteristics of the regression function may be unknown, e.g. we may know that the regression function is Hölder continuous but we ignore its level of smoothness. W e propose here an automated method for choosing the architecture of the network. W e introduce a cla ssical model selection framework based on the ELBO criterion ( Cherief-Abdellatif, 2019), and we show that the variational approximation associated with the selected structure does not overﬁt and adaptively achieves the optimal rate of convergence even without any oracle info rmation. The rest of this paper is organized as follows. Section 2 introduces the notations and the framework that will be considered in the paper, and prese nts sparse spike-and-slab variational inference for deep neural networks. Section 3 provides theoretical generalization error bounds for variational approximations of DNNs and sho ws the optimality of the method for estimating Hölder smooth functions. Finally , insights on the choice of the architecture of the network are given in Section 4 via the ELBO maximization framework. All the proofs are deferred to the appendix. 2. Sparse deep variational inference Let us introduce the notations and the statistical framework we adopt in this paper. F or any vector x = ( x1, ..., xd) ∈ [−1, 1]d and any real-valued function f deﬁned on [−1, 1]d, d > 0, we denote: ∥x∥∞ = max 1≤i≤d |xi| , ∥f∥2 = (∫ f2 ) 1/2 and ∥f∥∞ = sup y∈[−1,1]d |f(y)|. F or any k ∈ { 0, 1, 2, ...}d , we deﬁne |k| = ∑ d i=1 ki and the mixed partial derivatives when all partial derivatives up to order |k| exist: Dkf(x) = ∂|k|f ∂k1 x1...∂kd xd (x). 5Chérief-Abdellatif W e also introduce the notion of β-Hölder continuity for β > 0. W e denote ⌊β⌋ the largest integer strictly smaller than β. Then f is said to be β-Hölder continuous ( T sybakov, 2008) if all partial derivatives up to order ⌊β⌋ exist and are bounded, and if: ∥f∥Cβ := max |k|≤⌊β⌋ ∥Dkf∥∞ + max |k|=⌊β⌋ sup x,y∈[−1,1]d,x̸=y |Dkf(x) − Dkf(y)| ∥x − y∥β−⌊β⌋ ∞ < +∞. ∥f∥Cβ is the norm of the Hölder space Cβ = {f/∥f∥Cβ < +∞}. 2.1 Nonparametric regression W e consider the nonparametric regression framework. W e hav e a collection of random vari- ables (Xi, Yi) ∈ [−1, 1]d ×R for i = 1 , ..., n which are independent and identically distributed (i.i.d.) with the generating process: { Xi ∼ U ([−1, 1]d), Yi = f0(Xi) + ζi where U([−1, 1]d) is the uniform distribution on the interval [−1, 1]d, ζ1, ..., ζn are i.i.d. Gaus- sian random variables with mean 0 and known variance σ2, and f0 : [ −1, 1]d → R is the true unknown function. F or instance, the true regression functi on f0 may belong to the set Cβ of Hölder functions with level of smoothness β. 2.2 Deep neural networks W e call deep neural network any map fθ : Rd → R deﬁned recursively as follows:      x(0) := x, x(ℓ) := ρ(Aℓx(ℓ−1) + bℓ) for ℓ = 1 , ..., L − 1, fθ(x) := ALx(L−1) + bL where L ≥ 3. ρ is an activation function acting componentwise. F or instan ce, we can choose the ReLU activation function ρ(u) = max( u, 0). Each Aℓ ∈ RDℓ×Dℓ−1 is a weight matrix such that its (i, j) coeﬃcient, called edge weight, connects the j-th neuron of the (ℓ − 1)-th layer to the i-th neuron of the ℓ-th layer, and each bℓ ∈ RDℓ is a shift vector such that its i-th coeﬃcient, called node vector, represents the weight as sociated with the i-th node of layer ℓ. W e set D0 = d the number of units in the input layer, DL = 1 the number of units in the output layer and Dℓ = D the number of units in the hidden layers. The architecture of the network is characterized by its number o f edges S, i.e. the total number of nonzero entries in matrices Aℓ and vectors bℓ, its number of layers L ≥ 3 (excluding the input layer), and its width D ≥ 1. W e have S ≤ T where T = ∑ L ℓ=1 Dℓ(Dℓ−1 + 1) is the total number of coeﬃcients in a fully connected network. By no w, we consider that S, L and D are ﬁxed, and d = O(1) as n → +∞. In particular, we assume that d ≤ D, which implies that T ≤ LD(D + 1). W e also suppose that the absolute values of all coeﬃcients a re upper bounded by some positive constant B ≥ 2. This boundedness assumption will be relaxed in the appendix, see Appendix G. Then, the parameter of a DNN is θ = {(A1, b1), ..., (AL, bL)}, and we denote Θ S,L,D the set of all possible parameters. W e will also alternative ly consider the stacked coeﬃcients parameter θ = ( θ1, ..., θT ). 6Convergence Rates of V ariational Inference in Sparse Deep Learning 2.3 Bayesian modeling W e adopt a Bayesian approach, and we place a spike-and-slab pr ior π (Castillo et al. , 2015) over the parameter space Θ S,L,D (equipped with some suited sigma-algebra) that is deﬁned hierarchically . The spike-and-slab prior is known to be a re levant alternative to dropout for Bayesian deep learning, see Rockova and Polson (2018). First, we sample a vector of binary indicators γ = ( γ1, ..., γT ) ∈ { 0, 1}T uniformly among the set SS T of T -dimensional binary vectors with exactly S nonzero entries, and then given γt for each t = 1 , ..., T , we put a spike-and-slab prior on θt that returns 0 if γt = 0 and a random sample from a uniform distribution on [−B, B ] otherwise: { γ ∼ U (SS T ), θt|γt ∼ γt U([−B, B ]) + (1 − γt)δ{0}, t = 1 , ..., T where δ{0} is a point mass at 0 and U([−B, B ]) is a uniform distribution on [−B, B ]. W e recall that the sparsity level S is ﬁxed here and that this assumption will be relaxed in Section 4. Remark 2.1. We consider uniform distributions for simplicity as in simi lar works ( Rockova and Polson , 2018; Suzuki, 2018), but Gaussian distributions can be used as well when workin g on an un- bounded parameter set Θ S,L,D, see Theorem 7 in Appendix G. Then we deﬁne the tempered posterior distribution πn,α on parameter θ ∈ Θ S,L,D using prior π for any α ∈ (0, 1): πn,α(dθ) ∝ exp ( − α 2σ2 n∑ i=1 (Yi − fθ(Xi))2 ) π(dθ), which is a slight variant of the deﬁnition of the regular Bayes ian posterior (for which α = 1 ). This distribution is known to be easier to sample from, to req uire less stringent assumptions to obtain concentration, and to be robust to misspeciﬁcatio n, see respectively Behrens et al. (2012), Bhattacharya et al. (2016) and Grünwald and V an Ommen (2017). 2.4 Sparse variational inference The variational Bayes approximation ˜πn,α of the tempered posterior is deﬁned as the projec- tion (with respect to the Kullback-Leibler divergence) of t he tempered posterior onto some set FS,L,D: ˜πn,α = arg min q∈FS,L,D KL(q∥πn,α). which is equivalent to: ˜πn,α = arg min q∈FS,L,D { α 2σ2 n∑ i=1 ∫ (Yi − fθ(Xi))2q(dθ) + KL(q∥π) } (1) where the function inside the argmin operator in ( 1) is the opposite of the evidence lower bound Ln(q). 7Chérief-Abdellatif W e choose a sparse spike-and-slab variational set FS,L,D - see for instance T onolini et al. (2019) - which can be seen as an extension of the popular mean-ﬁeld v ariational set with a dependence assumption specifying the number of active neu rons. The mean-ﬁeld ap- proximation is based on a decomposition of the space of param eters Θ S,L,D as a prod- uct θ = ( θ1, ..., θT ) and consists in compatible product distributions on each pa rameter θt, t = 1 , ..., T . Here, we ﬁt a distribution in the family that matches the pri or: we ﬁrst choose a distribution πγ on the set SS T that selects a T -dimensional binary vector γ with S nonzero entries, and then we place a spike-and-slab variational app roximation on each θt given γt: { γ ∼ πγ , θt|γt ∼ γt U([lt, ut]) + (1 − γt)δ{0} for each t = 1 , ..., T where −1 ≤ lt ≤ ut ≤ 1, with the distribution πγ and the intervals [lt, ut], t = 1 , ..., T as the hyperparameters of the variational set FS,L,D. In particular, if we choose a deterministic πγ = δ{γ′} with γ′ ∈ S S T , then we will obtain a parametric mean-ﬁeld approximation. See Section 6.6 of the PhD thesis of Gal (2016) for a more detailed discussion on the connection between Gaussian mean-ﬁeld and sparse spike-and-slab post erior approximations. The generalization error of the tempered posterior πn,α and of its variational approxima- tion ˜πn,α is the expected average of the squared L2-distance to the true generating function over the Bayesian estimator: E [ ∫ ∥fθ − f0∥2 2πn,α(dθ) ] and E [ ∫ ∥fθ − f0∥2 2˜πn,α(dθ) ] . W e say that a Bayesian estimator is consistent at rate rn → 0 if its generalization error is up- per bounded by rn. Notice that consistency of the Bayesian estimator implies c oncentration to f0. Again, see Appendix A for the connection between these two notions. 3. Generalization of variational inference for neural networks The ﬁrst result of this section is an extension of the result o f Rockova and Polson (2018) on the Bayesian distribution for Hölder regression functions. Indeed, we provide a concentration result on the posterior distribution for the expected L2-distance instead of the empirical L2- distance, which enables generalization instead of reconst ruction on the training datapoints. This result is then extended again to the variational approx imation for our deﬁnition of consistency: we show that we can still achieve near-optimal ity using an approximation of the posterior without any additional assumption. Finally , we explain how we can incorporate optimization error in our generalization results. 3.1 Concentration of the posterior Rockova and Polson (2018) gives the ﬁrst posterior concentration result for deep ReL U net- works when estimating Hölder smooth functions in nonparame tric regression with empirical L2-distance. The authors highlight the ﬂexibility of DNNs ove r other methods for estimating β-Hölder smooth functions as there is a large range of values o f the level of smoothness β for which one can obtain concentration, e.g. 0 < β < d for a DNN against 0 < β < 1 for a Bayesian tree. 8Convergence Rates of V ariational Inference in Sparse Deep Learning The following theorem provides the concentration of the tem pered posterior distribution πn,α for deep ReLU neural networks when using the expected L2-distance for some suitable architecture of the network: Theorem 1. Let us assume that α ∈ (0, 1), that f0 is β-Hölder smooth with 0 < β < d and that the activation function is ReLU. We consider the archit ecture of Rockova and Polson (2018) for some positive constant CD independent of n: L = 8 + ( ⌊log2 n⌋ + 5)(1 + ⌈log2 d⌉), D = CD⌊n d 2β+d / log n⌋, S ≤ 94d2(β + 1)2dD(L + ⌈log2 d⌉). Then the tempered posterior distribution πn,α concentrates at the minimax rate rn = n −2β 2β+d up to a (squared) logarithmic factor for the expected L2-distance in the sense that: πn,α ( θ ∈ Θ S,L,D / ∥fθ − f0∥2 2> M n · n −2β 2β+d · log2 n ) − − − − − → n→+∞ 0 in probability as n → +∞ for any Mn → +∞. In order to prove Theorem 1, we actually have to check that the so-called prior mass condition is satisﬁed: π ( θ ∈ Θ S,L,D / ∥fθ − f0∥2 2≤ rn ) ≥ e−nrn . (2) This assumption, introduced in Ghosal et al. (2000) in order to obtain the concentration of the regular posterior distribution states that the prior must give enough mass to some neighborhood of the true parameter. As shown in Bhattacharya et al. (2016), this condition is even suﬃcient for tempered posteriors. Actually , this in equality was ﬁrst stated using the KL divergence instead of the expected L2-distance (see Condition 2.4 in Theorem 2.1 in Ghosal et al. (2000)), but the KL metric is equivalent to the squared L2-metric in regression problems with Gaussian noise. This prior mass condition giv es us the rate of convergence of the tempered posterior rn = n −2β 2β+d (up to a squared logarithmic factor) which is known to be optimal when estimating β-Hölder smooth functions ( T sybakov, 2008). Note that the log2 n term is common in the theoretical deep learning literature ( Imaizumi and F ukumizu , 2019; Suzuki, 2019; Schmidt-Hieber, 2017). Remark 3.1. The number of parameters of order n 2d 2β+d / log n ∈ [n2/3/ log(n), n2/ log(n)] is high compared to standard machine learning methods, whic h may lead to overﬁtting and hence prevent the procedure from achieving the minimax rate of convergence. The sparsity parameter S which gives a network with a small number of nonzero paramete rs along with the spike-and-slab prior help us tackle this issue and obtai n optimal rates of convergence (up to logarithmic factors). 9Chérief-Abdellatif 3.2 A generalization error bound The result we state in this subsection applies to a wide range of activation functions, includ- ing the popular ReLU activation and the identity map: Assumption 3.1. In the following, we assume that the activation function ρ is 1-Lispchitz continuous (with respect to the aboluste value) and is such t hat for any x ∈ R, |ρ(x)| ≤ | x|. W e do not assume any longer that the regression function is β-Hölder and we consider any structure (S, L, D). The following theorem gives a generalization error bound w hen using variational approximations instead of exact tempere d posteriors for DNNs. The proof is given in Appendix B and is based on P AC-Bayes theory ( Catoni, 2007; Guedj, 2019): Theorem 2. For any α ∈ (0, 1), E [ ∫ ∥fθ − f0∥2 2˜πn,α(dθ) ] ≤ 2 1 − α inf θ∗∈Θ S,L,D ∥fθ∗ − f0∥2 2+ 2 1 − α ( 1 + σ2 α ) rS,L,D n , (3) with rS,L,D n = LS n log(BD) + 2S n log(BLD) + S n log ( 7dL max (n S , 1 ) ) . The oracle inequality ( 3) ensures consistency of variational Bayes for estimating ne ural networks and provides the associated rate of convergence gi ven the structure (S, L, D). In- deed, if f0 is a neural network with structure (S, L, D), then the inﬁmum term on the right hand side of the inequality vanishes and we obtain a rate of co nvergence of order rS,L,D n ∼ max (S log(nL/S) n , LS log D n ) , which underlines a linear dependence on the number of layers and the sparsity . In fact, this rate of convergence is determined by the extended prior mass condition (Alquier and Ridgway , 2017; Chérief-Abdellatif and Alquier , 2018; Cherief-Abdellatif, 2019), which requires that in addition to the previous prior mass condition of Ghosal et al. (2000) and Bhattacharya et al. (2016), the variational set FS,L,D must contain probability distributions q that are concen- trated enough around the true generating function f0. One of the main ﬁndings of Theorem 2 is that our choice of the sparse spike-and-slab variational set FS,L,D is rich enough and that both conditions are actually similar and lead to the sam e rate of convergence. Hence, the rate of convergence is the one that satisﬁes the prior mas s condition ( 2). In particular, as the prior distribution is uniform over the parameter spac e, the negative logarithm of the prior mass of the neighborhood of the true regression functi on in Equation ( 2) is a local covering entropy , that is the logarithm of the number of rS,L,D n -balls needed to cover a neigh- borhood of the true regression function. Especially , it has been shown in previous studies that this local covering entropy fully characterizes the ra te of convergence of the empirical risk minimizer for DNNs ( Schmidt-Hieber, 2017; Suzuki, 2019). The rate rS,L,D n we obtain in this work is exactly of the same order than the upper bound on t he covering entropy number given in Lemma 5 in Schmidt-Hieber (2017) and in Lemma 3 in Suzuki (2019) which derive rates of convergence for the empirical risk minimizer using diﬀerent proof techniques. Note 10Convergence Rates of V ariational Inference in Sparse Deep Learning that replacing a uniform by a Gaussian in the prior and variat ional distributions leads to the same rate of convergence, see Appendix G. Nevertheless, deep neural networks are mainly used for thei r computational eﬃciency and their ability to approach complex functions, which makes th e task of estimating a neural network not so popular in machine learning. As said earlier, Imaizumi and F ukumizu (2019) used neural networks for estimating non-smooth functions. In such a context where the neural network model is misspeciﬁed, our generalization er ror bound is robust and still holds, and satisﬁes the best possible balance between bias a nd variance. Indeed, the upper bound on the generalization error on the ri ght-hand-side of ( 3) is mainly divided in two parts: the approximation error of f0 by a DNN fθ∗ in Θ S,L,D (i.e. the bias) and the estimation error rS,L,D n of a neural network fθ∗ in Θ S,L,D (i.e. the variance). F or instance, even if the generalization power is decreasing li nearly with respect to the number of layers compared to the logarithmic dependence on the width d ue to the variance term, this eﬀect is compensated by the beneﬁts of depth in the approxima tion theory of deep learning. Then, as there exists relationships between the bias/the va riance and the architecture of a neural network (respectively due to the approximation theo ry/the form of rS,L,D n ), Theorem 2 gives both a general formula for deriving rates of convergen ce for variational approximations and insight on the way to choose the architecture. W e choose t he architecture that minimizes the right-hand-side of ( 3), which can lead to minimax estimators for smooth functions . It also connects the approximation and estimation theories fo llowing previous studies. This was done for instance by Schmidt-Hieber (2017); Suzuki (2019); Imaizumi and F ukumizu (2019) who exploited the eﬀectiveness of ReLU activation function in terms of approximation ability (Y arotsky, 2016; Petersen and V oigtländer , 2017) for Hölder/Besov smooth and piecewise smooth generating functions. Now we illustrate Theorem 2 on Hölder smooth functions. The following result shows that the variational approximation achieves the same rate o f convergence than the posterior distribution it approximates, and even the minimax rate of c onvergence if the architecture is well chosen. W e present both consistency and concentrati on results. Corollary 3. Let us ﬁx α ∈ (0, 1). We consider the ReLU activation function. Assume that f0 is β-Hölder smooth with 0 < β < d . Then with L, D and S deﬁned as in Theorem 1, the variational approximation of the tempered posterior distr ibution ˜πn,α is consistent and hence concentrates at the minimax rate rn = n −2β 2β+d (up to a squared logarithmic factor): ˜πn,α ( θ ∈ Θ S,L,D / ∥fθ − f0∥2 2> M n · n −2β 2β+d · log2 n ) − − − − − → n→+∞ 0 in probability as n → +∞ for any Mn → +∞. 3.3 Optimization error In this subsection, we discuss the eﬀect of an optimization error that is independent on the previous statistical error. Indeed, in the variational Bayes community , people use ap- proximate algorithms in practice to solve the optimization problem ( 1) when the model is non-conjugate, i.e. the VB solution is not available in clos ed-form. This is the case here when considering a sparse spike-and-slab variational appr oximation in FS,L,D for DNNs with 11Chérief-Abdellatif hyperparameters φ = ( πγ, (φt)1≤t≤T ) and an algorithm that gives a sequence of hyperparam- eters (φk)k≥1 and associated variational approximations (˜πk n,α)k≥1. The following theorem gives a statistical guarantee for any approximation ˜πk n,α, k ≥ 1: Theorem 4. For any α ∈ (0, 1), E [ ∫ ∥fθ − f0∥2 2˜πk n,α(dθ) ] ≤ 2 1 − α inf θ∗ ∥fθ∗ − f0∥2 2+ 2 1 − α ( 1 + σ2 α ) rS,L,D n + 2σ2 α(1 − α) · E[L∗ n− Lk n] n , where L∗ n is the maximum of the evidence lower bound i.e. the ELBO evalu ated at ˜πn,α, while Lk nis the ELBO evaluated at ˜πk n,α. W e establish a clear connection between the convergence (in mean) of the ELBO Lk nto L∗ nand the consistency of our algorithm ˜πk n,α. Indeed, as soon as the ELBO Lk nconverges at rate ck,n, then our variational approximation ˜πk n,α is consistent at rate: max (ck,n n , S log(nL/S) n , SL log D n ) . In particular, as soon as k is such that ck,n ≤ max(S log n, S log D), then we obtain consis- tency of ˜πk n,α at rate rS,L,D n , i.e. ˜πk n,α and ˜πn,α have the same rate of convergence. However, deriving the convergence of the ELBO is a hard task. F or instance, when consid- ering a simple Gaussian mean-ﬁeld approximation without sp arsity , the variational objective Ln can be maximized using either stochastic ( Graves, 2011; Blundell et al. , 2015) or natu- ral gradient methods ( Khan et al. , 2018) on the parameters of the Gaussian approximation. The convergence of the ELBO is often met in practice ( Buchholz et al. , 2018; Mishkin et al. , 2018) and the recent work of Osawa et al. (2019) even showed that Bayesian deep learning enables practical deep learning and matches the performanc e of standard methods while pre- serving beneﬁts of Bayesian principles. Nevertheless, the o bjective is nonconvex and hence it is diﬃcult to prove the convergence to a global maximum in t heory . Some recent papers studied global convergence properties of gradient descent algorithms for frequentist classi- ﬁcation and regression losses ( Du et al. , 2019; Allen-Zhu et al. , 2019) that we may extend to gradient descent algorithms for the ELBO objective such as V ariational Online Gauss Newton or V adam ( Khan et al. , 2018; Osawa et al. , 2019). Another point is to develop and study more complex algorithm s than simple gradient descent that deal with spike-and-slab sparsity-inducing v ariational inference, as for instance Titsias and Lázaro-Gredilla (2011) did for multi-task and multiple kernel learning. Also, Louizos et al. (2018) connected sparse spike-and-slab variational inference w ith L0-norm regularization for neural networks and proposed a solution to the intractability of the L0- penalty term through the use of non-negative stochastic gat es, while Bellec et al. (2018) proposed an algorithm preserving sparsity during training . Nevertheless, these optimization concerns fall beyond the scope of this paper and are left for f urther research. 12Convergence Rates of V ariational Inference in Sparse Deep Learning 4. Architecture design via ELBO maximization W e saw in Section 3 that the choice of the architecture of the neural network is c rucial and can lead to faster convergence and better approximation. In this section, we formulate the architecture design of DNNs as a model selection problem and we investigate the ELBO maximization strategy which is very popular in the variatio nal Bayes community . This approach is diﬀerent from Rockova and Polson (2018) which is fully Bayesian and treats the parameters of the network architecture, namely the depth, t he width and the sparsity , as random variables. W e show that the ELBO criterion does not ove rﬁt and is adaptive: it provides a variational approximation with the optimal rate of convergence, and it does not require the knowledge of the unknown aspects of the regressi on function f0 (e.g. the level of smoothness for smooth functions) to select the optimal va riational approximation. W e denote MS,L,D the statistical model associated with the parameter set Θ S,L,D. W e consider a countable number of models, and we introduce prio r beliefs πS,L,D over the sparsity , the depth and the width of the network, that can be d eﬁned hierarchically and that are known beforehand. F or instance, the prior beliefs can be chosen such that πL = 2 −L, πD|L follows a uniform distribution over {d, ..., max(eL, d)} given L, and πS|L,D a uniform distribution over {1, ..., T } given L and D (we recall that T is the number of coeﬃcients in a fully connected network). This particular choice is sen sible as it allows to consider any number of hidden layers and (at most) an exponentially la rge width with respect to the depth of the network. W e still consider spike-and-slab p riors on θS,L,D ∈ Θ S,L,D given model MS,L,D. Each tempered posterior associated with model MS,L,D is denoted πS,L,D n,α . W e recall that the variational approximation ˜πS,L,D n,α associated with model MS,L,D is deﬁned as the distribution into the variational set FS,L,D that maximizes the Evidence Lower Bound: ˜πS,L,D n,α = arg max qS,L,D∈FS,L,D Ln(qS,L,D). W e will simply denote in the following L∗ n(S, L, D) the closest approximation to the log- evidence i.e., the value of the ELBO evaluated at its maximum: L∗ n(S, L, D) = Ln(˜πS,L,D n,α ). The model selection criterion we use here to select the archi tecture of the network is a slight penalized variant of the classical ELBO criterion ( Blei et al. , 2017) with strong theoretical guarantees ( Cherief-Abdellatif, 2019) : ( ˆS, ˆL, ˆD) = arg max S,L,D { L∗ n(S, L, D) − log ( 1 πS,L,D )} . F or any choice of the prior beliefs πS,L,D, compute the ELBO for each model MS,L,D using an algorithm that will converge to L∗ n(S, L, D) and choose the architecture that maximizes the penalized ELBO criterion. It is possible to restrict to a ﬁ nite number of layers in practice (for instance, a factor of n or log n). The following theorem shows that this ELBO criterion leads to a variational approxima- tion with the optimal rate of convergence: 13Chérief-Abdellatif Theorem 5. For any α ∈ (0, 1), E [ ∫ ∥fθ − f0∥2 2˜π ˆS, ˆL, ˆD n,α (dθ) ] ≤ inf S,L,D { 2 1 − α inf θ∗∈Θ S,L,D ∥fθ∗ − f0∥2 2+ 2 1 − α ( 1 + σ2 α ) rS,L,D n + 2σ2 α(1 − α) log( 1 πS,L,D ) n } . This inequality shows that as soon as the complexity term log(1/πS,L,D)/n that reﬂects the prior beliefs is lower than the eﬀective rate of converge nce that balances the accuracy and the estimation error rS,L,D n , the selected variational approximation adaptively achie ves the best possible rate. F or instance, it leads to (near-)min imax rates for Hölder smooth functions and selects the optimal architecture even withou t the knowledge of β, which was required in the previous section. Note that for the previous choice of prior beliefs πL = 2 −L, πD|L = 1 /(max(eL, d) − d + 1), πS|L,D = 1 /T , we get: log( 1 πS,L,D ) n ≤ 2 log(D + 1) + log L + max(L, log d) + L log 2 n that is lower than rS,L,D n (up to a factor) and hence the ELBO criterion does not overﬁt. 5. Discussion In this paper, we provided theoretical justiﬁcations for neural networks from a Bayesian point of view using sparse variational inference. W e derive d new generalization error bounds and we showed that sparse variational approximations of DNN s achieve (near-)minimax optimality when the regression function is Hölder smooth. A ll our results directly imply concentration of the approximation of the posterior distri bution. W e also proposed an automated method for selecting an architecture of the netwo rk with optimal consistency guarantees via the ELBO maximization framework. W e think that one of the main challenges here is the design of n ew computational algo- rithms for spike-and-slab deep learning in the wake of the wo rk of Titsias and Lázaro-Gredilla (2011) for multi-task and multiple kernel learning, or those of Louizos et al. (2018) and Bellec et al. (2018). In the latter paper, the authors designed an algorithm for training deep networks while simultaneously learning their sparse c onnectivity allowing for fast and computationally eﬃcient learning, whereas most approache s have focused on compressing already trained neural networks. In the same time, a future point of interest is the study of the global convergence of these approximate algorithms in nonconvex settings i.e. st udy of the theoretical conver- gence of the ELBO. This work was conducted for frequentist gra dient descent algorithms (Allen-Zhu et al. , 2019; Du et al. , 2019). Such studies should be investigated for Bayesian gradient descents, as well as for algorithms that preserve t he sparsity of the network during training. Acknowledgments W e would like to warmly thank Pierre Alquier for his helpful s uggestions on early versions of this work. 14Convergence Rates of V ariational Inference in Sparse Deep Learning References Zeyuan Allen-Zhu, Y uanzhi Li, and Zhao Song. A convergence theory for deep learning via over-parameterization. In Kamalika Chaudhuri and Ruslan S alakhutdinov, editors, Pro- ceedings of the 36th International Conference on Machine Le arning, volume 97 of Proceed- ings of Machine Learning Research , pages 242–252, Long Beach, California, USA, 09–15 Jun 2019. PMLR. URL http://proceedings.mlr.press/v97/allen-zhu19a.html. P . Alquier and J. Ridgway . Concentration of tempered poster iors and of their variational approximations. arXiv preprint arXiv:1706.09293 , 2017. P . Alquier, J. Ridgway , and N. Chopin. On the properties of va riational approximations of Gibbs posteriors. JMLR, 17(239):1–41, 2016. Pierre Baldi and Kurt Hornik. Neural networks and principal c omponent analysis: Learning from examples without local minima”, ne. Neural Networks , 2:53–58, 12 1989. doi: 10. 1016/0893-6080(89)90014-2. Andrew Barron. Barron, a.e.: Universal approximation bounds for superpositions of a sigmoidal function. ieee trans. on information theory 39, 9 30-945. Information Theory, IEEE Transactions on , 39:930 – 945, 06 1993. doi: 10.1109/18.256500. Andrew R Barron. Approximation and estimation bounds for art iﬁcial neural networks. Machine Learning , 14(1):115–133, 1994. Peter L Bartlett, Dylan J F oster, and Matus J T elgarsky . Spect rally-normalized margin bounds for neural networks. In I. Guyon, U. V. Luxburg, S. Beng io, H. W allach, R. F ergus, S. Vishwanathan, and R. Garnett, editors, Advances in Neural Infor- mation Processing Systems 30 , pages 6240–6249. Curran Associates, Inc., 2017. URL http://papers.nips.cc/paper/7204-spectrally-normalized-margin-bounds-for-neural-networks.pdf . Leonard E. Baum and T ed Petrie. Statistical inference for pro babilistic functions of ﬁnite state markov chains. Ann. Math. Statist. , 37(6):1554–1563, 12 1966. doi: 10.1214/aoms/ 1177699147. URL https://doi.org/10.1214/aoms/1177699147. G. Behrens, N. F riel, and M. Hurn. T uning tempered transition s. Statistics and computing , 22(1):65–78, 2012. Guillaume Bellec, David Kappel, W olfgang Maass, and Robert L egenstein. Deep rewiring: T raining very sparse deep networks. In International Conference on Learning Represen- tations, 2018. URL https://openreview.net/forum?id=BJ_wN01C-. Y oshua Bengio and Olivier Delalleau. On the expressive power of deep architectures. In Pro- ceedings of the 22Nd International Conference on Algorithm ic Learning Theory , AL T’11, pages 18–36, Berlin, Heidelberg, 2011. Springer-V erlag. IS BN 978-3-642-24411-7. URL http://dl.acm.org/citation.cfm?id=2050345.2050349. A. Bhattacharya, D. Pati, and Y. Y ang. Bayesian fractional pos teriors. arXiv preprint arXiv:1611.01125, to appear in the Annals of Statistics , 2016. 15Chérief-Abdellatif A. Bhattacharya, D. Pati, and Y. Y ang. On statistical optimal ity of variational Bayes. Proceedings of Machine Learning Research , 84 - AIST A T, 2018. David M Blei, Alp Kucukelbir, and Jon D McAuliﬀe. V ariational inference: A review for statisticians. Journal of the American Statistical Association , 112(518):859–877, 2017. Charles Blundell, Julien Cornebise, Koray Kavukcuoglu, and Daan Wierstra. W eight un- certainty in neural networks. In Proceedings of the 32Nd International Conference on International Conference on Machine Learning - Volume 37 , ICML’15, pages 1613–1622. JMLR.org, 2015. URL http://dl.acm.org/citation.cfm?id=3045118.3045290. Stéphane Boucheron, Gábor Lugosi, and Pascal Massart. Conce ntration inequalities us- ing the entropy method. Ann. Probab. , 31(3):1583–1614, 07 2003. doi: 10.1214/aop/ 1055425791. URL https://doi.org/10.1214/aop/1055425791. Alexander Buchholz, Florian W enzel, and Stephan Mandt. Quas i-Monte Carlo variational inference. In Jennifer Dy and Andreas Krause, editors, Proceedings of the 35th Interna- tional Conference on Machine Learning , volume 80 of Proceedings of Machine Learning Research, pages 668–677, Stockholmsmässan, Stockholm Sweden, 10–1 5 Jul 2018. PMLR. URL http://proceedings.mlr.press/v80/buchholz18a.html. T revor Campbell and Xinglong Li. Universal boosting variat ional inference. volume arXiv:1903.05220, 2019. Ismaël Castillo, Johannes Schmidt-Hieber, and Aad van der V aart. Bayesian linear regression with sparse priors. Ann. Statist. , 43(5):1986–2018, 10 2015. doi: 10.1214/15-AOS1334. URL https://doi.org/10.1214/15-AOS1334. O. Catoni. PAC-Bayesian supervised classiﬁcation: the thermodynami cs of statistical learn- ing. Institute of Mathematical Statistics Lecture Notes—Mono graph Series, 56. Institute of Mathematical Statistics, Beachwood, OH, 2007. B. Chérief-Abdellatif and P . Alquier. Consistency of variat ional bayes inference for estima- tion and model selection in mixtures. Electronic Journal of Statistics , 12(2):2995–3035, 2018. ISSN 1935-7524. doi: 10.1214/18-EJS1475. B.-E. Chérief-Abdellatif, P . Alquier, and M.E. Khan. A gener alization bound for online variational inference. Preprint arXiv:1904.03920v1, 201 9. Badr-Eddine Cherief-Abdellatif. Consistency of elbo maxim ization for model selection. In F rancisco Ruiz, Cheng Zhang, Dawen Liang, and Thang Bui, ed itors, Proceedings of The 1st Symposium on Advances in Approximate Bayesian Inf erence, volume 96 of Proceedings of Machine Learning Research , pages 11–31. PMLR, 02 Dec 2019. URL http://proceedings.mlr.press/v96/cherief-abdellatif19a.html. G. Cybenko. Approximation by superpositions of a sigmoidal function. Mathematics of Control, Signals, and Systems (MCSS) , 2(4):303–314, December 1989. ISSN 0932-4194. doi: 10.1007/BF02551274. URL http://dx.doi.org/10.1007/BF02551274. 16Convergence Rates of V ariational Inference in Sparse Deep Learning A. Doucet and A. Johansen. A tutorial on particle ﬁltering an d smoothing: Fifteen years later. Handbook of Nonlinear Filtering , 12, 01 2009. Simon Du, Jason Lee, Haochuan Li, Liwei W ang, and Xiyu Zhai. G radient descent ﬁnds global minima of deep neural networks. In Kamalika Chaudhur i and Ruslan Salakhutdinov, editors, Proceedings of the 36th International Conference on Machin e Learning , volume 97 of Proceedings of Machine Learning Research , pages 1675–1685, Long Beach, California, USA, 09–15 Jun 2019. PMLR. URL http://proceedings.mlr.press/v97/du19c.html. Y arin Gal. Uncertainty in Deep Learning . PhD thesis, University of Cambridge, 2016. Subhashis Ghosal, Jayanta K. Ghosh, and Aad W. van der V aart. Convergence rates of pos- terior distributions. Ann. Statist. , 28(2):500–531, 04 2000. doi: 10.1214/aos/1016218228. URL https://doi.org/10.1214/aos/1016218228. Ian Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing Xu, Da vid W arde-F arley , Sherjil Ozair, Aaron Courville, and Y oshua Bengio. Generative adver sarial nets. In Z. Ghahra- mani, M. W elling, C. Cortes, N. D. Lawrence, and K. Q. W einber ger, editors, Advances in Neural Information Processing Systems 27 , pages 2672–2680. Curran Associates, Inc., 2014. URL http://papers.nips.cc/paper/5423-generative-adversarial-nets.pdf. Ian Goodfellow, Y oshua Bengio, and Aaron Courville. Deep Learning . MIT Press, 2016. http://www.deeplearningbook.org. Alex Graves. Practical variational inference for neural ne tworks. In J. Shawe-T aylor, R. S. Zemel, P . L. Bartlett, F. Pereira, and K. Q. W einberger, edito rs, Advances in Neural Information Processing Systems 24 , pages 2348–2356. Curran Associates, Inc., 2011. URL http://papers.nips.cc/paper/4329-practical-variational-inference-for-neural-networks.pdf . Philipp Grohs, Dmytro Perekrestenko, Dennis Elbrächter, a nd Helmut Bölcskei. Deep neural network approximation theory , 01 2019. P . D. Grünwald and T. V an Ommen. Inconsistency of Bayesian inf erence for misspeciﬁed linear models, and a proposal for repairing it. Bayesian Analysis , 12(4):1069–1103, 2017. B. Guedj. A primer on pac-bayesian learning. arXiv preprint arXiv:1901.05353 , 2019. Satoshi Hayakawa and T aiji Suzuki. On the minimax optimalit y and superiority of deep neural network learning over sparse parameter spaces. arXiv preprint arXiv:1905.09195 , 2019. Geoﬀrey E. Hinton and Drew van Camp. Keeping the neural netwo rks simple by min- imizing the description length of the weights. In Proceedings of the Sixth Annual Conference on Computational Learning Theory , COL T ’93, pages 5–13, New Y ork, NY, USA, 1993. ACM. ISBN 0-89791-611-5. doi: 10.1145/168304 .168306. URL http://doi.acm.org/10.1145/168304.168306. M. D. Hoﬀman, D. M. Blei, C. W ang, and J. Paisley . Stochastic va riational inference. The Journal of Machine Learning Research , 14(1):1303–1347, 2013. 17Chérief-Abdellatif Jonathan H. Huggins, T revor Campbell, Mikolaj Kasprzak, an d T amara Broderick. Practical bounds on the error of bayesian posterior approximations: A nonasymptotic approach. ArXiv, abs/1809.09505, 2018. Masaaki Imaizumi and Kenji F ukumizu. Deep neural networks l earn non-smooth functions ef- fectively . In Kamalika Chaudhuri and Masashi Sugiyama, edi tors, Proceedings of Machine Learning Research, volume 89 of Proceedings of Machine Learning Research , pages 869–878. PMLR, 16–18 Apr 2019. URL http://proceedings.mlr.press/v89/imaizumi19a.html. P . Jaiswal, V. A. Rao, and H. Honnappa. Asymptotic consisten cy of α-rényi-approximate posteriors. Preprint arXiv:1902.01902, 2019a. Prateek Jaiswal, Harsha Honnappa, and Vinayak A. Rao. Risk- sensitive variational bayes: F ormulations and bounds. volume arXiv:1906.01235, 2019b. M. I. Jordan, Z. Ghahramani, T. S. Jaakkola, and L. K. Saul. An introduction to variational methods for graphical models. Machine Learning , 37:183–233, 1999. Kenji Kawaguchi. Deep learning without poor local minima. I n D. D. Lee, M. Sugiyama, U. V. Luxburg, I. Guyon, and R. Garnett, editors, Advances in Neural Informa- tion Processing Systems 29 , pages 586–594. Curran Associates, Inc., 2016. URL http://papers.nips.cc/paper/6112-deep-learning-with out-poor-local-minima.pdf. Kenji Kawaguchi, Jiaoyang Huang, and Leslie Pack Kaelbling . Eﬀect of depth and width on local minima in deep learning. Neural Computation , 31(6):1462–1498, 2019. Mohammad Khan, Didrik Nielsen, V oot T angkaratt, W u Lin, Y ar in Gal, and Akash Sri- vastava. F ast and scalable Bayesian deep learning by weight- perturbation in Adam. In Jennifer Dy and Andreas Krause, editors, Proceedings of the 35th International Con- ference on Machine Learning , volume 80 of Proceedings of Machine Learning Research , pages 2611–2620, Stockholmsmässan, Stockholm Sweden, 10– 15 Jul 2018. PMLR. URL http://proceedings.mlr.press/v80/khan18a.html. Diederik P Kingma and Max W elling. Auto-encoding variation al bayes. arXiv preprint arXiv:1312.6114, 2013. Y ann Lecun, Léon Bottou, Y oshua Bengio, and Patrick Haﬀner. Gr adient-based learning applied to document recognition. In Proceedings of the IEEE , pages 2278–2324, 1998. Y ann LeCun, Y oshua Bengio, and Geoﬀrey Hinton. Deep learning . Nature, 521(7553): 436–444, 5 2015. ISSN 0028-0836. doi: 10.1038/nature14539 . Christos Louizos, Max W elling, and Diederik P . Kingma. Lear ning sparse neural networks through l0-regularization. In International Conference on Learning Representations , 2018. URL https://openreview.net/forum?id=H1Y8hhg0b. David J. C. MacKay . A practical bayesian framework for backp ropagation networks. Neural Computation , 4(3):448–472, 1992a. doi: 10.1162/neco.1992.4.3.448. U RL https://doi.org/10.1162/neco.1992.4.3.448. 18Convergence Rates of V ariational Inference in Sparse Deep Learning David J. C. MacKay . Bayesian methods for adaptive models . PhD thesis, California Institute of T echnology , 1992b. T. P . Minka. Expectation propagation for approximate bayes ian inference. In Proceedings of the 17th Conference in Uncertainty in Artiﬁcial Intellig ence, UAI ’01, pages 362–369, San F rancisco, CA, USA, 2001. Morgan Kaufmann Publishers In c. ISBN 1-55860-800-1. URL http://dl.acm.org/citation.cfm?id=647235.720257. Aaron Mishkin, F rederik Kunstner, Didrik Nielsen, Mark Sch midt, and Mohammad Emtiyaz Khan. Slang: F ast structured covariance approximations fo r bayesian deep learning with natural gradient. In S. Bengio, H. W allach, H. Larochelle, K. Grauman, N. Cesa-Bianchi, and R. Garnett, editors, Advances in Neural Information Processing Systems 31 , pages 6245–6255. Curran Associates, Inc., 2018. Radford. M. Neal. Bayesian learning for neural networks . PhD thesis, University of T oronto, 1995. Behnam Neyshabur, Srinadh Bhojanapalli, and Nathan Srebro. A P AC-bayesian approach to spectrally-normalized margin bounds for neural networks. In International Conference on Learning Representations, 2018. URL https://openreview.net/forum?id=Skz_WfbCZ. Quynh Nguyen, Mahesh Chandra Mukkamala, and Matthias Hein. On the loss landscape of a class of deep neural networks with no bad local valleys. In International Conference on Learning Representations, 2019. URL https://openreview.net/forum?id=HJgXsjA5tQ. Manfred Opper and Cedric Archambeau. The variational gauss ian approximation revisited. Neural computation , 21:786–92, 10 2008. doi: 10.1162/neco.2008.08-07-592. Kazuki Osawa, Siddharth Swaroop, Anirudh Jain, Runa Eschen hagen, Richard E. T urner, Rio Y okota, and Mohammad Emtiyaz Khan. Practical deep learn ing with bayesian prin- ciples, 2019. URL http://arxiv.org/abs/1906.02506. cite arxiv:1906.02506Comment: Under review. Philipp Petersen and F elix V oigtländer. Optimal approxima tion of piecewise smooth func- tions using deep relu neural networks. Neural Networks , 09 2017. doi: 10.1016/j.neunet. 2018.08.019. V eronika Rockova and nicholas Polson. Posterior concentra tion for sparse deep learning. In S. Bengio, H. W allach, H. Larochelle, K. Grauman, N. Cesa-Bian chi, and R. Garnett, editors, Advances in Neural Information Processing Systems 31 , pages 930–941. Curran Associates, Inc., 2018. David Rolnick and Max T egmark. The power of deeper networks f or expressing natural functions. In 6th International Conference on Learning Representations , ICLR 2018, Van- couver, BC, Canada, April 30 - May 3, 2018, Conference Track P roceedings, 2018. URL https://openreview.net/forum?id=SyProzZAW. David E. Rumelhart, Geoﬀrey E. Hinton, and Ronald J. William s. Learning Representations by Back-propagating Errors. Nature, 323(6088):533–536, 1986. doi: 10.1038/323533a0. URL http://www.nature.com/articles/323533a0. 19Chérief-Abdellatif Johannes Schmidt-Hieber. Nonparametric regression using deep neural networks with relu activation function. ArXiv, arxiv:1708.06633, 2017. Rishit Sheth and Roni Khardon. Excess risk bounds for the bay es risk using variational inference in latent gaussian models. In I. Guyon, U. V. Luxbu rg, S. Bengio, H. W allach, R. F ergus, S. Vishwanathan, and R. Garnett, editors, Advances in Neural Information Processing Systems 30 , pages 5151–5161. Curran Associates, Inc., 2017. David Silver, Julian Schrittwieser, Karen Simonyan, Ioann is Antonoglou, Aja Huang, Arthur Guez, Thomas Hubert, Lucas Baker, Matthew Lai, Adrian Bolton, Y utian Chen, Timothy Lillicrap, F an Hui, Laurent Sifre, George van den Driessche , Thore Graepel, and Demis Hassabis. Mastering the game of go without human knowledge. Nature, 550:354–, October 2017. URL http://dx.doi.org/10.1038/nature24270. Daniel Soudry and Y air Carmon. No bad local minima: Data inde pendent training error guarantees for multilayer neural networks. 05 2016. Nitish Srivastava, Geoﬀrey Hinton, Alex Krizhevsky , Ilya S utskever, and Ruslan Salakhutdinov. Dropout: A simple way to prevent neural netw orks from over- ﬁtting. Journal of Machine Learning Research , 15:1929–1958, 2014. URL http://jmlr.org/papers/v15/srivastava14a.html. J.A. Stanford, K Giardina, G.A. Gerhardt, Kenji F ukumizu, a nd Shun-ichi Amari. Local minima and plateaus in hierarchical structures of multilay er perceptrons. Neural Networks , 13, 05 2000. doi: 10.1016/S0893-6080(00)00009-5. T aiji Suzuki. F ast generalization error bound of deep learn ing from a kernel perspective. In Amos Storkey and F ernando Perez-Cruz, editors, Proceedings of the Twenty-First Inter- national Conference on Artiﬁcial Intelligence and Statist ics, volume 84 of Proceedings of Machine Learning Research , pages 1397–1406, Playa Blanca, Lanzarote, Canary Islands, 09–11 Apr 2018. PMLR. URL http://proceedings.mlr.press/v84/suzuki18a.html. T aiji Suzuki. Adaptivity of deep reLU network for learning i n besov and mixed smooth besov spaces: optimal rate and curse of dimensionality . In International Conference on Learning Representations, 2019. URL https://openreview.net/forum?id=H1ebTsActm. Michalis K. Titsias and Miguel Lázaro-Gredilla. Spike and s lab variational inference for multi-task and multiple kernel learning. In J. Shawe-T aylo r, R. S. Zemel, P . L. Bartlett, F. Pereira, and K. Q. W einberger, editors, Advances in Neural Information Processing Systems 24 , pages 2339–2347. Curran Associates, Inc., 2011. F rancesco T onolini, Bjorn Sand Jensen, and Roderick Murray- Smith. V ariational sparse coding, 2019. URL https://openreview.net/forum?id=SkeJ6iR9Km. Alexandre B. T sybakov. Introduction to Nonparametric Estimation . Springer Publishing Company , Incorporated, 1st edition, 2008. ISBN 0387790519, 9780387790510. 20Convergence Rates of V ariational Inference in Sparse Deep Learning Mariia Vladimirova, Jakob V erbeek, Pablo Mesejo, and Julya n Arbel. Understand- ing priors in Bayesian neural networks at the unit level. In Ka malika Chaud- huri and Ruslan Salakhutdinov, editors, Proceedings of the 36th International Con- ference on Machine Learning , volume 97 of Proceedings of Machine Learning Re- search, pages 6458–6467, Long Beach, California, USA, 09–15 Jun 201 9. PMLR. URL http://proceedings.mlr.press/v97/vladimirova19a.html. Y. W ang and D. M. Blei. F requentist consistency of variationa l Bayes. Journal of the American Statistical Association (to appear), 2018. Dmitry Y arotsky . Error bounds for approximations with deep relu networks. Neural Net- works, 94, 10 2016. doi: 10.1016/j.neunet.2017.07.002. Chiyuan Zhang, Samy Bengio, Moritz Hardt, Benjamin Recht, and Oriol Vinyals. Understanding deep learning requires rethinking generali zation. 2017. URL https://arxiv.org/abs/1611.03530. F. Zhang and C. Gao. Convergence rates of variational poster ior distributions. arXiv preprint arXiv:1712.02519v1, 2017. 21Chérief-Abdellatif Appendix A. Connection between concentration and consistency In this appendix, we show the connection between the notions of consistency and concen- tration. The Bayesian estimator ρ (e.g. the tempered posterior πn,α or its variational approxima- tion ˜πn,α) is said to be consistent if its generalization error goes to zero as n → +∞: E [ ∫ ∥fθ − f0∥2 2ρ(dθ) ] − − − − − → n→+∞ 0. W e say that the Bayesian estimator ρ concentrates at rate rn ( Ghosal et al. , 2000) if in probability (with respect to the random variables distribu ted according to the generating process), the estimator concentrates asymptotically arou nd the true distribution as n → +∞, i.e.: ρ ( θ ∈ Θ S,L,D / ∥fθ − f0∥2 2> M nrn ) − − − − − → n→+∞ 0. in probability as n → +∞ for any Mn → +∞. The consistency of the Bayesian distribution ρ at rate rn implies its concentration at rate rn. Indeed, if we we assume that ρ is consistent at rate rn, i.e.: E [ ∫ ∥fθ − f0∥2 2ρ(dθ) ] ≤ rn, then, using Markov’s inequality for any Mn → +∞ as n → +∞: E [ ρ ( θ ∈ Θ S,L,D / ∥fθ − f0∥2 2> M nrn )] ≤ E [ ∫ ∥fθ − f0∥2 2ρ(dθ) ] Mnrn ≤ rn Mnrn = 1 Mn → 0. Hence, we have the convergence in mean of ρ ( θ ∈ Θ S,L,D / ∥fθ − f0∥2 2> M nrn ) to 0, and then the convergence in probability of ρ ( θ ∈ Θ S,L,D / ∥fθ − f0∥2 2 > M nrn ) to 0, i.e. the concentration of ρ to f0 at rate rn. Appendix B. Proof of Theorem 2 The structure of the proof of Theorem 2 is composed of three main steps. The ﬁrst one consists in obtaining the general shape of the inequality us ing P AC-Bayes inequalities, and the two others in ﬁnding a rate that satisﬁes the extended pri or mass condition. First step : we obtain the general inequality W e start from inequality 2.6 in Alquier and Ridgway (2017) that provides an upper bound on the generalization error but in α-Rényi divergence. W e denote P 0 the generating distribution of any (Xi, Yi) and Pθ the distribution characterizing the model. Then, for any α ∈ (0, 1): E [ ∫ Dα(Pθ, P 0)˜πn,α(dθ) ] ≤ inf q∈FS,L,D { α 1 − α ∫ KL(P 0, Pθ)q(dθ) + KL(q∥π) n(1 − α) } . 22Convergence Rates of V ariational Inference in Sparse Deep Learning Moreover, the α-Rényi divergence is equal to Dα(Pθ, P 0) = α 2σ2 ∥fθ − f0∥2 2 and the KL divergence is KL (P 0∥Pθ) = 1 2σ2 ∥fθ − f0∥2 2, and for any θ∗, ∥fθ − f0∥2 2 ≤ 2∥fθ − fθ∗ ∥2 2+ 2∥fθ∗ − f0∥2 2. Hence, for any θ∗ ∈ Θ S,L,D: E [ ∫ α 2σ2 ∥fθ − f0∥2 2˜πn,α(dθ) ] ≤ α 1 − α 2 2σ2 ∥fθ∗ − f0∥2 2+ inf q∈FS,L,D { α 1 − α ∫ 2 2σ2 ∥fθ − fθ∗ ∥2 2q(dθ) + KL(q∥π) n(1 − α) } , i.e. for any θ∗ ∈ Θ S,L,D, E [ ∫ ∥fθ − f0∥2 2˜πn,α(dθ) ] ≤ 2 1 − α∥fθ∗ − f0∥2 2+ inf q∈FS,L,D { 2 1 − α ∫ ∥fθ − fθ∗ ∥2 2q(dθ) + 2σ2 α KL(q∥π) n(1 − α) } . F rom now on, the rest of the proof consists in ﬁnding a distrib ution q∗ n ∈ F S,L,D that satisﬁes for θ∗ = arg min θ∈Θ S,L,D ∥fθ − f0∥2 the extended prior mass condition, i.e. that satisﬁes both: ∫ ∥fθ − fθ∗ ∥2 2q∗ n(dθ) ≤ rn (4) and KL(q∗ n∥π) ≤ nrn (5) with rn = SL n log(BD) + S n log(BL(D + 1)2) + S 2n log ( 4n S { 3 + ( d + 2)2L2 }) that is smaller than rS,L,D n as 3 + ( x + 2)2L2 ≤ 10x2L2 for x ≥ 1 and L ≥ 3. This will lead to: E [ ∫ ∥fθ − f0∥2 2˜πn,α(dθ) ] ≤ 2 1 − α inf θ∗∈Θ S,L,D ∥fθ∗ − f0∥2 2+ 2 1 − α ( 1 + σ2 α ) rS,L,D n . Second step : we prove Inequality (4) T o begin with, we deﬁne the loss of the ℓth layer of the neural network fθ: rℓ(θ) = sup x∈[−1,1]d sup 1≤i≤D |fℓ θ (x)i − fℓ θ∗ (x)i| where fℓ θ s are deﬁned as the partial networks: { f0 θ (x) := x, fℓ θ (x) := ρ(Aℓfℓ−1 θ (x) + bℓ) for ℓ = 1 , ..., L. W e also deﬁne the loss of the output layer: rℓ(θ) = sup x∈[−1,1]d |fL θ (x) − fL θ∗ (x)| = sup x∈[−1,1]d |fθ(x) − fθ∗ (x)|. 23Chérief-Abdellatif W e will prove by induction that for any ℓ = 1 , ..., L: rℓ(θ) ≤ (BD)ℓ ( d + 1 + 1 BD − 1 ) ℓ∑ u=1 ˜Au + ℓ∑ u=1 (BD)ℓ−u˜bu where ˜Au = sup i,j |Au,i,j − A∗ u,i,j| and ˜bu = sup j |bu,j − b∗ u,j|. T o do so, we will also prove by induction that: cℓ ≤ BℓDℓ−1 ( d + 1 + 1 BD − 1 ) where { cℓ = sup x∈[−1,1]d sup1≤i≤D |fℓ θ∗ (x)i| for ℓ = 1 , ..., L, cL = sup x∈[−1,1]d |fθ∗ (x)|, using the formula: xn ≤ unxn−1 + vn =⇒ xn ≤ n∑ i=2 ( n∏ j=i+1 uj ) vi + ( n∏ j=2 uj ) x1 (6) for any n ≥ 2 with the convention ∏ n j=n+1 uj = 1 . Indeed, we have according to Assumption 3.1: • Initialization: c1 = sup x∈[−1,1]d sup 1≤i≤D |f1 θ∗ (x)i| ≤ sup x∈[−1,1]d sup 1≤i≤D ⏐ ⏐ ⏐ ⏐ d∑ j=1 A∗ 1ij xj + b∗ 1i ⏐ ⏐ ⏐ ⏐ ≤ sup x∈[−1,1]d sup 1≤i≤D { d∑ j=1 |A∗ 1ij | · | xj | + |b∗ 1i| } ≤ d · B · 1 + B = ( d + 1)B. • F or any layer ℓ: cℓ ≤ sup x∈[−1,1]d sup 1≤i≤D ⏐ ⏐ ⏐ ⏐ D∑ j=1 A∗ ℓijfℓ−1 θ∗ (x)j + b∗ ℓi ⏐ ⏐ ⏐ ⏐ ≤ sup x∈[−1,1]d sup 1≤i≤D { D∑ j=1 |A∗ ℓij| · | fℓ−1 θ∗ (x)j | + |b∗ ℓi| } ≤ D · B · cℓ−1 + B. 24Convergence Rates of V ariational Inference in Sparse Deep Learning • Hence, using F ormula ( 6), we get: cℓ ≤ ℓ∑ u=2 ( ℓ∏ v=u+1 DB ) B + ( ℓ∏ v=2 BD ) c1 ≤ B ℓ∑ u=2 (DB)ℓ−u + (BD)ℓ−1(d + 1)B = B ℓ−2∑ u=0 (DB)u + (d + 1)Dℓ−1Bℓ = B (BD)ℓ−1 − 1 BD − 1 + (d + 1)Dℓ−1Bℓ ≤ BℓDℓ−1 ( d + 1 + 1 BD − 1 ) . Let us now come back to ﬁnding an upper bound on losses of the pa rtial networks fℓ θ s. As previously , we have: • Initialization: r1(θ) = sup x∈[−1,1]d sup 1≤i≤D |f1 θ∗ (x)i − f1 θ (x)i| ≤ sup x∈[−1,1]d sup 1≤i≤D { d∑ j=1 |A1ij − A∗ 1ij | · | xj| + |b1i − b∗ 1i| } ≤ d · ˜A1 + ˜b1. • F or any layer ℓ: rℓ(θ) ≤ sup x∈[−1,1]d sup 1≤i≤D { D∑ j=1 |Aℓij fℓ−1 θ (x)j − A∗ ℓijfℓ−1 θ∗ (x)j | + |bℓi − b∗ ℓi| } ≤ sup x∈[−1,1]d sup 1≤i≤D { D∑ j=1 [ |Aℓij − A∗ ℓij| · | fℓ−1 θ∗ (x)j | + |Aℓij | · | fℓ−1 θ∗ (x)j − fℓ−1 θ (x)j | ] + |bℓi − b∗ ℓi| } ≤ Dcℓ−1 ˜Aℓ + BDrℓ−1(θ) + ˜bℓ ≤ BDrℓ−1(θ) + ˜AℓBℓ−1Dℓ−1 ( d + 1 + 1 BD − 1 ) + ˜bℓ. 25Chérief-Abdellatif • Finally , using F ormula ( 6): rℓ(θ) ≤ ℓ∑ u=2 ( ℓ∏ v=u+1 BD )( ˜Au(BD)u−1 { d + 1 + 1 BD − 1 } + ˜bu ) + ( ℓ∏ v=2 BD ) r1(θ) = ℓ∑ u=2 (BD)ℓ−u ˜Au(BD)u−1 ( d + 1 + 1 BD − 1 ) + ℓ∑ u=2 (BD)ℓ−u˜bu + (BD)ℓ−1r1(θ) ≤ ( d + 1 + 1 BD − 1 ) ℓ∑ u=2 (BD)ℓ−1 ˜Au + ℓ∑ u=2 (BD)ℓ−u˜bu + (BD)ℓ−1d ˜A1 + (BD)ℓ−1˜b1 ≤ (BD)ℓ−1 ( d + 1 + 1 BD − 1 ) ℓ∑ u=1 ˜Au + ℓ∑ u=1 (BD)ℓ−u˜bu. Then, for any distribution q: ∫ ∥fθ − fθ∗ ∥2 2q(dθ) ≤ ∫ ∥fθ − fθ∗ ∥2 ∞q(dθ) = ∫ rL(θ)2q(dθ) ≤ ∫ 2(BD)2L−2 ( d + 1 + 1 BD − 1 ) 2( L∑ ℓ=1 ˜Aℓ ) 2 q(dθ) + ∫ 2 ( L∑ ℓ=1 (BD)L−ℓ˜bu ) 2 q(dθ) = 2( BD)2L−2 ( d + 1 + 1 BD − 1 ) 2(∫ L∑ ℓ=1 ˜A2 ℓq(dθ) + 2 ∫ L∑ ℓ=1 ℓ−1∑ k=1 ˜Aℓ ˜Akq(dθ) ) + 2 (∫ L∑ ℓ=1 (BD)2(L−ℓ)˜b2 lq(dθ) + 2 ∫ L∑ ℓ=1 ℓ−1∑ k=1 (BD)L−ℓ(BD)L−k˜bℓ˜bkq(dθ) ) . Here, we deﬁne q∗ n(θ) as follows: { γ∗ t = I(θ∗ t ̸= 0) , θt ∼ γ∗ t U([θ∗ t − sn, θ∗ t + sn]) + (1 − γ∗ t )δ{0} for each t = 1 , ..., T. with s2 n= S 4n (BD)−2L {( d + 1 + 1 BD−1 ) 2 L2 (BD)2 + 1 (BD)2−1 + 2 (BD−1)2 }−1 . Hence: ∫ ˜A2 ℓq∗ n(dθ) = ∫ sup i,j (Aℓ,i,j − A∗ ℓ,i,j)2q∗ n(dAℓ,i,j ) ≤ s2 n, and ∫ ˜Aℓ ˜Akq∗ n(dθ) = (∫ sup i,j |Aℓ,i,j − A∗ ℓ,i,j|q∗ n(dθ) )( ∫ sup i,j |Ak,i,j − A∗ k,i,j|q∗ n(dθ) ) ≤ | sn| · | sn| = s2 n, and similarly , ∫˜b2 ℓq∗ n(dθ) ≤ s2 nand ∫˜bℓ˜bkq∗ n(dθ) ≤ s2 n. 26Convergence Rates of V ariational Inference in Sparse Deep Learning Then ∫ ∥fθ − fθ∗ ∥2 2q∗ n(dθ) ≤ 2(BD)2L−2 ( d + 1 + 1 BD − 1 ) 2(∫ L∑ ℓ=1 ˜A2 ℓq(dθ) + 2 ∫ L∑ ℓ=1 ℓ−1∑ k=1 ˜Aℓ ˜Akq(dθ) ) + 2 (∫ L∑ ℓ=1 (BD)2(L−ℓ)˜b2 lq(dθ) + 2 ∫ L∑ ℓ=1 ℓ−1∑ k=1 (BD)L−ℓ(BD)L−k˜bℓ˜bkq(dθ) ) ≤ 2(BD)2L−2 ( d + 1 + 1 BD − 1 ) 2 s2 n ( L + 2 L−1∑ ℓ=0 ℓ ) + 2s2 n L−1∑ ℓ=0 (BD)2ℓ + 4s2 n L∑ ℓ=1 L−1∑ k=L−ℓ+1 (BD)L−ℓ(BD)k = 2( BD)2L−2 ( d + 1 + 1 BD − 1 ) 2 s2 nL2 + 2s2 n (BD)2L − 1 (BD)2 − 1 + 4s2 n L∑ ℓ=1 ℓ−2∑ k=0 (BD)L−ℓ(BD)k(BD)L−ℓ+1 = 2 s2 n(BD)2L−2 ( d + 1 + 1 BD − 1 ) 2 L2 + 2s2 n (BD)2L − 1 (BD)2 − 1 + 4s2 n L∑ ℓ=1 (BD)L−ℓ (BD)ℓ−1 − 1 BD − 1 (BD)L−ℓ+1 ≤ 2s2 n(BD)2L−2 ( d + 1 + 1 BD − 1 ) 2 L2 + 2s2 n (BD)2L − 1 (BD)2 − 1 + 4s2 n 1 BD − 1 L∑ ℓ=1 (BD)2L−ℓ = 2 s2 n(BD)2L−2 ( d + 1 + 1 BD − 1 ) 2 L2 + 2s2 n (BD)2L − 1 (BD)2 − 1 + 4s2 n 1 BD − 1(BD)L (BD)L − 1 BD − 1 ≤ 2s2 n(BD)2L−2 ( d + 1 + 1 BD − 1 ) 2 L2 + 2s2 n (BD)2L − 1 (BD)2 − 1 + 4s2 n 1 (BD − 1)2 (BD)2L ≤ 2s2 n(BD)2L {( d + 1 + 1 BD − 1 ) 2 L2 (BD)2 + 1 (BD)2 − 1 + 2 (BD − 1)2 } = S 2n ≤ rn which proves Equation ( 4). 27Chérief-Abdellatif Third step : we prove Inequality (5) W e will use the fact that for any K, any p, p0 ∈ [0, 1]K such that ∑ K k=1 pk = ∑ K k=1 p0 k= 1 and any distributions Qk, Q0 kfor k = 1 , ..., K , we have: K (K∑ k=1 p0 kQ0 k     K∑ k=1 pkQk ) ≤ K (p0∥p) + K∑ k=1 p0 kK(Q0 k∥Qk). (7) Please refer to Lemma 6.1 in Chérief-Abdellatif and Alquier (2018) for a proof. Then we write q∗ n and π as mixtures of independent products of mixtures of two compo nents: q∗ n = ∑ γ∈SS T I(γ = γ∗) T⨂ t=1 { γt U([lt, ut]) + (1 − γt)δ{0} } and π = ∑ γ∈SS T (T S∗ ) −1 T⨂ t=1 { γt U([−B, B ]) + (1 − γt)δ{0} } Hence, using Inequality 7 twice and the additivity of KL for independent distribution s: KL(q∗ n∥π) ≤ KL ( {I(γ = γ∗)}γ∈SS T     {(T ∗ S∗ ) −1} γ∈SS T ) + ∑ γ∈SS T I(γ = γ∗) KL ( T⨂ t=1 { γt U([lt, ut]) + (1 − γt)δ{0} }    T⨂ t=1 { γt U([−B, B ]) + (1 − γt)δ{0} }) = log (T S ) + T∑ t=1 KL ( γ∗ t U([lt, ut]) + (1 − γ∗ t )δ{0}    γ∗ t U([−B, B ]) + (1 − γ∗ t )δ{0} ) ≤ log (T S ) + T∑ t=1 γ∗ t KL ( U([lt, ut])    U([−B, B ]) ) + T∑ t=1 (1 − γ∗ t )KL ( δ{0}∥δ{0} ) ≤ S log(T ) + T∑ t=1 γ∗ t log ( 2B ut − lt ) = S log(T ) + T∑ t=1 γ∗ t log (2B 2sn ) = S log(T ) + S log(B) + S 2 log (1 s2 n ) = S log(T ) + S log(B) + S 2 log (4n S (BD)2L {( d + 1 + 1 BD − 1 ) 2 L2 + 1 (BD)2 − 1 + 2 (BD − 1)2 }) , 28Convergence Rates of V ariational Inference in Sparse Deep Learning and hence, KL(q∗ n∥π) ≤ S log(T ) + S log(B) + S 2 log (4n S (BD)2L {( d + 1 + 1 BD − 1 ) 2 L2 + 1 (BD)2 − 1 + 2 (BD − 1)2 }) ≤ S log(L(D + 1)2) + S log(B) + LS log(BD) + S 2 log (4n S {( d + 1 + 1 BD − 1 ) 2 L2 + 1 (BD)2 − 1 + 2 (BD − 1)2 }) ≤ nrn, which ends the proof. Appendix C. Proof of Corollary 3 Corollary 3 is a direct consequence of Theorem 2, and we just need to ﬁnd an upper bound on infθ∗∈Θ S,L,D ∥fθ∗ − f0∥2 ∞and rS,L,D n . Indeed, according to Theorem 2: E [ ∫ ∥fθ − f0∥2 2˜πn,α(dθ) ] ≤ 2 1 − α inf θ∗∈Θ S,L,D ∥fθ∗ − f0∥2 ∞+ 2 1 − α ( 1 + σ2 α ) rn. (8) W e directly use the rate rn in the proof of Theorem 2 rather than rS,L,D n . Let us assume that f0 is β-Hölder smooth with 0 < β < d . Then according to Lemma 5.1 in Rockova and Polson (2018), we have for some positive constant CD independent of n (see Theorem 6.1 in Rockova and Polson (2018)) a neural network with architecture : L = 8 + ( ⌊log2 n⌋ + 5)(1 + ⌈log2 d⌉), D = CD⌊n d 2β+d / log n⌋, S ≤ 94d2(β + 1)2dD(L + ⌈log2 d⌉), with an error ∥f −f0∥∞ that is at most a constant multiple of D n +D−β/d ≤ CDn −2β 2β+d / log n+ C−β/d D n −β 2β+d logβ/d n ≤ (CD/ log n + C−β/d D log n)n −β 2β+d , which gives an upper bound on the ﬁrst term of the right-hand-side of Inequality 8 of order n −2β 2β+d log2 n. In the same time, we have for some constants C, C ′ that do not depend on n: rn ≤ SL n log(BD) + S n log(2BL(D + 1)2) + S 2n log (4n S { 3 + ( d + 2)2L2 }) ≤ C (DL2 n log D + DL n log(LD) + DL n log n ) ≤ C′ n d 2β+d n log2 n = C′n −2β 2β+d log2 n. 29Chérief-Abdellatif Then the tempered posterior distribution πn,α concentrates at the minimax rate rn = n −2β 2β+d up to a (squared) logarithmic factor for the expected L2-distance in the sense that: πn,α ( θ ∈ Θ S,L,D / ∥fθ − f0∥2 2> M nn −2β 2β+d log2 n ) − − − − − → n→+∞ 0. in probability as n → +∞ for any Mn → +∞. Appendix D. Proof of Theorem 1 W e could prove Theorem 1 using the prior mass condition ( 2) but we will use instead the same proof than for Theorem 2. Indeed, we can easily show that for any θ∗ ∈ Θ S,L,D, E [ ∫ ∥fθ−f0∥2 2πn,α(dθ) ] ≤ 2 1 − α∥fθ∗ −f0∥2 2+inf q { 2 1 − α ∫ ∥fθ−fθ∗ ∥2 2q(dθ)+2σ2 α KL(q∥π) n(1 − α) } where the inﬁmum is taken over all the probability distribut ions on Θ S,L,D. W e have: inf q { 2 1 − α ∫ ∥fθ − fθ∗ ∥2 2q(dθ) + 2σ2 α KL(q∥π) n(1 − α) } ≤ inf q∈FS,L,D { 2 1 − α ∫ ∥fθ − fθ∗ ∥2 2q(dθ) + 2σ2 α KL(q∥π) n(1 − α) } ≤ 2 1 − α ( 1 + σ2 α ) rS,L,D n , which implies E [ ∫ ∥fθ − f0∥2 2˜πn,α(dθ) ] ≤ 2 1 − α inf θ∗∈Θ S,L,D ∥fθ∗ − f0∥2 2+ 2 1 − α ( 1 + σ2 α ) rS,L,D n ≤ 2 1 − α inf θ∗∈Θ S,L,D ∥fθ∗ − f0∥2 ∞+ 2 1 − α ( 1 + σ2 α ) rS,L,D n . The rest of the proof follows the same lines than the one of Cor ollary 3. Appendix E. Proof of Theorem 4 First, we need Donsker and V aradhan’s variational formula. Refer to Lemma 1.1.3. in Catoni (2007) for a proof. Theorem 6. For any probability λ on some measurable space (E, E) and any measurable function h : E → R such that ∫ ehdλ < ∞, log ∫ ehdλ = sup q { ∫ hdq − KL(q, λ) } , where the supremum is taken over all probability distributi ons over E and with the convention ∞ − ∞ = −∞. Moreover, if h is upper-bounded on the support of λ, then the supremum is reached by the distribution of the form: λh(dβ) = eh(β) ∫ ehdλλ(dβ). 30Convergence Rates of V ariational Inference in Sparse Deep Learning Let us come back to the proof of Theorem 4. Here, we can not directly use Theorem 2.6 in Alquier and Ridgway (2017). Thus we begin from scratch. F or any α ∈ (0, 1) and θ ∈ Θ S,L,D, using the deﬁnition of Rényi divergence and Dα(P ⊗n, R⊗n) = nDα(P, R) as data are i.i.d. E [ exp ( − αrn(Pθ, P 0) + (1 − α)nDα(Pθ, P 0) )] = 1 where rn(Pθ, P 0) = 1 2σ2 ∑ n i=1{(Yi − fθ(Xi))2 − (Yi − f0(Xi))2} is the negative log-likelihood ratio. Then we integrate and use F ubini’s theorem, E [ ∫ exp ( − αrn(Pθ, P 0) + (1 − α)nDα(Pθ, P 0) ) π(dθ) ] = 1 . According to Theorem 6, E [ exp ( sup q { ∫ ( − αrn(Pθ, P 0) + (1 − α)nDα(Pθ, P 0) ) q(dθ) − KL(q||π) })] = 1 where the supremum is taken over all probability distributi ons over Θ S,L,D. Then, using Jensen’s inequality , E [ sup q { ∫ ( − αrn(Pθ, P 0) + (1 − α)nDα(Pθ, P 0) ) q(dθ) − KL(q||π) }] ≤ 0, and then, E [ ∫ ( − αrn(Pθ, P 0) + (1 − α)nDα(Pθ, P 0) ) ˜πk n,α(dθ) − KL(˜πk n,α||π) ] ≤ 0. W e rearrange terms: E [ ∫ Dα(Pθ, P 0)˜πk n,α(dθ) ] ≤ E [ α 1 − α ∫ rn(Pθ, P 0) n ˜πk n,α(dθ) + KL(˜πk n,α||π) n(1 − α) ] , that we can write: E [ ∫ Dα(Pθ, P 0)˜πk n,α(dθ) ] ≤ E [ α 1 − α ∫ rn(Pθ, P 0) n ˜πn,α(dθ) + KL(˜πn,α||π) n(1 − α) ] + E [ α 1 − α ∫ rn(Pθ, P 0) n ˜πk n,α(dθ) + KL(˜πk n,α||π) n(1 − α) ] − E [ α 1 − α ∫ rn(Pθ, P 0) n ˜πn,α(dθ) + KL(˜πn,α||π) n(1 − α) ] . Let us precise that E [ rn(Pθ,P 0) n ] = KL(P 0||Pθ) = ∥f0−fθ∥2 2 2σ2 , and: Ln(q) = − α 2σ2 n∑ i=1 ∫ (Yi − fθ(Xi))2q(dθ) − KL(q∥π) up to a constant. 31Chérief-Abdellatif Then: E [ ∫ Dα(Pθ, P 0)˜πk n,α(dθ) ] ≤ E [ α 1 − α ∫ rn(Pθ, P 0) n ˜πn,α(dθ) + KL(˜πn,α||π) n(1 − α) ] + E[L∗ n− Lk n] n(1 − α) . W e conclude by interverting the inﬁmum and the expectation a nd the same inequalities than in Theorem 2: E [ α 1 − α ∫ rn(Pθ, P 0) n ˜πn,α(dθ) + KL(˜πn,α∥π) n(1 − α) ] = E [ inf q∈FS,L,D { α 1 − α ∫ rn(Pθ, P 0) n q(dθ) + KL(q∥π) n(1 − α) }] ≤ inf q∈FS,L,D { E [ α 1 − α ∫ rn(Pθ, P 0) n q(dθ) + KL(q∥π) n(1 − α) ]} ≤ α 1 − α 2 2σ2 inf θ∗∈Θ S,L,D ∥fθ∗ − f0∥2 2+ α 2σ2 2 1 − α ( 1 + σ2 α ) rS,L,D n . Appendix F. Proof of Theorem 5 W e start from the last inequality obtained in the proof of The orem 3 in Cherief-Abdellatif (2019) that provides an upper bound in α-Rényi divergence for the ELBO model selection framework. W e still denote P 0 the generating distribution and Pθ the distribution charac- terizing the model. Then, for any α ∈ (0, 1): E [ ∫ Dα(Pθ, P 0)˜π ˆS, ˆL, ˆD n,α (dθ) ] ≤ inf S,L,D { inf q∈FS,L,D { α 1 − α ∫ KL(P 0, PθS,L,D )q(dθS,L,D) + KL(q, Π S,L,D) n(1 − α) } + log( 1 πS,L,D ) n(1 − α) } where Π S,L,D denotes the prior over the parameter set Θ S,L,D and πS,L,D the prior belief over model (S, L, D). As for the proof of Theorem 2, for any S, L, D and any θ∗ ∈ Θ S,L,D: E [ ∫ α 2σ2 ∥fθ − f0∥2 2˜π ˆS, ˆL, ˆD n,α (dθ) ] ≤ α 1 − α 2 2σ2 ∥fθ∗ − f0∥2 2+ inf q∈FS,L,D { α 1 − α ∫ 2 2σ2 ∥fθ − fθ∗ ∥2 2q(dθ) + KL(q, Π S,L,D) n(1 − α) } + log( 1 πS,L,D ) n(1 − α) , and then for any S, L, D and any θ∗ ∈ Θ S,L,D, E [ ∫ ∥fθ − f0∥2 2˜π ˆS, ˆL, ˆD n,α (dθ) ] ≤ 2 1 − α∥fθ∗ − f0∥2 2+ 2 1 − α ( 1+σ2 α ) rS,L,D n + 2σ2 α(1 − α) log( 1 πS,L,D ) n , which ﬁnally leads to Theorem 5. 32Convergence Rates of V ariational Inference in Sparse Deep Learning Appendix G. Result for sparse Gaussian approximations In this appendix, we consider non-bounded parameter sets Θ S,L,D and Gaussians instead of uniform distributions in spike-and-slab priors on θ ∈ Θ S,L,D: { γ ∼ U (SS T ), θt|γt ∼ γt N (0, 1) + (1 − γt)δ{0}, t = 1 , ..., T and Gaussian-based sparse spike-and-slab approximations : { γ ∼ πγ , θt|γt ∼ γt N (mt, s2 n) + (1 − γt)δ{0} for each t = 1 , ..., T. The following theorem states that using Gaussians instead o f uniform distributions still leads to consistency with the same rate of convergence. Note that t he inﬁmum in the RHS of the inequality is taken over a bounded neural network model. Theorem 7. Let us introduce the sets Θ B S,L,Dthat contain the neural network parameters upper bounded by B (in L∞-norm). Then for any α ∈ (0, 1), for any B ≥ 2, E [ ∫ ∥fθ − f0∥2 2˜πn,α(dθ) ] ≤ 2 1 − α inf θ∗∈Θ B S,L,D ∥fθ∗ − f0∥2 2+ 2 1 − α ( 1 + σ2 α ) rS,L,D n with rS,L,D n = SL n log(2BD) + S 4n ( 12 log(LD) + B2 ) + S n log ( 11d max( n S , 1) ) . Proof. The proof follows the same structure than for Theorem 2. W e ﬁx B ≥ 2. First step : we obtain the general inequality W e can directly write for any θ∗ ∈ Θ S,L,D, E [ ∫ ∥fθ − f0∥2 2˜πn,α(dθ) ] ≤ 2 1 − α∥fθ∗ − f0∥2 2+ inf q∈FS,L,D { 2 1 − α ∫ ∥fθ − fθ∗ ∥2 2q(dθ) + 2σ2 α KL(q∥π) n(1 − α) } . W e deﬁne θ∗ = arg min θ∈Θ B S,L,D ∥fθ − f0∥2. Again, the rest of the proof consists in ﬁnding a distribution q∗ n ∈ F S,L,D that satisﬁes the extended prior mass condition: ∫ ∥fθ − fθ∗ ∥2 2q∗ n(dθ) ≤ rn (9) and KL(q∗ n∥π) ≤ nrn (10) with rn = SL n log(2BD)+ S n log(L(D+1)2)+ S log log(3D) n + SB2 4n + S 2n log ( 16n S { 3+(d+2)2 }) ≤ rS,L,D n as 3 + ( x + 2)2 ≤ 7x2 for x ≥ 1. 33Chérief-Abdellatif Second step : we prove Inequality (9) All coeﬃcients of parameter θ∗ are upper bounded by B. Hence, we still have: cℓ ≤ BℓDℓ−1 ( d + 1 + 1 BD − 1 ) . However, the upper bound on rℓ(θ) is not the same, as |Aℓ,i,j | can not be upper bounded by B directly and must be upper bounded by |A∗ ℓ,i,j| + ˜Aℓ ≤ B + ˜Aℓ: rℓ(θ) ≤ sup x∈[−1,1]d sup 1≤i≤D { D∑ j=1 [ |Aℓij − A∗ ℓij| · | fℓ−1 θ∗ (x)j | + |Aℓij | · | fℓ−1 θ∗ (x)j − fℓ−1 θ (x)j | ] + |bℓi − b∗ ℓi| } ≤ sup x∈[−1,1]d sup 1≤i≤D { D∑ j=1 [ |Aℓij − A∗ ℓij| · | fℓ−1 θ∗ (x)j | + (B + ˜Aℓ) · |fℓ−1 θ∗ (x)j − fℓ−1 θ (x)j | ] + |bℓi − b∗ ℓi| } ≤ Dcℓ−1 ˜Aℓ + (B + ˜Aℓ)Drℓ−1(θ) + ˜bℓ ≤ (B + ˜Aℓ)Drℓ−1(θ) + ˜AℓBℓ−1Dℓ−1 ( d + 1 + 1 BD − 1 ) + ˜bℓ. Then, using F ormula 6: rℓ(θ) ≤ ℓ∑ u=2 ( ℓ∏ v=u+1 (B + ˜Av)D )( ˜Au(BD)u−1 { d + 1 + 1 BD − 1 } + ˜bu ) + ( ℓ∏ v=2 (B + ˜Av)D ) r1(θ) ≤ ℓ∑ u=2 Dℓ−u ℓ∏ v=u+1 (B + ˜Av) ˜Au(BD)u−1 ( d + 1 + 1 BD − 1 ) + ℓ∑ u=2 Dℓ−u ℓ∏ v=u+1 (B + ˜Av)˜bu + Dℓ−1 ℓ∏ v=2 (B + ˜Av)r1(θ), and using inequality r1(θ) ≤ d · ˜A1 + ˜b1: rℓ(θ) ≤ Dℓ−1 ( d + 1 + 1 BD − 1 ) ℓ∑ u=2 Bu−1 ℓ∏ v=u+1 (B + ˜Av) ˜Au + ℓ∑ u=2 Dℓ−u ℓ∏ v=u+1 (B + ˜Av)˜bu + dDℓ−1 ℓ∏ v=2 (B + ˜Av) ˜A1 + Dℓ−1 ℓ∏ v=2 (B + ˜Av)˜b1 ≤ Dℓ−1 ( d + 1 + 1 BD − 1 ) ℓ∑ u=1 Bu−1 ℓ∏ v=u+1 (B + ˜Av) ˜Au + ℓ∑ u=1 Dℓ−u ℓ∏ v=u+1 (B + ˜Av)˜bu. 34Convergence Rates of V ariational Inference in Sparse Deep Learning Then we have for any distribution q(θ) = q1(θ1) × ... × qT (θT ): ∫ ∥fθ − fθ∗ ∥2 2q(dθ) ≤ ∫ ∥fθ − fθ∗ ∥2 ∞q(dθ) = ∫ rL(θ)2q(dθ) ≤ ∫ 2D2L−2 ( d + 1 + 1 BD − 1 ) 2( L∑ ℓ=1 Bℓ−1 L∏ v=ℓ+1 (B + ˜Av) ˜Aℓ ) 2 q(dθ) + ∫ 2 ( L∑ ℓ=1 DL−ℓ L∏ v=ℓ+1 (B + ˜Av)˜bℓ ) 2 q(dθ) = 2 D2L−2 ( d + 1 + 1 BD − 1 ) 2(∫ L∑ ℓ=1 B2ℓ−2 L∏ v=ℓ+1 (B + ˜Av)2 ˜A2 ℓq(dθ) + 2 ∫ L∑ ℓ=1 ℓ−1∑ k=1 Bℓ−1Bk−1 L∏ v=ℓ+1 (B + ˜Av) ˜Aℓ L∏ v=k+1 (B + ˜Av) ˜Akq(dθ) ) + 2 (∫ L∑ ℓ=1 D2(L−ℓ) L∏ v=ℓ+1 (B + ˜Av)2˜b2 ℓq(dθ) + 2 ∫ L∑ ℓ=1 ℓ−1∑ k=1 DL−ℓDL−k L∏ v=ℓ+1 (B + ˜Av)˜bℓ L∏ v=k+1 (B + ˜Av)˜bkq(dθ) ) = 2 D2L−2 ( d + 1 + 1 BD − 1 ) 2( L∑ ℓ=1 B2ℓ−2 L∏ v=ℓ+1 ∫ (B + ˜Av)2q(dθ) ∫ ˜A2 ℓqℓ(dθℓ) + 2 L∑ ℓ=1 ℓ−1∑ k=1 Bℓ−1Bk−1 L∏ v=ℓ+1 ∫ (B + ˜Av)2q(dθ) ∫ ˜Aℓqℓ(dθℓ) ℓ∏ v=k+1 ∫ (B + ˜Av)q(dθ) ∫ ˜Akq(dθ) ) + 2 ( L∑ ℓ=1 D2(L−ℓ) L∏ v=ℓ+1 ∫ (B + ˜Av)2q(dθ) ∫ ˜b2 ℓq(dθ) + 2 L∑ ℓ=1 ℓ−1∑ k=1 DL−ℓDL−k L∏ v=ℓ+1 ∫ (B + ˜Av)2q(dθ) ∫ ˜bℓq(dθ) ℓ∏ v=k+1 ∫ (B + ˜Av)q(dθ) ∫ ˜bkq(dθ) ) . Here, we deﬁne q∗ n(θ) as follows: { γ∗ t = I(θ∗ t ̸= 0) , θt ∼ γ∗ t N (θ∗ t , s2 n) + (1 − γ∗ t )δ{0} for each t = 1 , ..., T. with s2 n= S 16n log(3D)−1(2BD)−2L {( d + 1 + 1 BD−1 ) 2 + 1 (2BD)2−1 + 2 (2BD−1)2 }−1 . W e upper bound the expectation of the supremum of absolute va lues of Gaussian vari- ables: ∫ ˜Aℓq∗ n(dθ) = ∫ sup i,j |Aℓ,i,j − A∗ ℓ,i,j|q∗ n(dθ) ≤ √ 2s2 nlog(2D2) = √ 4s2 nlog(3D), 35Chérief-Abdellatif and use Example 2.7 in Boucheron et al. (2003): ∫ ˜A2 ℓq∗ n(dθ) = ∫ sup i,j (Aℓ,i,j − A∗ ℓ,i,j)2q∗ n(dθ) ≤ s2 n(1 + 2 √ log(D2) + log( D2)) = 4 s2 nlog(3D), which also give: ∫ (B + ˜Aℓ)q∗ n(dθ) = B + ∫ ˜Aℓq∗ n(dθ) ≤ B + √ 4s2 nlog(3D) ≤ 2B, and ∫ (B + ˜Aℓ)2q∗ n(dθ) = B2 + 2B ∫ ˜Aℓq∗ n(dθ) + ∫ ˜A2 ℓq∗ n(dθ) ≤ B2 + 2B √ 4s2 nlog(3D) + 4 s2 nlog(3D) ≤ 4B2 as √ 4s2 nlog(3D) ≤ B (s2 n≤ LD(D+1) 16n (2BD)−2L ≤ 2LD2 16n 4−2LD−2L ≤ 1). Similarly , ∫ ˜bℓq∗ n(dθ) ≤ √ 4s2 nlog(3D) and ∫ ˜b2 ℓq∗ n(dθ) ≤ 4s2 nlog(3D). Then ∫ ∥fθ − fθ∗ ∥2 2q∗ n(dθ) ≤ 2D2L−2 ( d + 1 + 1 BD − 1 ) 2( L∑ ℓ=1 B2ℓ−2(4B2)L−ℓ4s2 nlog(3D) + 2 L∑ ℓ=1 ℓ−1∑ k=1 Bℓ−1Bk−1(4B2)L−ℓ√ 4s2 nlog(3D)(2B)ℓ−k√ 4s2 nlog(3D) ) + 2 ( L∑ ℓ=1 D2(L−ℓ)(4B2)L−ℓ4s2 nlog(3D) + 2 L∑ ℓ=1 ℓ−1∑ k=1 DL−ℓDL−k(4B2)L−ℓ√ 4s2 nlog(3D)(2B)ℓ−k√ 4s2 nlog(3D) ) , 36Convergence Rates of V ariational Inference in Sparse Deep Learning i.e. ∫ ∥fθ − fθ∗ ∥2 2q∗ n(dθ) ≤ 2D2L−2 ( d + 1 + 1 BD − 1 ) 2( B2L−24s2 nlog(3D) L−1∑ ℓ=0 4ℓ + 2B2L−24s2 nlog(3D) L∑ ℓ=1 ℓ−1∑ k=1 2L−ℓ2L−k ) + 2 ( 4s2 nlog(3D) L∑ ℓ=1 (2BD)2L−2ℓ + 8s2 nlog(3D) L∑ ℓ=1 ℓ−1∑ k=1 (2BD)L−ℓ(2BD)L−k ) ≤ 2D2L−2 ( d + 1 + 1 BD − 1 ) 2( B2L−24s2 nlog(3D)4L − 1 4 − 1 + 2B2L−24s2 nlog(3D) L∑ ℓ=1 2L−ℓ2L−ℓ+1 ℓ−2∑ k=0 2k ) + 2 ( 4s2 nlog(3D) L−1∑ ℓ=0 (2BD)2ℓ + 8s2 nlog(3D) L∑ ℓ=1 (2BD)L−ℓ(2BD)L−ℓ+1 ℓ−2∑ k=0 (2BD)k ) ≤ 2D2L−2 ( d + 1 + 1 BD − 1 ) 2( B2L−24s2 nlog(3D)4L 3 + 2B2L−24s2 nlog(3D) L∑ ℓ=1 2L−ℓ2L−ℓ+12ℓ−1 ) + 2 ( 4s2 nlog(3D) (2BD)2L (2BD)2 − 1 + 8s2 nlog(3D) L∑ ℓ=1 (2BD)L−ℓ(2BD)L−ℓ+1 (2BD)ℓ−1 2BD − 1 ) ≤ 2D2L−2 ( d + 1 + 1 BD − 1 ) 2( B2L−24s2 nlog(3D)4L 3 + 2B2L−24s2 nlog(3D)2L L−1∑ ℓ=0 2ℓ ) + 2 ( 4s2 nlog(3D) (2BD)2L (2BD)2 − 1 + 8s2 nlog(3D) L−1∑ ℓ=0 (2BD)ℓ (2BD)L 2BD − 1 ) ≤ 2D2L−2 ( d + 1 + 1 BD − 1 ) 2( B2L−24s2 nlog(3D)4L 3 + 2B2L−24s2 nlog(3D)22L ) + 2 ( 4s2 nlog(3D) (2BD)2L (2BD)2 − 1 + 8s2 nlog(3D) (2BD)2L (2BD − 1)2 ) = 2 D2L−2 ( d + 1 + 1 BD − 1 ) 2 4s2 nlog(3D) ( B2L−2 4L 3 + 2B2L−222L ) + 2 ( (2BD)2L (2BD)2 − 1 + 2 (2BD)2L (2BD − 1)2 ) 4s2 nlog(3D), 37Chérief-Abdellatif and consequently , as BD ≥ 2, ∫ ∥fθ − fθ∗ ∥2 2q∗ n(dθ) ≤ 8s2 nlog(3D) { D2L−2 ( d + 1 + 1 BD − 1 ) 2 7 3B2L−222L + (2BD)2L ( 1 (2BD)2 − 1 + 2 (2BD − 1)2 )} = 8 s2 nlog(3D) { (2BD)2L 1 (BD)2 ( d + 1 + 1 BD − 1 ) 2 7 3 + (2BD)2L ( 1 (2BD)2 − 1 + 2 (2BD − 1)2 )} ≤ 8s2 nlog(3D)(2BD)2L {( d + 1 + 1 BD − 1 ) 2 + 1 (2BD)2 − 1 + 2 (2BD − 1)2 } = S 2n ≤ rn. which ends Step 2. Third step : we prove Inequality (10) W e end the proof: KL(q∗ n∥π) ≤ log (T S ) + T∑ t=1 γ∗ t KL ( N (θ∗ t , s2 n)    N (0, 1) ) ≤ S log(T ) + T∑ t=1 γ∗ t {1 2 log (1 s2 n ) + s2 n+ θ∗2 t 2 − 1 2 } ≤ S log(T ) + T∑ t=1 γ∗ t {1 2 log (1 s2 n ) + s2 n+ B2 2 − 1 2 } = S log(T ) + S 2 s2 n+ S 2 B2 − 1 2 + S 2 log (1 s2 n ) ≤ S log(T ) + S 2 + S 2 B2 − 1 2 + S 2 log (16n S log(3D)(2BD)2L {( d + 1 + 1 BD − 1 ) 2 + 1 (2BD)2 − 1 + 2 (2BD − 1)2 }) ≤ S log(L(D + 1)2) + B2S 4 + LS log(2BD) + S 2 log log(3D) + S 2 log (16n S {( d + 1 + 1 BD − 1 ) 2 + 1 (BD)2 − 1 + 2 (BD − 1)2 }) ≤ nrn. 38",
      "meta_data": {
        "arxiv_id": "1908.04847v2",
        "authors": [
          "Badr-Eddine Chérief-Abdellatif"
        ],
        "published_date": "2019-08-09T18:50:09Z",
        "pdf_url": "https://arxiv.org/pdf/1908.04847v2.pdf"
      },
      "llm_extracted_info": {
        "main_contributions": "This paper presents the first theoretical generalization error bounds for variational inference (VI) in Bayesian deep learning, showing that sparse VI in deep neural networks (DNNs) achieves near-minimax convergence rates for Hölder smooth functions, akin to exact Bayesian inference. It introduces a robust notion of consistency for Bayesian estimators using tempered posteriors to handle model misspecification. The research demonstrates that architecture selection through Evidence Lower Bound (ELBO) maximization adaptively achieves optimal convergence rates without prior knowledge of the regression function's smoothness and does not overfit. It also analyzes the impact of optimization errors on generalization, linking ELBO convergence to algorithm consistency.",
        "methodology": "The methodology centers on sparse variational inference (VI) for Bayesian Deep Neural Networks in a nonparametric regression framework. It employs a hierarchical spike-and-slab prior to induce sparsity in network parameters. The paper uses tempered posterior distributions (likelihood raised to an α-power, α<1) for robustness and theoretical tractability. Variational approximations are obtained by minimizing the Kullback-Leibler (KL) divergence, which is equivalent to maximizing the ELBO. Theoretical generalization error bounds are derived using PAC-Bayes theory. An ELBO maximization framework is introduced for adaptive model selection, guiding the optimal choice of network architecture (depth, width, and sparsity).",
        "experimental_setup": "The paper primarily presents a theoretical study. The framework considers nonparametric regression where observed data (Xi, Yi) are i.i.d. with Yi = f0(Xi) + ζi, where ζi is i.i.d. Gaussian noise and f0 is a β-Hölder smooth function. For the theoretical results, ReLU activation functions are considered, and specific network architectures (L, D, S for layers, width, and sparsity, respectively) are defined to achieve minimax rates of convergence. No empirical datasets, benchmarks, or practical experiments are performed; the validation is purely through mathematical proofs of convergence rates and generalization bounds.",
        "limitations": "The paper explicitly states that the challenges related to optimization, particularly proving the global convergence of ELBO maximization algorithms (which are often non-convex), fall outside its scope. While it discusses the impact of optimization error on generalization, it does not provide concrete algorithms or guarantees for achieving the ideal variational solution. The theoretical framework also relies on assumptions like 1-Lipschitz continuous activation functions and a bounded range for network coefficients (though a relaxation for unbounded parameters is discussed in the appendix).",
        "future_research_directions": "Future research directions include the design of new, computationally efficient algorithms for spike-and-slab deep learning that can handle sparsity-inducing variational inference, building upon existing works. Another key area is the theoretical study of global convergence for these approximate algorithms, especially in non-convex settings, focusing on Bayesian gradient descent methods and the convergence of the ELBO. Developing algorithms that effectively preserve network sparsity during the training process is also highlighted as an important extension."
      }
    },
    {
      "title": "Test Time Adaptation via Conjugate Pseudo-labels",
      "abstract": "Test-time adaptation (TTA) refers to adapting neural networks to distribution\nshifts, with access to only the unlabeled test samples from the new domain at\ntest-time. Prior TTA methods optimize over unsupervised objectives such as the\nentropy of model predictions in TENT [Wang et al., 2021], but it is unclear\nwhat exactly makes a good TTA loss. In this paper, we start by presenting a\nsurprising phenomenon: if we attempt to meta-learn the best possible TTA loss\nover a wide class of functions, then we recover a function that is remarkably\nsimilar to (a temperature-scaled version of) the softmax-entropy employed by\nTENT. This only holds, however, if the classifier we are adapting is trained\nvia cross-entropy; if trained via squared loss, a different best TTA loss\nemerges. To explain this phenomenon, we analyze TTA through the lens of the\ntraining losses's convex conjugate. We show that under natural conditions, this\n(unsupervised) conjugate function can be viewed as a good local approximation\nto the original supervised loss and indeed, it recovers the best losses found\nby meta-learning. This leads to a generic recipe that can be used to find a\ngood TTA loss for any given supervised training loss function of a general\nclass. Empirically, our approach consistently dominates other baselines over a\nwide range of benchmarks. Our approach is particularly of interest when applied\nto classifiers trained with novel loss functions, e.g., the recently-proposed\nPolyLoss, where it differs substantially from (and outperforms) an\nentropy-based loss. Further, we show that our approach can also be interpreted\nas a kind of self-training using a very specific soft label, which we refer to\nas the conjugate pseudolabel. Overall, our method provides a broad framework\nfor better understanding and improving test-time adaptation. Code is available\nat https://github.com/locuslab/tta_conjugate.",
      "full_text": "Test-Time Adaptation via Conjugate Pseudo-labels Sachin Goyal⋆1 Mingjie Sun⋆1 Aditi Raghunathan1 Zico Kolter1,2 1Carnegie Mellon University, 2Bosch Center for AI {sachingo, mingjies, raditi, zkolter}@cs.cmu.edu Abstract Test-time adaptation (TTA) refers to adapting neural networks to distribution shifts, with access to only the unlabeled test samples from the new domain at test-time. Prior TTA methods optimize over unsupervised objectives such as the entropy of model predictions in TENT [50], but it is unclear what exactly makes a good TTA loss. In this paper, we start by presenting a surprising phenomenon: if we attempt to meta-learn the “best” possible TTA loss over a wide class of functions, then we recover a function that isremarkably similar to (a temperature-scaled version of) the softmax-entropy employed by TENT. This only holds, however, if the classiﬁer we are adapting is trained via cross-entropy loss; if the classiﬁer is trained via squared loss, a different “best” TTA loss emerges. To explain this phenomenon, we analyze test-time adaptation through the lens of the training losses’sconvex conjugate. We show that under natural conditions, this (unsupervised) conjugate function can be viewed as a good local approximation to the original supervised loss and indeed, it recovers the “best” losses found by meta-learning. This leads to a generic recipe that can be used to ﬁnd a good TTA loss for any given supervised training loss function of a general class. Empirically, our approach consistently dominates other TTA alternatives over a wide range of domain adaptation benchmarks. Our approach is particularly of interest when applied to classiﬁers trained with novel loss functions, e.g., the recently-proposed PolyLoss [25] function, where it differs substantially from (and outperforms) an entropy-based loss. Further, we show that our conjugate based approach can also be interpreted as a kind of self-training using a very speciﬁc soft label, which we refer to as the conjugate pseudo-label. Overall, our method provides a broad framework for better understanding and improving test-time adaptation. Code is available at https://github.com/locuslab/ tta_conjugate. 1 Introduction Modern deep networks perform exceeding well on new test inputs that are close to the training distribution. However, this performance dramatically decreases on test inputs drawn from a different distribution. While there is a large body of work on improving the robustness of models, most robust training methods are highly specialized to the setting they cater to. For e.g., they assume pre-speciﬁed perturbations, subpopulations, and spurious correlations, or access to unlabeled data from the target distribution, and most methods offer close to no improvement on general distribution shifts beyond what they were trained for [12, 21]. In practice, it is often cumbersome (or even impossible) to precisely characterize all possible distri- bution shifts a model could encounter and then train accordingly. Instead, a model already trained on some source data must be able to adapt at test-time to new inputs from a different domain. This setting of test-time adaptation (TTA) has gained interest in recent years [ 6, 47, 50, 54]. TTA is typically accomplished by updating the source model parameters via a few steps of optimization on an unsupervised objective involving the new test sample from the target distribution. The choice ⋆ Equal Contribution 36th Conference on Neural Information Processing Systems (NeurIPS 2022). arXiv:2207.09640v2  [cs.LG]  23 Nov 2022of this unsupervised objective, which we call the TTA loss, dictates the success of the adaptation procedure. [47] uses a self-supervised objective on the test sample, [50] uses the entropy of model predictions, and several follow-ups have proposed variants or alternatives [ 40, 54]. However, it remains unclear as to how to choose or guide the selection of this TTA loss, and thus far the choice of these losses has remained largely heuristic in nature. In this work, we begin by presenting a set of intriguing experiments where we attempt to learn the “best” TTA loss for a given source classiﬁer and distribution shift. We parameterize the TTA loss by another neural network whose parameters are learnt via meta-learning [ 3, 9] where we differentiate through the adaptation process to ﬁnd the TTA loss that achieves the best adaptation on distribution shifts. Surprisingly, we ultimately learn a TTA loss that looksremarkably similar to (a temperature-scaled version of) the softmax-entropy loss, which was already proposed by [50]. Why did we recover the commonly used softmax-entropy loss despite the fact that the procedure is capable of learning a very general class of losses and the meta-learning process could potentially specialize to both the source classiﬁer and the distribution shift of interest? Furthermore, we ﬁnd that this pattern only holds when the loss used to train the source classiﬁer is cross-entropy loss; when a different loss such as squared loss is used instead, the meta-learning procedure recovers a TTA loss that itself looks more like a negative squared error, and is very different from the softmax-entropy loss (Section 3). In order to explain this phenomenon, we propose to consider TTA through the lens of the convex conjugate function. Speciﬁcally, given a hypothesis function h(x) and label y, several common losses (cross-entropy and the squared loss amongst them, but not limited to these) can be written in the form L(h(x),y) = f(h(x)) −yTh(x) for some function f. In these cases, we show that “natural” TTA loss for such classiﬁers is precisely the (negation of) the convex conjugate evaluated at the gradient of h, LTTA(x) = −f∗(∇f(h(x)), where f∗is the convex conjugate of f. This framework not only recovers the results of our meta-learning experiments, but also justiﬁes why some speciﬁc choices of TTA loss in the previous literature work well (e.g., this framework recovers TENT’s choice of softmax-entropy for cross-entropy-trained classiﬁer). Moreover, it also provides a broad framework for what the TTA loss should be when the source model is trained using various different loss functions (for example the recently-proposed PolyLoss [25, 29]) as is becoming increasingly common in machine learning. Further, we show that our proposed conjugate adaptation loss is in fact a kind of self-training with pseudo-labels [42], a classic approach in machine learning. Various formulations of the pseudo-label have been proposed in the literature, and our conjugate analysis provides a general recipe for the “correct” choice of soft pseudo-labels given byˆy(x) = ∇f(h(x)). We thus refer to these as conjugate pseudo-labels (Conjugate PL’s), and believe our work provides a broad framework for understanding adaptation with unlabeled data in general. Finally, we empirically verify the effectiveness of our proposed conjugate adaptation loss across several datasets and training losses, such as cross-entropy and squared loss, along with the recently- proposed PolyLoss [ 25] (which itself has shown higher standard test accuracy on a wide range of vision tasks). Over all models, datasets and training losses, we ﬁnd our proposed conjugate pseudo-labeling consistently outperforms prior TTA losses and improves TTA performance over the current state of the art. 2 Background and preliminaries. Test-time adaptation. We are interested in mapping an input x∈Rd to a label y∈Y. We learn a model hθ : Rd ↦→R|Y|parameterized by θthat maps an input xto predictions hθ(x). We assume access to a trained source model and adapt at test-time over the test input, before making the ﬁnal prediction. This is the standard test-time adaptation (TTA) setting [47, 50]. During TTA, we update the model parameters on an unsupervised objective L(x,hθ). For example, in TENT [50], this loss is the entropy of the softmax-normalized predictions of the model. At each time step of adaptation, we observe a batch of test inputs and we take a gradient step towards optimizing the TTA loss on this test batch. As is standard, we measure the average online performance of models across all steps (number of test batch inputs seen) in the adaptation process. Meta learning the loss function. In order to explore the existence of different TTA losses, we employ the meta-learning procedure where we attempt to learn the TTA loss. We use a similar procedure as prior work on meta-learning loss functions [3, 37] and parameterize the loss function via a neural network mφ : R|Y| ↦→R that takes in the model predictions/logits and outputs a loss value. We want to learn parameter φsuch that when we update θvia the loss function mφ, our ﬁnal 2performance is optimal. In order to do so, let xbe the unlabeled test samples to adapt to, and ybe the corresponding labels. We update θand φalternatively as follows. θt+1 ←θt −α∂mφt(hθt(x)) ∂θt , φt+1 ←φt −β∂L(hθt+1 (x′),y′) ∂φt , (1) where Lis some supervised surrogate loss function such as cross-entropy. Please refer to Appendix A3 for further details regarding meta-learning setup. Note that the meta-learning process above assumes access to labels yof test inputs. In this paper, we do not propose meta-learning the TTA loss as an approach. Rather, we use meta-learning to explore what the “best” TTA losses look like. We discuss our ﬁndings from this exploration in the next section. 3 Test-time Adaptation via Meta-Learnt Losses The objective used in TENT is the softmax-entropy of the model predictions which essentially makes the classiﬁer more conﬁdent in its current predictions. The same can be achieved by various other loss formulations such as those mentioned in [40]. With so many possible choices for the loss function, what should we use for TTA? In this section, we attempt to answer this empirically and present some intriguing observations. (a)  (b) Figure 1: Visualization of meta loss (blue) by varying one input prediction score. (a) For cross-entropy loss trained model, the learnt meta loss can be approximated with a scaled softmax-entropy function (dashed red). (b) When the source model is trained with a squared loss for classiﬁcation, the learnt meta loss (blue) can be ﬁtted closely with a quadratic function (dashed red), shown in Figure 1b. The range (max/min) of the prediction score (logit) in x-axis is chosen to cover the empirical range of the predicted logits. Experiment 1. We learn the TTA loss parameterized by a neural network via meta-learning as described in Section 2. Our source classiﬁer is a ResNet-26 trained on CIFAR-10 and we adapt to distribution shifts in CIFAR-10-C. We use the 4 labeled validation noises in CIFAR-10-C to learn the meta-loss network parameters and we denote the resulting learnt loss function by meta-TTA loss. We then adapt the source classiﬁer to the test set of 15 corruptions by optimizing the meta-TTA loss. Observations. First, we ﬁnd that TTA using meta-TTA loss performs better than TENT (12.35% vs 13.14%), suggesting that there are better TTA losses than previous losses based on softmax-entropy. However, on examining this meta-TTA loss, we ﬁnd a surprising observation. Figure 1a (blue curve) visualizes the learnt meta-loss over model predictions as we vary a single class prediction with the rest ﬁxed. Qualitatively, the learnt meta-loss looks very similar to softmax-entropy in one dimension. In fact, we can ﬁt it closely with a scaled softmax-entropy function (dashed red curve): α·H(softmax(hθ(x)/T)), where αis a magnitude parameter and T is a temperature scaler. We want to test if the meta-loss is basically learning the softmax-entropy function. Hence, we perform test-time adaptation with the ﬁtted softmax-entropy function instead (dashed red curve) and achieve an error of 12.32%, essentially recovering the performance of meta-TTA. 3Despite the ability to represent many different loss functions and potentially specialize to the CIFAR- 10-C setting, the meta-loss procedure gave back the standard entropy objective.Do we always recover a loss that looks like softmax-entropy? Experiment 2. In an attempt to isolate when we get back the entropy objective, we vary several things. We tried different architectures for the source classiﬁer, different lossesLduring the meta- learning process (1) and different training losses for the source classiﬁer. Results. We observed that we consistently recovered the temperature scaled softmax-entropy function in all cases except when we varied the training loss for the source classiﬁer (Appendix A.10). On using the squared loss function [18], a strikingly different meta-TTA loss emerges. Figure 1b (blue curve) shows the learnt meta-loss (13.48% error) for this network. Here again, the meta-TTA loss outperforms entropy (14.57%) but it is not simply due to a scaling factor. The loss now looks like the negative squared error (red curve). Like previously, we tried ﬁtting a quadratic loss directly to the meta loss in Figure 1b, and this time we even slightly outperformed the meta-TTA loss. To summarize, we used a meta-learning procedure to search for the “best” TTA loss, where the loss itself was parameterized by a neural network that could potentially represent arbitrarily complex loss functions. However, we ended up with loss functions displaying remarkable structure: across different architectures and different variants of meta-learning, for a classiﬁer trained with cross-entropy, the meta-TTA loss was temperature scaled softmax-entropy and for a classiﬁer trained with squared loss, the meta-TTA loss was a negative squared loss. This is interesting from both a practical and conceptual standpoint where the “best” TTA loss depends on the loss used to train the source classiﬁer in a clean fashion. We attempt to understand and explain this phenomenon in the next section. 4 Conjugate Pseudo Labels Results in the previous section raise an obvious question: why does softmax-entropy as used in TENT seem to be the “best” possible test time adaptation loss for classiﬁers trained via cross-entropy (at least, best in the sense that meta-learning consistently recovers something which essentially mimics softmax-entropy, even though meta-loss is parameterized by a neural network and hence could learn much more complex functions speciﬁc to the model and the particular shift)? And why, alternatively, does a quadratic TTA loss seem to perform best when the classiﬁer is trained via squared loss? In this section, we offer an explanation of this phenomenon via the construct of the convex conjugate function [1]. As we will see, our method recovers softmax-entropy and quadratic loss as the “natural” objectives for classiﬁers trained via cross-entropy and squared loss respectively. Furthermore, for classiﬁers trained via other loss functions, as is becoming increasingly common in deep learning, our approach naturally suggests corresponding test-time adaptation losses, which we show in the next section to comparatively outperform alternatives. Thus, we argue that our framework overall provides a compelling recipe for specifying the “correct” method for TTA for a large class of possible losses. 4.1 Losses and the convex conjugate We begin by formally considering loss functions between a hypothesis outputhθ(x) (e.g., the logit outputs of a classiﬁer, or the direct prediction of a regressor) and targetythat take the following form L(hθ(x),y) = f(hθ(x)) −yThθ(x) (2) for some function f; when there is no risk of confusion, we will use hin place of hθ(x) for simplicity of notation. While not every loss can be expressed in such a form, this captures a wide variety of common losses (possibly scaled by a constant value). For example, cross-entropy loss corresponds to the choice f(h) = log ∑ iexp(hi) and where y denotes a one-hot encoding of the class label; similarly, squared loss corresponds to the choice f(h) = 1 2 ∥h∥2 2. When training an over-parameterized classiﬁer, we can roughly view the training process as (approxi- mately) attaining the minimum over hypotheses hfor each training example min θ 1 t t∑ i=1 L(hθ(xi),yi) ≈1 t t∑ i=1 min h L(h,yi) (3) 4where t is the number of training samples. However, in the case of losses in the form (2), the minimization over hin this form represents a very speciﬁc and well-known optimization problem: it is known as the convex conjugate [1] of the function f min h L(h,y) = min h {f(h) −yTh}= −f⋆(y) (4) where f⋆ denotes the convex conjugate of f. f⋆ is a convex function in y(and indeed, is convex regardless of whether or not f is convex). Furthermore, for the case that f is convex differentiable, the optimality condition of this minimization problem is given by ∇f(hopt) = y, so we also have that f⋆(y) = f⋆(∇f(hopt)) (5) where hopt refers to the optimal classiﬁer (used interchangeably with hθopt ). Putting this all together, we can state (admittedly, in a rather informal manner) that under the assumption that θopt is chosen so as to approximately minimize the empirical loss on the source data in the over-parameterized setting, we have that for tinputs 1 t t∑ i=1 L(hθopt (xi),yi) ≈1 t t∑ i=1 −f⋆(∇f(hθopt (xi))) (6) i.e., the empirical loss can be approximated by the (negative) conjugate applied to the gradient of the f, at least in a region close to the optimal θopt that minimizes the empirical loss. But the later expression has the notable beneﬁt that it does not require any label yi in order to compute the loss, and thus can be used as a basis for TTA on target domain of the hypothesis function hθopt . Deﬁnition 1 (conjugate adaptation loss) Consider a loss function that takes the form given in 2, used for training a hypothesis hθ in the over-parameterized regime. We deﬁne the conjugate adaptation loss Lconj(hθ(x)) : R|Y|↦→R as follows. Lconj(hθ(x)) = −f⋆(∇f(hθ(x))) = f(hθ(x)) −∇f(hθ(x))⊤hθ(x). (7) 4.2 Recovery of existing test-time adaptation strategies Cross-entropy The interesting aspect to this formalism is that when applied to classiﬁers trained with cross-entropy, it recovers exactly the TENT approach to TTA : minimizing the softmax-entropy of hθ(x). And indeed, this loss was also recovered when using meta-learning to learn the “optimal” test-time adaptation loss. To see this, note that for cross-entropy, we have thatf(h) = log ∑ iexp(hi), giving the optimality condition y= ∇f(hopt) = exp(hopt)∑ iexp(hopt i ) and the conjugate function f⋆(y) = { ∑ iyilog yi if ∑ iyi = 1 ∞ otherwise . (8) In other words, Lconj(hθ(x)) = −f⋆(∇f(hθ(x))) = − ∑ i exp(hi)∑ jexp(hj) log exp(hi)∑ jexp(hj) (9) i.e. softmax-entropy of the model prediction, which is exactly the TTA loss that TENT uses. Squared loss For the squared loss, we have thatf(h) = 1 2 ∥h∥2 2, leading to the optimality condition y = hand conjugate function f⋆(y) = 1 2 ∥y∥2 2. Hence, the adaptation loss in this case would be simply given by Lconj(hθ(x)) = −f⋆(∇f(hθ(x))) = −1 2 ∥h∥2 2 which is also what we observed in the meta-learning experiments discussed in Section 3. 4.3 Conjugate pseudo-labels We now emphasize that by the nature of our approximations, there is an additional simple interpre- tation of the conjugate loss: it is also equal to the original loss (2) applied to the “psuedo-labels” ˜yCPL θ (x) = ∇f(hθ(x)), where CPL refers to conjugate pseudo-labels, i.e., Lconj(hθ(x)) = −f⋆(∇f(hθ(x))) = f(hθ(x)) −∇f(hθ(x))Thθ(x) = L(hθ(x),∇f(hθ(x))). (10) 5This property is known as the Fenchel-Young inequality, that isf(x) + f⋆(u) ≥xTuholding with equality when u = ∇f(x). In other words, our conjugate adaptation loss is precisely equivalent to self-training under the speciﬁc soft pseudo-labels given by ˜yCPL = ∇f(hθ(x)). And indeed, for many cases, this may be a more convenient form to compute than explicitly computing the conjugate function at all. For this reason, we refer to our method as that of conjugate pseudo-labels. In the case of cross-entropy loss, this approach then corresponds exactly to self-training using labels given by the softmax applied to the current hypothesis. We must emphasize, however, that while our conjugate formulation indeed has this “simple” form for the case of cross-entropy loss, the real advantage comes in that it provides the “correct”pseudo-label for use with other losses, which may result in pseudo-labels different from the “common” softmax operation. Example: conjugate pseudo-labels for PolyLoss. PolyLoss [25] is a recently-proposed simple alternative to cross-entropy loss than has been shown to improve performance across a wide variety of compute tasks. This loss is given by the form Lpoly(hθ(x),y) = Lce(hθ(x),y) + ϵ·yT(1 −softmax(hθ(x))) (11) We note that this can be put exactly into our conjugate form (equation 2) by writing the loss in a slightly more involved fashion, which we refer to as the expanded conjugate form Lpoly(hθ(x),y) = f(hθ(x)) −yTg(hθ(x)). (12) where f is the log-sum-exp function as before, and g(h) = h−ϵ(1 −softmax(h)). In order to formally put this into the form of the previous loss function (equation 2), we can simply deﬁne an alternative hypothesis as the function h′ θ(x) = g(hθ(x)), and then deﬁne PolyLoss in the conjugate form as Lpoly(h′ θ(x),y) = f(g−1(h′ θ(x))) −yTh′ θ(x). (13) Typically, however, it is easier to simply operate on the expanded conjugate form, which yields the optimality condition for the pseudo-label ∇f(hopt) = Dg(hopt)˜yCPL θ (x), where D is the Jacobian operator. For the case of PolyLoss, this leads to the conjugate pseudo-label of the following form: ˜yCPL θ (x) = (I+ ϵdiag(z) −ϵzzT)−1z, z ≡softmax(hθ(x)). Test-time adaptation. Finally, we note that the above discussion doesn’t actually address any topics related to test-time adaptation to OOD data, but merely provides a generic characterization of a self- training procedure for generic loss functions of the form(2). However, the application toTTA on OOD data is fairly straightforward: as long as the learnt source parameters θis a reasonable approximation to the true optimal θopt on the shifted domain, self-training with the conjugate pseudo-labels provides a reasonable proxy for ﬁne-tuning the network on the true OOD loss. We emphasize that, common to most approaches for TTA , there are still some amount of design decisions that must be put in place; these are detailed in Section 5.1. In practice, we observe OOD generalization typically beneﬁts (across all baselines) from an additional “temperature” scaling, i.e., applying the TTA loss to hθ(x)/T for some ﬁxed temperature T, although it requires a held-out validation dataset for tuningT. However, we should emphasize that truly unsupervisedTTA would require making an informed guess for the value of these hyper-parameters. The full procedure for test time adaptation via conjugate pseudo-labels is shown in Algorithm 1. Algorithm 1 Conjugate pseudo-labeling (Conjugate PL) Input: Source classiﬁer θ0 trained using loss L(hθ(x),y) = f(hθ(x)) −hθ(x)⊤y. N batches of test data Dtest = [x1,x2,...,x N] Hyperparams: learning rate ηand temperature T. Let ¯hθ(x) def = hθ(x)/T be the temperature scaled predictor. Let ˜yCPL θ (x) denote the conjugate pseudo-label function ˜yCPL θ (x) = ∇(f(¯hθ(x))). for n= 0,1,...N −1 do θn+1 = θn −η∇L ( ¯hθ(xn),˜yCPL θ (xn) ) [Self-training with conjugate pseudo-labels] 65 Experiments In this section, we empirically evaluate the effectiveness and generality of the proposed conjugate pseudo-labeling procedure (Algorithm 1) for test-time adaptation on a variety of datasets. 5.1 Setup Datasets. We evaluate on the three common corruption benchmarks: adapting a classiﬁer trained on CIFAR-10 to CIFAR-10-C, CIFAR-100 to CIFAR-100-C and ImageNet to ImageNet-C [ 15]. Following the previous works [47, 50], we report the error averaged across corruptions at the highest severity for CIFAR-10/100-C and averaged across corruptions and severity level for ImageNet-C. We also evaluate on three domain adaptation datasets: adapting a classiﬁer trained on SVHN to MNIST, an ImageNet classiﬁer to ImageNet-R [16] and adapting from synthetic to real data in VISDA-C [38]. Models and Training losses. Following previous works on TTA[47, 50], we use ResNet-26 [14] as the source classiﬁer architecture for CIFAR-10/100 experiments, ResNet-18 for SVHN to MNIST and a ResNet-50 for ImageNet and source synthetic data on VisDA-C. We consider source classiﬁers trained via the following loss functions: the de-facto cross-entropy, recently proposed polyloss [25] and squared loss [18]. Baselines. Our proposed conjugate pseudo-label is the classic approach of self-training with a speciﬁc form of pseudo-labels. In self-training, we replace the label ywith a pseudo-label ˜y(x) and adapt by optimizing the loss function L(hθ(x),˜y(x)). Note that we could either instantaneously update the pseudo-labels using the current classiﬁer, or generate pseudo-labels once with just the source classiﬁer. Instantaneous updates have been shown to work better for domain adaptation [7, 40], and we perform instantaneous updates for all methods. While we propose using ˜yCPL(x) = ∇f(hθ(x)) (See Section 4.3), we compare to the standard pseudo-labels used in the literature: • (i) the “hard” pseudo-label (hard PL) where ˜y(x) = arg maxi ( hθ(x) ) i is the most likely class as predicted by hθ. As is common in the self-training literature, we perform conﬁdence thresholding. • (ii) The “soft” pseudo-label (soft PL) where ˜y(x) is obtained by applying a softmax function to the model predictions hθ(x). We also compare with the following recently proposed test-time adaptation methods. • Entropy Minimization (ENT) [50] minimizes the entropy of model predictions. • Robust Pseudo-Label [40] where we minimize a robust classiﬁcation loss, Lrpl = q−1(1 −p(i|x)q) where i= argmaxjp(j|x) and q∈[0,1]. • MEMO [54] minimizes entropy of a model’s outputs across different augmentations of a test input. We implement a batch version, where we see multiple test points at once, for fair comparisons. TTA methodology. Following [ 50] and [40], we ﬁne-tune by updating the learnable scale and shift parameters of the batch normalization layers across all adaptation losses. For each batch, batch normalization statistics is also updated, as suggested in [41]. We report performance at the end of one round of test-time adaptation over the entire test set. We tune the learning rate (LR) and temperature (T) on the validation noises in the corruption benchmark by grid-search. LR is selected from {1e−1,1e−2,... 1e−4}and T from {1,2 ... 5}. All the experiments have been performed on A6000 GPU’s. On domain adaptation benchmarks, where there is no held-out target domain, we set T to be 1 and use the LR suggested by [ 6, 50]. We use the same hyperparameter tuning protocol across all methods. We single out temperature as a very important hyperparameter, as we discuss in the results below. 5.2 Results on classiﬁers trained with cross-entropy We study the effectiveness of our proposed conjugate pseudo-labels when the source classiﬁer is trained via cross-entropy loss. In this case, baselines Softmax PL and ENT are the same as Conjugate PL. Thus we omit them in our results. Table 1, reports the performance of various TTA methods. When the source classiﬁer is trained via cross-entropy, our conjugate pseudo-label algorithm exactly corresponds to entropy minimization with an additional temperature scaling. Entropy minimization as 7Dataset Temperature (T) Hard PL Robust PL MEMO Conjugate PL (ENT) CIFAR-10-C \u0017 13.95 (±0.06) 13.97 ( ±0.04) 12.60(±0.04) 13.07 (±0.05) \u0013 13.95 (±0.06) 12.85 ( ±0.04) 12.51(±0.01) 12.51(±0.03) CIFAR-100-C \u0017 45.22 (±0.4) 39.80 ( ±0.18) 38.52(±0.16) 41.15 (±0.25) \u0013 45.22 (±0.4) 36.37 ( ±0.10) 37.38 ( ±0.06) 36.10(±0.07) ImageNet-C \u0017 45.43(±0.05) 45.68 ( ±0.01) 48.91( ±0.03) 45.82(±0.01) \u0013 45.43 (±0.05) 45.61 ( ±0.01) 48.91( ±0.04) 45.36(±0.01) Table 1: Mean errors when adapting to corruptions using a source classiﬁer trained via cross- entropy loss. Here, conjugate pseudo-labeling becomes softmax-entropy minimization. With the right temperature scaling, softmax-entropy minimization matches or outperforms other approaches. Prior reported gains of other methods over softmax-entropy minimization disappear when we use temperature scaling. For additional context, the source classiﬁer errors without adaptation are: CIFAR-10-C (29.54%), CIFAR-100-C (62.26%), ImageNet-C (61.89%) proposed in prior work [50] does not tune the temperature parameter, and some newer objectives such as robust PL or MEMO outperform vanilla entropy minimization. For example, on CIFAR-100-C, vanilla ENT obtaines 41.15% average error, while robust PL improves this to39.80% and MEMO to 38.52%. However, with the right temperature scaling, entropy minimization obtains 36.10% error which outperforms the newer objectives (with and without temperature scaling). A similar observation holds for CIFAR-10-C and ImageNet-C as well. Essentially, the gains over vanilla entropy minimization vanish when we do temperature scaling, and entropy minimization (i.e. conjugate pseudo-labeling corresponding to cross-entropy) turns out to be the best objective after all. 5.3 Results on classiﬁers trained with polyloss and squared loss In the case of cross-entropy, conjugate pseudo-labeling reduces to the familiar notion of entropy minimization. We now explore the performance of our method on different loss functions where the conjugate pseudo-labels differ substantially from entropy minimization (section 4.3). Table 2 presents the results on the corruption benchmarks and Table 3 presents the results on the other domain adaptation datasets for source classiﬁers trained with PolyLoss. Dataset T Hard PL Robust PL ENT MEMO Softmax PL Conjugate PL (Ours) CIFAR-10-C \u0017 13.81(±0.12) 14.23(±0.02) 13.46(±0.06) 13.23(±0.07) 14.64(±0.11) 13.02(±0.09) \u0013 13.81(±0.12) 12.45(±0.05) 12.23(±0.06) 12.33(±0.04) 12.26(±0.04) 12.08(±0.05) CIFAR-100-C\u0017 40.47(±0.05) 42.86(±0.11) 40.12(±0.08) 39.90(±0.05) 41.00(±0.11) 38.17(±0.17) \u0013 40.47(±0.05) 39.80(±0.08) 38.23(±0.05) 39.23(±0.04) 37.04(±0.06) 36.83(±0.08) ImageNet-C \u0017 45.44(±0.21) 46.27(±0.03) 46.10(±0.03) 48.21(±0.05) 44.63(±0.03) 44.01(±0.01) \u0013 45.44(±0.21) 46.27(±0.03) 45.50(±0.02) 48.21(±0.04) 44.45(±0.03) 44.01(±0.01) Table 2: Mean errors when adapting to corruptions using a source classiﬁer trained via recently proposed Poly-1 Loss [ 25]. Conjugate pseudo-labeling consistently outperforms all previous ap- proaches. For additional context, source classiﬁer errors without adaptation : CIFAR-10-C (30.22%), CIFAR-100-C (63.91%) and ImageNet-C (62.18%). First, we note that, across all datasets in Table 2 and Table 3, our conjugate PL approach outperforms all other TTA losses. With polyloss classiﬁers, entropy minimization is no longer the best method—on CIFAR-100-C, entropy minimization achieves38.23% error while our conjugate PL achieves36.83%. We see similar consistent gains on CIFAR-10-C, ImageNet-C, ImageNet-R and VisDA-C. On digit adaptation tasks from SVHN to MNIST/USPS/MNISTM, where there is a larger shift between source and target, the gains are especially pronounced. Figure 2 compares how the task loss (polyloss ϵ= 6) on the test data decreases as we adapt the model through conjugate PL and other baselines. We use CIFAR-10-C as an example. Observe that our proposed conjugate PL indeed reduces the task loss the most among other baselines. 8Dataset Source Error Hard PL Robust PL EntropySoftmax PL Conjugate PL Ours SVHN→MNIST 28.33 20.21 19.73 14.28 16.54 10.73 SVHN→USPS 31.58 23.32 26.12 23.12 24.07 21.62 SVHN→MNISTM61.69 50.73 51.35 49.33 50.47 47.59 ImageNet-R 64.19 58.52 59.46 58.25 56.62 55.63 VisDA-C 58.13 40.43 45.44 44.11 39.63 38.42 Table 3: Target error when adapting models trained via polyloss on source domains across different domain adaptation bench- marks. Conjugate pseudo-labeling offers consistent and substan- tial gains over previous approaches across three datasets. Figure 2: Task Loss (PolyLoss ϵ= 6) evaluated on CIFAR-10-C test data during test-time adaptation. Furthermore, on CIFAR-10-C and ImageNet-C, we ﬁnd that adapting polyloss classiﬁers via conjugate PL improves the performance over all methods applied to cross-entropy trained source classiﬁers. For e.g., on ImageNet-C, the performance improves from 45.34% to 44.01%. However, this is only true when using the proposed conjugate PL. If we just did softmax-entropy minimization (even with temperature scaling), the ﬁnal adapted performance of a polyloss classiﬁer (45.5%) is in fact worse than that of a cross-entropy classiﬁer (45.34%). Our results suggest that as we develop new training losses that improve the source classiﬁers, it is important to adapt via conjugate pseudo-labeling to reap the maximum gains. Similarly, we experiment with the case when the source classiﬁer is trained using squared loss on the CIFAR-10 and CIFAR-100 datasets, and observe consistent gains using the proposed conjugate pseudo-labels over the baselines. For example, on CIFAR-10-C, TTA using conjugate PL gives and error of 12.87%, outperforming baselines like ENT (13.24%) and Softmax PL (31.81%). Table 5 in Appendix A.7 shows the detailed results. Comparing Table 1 and Table 2, we see that the relative ordering between the various baselines differs. This is further evidence that the adaptation loss has to depend on the training loss, and we believe our conjugate pseudo-label approach captures this appropriately by offering consistent gains across the various settings we experimented with. 6 Related Works Test-time adaptation methods. In recent years, the setting of test-time adaptation has gained a lot of interest with a host of different approaches proposed in the literature. One family of TTA approaches update the source classiﬁer by minimizing an unsupervised loss on the target distribution [4, 6, 20, 22, 35, 36, 40, 43, 44, 50, 51, 54]. TENT [ 50] proposes to minimize the entropy of model predictions at test time. Several follow ups like [ 6, 35, 40, 44, 54] propose alternative TTA objectives, e.g. robust pseudo-labelling [40], likelihood ratio loss [35], entropy of marginal probability averaged across augmentations [54] and self-supervised contrastive losses [6, 49]. However, most of these objectives are heuristically designed or chosen. In this paper, we provide a principled approach of designing unsupervised objectives for TTA . Another family of approaches for test-time adaptation such as [ 2, 8, 13, 31, 34, 47] leverage an auxiliary self-supervised task (e.g. rotation prediction [ 47], masked autoencoders [10]) to update model parameters on each test sample. Crucially, these methods require modifying the source model training by augmenting the supervised training objective with an auxiliary self-supervised loss. Hence it cannot be applied to typical standard classiﬁers that are trained by minimizing a supervised loss on the source data. Source-free domain adaptation. A very related setting to test-time adaptation is source-free domain adaptation, where a trained source classiﬁer must be adapted to a target distribution of interest, although the entire target unlabeled data is available at once. SHOT [28] proposes to optimize the source hypothesis (i.e. feature extractor) with a combination of entropy minimization, diversity and self-training on pseudo-labels on the unlabeled target data. [53] promotes feature clustering on features from target distributions. [24, 26] use generative modeling to estimate the underlying source distributions for enforcing feature invariance. Such approaches typically require multiple epochs over the target data and cannot be easily adopted to work in an online fashion. 9Unsupervised domain adaptation. The most canonical setting of domain adaptation involves access to labeled source data and unlabeled target data, all during training. The availability of source and target data during training lends itself to approaches that “align” the source and target representations in some way: [ 32, 33, 45, 48] match distribution statistics, [ 11] uses a discriminator, [ 46] uses self-supervised learning. However, such approaches require access to source data which might not always be feasible due to data privacy and efﬁciency issues. Pseudo-labels and self-training. Self-training is a classic idea for leveraging unlabeled data, devel- oped ﬁrst for the semi-supervised setting. Self-training generates pseudo-labels on the unlabeled data, allowing us to use any “supervised” loss on this pseudo-labeled data. Self-training has shown promising results in various settings like semi-supervised learning [ 19] and improving adversarial robustness [ 5]. Self-training has also been gaining attention in the setting of unsupervised domain adaptation [28, 39], where pseudo-labels generated on the unlabeled data from target domain is used to supervise the adaptation process. [ 7, 23, 52] provide theoretical insights into how self-training with pseudo-labels can help under distribution shift. TENT [50] (i.e entropy minimization) can be viewed as a form of self-training with instantaneous softmax pseudo-labels. Our work provides a general framework for the choice of soft pseudo-labels based on the conjugate analysis of the source training objective. Some prior works like [7, 17, 27, 30, 55, 56] have documented the improvement in performance when using instantaneous pseudo-labels over pre-computed pseudo-labels, and thus lend further support to the beneﬁts of our proposed conjugate pseudo-labeling approach. The ex- periment results presented in this work supporting conjugate pseudo-labels suggest that conjugate pseudo-labels is a promising direction of pseudo-labeling in a broader context. 7 Conclusion, Limitations and Future Directions In this work, we proposed a general test-time adaptation loss, based on the convex conjugate formulation which in turn was motivated by the intriguing meta learning experiments. The fact that meta-learning recovers the proposed loss hints at some kind of optimality of the loss. In Section 4, we prove that for a broad set of loss functions, the proposed (unsupervised) conjugate loss is close to the oracle supervised loss. However, this still does not completely answer what the optimal test-time adaptation loss is and why. The meta-learning framework in this work was constrained to learn functions over the logits of each individual input. It can be expanded to more involved setups, where we consider functions over the intermediate representations too and also consider learning functions over a batch of input while accounting for their interactions. Beyond the choice of the adaptation loss itself, achieving good test-time adaptation generally involves several heuristics like updating only the batch norm parameters [50]. While our work was motivated by the loss function, via the meta-learning experiments, we discovered that temperature scaling is another important hyper-parameter that improves the performance of all previous baselines as well. At a high level, test-time adaptation has to be appropriately regularized to prevent the updates over batches from taking the model too far: updating only a few batch norm parameters is one way to do that, and perhaps temperature scaling provides a similar beneﬁcial regularization effect by making the network predictions on unlabeled inputs less conﬁdent. Understanding the role of these heuristics more concretely is an interesting direction for future work. It also remains an open problem to understand under what sort of real-world distribution shifts would self-training based approaches would help. Finally, it is also worth extending and applying the conjugate pseudo-labeling to other settings like semi-supervised learning. 8 Acknowledgments We thank Shubhang Bhatnagar and Asher Trockman for helping with running the ImageNet experi- ments. We thank Zhili Feng for useful feedback. Sachin Goyal and Mingjie Sun were supported by funding from the Bosch Center for Artiﬁcial Intelligence. Aditi Raghunathan was supported by an Open Philanthropy AI Fellowship. 10References [1] https://en.wikipedia.org/wiki/Convex_conjugate. [2] Pratyay Banerjee, Tejas Gokhale, and Chitta Baral. Self-supervised test-time learning for reading comprehension. In Annual Conference of the North American Chapter of the Association for Computational Linguistics, 2021. [3] Sarah Bechtle, Artem Molchanov, Yevgen Chebotar, Edward Grefenstette, Ludovic Righetti, Gaurav Sukhatme, and Franziska Meier. Meta-learning via learned loss. arXiv preprint arXiv:1906.05374, 2019. [4] Malik Boudiaf, Romain Mueller, Ismail Ben Ayed, and Luca Bertinetto. Parameter-free online test-time adaptation. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), 2022. [5] Yair Carmon, Aditi Raghunathan, Ludwig Schmidt, John C Duchi, and Percy S Liang. Un- labeled data improves adversarial robustness. In H. Wallach, H. Larochelle, A. Beygelzimer, F. d'Alché-Buc, E. Fox, and R. Garnett, editors, Advances in Neural Information Processing Systems, volume 32. Curran Associates, Inc., 2019. URL https://proceedings.neurips. cc/paper/2019/file/32e0bd1497aa43e02a42f47d9d6515ad-Paper.pdf. [6] Dian Chen, Dequan Wang, Trevor Darrell, and Sayna Ebrahimi. Contrastive test-time adaptation. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), 2022. [7] Yining Chen, Colin Wei, Ananya Kumar, and Tengyu Ma. Self-training avoids using spurious features under domain shift. In Advances in Neural Information Processing Systems, 2020. [8] Mohammad Zalbagi Darestani, Jiayu Liu, and Reinhard Heckel. Test-time training can close the natural distribution shift performance gap in deep learning based compressed sensing. In Proceedings of the 39th International Conference on Machine Learning (ICML), 2022. [9] Chelsea Finn, Pieter Abbeel, and Sergey Levine. Model-agnostic meta-learning for fast adap- tation of deep networks. In Proceedings of the 34th International Conference on Machine Learning (ICML), 2017. [10] Yossi Gandelsaman, Yu Sun, Xinlei Chen, and Alexei A. Efros. Test-time training with masked autoencoders. In Advances in Neural Information Processing Systems, 2022. [11] Yaroslav Ganin, Evgeniya Ustinova, Hana Ajakan, Pascal Germain, Hugo Larochelle, François Laviolette, Mario March, and Victor Lempitsky. Domain-adversarial training of neural networks. Journal of Machine Learning Research, 17(59):1–35, 2016. [12] Ishaan Gulrajani and David Lopez-Paz. In search of lost domain generalization. InInternational Conference on Learning Representations, 2021. [13] Nicklas Hansen, Rishabh Jangir, Yu Sun, Guillem Alenya, Pieter Abbeel, Alexei A. Efros, Lerrel Pinto, and Xiaolong Wang. Self-supervised policy adaptation during deployment. In International Conference on Learning Representations, 2021. [14] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image recognition. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), 2016. [15] Dan Hendrycks and Thomas Dietterich. Benchmarking neural network robustness to common corruptions and perturbations. In International Conference on Learning Representations, 2019. [16] Dan Hendrycks, Steven Basart, Norman Mu, Saurav Kadavath, Frank Wang, Evan Dorundo, Rahul Desai, Tyler Zhu, Samyak Parajuli, Mike Guo, Dawn Song, Jacob Steinhardt, and Justin Gilmer. The many faces of robustness: A critical analysis of out-of-distribution generalization. In In IEEE/CVF International Conference on Computer Vision (ICCV), 2021. [17] Yosuke Higuchi, Niko Moritz, Jonathan Le Roux, and Takaaki Hori. Advancing momentum pseudo-labeling with conformer and initialization strategy. In IEEE International Conference on Acoustics, Speech and Signal Processing, 2022. 11[18] Like Hui and Mikhail Belkin. Evaluation of neural architectures trained with square loss vs cross-entropy in classiﬁcation tasks. In International Conference on Learning Representations, 2021. [19] Dong hyun Lee. Pseudo-label: The simple and efﬁcient semi-supervised learning method for deep neural networks. [20] Yusuke Iwasawa and Yutaka Matsuo. Test-time classiﬁer adjustment module for model-agnostic domain generalization. In Advances in Neural Information Processing Systems, 2021. [21] Pang Wei Koh, Shiori Sagawa, Henrik Marklund, Sang Michael Xie, Marvin Zhang, Akshay Balsubramani, Weihua Hu, Michihiro Yasunaga, Richard Lanas Phillips, Irena Gao, Tony Lee, Etienne David, Ian Stavness, Wei Guo, Berton A. Earnshaw, Imran S. Haque, Sara Beery, Jure Leskovec, Anshul Kundaje, Emma Pierson, Sergey Levine, Chelsea Finn, and Percy Liang. Wilds: A benchmark of in-the-wild distribution shifts. In Proceedings of the 38th International Conference on Machine Learning (ICML), 2021. [22] Takeshi Kojima, Yutaka Matsuo, and Yusuke Iwasawa. Robustifying vision transformer without retraining from scratch by test-time class-conditional feature alignment. In International Joint Conference on Artiﬁcial Intelligence, 2022. [23] Ananya Kumar, Tengyu Ma, and Percy Liang. Understanding self-training for gradual domain adaptation. In Proceedings of the 37 th International Conference on Machine Learning (ICML), 2020. [24] Vinod K Kurmi, Venkatesh K Subramanian, and Vinay P Namboodiri. Domain impression: A source data free domain adaptation method. In IEEE Winter Conference on Applications of Computer Vision (WACV), 2021. [25] Zhaoqi Leng, Mingxing Tan, Chenxi Liu, Ekin Dogus Cubuk, Jay Shi, Shuyang Cheng, and Dragomir Anguelov. Polyloss: A polynomial expansion perspective of classiﬁcation loss functions. In International Conference on Learning Representations, 2022. [26] Rui Li, Qianfen Jiao, Wenming Cao, Hau-San Wong, and Si Wu. Model adaptation: Unsuper- vised domain adaptation without source data. In IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), 2020. [27] Xinzhe Li, Qianru Sun, Yaoyao Liu, Qin Zhou, Shibao Zheng, Tat-Seng Chua, and Bernt Schiele. Learning to self-train for semi-supervised few-shot classiﬁcation. In Advances in Neural Information Processing Systems, 2019. [28] Jian Liang, Dapeng Hu, and Jiashi Feng. Do we really need to access the source data? source hypothesis transfer for unsupervised domain adaptation. InProceedings of the 37th International Conference on Machine Learning (ICML), 2020. [29] Tsung-Yi Lin, Priya Goyal, Ross Girshick, Kaiming He, and Piotr Dollár. Focal loss for dense object detection. In IEEE/CVF International Conference on Computer Vision (ICCV), 2017. [30] Hong Liu, Jianmin Wang, and Mingsheng Long. Cycle self-training for domain adaptation. In Advances in Neural Information Processing Systems, 2021. [31] Yuejiang Liu, Parth Kothari, Bastien van Delft, Baptiste Bellot-Gurlet, Taylor Mordan, and Alexandre Alahi. Ttt++: When does self-supervised test-time training fail or thrive? In Advances in Neural Information Processing Systems, 2021. [32] Mingsheng Long, Jianmin Wang, Guiguang Ding, Jiaguang Sun, and Philip S. Yu. Transfer feature learning with joint distribution adaptation. In IEEE/CVF International Conference on Computer Vision (ICCV), 2013. [33] Mingsheng Long, Yue Cao, Jianmin Wang, and Michael I. Jordan. Learning transferable features with deep adaptation networks. In Proceedings of the 32nd International Conference on Machine Learning, 2015. [34] Xuan Luo, Jia-Bin Huang, Richard Szeliski, Kevin Matzen, and Johannes Kopf. Consistent video depth estimation. In SIGGRAPH, 2020. 12[35] Chaithanya Kumar Mummadi, Robin Hutmacher, Kilian Rambach, Evgeny Levinkov, Thomas Brox, and Jan Hendrik Metzen. Test-Time Adaptation to Distribution Shift by Conﬁdence Maximization and Input Transformation. arXiv preprint arXiv: 2106.14999, 2021. [36] Shuaicheng Niu, Jiaxiang Wu, Yifan Zhang, Yaofo Chen, Shijian Zheng, Peilin Zhao, and Mingkui Tan. Efﬁcient test-time model adaptation without forgetting. In Proceedings of the 39th International Conference on Machine Learning (ICML), 2022. [37] Junhyuk Oh, Matteo Hessel, Wojciech M. Czarnecki, Zhongwen Xu, Hado P van Hasselt, Satinder Singh, and David Silver. Discovering reinforcement learning algorithms. In Advances in Neural Information Processing Systems, 2020. [38] Xingchao Peng, Ben Usman, Neela Kaushik, Judy Hoffman, Dequan Wang, and Kate Saenko. Visda: The visual domain adaptation challenge, 2017. [39] Viraj Prabhu, Shivam Khare, Deeksha Kartik, and Judy Hoffman. Sentry: Selective entropy optimization via committee consistency for unsupervised domain adaptation. In Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV), 2021. [40] Evgenia Rusak, Steffen Schneider, George Pachitariu, Luisa Eck, Peter Vincent Gehler, Oliver Bringmann, Wieland Brendel, and Matthias Bethge. If your data distribution shifts, use self- learning, 2022. URL https://openreview.net/forum?id=1oEvY1a67c1. [41] Steffen Schneider, Evgenia Rusak, Luisa Eck, Oliver Bringmann, Wieland Brendel, and Matthias Bethge. Improving robustness against common corruptions by covariate shift adaptation. In Advances in Neural Information Processing Systems, 2020. [42] H. Scudder. Probability of error of some adaptive pattern-recognition machines. IEEE Transac- tions on Information Theory, 1965. [43] Manli Shu, Weili Nie, De-An Huang, Zhiding Yu, Tom Goldstein, Anima Anandkumar, and Chaowei Xiao. Test-time prompt tuning for zero-shot generalization in vision-language models. In Advances in Neural Information Processing Systems, 2022. [44] Prabhu Teja Sivaprasad and François Fleuret. Test time adaptation through perturbation robust- ness. arXiv preprint arXiv: 2110.10232, 2021. [45] Baochen Sun, Jiashi Feng, and Kate Saenko. Correlation alignment for unsupervised domain adaptation. arXiv preprint arXiv: 1612.01939, 2016. [46] Yu Sun, Eric Tzeng, Trevor Darrell, and Alexei A. Efros. Unsupervised domain adaptation through self-supervision. arXiv preprint arXiv:1909.11825, 2019. [47] Yu Sun, Xiaolong Wang, Zhuang Liu, John Miller, Alexei A. Efros, and Moritz Hardt. Test-time training with self-supervision for generalization under distribution shifts. In Proceedings of the 36th International Conference on Machine Learning (ICML), 2019. [48] Eric Tzeng, Judy Hoffman, Ning Zhang, Kate Saenko, and Trevor Darrell. Deep domain confusion: Maximizing for domain invariance. arXiv preprint arXiv:1412.3474, 2014. [49] Dequan Wang, Shaoteng Liu, Sayna Ebrahimi, Evan Shelhamer, and Trevor Darrell. On-target adaptation. arXiv preprint arXiv: 2109.01087, 2021. [50] Dequan Wang, Evan Shelhamer, Shaoteng Liu, Bruno Olshausen, and Trevor Darrell. Tent: Fully test-time adaptation by entropy minimization. In International Conference on Learning Representations, 2021. [51] Qin Wang, Olga Fink, Luc Van Gool, and Dengxin Dai. Continual test-time domain adaptation. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), 2022. [52] Sang Michael Xie, Ananya Kumar, Robbie Jones, Fereshte Khani, Tengyu Ma, and Percy Liang. In-n-out: Pre-training and self-training using auxiliary information for out-of-distribution robustness. In International Conference on Learning Representations, 2021. 13[53] Shiqi Yang, Yaxing Wang, Joost van de Weijer, Luis Herranz, and Shangling Jui. Generalized source-free domain adaptation. In Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV), 2021. [54] Marvin Zhang, Sergey Levine, and Chelsea Finn. Memo: Test time robustness via adaptation and augmentation. In Advances in Neural Information Processing Systems, 2022. [55] Yang Zou, Zhiding Yu, B. V . K. Vijaya Kumar, and Jinsong Wang. Domain adaptation for semantic segmentation via class-balanced self-training. European Conference on Computer Vision, 2018. [56] Yang Zou, Zhiding Yu, Xiaofeng Liu, B. V . K. Vijaya Kumar, and Jinsong Wang. Conﬁdence regularized self-training. In IEEE/CVF International Conference on Computer Vision (ICCV), 2019. 14A Appendix A.1 Conjugate Derivations Cross-Entropy Loss : L(h,y) = − c∑ i=1 yilog exp(hi)∑c j=1 exp(hj) = − c∑ i=1 yi ∗hi + log c∑ j=1 exp(hj) = f(h) −y⊤h, (14) where f(h) is log ∑c j=1 exp(hj) and the constraint that ∑c i=1 yi = 1. Now, the conjugate f⋆(y) is given by : f⋆(y) = −min h {f(h) −yTh}= −min h {log c∑ j=1 exp(hj) −yTh} (15) with the constraint ∑c i=1 yi = 1. At the optimality, yi = (∇f(h))i = exp(hi)∑ jexp(hj) (16) Then, f⋆(y) = −log c∑ j=1 exp(hj) + c∑ i=1 hi exp(hi)∑ jexp(hj) = ∑ i exp(hi)∑ jexp(hj) log exp(hi)∑ jexp(hj), (17) if the constraint ∑c i=1 yi = 1 is satisﬁed, otherwise f⋆(y) = ∞by duality. This in turn gives, the conjugate loss for cross-entropy (when the constraint is satisﬁed) : Lconj(h) = −f⋆(y) = −f⋆(∇f(h)) = − ∑ i exp(hi)∑ jexp(hj) log exp(hi)∑ jexp(hj) (18) Squared Loss : L(h,y) = 1 2||h−y||2 2 ≈1 2||h||2 2 −y⊤h [ignoring the constant term] = f(h) −y⊤h, (19) Now, the conjugate f⋆(y) is given by: f⋆(y) = −min h {f(h) −yTh}= −min h {1 2||h||2 2 −yTh} = −1 2||h||2 2 (20) A.2 Experiments on Binary Classiﬁcation with Exponential Loss Here we present the results on a binary classiﬁcation task over a synthetic dataset of 100 dimensional gaussian clusters. 15Dataset Creation For the binary classiﬁcation task, we create a synthetic dataset similar to [23]. Speciﬁcally, let the data X ∼ N(µ,Σ) ∈ R100 and labels Y ∈ {−1,+1}. We sample µ ∼ N(k,I100). For Σ, similar to [ 23], we sample a diagonal matrix D, where each entry is sampled uniformly from a speciﬁed range, and a rotation matrix U from a HAAR distribution, giving Σ = UDUT. For the source data, we sample µ−1 s ,µ+1 s ,Σ−1 s ,Σ+1 s as speciﬁed above with k= 0. Now to create a distribution shifted data of various severity, we sampleµ−1 t ,µ+1 t ,Σ−1 t ,Σ+1 t as speciﬁed above with k= 1, which are then used to sample the shifted data as follows : µ1 λ = λµ1 t + (1 −λ)µ1 s µ−1 λ = λµ−1 t + (1 −λ)µ−1 s Σ1 λ = λΣ1 t + (1 −λ)Σ1 s Σ−1 λ = λΣ−1 t + (1 −λ)Σ−1 s Xλ ∼N(µλ,Σλ) In the following experiments, easy shift refers to λ= 0.6, moderate shift to λ= 0.65 and hard shift to λ= 0.7. Exponential Loss for Binary Classiﬁcation Let zbe the classiﬁcation score hθ(x). For logistic training loss, conjugate adaptation loss would default to entropy with sigmoid probability. Thus, here we experiment with a different but also commonly used surrogate loss to 0/1 loss: exponential loss, which is deﬁned as: Lexp(z,y) = exp(−yz) (21) where y∈{−1,+1}. It can be rewritten in the expanded conjugate form of: Lexp(z,y) = 1 2 · ( ez + e−z) −1 2 ·y· ( ez −e−z) (22) For exponential loss, the conjugate pseudo-label function and the conjugate pseudo-label loss are: yCPL exp (z) = ez −e−z ez + e−z, LCPL exp (z) = 2 ez + e−z (23) The model is adapted on shifted gaussian clusters and we compare the conjugate loss with two baseline approaches: 1) Hard pseudo-labelling exp(−yhard pl ·z); 2) Entropy applied to sigmoid probability P(y= +1) = σ(z). The losses are compared on three degrees of shift (easy, moderate and hard), which is controlled by the drifted distance of Gaussian clusters. The results are shown in Figure 3, where we plot the accuracy curve with respect to adaptation iterations. With easy and moderate shift, conjugate loss (green) generalizes faster to shifted test data; with hard shift, only conjugate loss improves model accuracy on shifted test data while entropy (blue) deteriorates model performance. Figure 3: Test-time adaptation result on synthetic data with three shift levels ranging from easy, moderate and hard (detailed in section A.2). The source model is a linear classiﬁer trained with exponential loss Lexp = e−yhθ(x). Adaptation with the conjugate loss generalizes better compared to baseline losses. 16A.3 Meta Learning Experiment Details In section 3 we talked about learning the meta-loss function parameterized by a neural network mφ : R|Y|↦→R, that takes in the model predictions/logits and outputs a loss value. Here we discuss the architecture chosen and the implementation details. Further, in Appendix A.4 we empirically show that the learnt meta-loss is not affected by the choice of task loss / surrogate loss used in meta learning (Lin Equation 1). Note that the task loss / surrogate loss function is used to update the meta-loss mφ during meta-learning. The surrogate loss is calculated on updated source model’s predictions on labeled samples from test domain. The surrogate loss tries to update the meta-loss in the outer loop such that when meta-loss is later used to update the source model in the inner loop, the source model generalizes better to the test domain. Architecture and Implementation Details Figure 4 gives an overall schema for meta-learning the loss function and algorithm 2 gives the pseudo-code for meta-learning the loss function. Below we describe this in further detail. We use a transformer (denoted by T) with a MLP (denoted by P) over the output of transformer as the architecture for mφ, i.e. mφ(x) = P(T(x)). Speciﬁcally, for a given source trained model hθ and input x∼Dtest : 1. Let hθ(x) ∈R|Y|be the model predictions/logits, where |Y|denotes the number of classes. 2. Let hj θ(x) ∈R,∀j ∈|Y| be the prediction corresponding to class j. 3. The input to transformer is then given by z ∈R|Y|×(1+e), where zj ∈R1+e,∀j ∈|Y| is the concatenation of hj θ(x) and the learnable positional embedding pej ∈Re. 4. The transformer output is given by w= T(z) ∈Rd, where ddenotes the feed-forward dimension of the transformer. 5. The transformer output wis ﬁnally passed through a MLP to get the meta-loss valuemφ(hθ(x)) = P(w) ∈R 6. The source model is updated by optimizing over the meta-loss. θt+1 ←θt −α∂mφt(hθt(x)) ∂θt (24) 7. The updated source model is then used to update the meta-loss by optimizing over some supervised loss function Ltask. φt+1 ←φt −β∂Ltask(hθt+1 (x′),y′) ∂φt , where (x′,y′) ∼Dtest (25) Note that the last step assumes access to labels of test inputs. In this paper, we do not propose meta-learning the TTA loss as an approach. Rather, we use meta-learning to explore what the “best” TTA losses look like. We select the trasformer input embedding dimension (1 + e) from {16,32,64}and transformer feed-forward dimension dfrom {32,64,128}. The number of transformer layers and the hidden layers in MLP are selected from {1,2}. We use Adam optimizer with a learning rate of 1e−3 for learning the meta-loss (i.e. the transformer + MLP). We train the meta-loss for 100 epochs with a batch size of 200. A.4 Effect of Task Loss in Meta Learning In section 3, we show that the meta losses learned on different source classiﬁers differ substantially if the source classiﬁers are trained using different source loss functions. Here we further empirically verify that the learnt meta loss is not affected by the task loss used in meta learning (Lin Equation 1). Thus the learnt meta loss is determined by the source model. In Figure 5, we show the meta loss learnt on a ResNet-26 trained with Cross Entropy loss for two meta task losses: Cross Entropy Figure 5a and Squared Loss Figure 5b. We plot the meta loss as a function over one of its input prediction scores, while keeping other ﬁxed. We can see that the task loss barely affects the learnt meta loss. Similar observations can be made for the classiﬁer trained with squared loss Figure 6. 17Meta-Loss  Backpropogate  Figure 4: Meta-Loss learning procedure : The model predictions hθt(x) are passed through the parameterized loss function mφt, which outputs a loss value. We optimize φ such that when optimizing the source model over the loss mφt(hθt(x)), the updated θt+1 has a better performance on the test domain. To do this, we take one gradient step over the meta-loss to get the update source model parameters θt+1, and then update φby evaluating θt+1 on the labeled validation data using some task loss Ltask. Algorithm 2 Learning the Meta-Loss Input: Source trained classiﬁer hθ0 . Randomly initialized meta-loss mφ0 . Task loss / Surrogate loss Ltask like cross-entropy or squared loss for meta learning N batches of test data Dtest = [(x1,y1),..., (xN,yN)] Hyperparams: learning rates αand β. for epoch= 0,1,2,... do for n= 0,1,...N −1 do θt+1 ←θt −α ∂mφt(hθt(xn)) ∂θt Sample (xr,yr) ∼Dtest. φt+1 ←φt −β∂Ltask(hθt+1 (xr),yr) ∂φt A.5 Test-Time Adaptation Detail For completeness, we also give the test-time adaptation setup in Algorithm 3. A.6 ImageNet results on each severity level In continuation with results shown in Table 2 in Section 5.3, Table 4 shows the mean errors averaged across the 15 corruption types for each of the severity level on ImageNet-C, for a source classiﬁer trained with PolyLoss (ϵ= 8). A.7 Square Loss Trained Source Classiﬁer In Section 5.3, we brieﬂy discussed that similar to the other source training losses like cross-entropy and polyloss, our proposed conjugate loss outperforms the baselines when the source classiﬁer is 18(a)  (b) Figure 5: Visualizations of meta loss by varying one input dimension (prediction score). The source model is a ResNet-26 trained with Cross Entropy. Here we show meta loss trained by two different task losses: Cross Entropy Figure 5a and Squared Loss Figure 5b. (a)  (b) Figure 6: Visualizations of meta loss by varying one input dimension (prediction score). The source model is a ResNet-26 trained with Squared Loss. Here we show meta loss trained by two different task losses: Cross Entropy Figure 6a and Squared Loss Figure 6b. Algorithm 3 Test-Time Adaptation Input: Source classiﬁer θ0 trained using loss L(hθ(x),y), An unsupervised loss function for test-time adaptation Ltta(x), N batches of test data Dtest = [x1,...,x N] Hyperparams: learning rate η. for n= 0,1,...N −1 do θn+1 = θn −η∇Ltta(xn) ˆyn = hθn+1 (xn) [Predictions for the nth batch] 19Corrution Severity Temperature Robust PL Entropy MEMO Softmax PL Conjugate 1 \u0017 34.27 33.17 34.39 32.49 32.26 \u0013 34.27 32.84 34.39 32.70 32.26 2 \u0017 41.25 39.04 40.38 37.78 37.40 \u0013 41.25 38.50 40.38 37.75 37.40 3 \u0017 47.37 44.04 45.67 42.30 41.72 \u0013 47.37 43.33 45.67 42.14 41.72 4 \u0017 56.63 51.88 54.49 49.61 48.84 \u0013 56.63 51.03 54.49 49.39 48.84 5 \u0017 67.11 62.53 66.13 60.94 59.90 \u0013 67.11 61.80 66.13 60.30 59.90 Mean \u0017 49.32 46.13 48.21 44.62 44.02 \u0013 49.32 45.50 48.21 44.45 44.02 Table 4: Mean Errors across the 15 noises for various severity level on the ImageNet-C dataset, with source model trained using Poly-1 Loss. Note that Temperature scaling helped only in the case of Entropy and Softmax PL. trained using a squared loss. Table 5 shows a detailed comparison with the baselines. We note that for the conjugate of squared loss, the temperature scaling can be wrapped into the learning rate as shown in Section 4.2. Further, on the CIFAR-10-C dataset we observe temperature scaling doesn’t help any of the other baselines too, hence we do not include the temperature row in CIFAR-10-C. Dataset Temperature Hard PL Robust PL ENT MEMO Softmax PL Conjugate PL CIFAR-10-C \u0017 13.71 (±0.07) 13.06 (±0.05) 13.24 (±0.02) 13.22 (±0.04) 14.85 (±0.08)12.99(±0.04) CIFAR-100-C \u0017 50.82 (±0.31) 44.53 (±0.13) 43.55 (±0.12) 51.35 (±0.04) 51.99 (±0.03)43.39(±0.11) \u0013 50.82 (±0.31) 43.99 (±0.15)43.21(±0.08) 51.35 (±0.04) 51.99 (±0.03) 43.39 (±0.11) Table 5: Mean Errors on the common corruptions datasets for source classiﬁer trained using squared loss. We note that temperature scaling didn’t help on the CIFAR-10-C dataset. Source Classiﬁer Errors without adaptation : CIFAR-10-C (28.34%), CIFAR-100-C (68.79%) Dataset Temperature (T) Hard PL Robust PL MEMO Conjugate PL (ENT) CIFAR-10-C \u0017 SGD,1e−3, 1 SGD,1 e−3, 1 SGD,1 e−3, 1 SGD, 1e−3, 1 \u0013 SGD,1e−3, 1 SGD,1 e−2, 2 SGD,5 e−3, 3 Adam,1e−3, 2 CIFAR-100-C \u0017 SGD,1e−2, 1 SGD,1 e−2, 1 SGD,5 e−3, 1 SGD, 1e−2, 1 \u0013 SGD,1e−2, 1 SGD,1 e−2, 2 SGD,1 e−2, 2 SGD,1e−2, 2 ImageNet-C \u0017 SGD,1e−2, 1 SGD,2.5 e−3, 1 SGD,1 e−3, 1 SGD,2.5e−3, 1 \u0013 SGD,1e−2, 1 SGD,2.5e−3, 1.5 SGD,1e−3, 1 SGD,2.5e−3, 1.5 Table 6: Hyper-parameters (Optimizer, Learning Rate, Temperature) for the results in Table 1, where we showed the mean errors on the common corruptions dataset for a source classiﬁer trained using cross-entropy loss. A.8 Hyper-Parameters We share the exact hyper-parameters found using gridsearch over the 4 validation noises for the common corruptions dataset. 20Cross Entropy Classiﬁer Experiments In Section 5.2, Table 1 shows the results when adapting a cross entropy trained classiﬁer on various common corruptions dataset. Table 6 gives the optimizer, learning rate and optimal temperature for each of the baseline and our proposed conjugate loss. PolyLoss Classiﬁer Experiments In Section 5.3, Table 2 shows the results when adapting a polyloss trained classiﬁer on various common corruptions dataset. Table 7 gives the optimizer, learning rate and optimal temperature for each of the baseline and our proposed conjugate loss. Dataset T Hard PL Robust PL ENT MEMO Softmax PL Conjugate PL (Ours) CIFAR-10-C\u0017 SGD,1e−3, 1 SGD,1e−3, 1 SGD,1 e−3, 1 SGD,5 e−3, 1 SGD, 1e−3, 1 SGD, 1e−3, 1 \u0013 SGD,1e−3, 1 SGD,1e−2, 3 SGD,1 e−2, 3 SGD,5 e−3, 3 SGD, 1e−3, 2 SGD, 1e−3, 1.5 CIFAR-100-C\u0017 SGD,1e−2, 1 SGD,1e−2, 1 SGD,1 e−2, 1 SGD,1 e−2, 1 SGD, 1e−2, 1 SGD, 1e−2, 1 \u0013 SGD,1e−2, 1 Adam,1e−3, 3 SGD,1 e−2, 2 SGD,1 e−2, 2 SGD, 1e−2, 2.5 SGD, 1e−2, 1.5 ImageNet-C\u0017 SGD,1e−2, 1 SGD,2.5e−3, 1 SGD,2.5e−3, 1 SGD,5e−3, 1 SGD, 2.5e−3, 1 SGD, 2.5e−3, 1 \u0013 SGD,1e−2, 1 SGD,2.5e−3, 1 SGD,2.5e−3, 1.5 SGD,5e−3, 1 SGD, 2.5e−3, 2 SGD, 2.5e−3, 1 Table 7: Hyper-parameters (Optimizer, Learning Rate, Temperature) for the results in Table 2, where we showed the mean errors on the common corruptions dataset for a source classiﬁer trained using poly-loss. Squared Loss Classiﬁer Experiments In Section 5.3, we brieﬂy discussed the results when adapt- ing a squared loss trained classiﬁer on various common corruptions dataset. Table 8 gives the optimizer, learning rate and optimal temperature for each of the baseline and our proposed conjugate loss for the results in Table 5. Digit Adaptation Datasets For the experiments on digits adaptation tasks, we do not have any validation set. Hence, we don’t use temperature scaling here (T = 1) and ﬁx the optimizer and LR as Adam and 1e−2 respectively for all the baselines. A.9 Additional Experiments on Digit Adaptation Datasets Similar to the setting of Table 1, we perform additional experiments on digit adaptation datasets when the source classiﬁer is trained using the cross-entropy loss. Note that when the source classiﬁer is trained using cross-entropy loss, the conjugate loss is equal to the softmax-entropy. In the absence of validation dataset in digit adaptation benchmarks, we used a ﬁxed learning rate of 0.01 for all the baselines, optimizer as Adam and an informed temperature scaling guess of T=2. Table 9 compares softmax-entropy minimization with various baselines. Here, again we observe that on SVHN →MNIST benchmark, without temperature scaling, MEMO (10.67% error) outperforms softmax-entropy (14.41% error). However, similar to the observations in Table 1, with temperature scaling, softmax-entropy minimization (9.26% error) is able to match the performance of MEMO (9.36% error). Further, on the SVHN →USPS benchmark, softmax-entropy (conjugate) and MEMO perform similar even without temperature scaling. A.10 Additional Meta Learning the TTA Loss Experiments In Section 3, we tried to learn a test-time adaptation (TTA) loss via meta-learning for adapting a CIFAR10 trained ResNet26 to distribution shifts on CIFAR10 corruptions. Figure 1 showed that the learnt meta-loss looks like a temperature scaled softmax-entropy. In this section, we show the learnt meta loss across a range of settings as described below : 1. Digit Adaptation: Figure 7a and 7b show the learnt meta-loss when adapting a SVHN trained ResNet26 to MNIST dataset and USPS dataset respectively. We observe that the learnt meta-loss can be well approximated by a temperature scaled softmax-entropy. 2. Various Noise Types: In Figure 8, we show the learnt meta-loss when adapting a ResNet26 trained on CIFAR10 dataset using cross-entropy loss, to various noise types like speckle, gaussian, saturate and spatter. The severity level is kept ﬁxed at the maximum i.e. 5. 21Dataset T Hard PL Robust PL ENT MEMO Softmax PL Conjugate PL (Ours) CIFAR-10-C\u0017 SGD,1e−2, 1 SGD,1 e−2, 1 SGD,1 e−2, 1 SGD,1e−2, 1 SGD,1 e−4, 1 SGD,1e−2, 1 CIFAR-100-C\u0017 Adam,1e−3, 1 Adam,1e−3, 1 Adam,1e−3, 1 Adam,1e−3, 1 Adam, 1e−4, 1 Adam, 1e−3, 1 \u0013 Adam,1e−3, 1 Adam,1e−3, 0.5 Adam,1e−3, 2 Adam,1e−3, 2 Adam, 1e−4, 2.5 Adam, 1e−3, 1 Table 8: Hyper-parameters (Optimizer, Learning Rate, Temperature) for the results in Table 5, where we showed the mean errors on the common corruptions dataset for a source classiﬁer trained using squared loss. Dataset Temperature (T) Hard PL Robust PL MEMO Conjugate PL (ENT) SVHN→MNIST \u0017 21.54 27.44 10.67 14.41 \u0013 21.54 13.26 9.36 9.26 SVHN→USPS \u0017 26.06 26.81 22.72 22.57 \u0013 26.06 22.32 22.42 22.27 Table 9: Mean errors when adapting to digit adaptation benchmarks using a source classiﬁer trained via cross-entropy loss. Here, conjugate pseudo-labeling becomes softmax-entropy minimization. Again we observe that with the right temperature scaling, softmax-entropy minimization matches other approaches. For additional context, the source classiﬁer errors without adaptation are: SVHN →MNIST (34.17%), SVHN →USPS (31.84%). 20  10  0 10 20 prediction score 5 0 5 10loss value meta loss (error 10.44%) softmax entropy (error 14.41) fitted entropy (error 9.26) Meta Loss for SVHN -> MNIST (a) 20  10  0 10 20 prediction score 6 4 2 0 2 4 6 8 loss value meta loss (error 20.13%) softmax entropy (error 22.57) fitted entropy (error 22.22) Meta Loss for SVHN -> USPS adpatation (b) Figure 7: Visualizations of the learnt meta-loss by varying one input dimension (prediction score). The source model is a ResNet-26 trained with cross-entropy on the SVHN dataset. (a) The learnt meta-loss when adapting to the MNIST test dataset. (b) The learnt meta-loss when adapting to the USPS test dataset. 3. Various Severity Levels: In Figure 9, we vary the severity level of the noise, keeping the noise type ﬁxed. 4. Dataset and Architecture: In Figure 10, we compare the learnt meta-loss when adapting to speckle noise, for different source classiﬁer architectures (ResNet26 and ResNet50) and different source training dataset (CIFAR10 and CIFAR100). In all the cases, we again observe that the learnt meta-loss can be well approximated by a temperature scaled softmax-entropy. 5. Squared Loss : Finally, in Figure 11 we show the learnt meta-loss for classiﬁers trained with squared loss function instead of cross-entropy. We observe that in this case, the learnt meta loss mimics a quadratic function as expected from the conjugate formulation. 22For each of the learnt meta losses, we also show the values (α,T,C ) we use to ﬁt the meta loss with softmax entropy function: α·H(softmax(x/T)) −C. Note that although the learnt meta-loss can be approximated by the conjugate, the parameters α,T,C differ across the settings. In the case of classiﬁers trained with squared loss, we ﬁt the meta loss with a quadratic function∑K i=1(A·x2 i + C), where Kis the number of classes and xis the logit vector. Again, we also show the ﬁtted parameter value A,C. The meta loss follows the trend of a quadratic function. The ﬁtted quadratic function performs better or similar as the meta loss, while the parameters of the ﬁtted quadratic function remain different across the meta learning setup (base classiﬁer architectures and noise types). (a)  (b) (c)  (d) Figure 8: Visualization of meta loss (blue) learnt from various noise types in CIFAR-10-C validation set, where base classiﬁers are trained with cross-entropy loss. We show the error of meta loss, softmax entropy and ﬁtted entropy for test-time adaptation on the corresponding noise types. We also show the parameters (α,T,C ) in the ﬁtted entropy. 23(a)  (b) (c)  (d) Figure 9: Visualization of meta loss (blue) learnt on speckle noise with different severity level for CIFAR-10-C, where base classiﬁers are trained with cross-entropy loss. We show the error of meta loss, softmax entropy and ﬁtted entropy for test-time adaptation on the corresponding noise types. We also show the parameters (α,T,C ) in the ﬁtted entropy. 24(a)  (b) (c)  (d) Figure 10: Visualization of meta loss (blue) learnt across datasets (CIFAR-10-C/CIFAR-100-C) and base classiﬁer architectures (ResNet-26/ResNet-50), where base classiﬁers are trained with cross-entropy loss. We show the error of meta loss, softmax entropy and ﬁtted entropy for test-time adaptation on the corresponding noise types. We also show the parameters ( α,T,C ) in the ﬁtted entropy. (a)  (b) Figure 11: Visualization of meta loss (blue), where base classiﬁer is trained with quadratic loss. We show the error of meta loss, softmax entropy and ﬁtted quadratic function for test-time adaptation on the corresponding noise types. We also show the parameters ( A,B,C ) in the ﬁtted quadratic function. 25",
      "meta_data": {
        "arxiv_id": "2207.09640v2",
        "authors": [
          "Sachin Goyal",
          "Mingjie Sun",
          "Aditi Raghunathan",
          "Zico Kolter"
        ],
        "published_date": "2022-07-20T04:02:19Z",
        "pdf_url": "https://arxiv.org/pdf/2207.09640v2.pdf"
      },
      "llm_extracted_info": {
        "main_contributions": "The paper addresses the unclear choice of unsupervised objectives for Test-Time Adaptation (TTA) to distribution shifts. It presents a surprising phenomenon: meta-learning the 'best' TTA loss recovers softmax-entropy for cross-entropy trained classifiers, and negative squared error for squared-loss trained classifiers. To explain this, the authors propose a generic recipe for TTA loss based on the convex conjugate of the supervised training loss, which is shown to locally approximate the original supervised loss without labels. This framework recovers prior empirical findings and justifies existing TTA strategies. They introduce 'conjugate pseudo-labels' as a form of self-training, providing a principled way to derive soft pseudo-labels for various loss functions (e.g., PolyLoss). Empirically, their conjugate pseudo-labeling approach consistently outperforms other TTA methods across diverse benchmarks and training losses, particularly for novel loss functions where it substantially differs from (and outperforms) entropy-based losses.",
        "methodology": "The methodology involves two main parts. First, an empirical meta-learning approach is used to discover effective TTA losses. This involves parameterizing the TTA loss as a neural network and learning its parameters via meta-learning, differentiating through the adaptation process on a validation set. Second, a theoretical framework based on convex conjugate functions is proposed. For supervised losses of the form L(h(x),y) = f(h(x)) - y^T h(x), the conjugate adaptation loss is defined as Lconj(h(x)) = -f*(∇f(h(x))). This loss is shown to be equivalent to self-training with 'conjugate pseudo-labels' ỹCPL(x) = ∇f(h(x)). The full adaptation procedure (Algorithm 1) involves iteratively updating model parameters on test batches using the original loss with conjugate pseudo-labels, often with an additional temperature scaling.",
        "experimental_setup": "Experiments are conducted on three common corruption benchmarks: CIFAR-10-C, CIFAR-100-C, and ImageNet-C, evaluating average error across corruptions. Three domain adaptation datasets are also used: SVHN to MNIST, ImageNet to ImageNet-R, and synthetic to real data in VISDA-C. Source classifiers include ResNet-26 (CIFAR), ResNet-18 (SVHN), and ResNet-50 (ImageNet, VisDA-C), trained with cross-entropy, PolyLoss, or squared loss. Baselines include Hard Pseudo-Label, Soft Pseudo-Label (softmax), Entropy Minimization (TENT), Robust Pseudo-Label, and MEMO. TTA fine-tuning updates only the learnable scale and shift parameters of batch normalization layers, with batch normalization statistics updated per batch. Hyperparameters (learning rate and temperature) are tuned via grid-search on validation noises for corruption benchmarks, while fixed values are used for domain adaptation tasks without held-out target domains.",
        "limitations": "The work does not fully answer what the optimal test-time adaptation loss is and why, despite meta-learning results hinting at optimality. The meta-learning framework was constrained to learning functions over the logits of individual inputs and could be expanded to intermediate representations or batch-level interactions. Achieving good TTA still involves several heuristics, such as updating only batch norm parameters and temperature scaling, whose roles need a more concrete understanding. It remains an open problem to understand under what real-world distribution shifts self-training based approaches would be most effective.",
        "future_research_directions": "Future research directions include expanding the meta-learning framework to learn functions over intermediate representations or batch-level interactions. Further investigation is needed to more concretely understand the role and regularization effects of TTA heuristics like updating batch normalization parameters and temperature scaling. Another area for exploration is to understand the specific real-world distribution shifts for which self-training based approaches would be most beneficial. Finally, the authors suggest extending and applying the conjugate pseudo-labeling framework to other settings beyond TTA, such as semi-supervised learning."
      }
    },
    {
      "title": "Active Test-Time Adaptation: Theoretical Analyses and An Algorithm",
      "abstract": "Test-time adaptation (TTA) addresses distribution shifts for streaming test\ndata in unsupervised settings. Currently, most TTA methods can only deal with\nminor shifts and rely heavily on heuristic and empirical studies.\n  To advance TTA under domain shifts, we propose the novel problem setting of\nactive test-time adaptation (ATTA) that integrates active learning within the\nfully TTA setting.\n  We provide a learning theory analysis, demonstrating that incorporating\nlimited labeled test instances enhances overall performances across test\ndomains with a theoretical guarantee. We also present a sample entropy\nbalancing for implementing ATTA while avoiding catastrophic forgetting (CF). We\nintroduce a simple yet effective ATTA algorithm, known as SimATTA, using\nreal-time sample selection techniques. Extensive experimental results confirm\nconsistency with our theoretical analyses and show that the proposed ATTA\nmethod yields substantial performance improvements over TTA methods while\nmaintaining efficiency and shares similar effectiveness to the more demanding\nactive domain adaptation (ADA) methods. Our code is available at\nhttps://github.com/divelab/ATTA",
      "full_text": "Published as a conference paper at ICLR 2024 ACTIVE TEST-TIME ADAPTATION : T HEORETICAL ANALYSES AND AN ALGORITHM Shurui Gui∗ Texas A&M University College Station, TX 77843 shurui.gui@tamu.edu Xiner Li* Texas A&M University College Station, TX 77843 lxe@tamu.edu Shuiwang Ji Texas A&M University College Station, TX 77843 sji@tamu.edu ABSTRACT Test-time adaptation (TTA) addresses distribution shifts for streaming test data in unsupervised settings. Currently, most TTA methods can only deal with minor shifts and rely heavily on heuristic and empirical studies. To advance TTA under domain shifts, we propose the novel problem setting of active test-time adaptation (ATTA) that integrates active learning within the fully TTA setting. We provide a learning theory analysis, demonstrating that incorporating limited labeled test instances enhances overall performances across test domains with a theoretical guarantee. We also present a sample entropy balancing for implementing ATTA while avoiding catastrophic forgetting (CF). We introduce a simple yet effective ATTA algorithm, known as SimATTA, using real-time sample selection techniques. Extensive experimental results confirm consistency with our theoretical analyses and show that the proposed ATTA method yields substantial performance improvements over TTA methods while maintaining efficiency and shares similar effectiveness to the more demanding active domain adaptation (ADA) methods. Our code is available at https://github.com/divelab/ATTA. 1 I NTRODUCTION Deep learning has achieved remarkable success across various fields, attaining high accuracy in numerous applications (Krizhevsky et al., 2017; Simonyan and Zisserman, 2014). Nonetheless, When training and test data follow distinct distributions, models often experience significant performance degradation during test. This phenomenon, known as the distribution shift or out-of-distribution (OOD) problem, is extensively studied within the context of both domain generalization (DG) (Gulra- jani and Lopez-Paz, 2020; Koh et al., 2021; Gui et al., 2022) and domain adaptation (DA) (Ganin et al., 2016; Sun and Saenko, 2016). While these studies involve intensive training of models with considerable generalization abilities towards target domains, they overlook an important application property; namely, continuous adaptivity to real-time streaming data under privacy, resource, and efficiency constraints. This gap leads to the emergence of test-time adaptation (TTA) tasks, targeting on-the-fly adaptation to continuous new domains during the test phase or application deployment. The study of TTA encompasses two main categories; namely test-time training (TTT) methods (Sun et al., 2020; Liu et al., 2021c) and fully test-time adaptation (FTTA) (Niu et al., 2023; Wang et al., 2021). The TTT pipeline incorporates retraining on the source data, whereas FTTA methods adapt arbitrary pre-trained models to the given test mini-batch by conducting entropy minimization, without access to the source data. Nevertheless, most TTA methods can only handle corrupted distribution shifts (Hendrycks and Dietterich, 2019b) (e.g., Gaussian noise,) and rely heavily on human intuition or empirical studies. To bridge this gap, our paper focuses on tackling significant domain distribution shifts in real time with theoretical insights. We investigate FTTA, which is more general and adaptable than TTT, particularly under data ac- cessibility, privacy, and efficiency constraints. Traditional FTTA aims at adapting a pre-trained model to streaming test-time data from diverse domains under unsupervised settings. However, recent works (Lin et al., 2022; Pearl, 2009) prove that it is theoretically infeasible to achieve OOD generalization without extra information such as environment partitions. Since utilizing environment partitions requires heavy pretraining, contradicting the nature of TTA, we are motivated to incorporate extra information in a different way,i.e., integrating a limited number of labeled test-time samples to alleviate distribution shifts, following the active learning (AL) paradigm (Settles, 2009). To this end, we propose the novel problem setting of active test-time adaptation (ATTA) by incorporating ∗Equal contributions 1 arXiv:2404.05094v1  [cs.LG]  7 Apr 2024Published as a conference paper at ICLR 2024 AL within FTTA. ATTA faces two major challenges; namely, catastrophic forgetting (CF) (Kemker et al., 2018; Li and Hoiem, 2017) and real-time active sample selection. CF problem arises when a model continually trained on a sequence of domains experiences a significant performance drop on previously learned domains, due to the inaccessibility of the source data and previous test data. Real-time active sample selection requires AL algorithms to select informative samples from a small buffer of streaming test data for annotation, without a complete view of the test distribution. In this paper, we first formally define the ATTA setting. We then provide its foundational analysis under the learning theory’s paradigm to guarantee the mitigation of distribution shifts and avoid CF. Aligned with our empirical validations, while the widely used entropy minimization (Wang et al., 2021; Grandvalet and Bengio, 2004) can cause CF, it can conversely become the key to preventing CF problems with our sample selection and balancing techniques. Building on the analyses, we then introduce a simple yet effective ATTA algorithm, SimATTA, incorporating balanced sample selections and incremental clustering. Finally, we conducted a comprehensive experimental study to evaluate the proposed ATTA settings with three different settings in the order of low to high requirement restrictiveness, i.e., TTA, Enhanced TTA, and Active Domain Adaptation (ADA). Intensive experiments indicate that ATTA jointly equips with the efficiency of TTA and the effectiveness of ADA, rendering an uncompromising real-time distribution adaptation direction. Comparison to related studies. Compared to TTA methods, ATTA requires extra active labels, but the failure of TTA methods (Sec. 5.1) and the theoretical proof of Lin et al. (2022); Pearl (2009) justify its necessity and rationality. Compared to active online learning, ATTA focuses on lightweight real-time fine-tuning without round-wise re-trainings as Saran et al. (2023) and emphasizes the importance of CF avoidance instead of resetting models and losing learned distributions. In fact, active online learning is partially similar to our enhanced TTA setting (Sec. 5.2. Compared to ADA methods (Prabhu et al., 2021; Ning et al., 2021), ATTA does not presuppose access to source data, model parameters, or pre-collected target samples. Furthermore, without this information, ATTA can still perform on par with ADA methods (Sec. 5.3). The recent source-free active domain adaptation (SFADA) method SALAD (Kothandaraman et al., 2023) still requires access to model parameter gradients, pre-collected target data, and training of additional networks. Our ATTA, in contrast, with non-regrettable active sample selection on streaming data, is a much lighter and more realistic approach distinct from ADA and SFADA. More related-work discussions are provided in Appx. C. 2 T HE ACTIVE TEST-TIME ADAPTATION FORMULATION TTA methods aim to solve distribution shifts by dynamically optimizing a pre-trained model based on streaming test data. We introduce the novel problem setting of Active Test-Time Adaptation (ATTA), which incorporates active learning during the test phase. In ATTA, the model continuously selects the most informative instances from the test batch to be labeled by an explicit or implicit oracle (e.g., human annotations, self-supervised signals) and subsequently learned by the model, aiming to improve future adaptations. Considering the labeling costs in real-world applications, a “budget” is established for labeled test instances. The model must effectively manage this budget distribution and ensure that the total number of label requests throughout the test phase does not surpass the budget. We now present a formal definition of the ATTA problem. Consider a pre-trained modelf(x; ϕ) with parameters ϕ trained on the source dataset DS = (x, y)|DS|, with each data sample x ∈ Xand a label y ∈ Y. We aim to adapt model parameters θ, initialized as ϕ, to an unlabeled test-time data stream. The streaming test data exhibit distribution shifts from the source data and varies continuously with time, forming multiple domains to which we must continuously adapt. The test phase commences at time step t = 1 and the streaming test data is formulated in batches. The samples are then actively selected, labeled (by the oracle) and collected as Dte(t) = ActAlg(Ute(t)), where ActAlg(·) denotes an active selection/labeling algorithm. The labeled samples Dte(t) are subsequently incorporated into the ATTA training setDtr(t). Finally, we conclude time step t by performing ATTA training, updating model parameters θ(t) using Dtr(t), with θ(t) initialized as the previous final state θ(t − 1). Definition 1 (The ATTA problem). Given a model f(x; θ), with parameters θ, initialized with parameters θ(0) = ϕ obtained by pre-training on source domain data, and streaming test data batches Ute(t) continually changing over time, the ATTA task aims to optimize the model at any time stept (with test phase commencing at t = 1) as θ(t)∗ := argmin θ(t) (E(x,y,t)∈Dtr(t)[ℓCE (f(x; θ(t)), y)] + E(x,t)∈Ute(t)[ℓU (f(x; θ(t)))]), (1) 2Published as a conference paper at ICLR 2024 where Dtr(t) = ( ∅, t = 0 Dtr(t − 1) ∪ Dte(t), t ≥ 1, s.t. |Dtr(t)| ≤ B, (2) Dte(t) = ActAlg(Ute(t)) is actively selected and labeled, ℓCE is the cross entropy loss, ℓU is an unsupervised learning loss, and B is the budget. 3 T HEORETICAL STUDIES In this section, we conduct an in-depth theoretical analysis of TTA based on learning theories. We mainly explore two questions: How can significant distribution shifts be effectively addressed under the TTA setting? How can we simultaneously combat the issue of CF? Sec. 3.1 provides a solution with theoretical guarantees to the first question, namely, active TTA (ATTA), along with the conditions under which distribution shifts can be well addressed. Sec. 3.2 answers the second question with an underexplored technique, i.e., selective entropy minimization, building upon the learning bounds established in Sec. 3.1. We further validate these theoretical findings through experimental analysis. Collectively, we present a theoretically supported ATTA solution that effectively tackles both distribution shift and CF. 3.1 A LLEVIATING DISTRIBUTION SHIFTS THROUGH ACTIVE TEST-TIME ADAPTATION Traditional TTA is performed in unsupervised or self-supervised context. In contrast, ATTA introduces supervision into the adaptation setting. In this subsection, we delve into learning bounds and establish generalization bounds to gauge the efficacy of ATTA in solving distribution shifts. We scrutinize the influence of active learning and evidence that the inclusion of labeled test instances markedly enhances overall performances across incremental test domains. Following Kifer et al. (2004), we examine statistical guarantees for binary classification. A hypothesis is a function h : X → {0, 1}, which can serve as the prediction function within this context. In the ATTA setting, the mapping ofh varies with time as h(x, t). We use H∆H-distance following Ben- David et al. (2010), which essentially provides a measure to quantify the distribution shift between two distributions D1 and D2, and can also be applied between datasets. The probability that an estimated hypothesis h disagrees with the true labeling function g : X → {0, 1} according to distribution D is defined as ϵ(h(t), g) = E(x)∼D[|h(x, t) − g(x)|], which we also refer to as the error or risk ϵ(h(t)). While the source data is inaccessible under ATTA settings, we consider the existence of source dataset DS for accurate theoretical analysis. Thus, we initialize Dtr as Dtr(0) = DS. For every time step t, the test and training data can be expressed asUte(t) and Dtr(t) = DS ∪Dte(1) ∪Dte(2) ∪···∪ Dte(t). Building upon two lemmas (provided in Appx. D), we establish bounds on domain errors under the ATTA setting when minimizing the empirical weighted error using the hypothesish at time t. Theorem 1. Let H be a hypothesis class of VC-dimension d. At time step t, for ATTA data domains DS, Ute(1), ··· , Ute(t), ··· , Si are unlabeled samples of sizem sampled from each of thet+1 domains respectively. The total number of samples in Dtr(t) is N and the ratio of sample numbers in each component is λ = (λ0, ··· , λt). If ˆh(t) ∈ Hminimizes the empirical weighted error ˆϵw(h(t)) with the weight vector w = (w0, ··· , wt) on Dtr(t), and h∗ j (t) = arg minh∈H ϵj(h(t)) is the optimal hypothesis on the jth domain, then for any δ ∈ (0, 1), with probability of at least 1 − δ, we have ϵj(ˆh(t)) ≤ ϵj(h∗ j (t)) + 2 tX i=0,i̸=j wi  1 2 ˆdH∆H(Si, Sj) + 2 s 2d log(2m) + log 2 δ m + γi   + 2C, where C = r\u0010Pt i=0 w2 i λi \u0011\u0010 d log(2N)−log(δ) 2N \u0011 and γi = minh∈H{ϵi(h(t)) + ϵj(h(t))}. For future test domains j = t + k (k >0), assuming k′ = argmink′∈{0,1,...t} dH∆H(D(k′), Ute(t + k)) and min dH∆H (D(k′), Ute(t + k)) ≤ δD, where 0 ≤ δD ≪ +∞, then ∀δ, with probability of at least 1 − δ, we have ϵt+k(ˆh(t)) ≤ ϵt+k(h∗ t+k(t)) + tX i=0 wi  ˆdH∆H(Si, Sk′ ) + 4 s 2d log(2m) + log 2 δ m + δD + 2γi   + 2C. The adaptation performance on a test domain is majorly bounded by the composition of (labeled) training data, estimated distribution shift, and ideal joint hypothesis performance, which correspond to C, ˆdH∆H(Si, Sj), and γi, respectively. The ideal joint hypothesis error γi gauges the inherent adaptability between domains. Further theoretical analysis are in Appx. D. 3Published as a conference paper at ICLR 2024 Figure 1: (a) Empirical validation of Thm. 1. We train a series of models on N = 2000 samples from the PACS (Li et al., 2017) dataset given differentλ0 and w0 and display the test domain loss of each model. Red points are the test loss minimums given a fixed λ0. The orange line is the reference where w0 = λ0. We observe that w0 with loss minimums are located closed to the orange line but slightly smaller than λ0, which validates our findings in Eq. (4). (b) Empirical analysis with an uncertainty balancing. Given source pre-trained models, we fine-tune the models on 500 samples with different λ0 and w0, and display the combined error surface of test and source error. Although a small λ0 is good for test domain error, it can lead to non-trivial source error exacerbation. Therefore, we can observe that the global loss minimum (green X) locates in a relatively high-λ0 region. If we consider the multiple test data distributions as a single test domain,i.e., St i=1 Ute(i), Thm. 1 can be reduced into bounds for the source domain error ϵS and test domain error ϵT . Given the optimal test/source hypothesis h∗ T (t) = arg minh∈H ϵT (h(t)) and h∗ S(t) = arg minh∈H ϵS(h(t)), we have |ϵT (ˆh(t)) − ϵT (h∗ T (t))| ≤w0A + s w2 0 λ0 + (1 − w0)2 1 − λ0 B, (3a) |ϵS(ˆh(t)) − ϵS(h∗ S(t))| ≤(1 − w0)A + s w2 0 λ0 + (1 − w0)2 1 − λ0 B, (3b) where the distribution divergence termA = ˆdH∆H(S0, ST )+4 q 2d log(2m)+log 2 δ m +2γ, the empirical gap term B = 2 q d log(2N)−log(δ) 2N , ST is sampled from St i=1 Ute(i), and γ = minh∈H{ϵ0(h(t)) + ϵT (h(t))}. Our learning bounds demonstrates the trade-off between the small amount of budgeted test-time data and the large amount of less relevant source data. Next, we provide an approximation of the condition necessary to achieve optimal adaptation performance, which is calculable from finite samples and can be readily applied in practical ATTA scenarios. Following Eq. (3.a), with approximatelyB = c1 p d/N, the optimal value w∗ 0 to tighten the test error bound is a function of λ0 and A: w∗ 0 = λ0 − s A2N c2 1d − A2Nλ0(1 − λ0), for λ 0 ≥ 1 − d A2N , (4) where c1 is a constant. Note that λ0 ≥ 1 − d A2N should be the satisfied condition in practical ATTA settings, where the budget is not sufficiently big while the source data amount is relatively large. The following theorem offers a direct theoretical guarantee that ATTA reduces the error bound on test domains in comparison to TTA without the integration of active learning. Theorem 2. Let H be a hypothesis class of VC-dimension d. For ATTA data domains DS, Ute(1), Ute(2), ··· , Ute(t), considering the test-time data as a single test domain St i=1 Ute(i), if ˆh(t) ∈ H minimizes the empirical weighted error ˆϵw(h(t)) with the weight vector w on Dtr(t), let the test error be upper-bounded with |ϵT (ˆh(t)) − ϵT (h∗ T (t))| ≤EBT (w, λ, N, t). Let w′ and λ′ be the weight and sample ratio vectors when no active learning is included, i.e., w′ and λ′ s.t. w′ 0 = λ′ 0 = 1 and w′ i = λ′ i = 0 for i ≥ 1, then for any λ ̸= λ′, there exists w s.t. EBT (w, λ, N, t) < EBT (w′, λ′, N, t). (5) Therefore, the incorporation of labeled test instances in ATTA theoretically enhances the overall performance across test domains, substantiating the significance of the ATTA setting in addressing distribution shifts. All proofs are provided in Appx. E. Finally, we support the theoretical findings with experimental analysis and show the numerical results of applying the principles on real-world datasets, as shown in Fig. 1. For rigorous analysis, note that our theoretical results rest on the underlying condition that N should at least be of the same scale as d, according to the principles of VC-dimension theory. The empirical alignment of our experiments with the theoretical framework can be attributed to the assumption that fine-tuning a model is roughly equivalent to learning a model with a relatively small d. Experiment details and other validations can be found in Appx. H. 4Published as a conference paper at ICLR 2024 3.2 M ITIGATING CATASTROPHIC FORGETTING WITH BALANCED ENTROPY MINIMIZATION Catastrophic forgetting (CF), within the realm of Test-Time Adaptation (TTA), principally manifests as significant declines in overall performance, most notably in the source domain. Despite the lack of well-developed learning theories for analyzing training with series data, empirical studies have convincingly illustrated the crucial role of data sequential arrangement in model learning, thereby accounting for the phenomenon of CF. Traditionally, the mitigation of CF in adaptation tasks involves intricate utilization of source domain data. However, under FTTA settings, access to the source dataset is unavailable, leaving the problem of CF largely unexplored in the data-centric view. Table 1: Correlation analysis of high/low en- tropy samples and domains. We use a source pre-trained model to select samples with low- est/highest entropy, and 1.retrain the model on 2000 samples; 2.fine-tune the model on 300 sam- ples. We report losses on source/test domains for each setting, showing that low-entropy samples form distributions close to the source domain. Sample type Retrain Fine-tune ϵS ϵT ϵS ϵT Low entropy 0.5641 0.8022 0.0619 1.8838 High entropy 2.5117 0.3414 0.8539 0.7725 To overcome this challenge of source dataset ab- sence, we explore the acquisition of “source-like” data. In TTA scenarios, it is generally assumed that the amount of source data is considerably large. We also maintain this assumption in ATTA, practically assuming the volume of source data greatly surpasses the test-time budget. As a re- sult, we can safely assume that the pre-trained model is well-trained on abundant source do- main data DS. Given this adequately trained source model, we can treat it as a “true” source data labeling function f(x; ϕ). The model es- sentially describes a distribution, Dϕ,S(X, Y) = {(x, ˆy) ∈ (X, Y) | ˆy = f(x; ϕ), x∈ DS}. The entropy of the model prediction is defined as H(ˆy) = −P c p(ˆyc) logp(ˆyc), ˆy = f(x; ϕ), where c denotes the class. Lower entropy indicates that the model assigns high probability to one of the classes, suggesting a high level of certainty or confidence in its prediction, which can be interpreted as the sample being well-aligned or fitting closely with the model’s learned distribution. In other words, the model recognizes the sample as being similar to those it was trained on. Thus entropy can be used as an indicator of how closely a sample x aligns with the model distribution Dϕ,S. Since the model distribution is approximately the source distribution, selecting (and labeling) low-entropy samples using f(x; ϕ) essentially provides an estimate of sampling from the source dataset. Therefore, in place of the inaccessible DS, we can feasibly include the source-like dataset into the ATTA training data at each time stept: Dϕ,S(t) = {(x, f(x; ϕ))|x ∈ Ute(t), H(f(x; ϕ)) < el}, (6) where el is the entropy threshold. The assumption that Dϕ,S(t) is an approximation of DS can be empirically validated, as shown by the numerical results on PACS in Tab. 1. In contrast, high-entropy test samples typically deviate more from the source data, from which we select Dte(t) for active labeling. Following the notations in Thm. 1, we are practically minimizing the empirical weighted error of hypothesis h(t) as ˆϵ′ w(h(t)) = tX j=0 wjˆϵj(h(t)) = w0 λ0N X x∈Dϕ,S(t) |h(x, t) − f(x; ϕ)| + tX j=1 wj λjN X x,y∈Dte(j) |h(x, t) − y|. (7) By substituting DS with Dϕ,S(t) in Thm. 1, the bounds of Thm. 1 continue to hold for the test domains. In the corollary below, we bound the source error for practical ATTA at each time stept. Corollary 3. At time step t, for ATTA data domains Dϕ,S(t), Ute(1), Ute(2), ··· , Ute(t), Si are unla- beled samples of size m sampled from each of the t + 1 domains respectively, and SS is unlabeled samples of size m sampled from DS. If ˆh(t) ∈ Hminimizes ˆϵ′ w(h(t)) while other conditions remain identical to Thm. 1, then ϵS(ˆh(t)) ≤ ϵS(h∗ S(t)) + tX i=0 wi  ˆdH∆H(Si, SS) + 4 s 2d log(2m) + log 2 δ m + 2γi   + 2C, with probability at least 1 − δ, where C follows Thm. 1 and γi = minh∈H{ϵi(h(t)) + ϵS(h(t))}. Further analysis and proofs are in Appx. D and E. The following corollary provides direct theoretical support that our strategy conditionally reduces the error bound on the source domain. Corollary 4. At time step t, for ATTA data domains Dϕ,S(t), Ute(1), Ute(2), ··· , Ute(t), suppose that ˆh(t) ∈ Hminimizes ˆϵw′(h(t)) under identical conditions to Thm. 2. Let’s denote the source error upper bound with |ϵS(ˆh(t)) − ϵS(h∗ S(t))| ≤EBS(w, λ, N, t). Let w′ and λ′ be the weight 5Published as a conference paper at ICLR 2024 <latexit sha1_base64=\"NxhXSyFABPQk4q8627/odirDspg=\">AAAB9XicbVDLSgMxFM34rPVVdekmWARXZab4WhbcuKzYF7S1ZNI7bWgmMyR3lDL0P9y4UMSt/+LOvzHTdqGtBwKHc87l3hw/lsKg6347K6tr6xubua389s7u3n7h4LBhokRzqPNIRrrlMwNSKKijQAmtWAMLfQlNf3ST+c1H0EZEqobjGLohGygRCM7QSg/3mIWFGtAaGOwVim7JnYIuE29OimSOaq/w1elHPAlBIZfMmLbnxthNmUbBJUzyncRAzPiIDaBtqWIhmG46vXpCT63Sp0Gk7VNIp+rviZSFxoxD3yZDhkOz6GXif147weC6mwoVJwiKzxYFiaQY0awC2hcaOMqxJYxrYW+lfMg042iLytsSvMUvL5NGueRdli7uysXK+byOHDkmJ+SMeOSKVMgtqZI64USTZ/JK3pwn58V5dz5m0RVnPnNE/sD5/AFnsJJq</latexit> Streaming Test <latexit sha1_base64=\"a41BOKrutEYSWO9+8CjkPZKHvb8=\">AAAB73icbVBNS8NAEJ3Ur1q/qh69BIvgqSTiR48FLx4r2A9oQ9lsN+3SzSbuToQQ+ie8eFDEq3/Hm//GTZuDtj4YeLw3w8w8PxZco+N8W6W19Y3NrfJ2ZWd3b/+genjU0VGiKGvTSESq5xPNBJesjRwF68WKkdAXrOtPb3O/+8SU5pF8wDRmXkjGkgecEjRSbzAhmKWzyrBac+rOHPYqcQtSgwKtYfVrMIpoEjKJVBCt+64To5cRhZwKNqsMEs1iQqdkzPqGShIy7WXze2f2mVFGdhApUxLtufp7IiOh1mnom86Q4EQve7n4n9dPMGh4GZdxgkzSxaIgETZGdv68PeKKURSpIYQqbm616YQoQtFElIfgLr+8SjoXdfe6fnV/WWs2ijjKcAKncA4u3EAT7qAFbaAg4Ble4c16tF6sd+tj0Vqyiplj+APr8wfpIY/e</latexit> ˆy <latexit sha1_base64=\"SJEOE2ZYxLL1SU/QahOlMH6fop4=\">AAAB8HicbVBNSwMxEM3Wr1q/qh69BItQL2VX/Oix4MVjBbettEvJptk2NMkuyaxQlv4KLx4U8erP8ea/MW33oK0PBh7vzTAzL0wEN+C6305hbX1jc6u4XdrZ3ds/KB8etUycasp8GotYd0JimOCK+cBBsE6iGZGhYO1wfDvz209MGx6rB5gkLJBkqHjEKQErPfr9DNi0Cuf9csWtuXPgVeLlpIJyNPvlr94gpqlkCqggxnQ9N4EgIxo4FWxa6qWGJYSOyZB1LVVEMhNk84On+MwqAxzF2pYCPFd/T2REGjORoe2UBEZm2ZuJ/3ndFKJ6kHGVpMAUXSyKUoEhxrPv8YBrRkFMLCFUc3srpiOiCQWbUcmG4C2/vEpaFzXvunZ1f1lp1PM4iugEnaIq8tANaqA71EQ+okiiZ/SK3hztvDjvzseiteDkM8foD5zPH2KnkB4=</latexit> U te ( t ) <latexit sha1_base64=\"7rdY0fXtveVAqOkqa7z+i6K3Rp0=\">AAAB+XicbVDLSsNAFJ34rPUVdelmsAh1UxLxUXBTcOOygn1AE8pkMmmHTiZh5qZQQv/EjQtF3Pon7vwbp20W2nrgwuGce7n3niAVXIPjfFtr6xubW9ulnfLu3v7BoX103NZJpihr0UQkqhsQzQSXrAUcBOumipE4EKwTjO5nfmfMlOaJfIJJyvyYDCSPOCVgpL5tR1WPhgncYQ+GDMhF3644NWcOvErcglRQgWbf/vLChGYxk0AF0brnOin4OVHAqWDTspdplhI6IgPWM1SSmGk/n18+xedGCXGUKFMS8Fz9PZGTWOtJHJjOmMBQL3sz8T+vl0FU93Mu0wyYpItFUSYwJHgWAw65YhTExBBCFTe3YjokilAwYZVNCO7yy6ukfVlzb2rXj1eVRr2Io4RO0RmqIhfdogZ6QE3UQhSN0TN6RW9Wbr1Y79bHonXNKmZO0B9Ynz9h0pLV</latexit> f ( · ; ✓ ) <latexit sha1_base64=\"ud3dFXm+F2nsLD2/MdusutzkLvU=\">AAAB9HicbVDLSgNBEJyNrxhfUY9eBoPgKeyKr2PAixchgnlAsoTZ2d5kyMzOOjMbDEu+w4sHRbz6Md78GyfJHjSxoKGo6qa7K0g408Z1v53Cyura+kZxs7S1vbO7V94/aGqZKgoNKrlU7YBo4CyGhmGGQztRQETAoRUMb6Z+awRKMxk/mHECviD9mEWMEmMlvysC+ZTdyRD4pNQrV9yqOwNeJl5OKihHvVf+6oaSpgJiQznRuuO5ifEzogyjHCalbqohIXRI+tCxNCYCtJ/Njp7gE6uEOJLKVmzwTP09kRGh9VgEtlMQM9CL3lT8z+ukJrr2MxYnqYGYzhdFKcdG4mkCOGQKqOFjSwhVzN6K6YAoQo3NaRqCt/jyMmmeVb3L6sX9eaV2nsdRREfoGJ0iD12hGrpFddRAFD2iZ/SK3pyR8+K8Ox/z1oKTzxyiP3A+fwCmlpH9</latexit> Model SimATTA <latexit sha1_base64=\"bhVea6W/pzUPuDRNfs2xbDF7qAk=\">AAAB73icbVC7SgNBFL3rM8ZX1NJmMAhWYTf4KgM2FhYRzAOSJcxOZpMhs7PrzF0hhPyEjYUitv6OnX/jbLKFJh4YOJxzD3PvCRIpDLrut7Oyura+sVnYKm7v7O7tlw4OmyZONeMNFstYtwNquBSKN1Cg5O1EcxoFkreC0U3mt564NiJWDzhOuB/RgRKhYBSt1L6jQRYd9Eplt+LOQJaJl5My5Kj3Sl/dfszSiCtkkhrT8dwE/QnVKJjk02I3NTyhbEQHvGOpohE3/mS275ScWqVPwljbp5DM1N+JCY2MGUeBnYwoDs2il4n/eZ0Uw2t/IlSSIlds/lGYSoIxyY4nfaE5Qzm2hDIt7K6EDammDG1FRVuCt3jyMmlWK95l5eK+Wq6d53UU4BhO4Aw8uIIa3EIdGsBAwjO8wpvz6Lw4787HfHTFyTNH8AfO5w/1SI/i</latexit> Labeling <latexit sha1_base64=\"7rdY0fXtveVAqOkqa7z+i6K3Rp0=\">AAAB+XicbVDLSsNAFJ34rPUVdelmsAh1UxLxUXBTcOOygn1AE8pkMmmHTiZh5qZQQv/EjQtF3Pon7vwbp20W2nrgwuGce7n3niAVXIPjfFtr6xubW9ulnfLu3v7BoX103NZJpihr0UQkqhsQzQSXrAUcBOumipE4EKwTjO5nfmfMlOaJfIJJyvyYDCSPOCVgpL5tR1WPhgncYQ+GDMhF3644NWcOvErcglRQgWbf/vLChGYxk0AF0brnOin4OVHAqWDTspdplhI6IgPWM1SSmGk/n18+xedGCXGUKFMS8Fz9PZGTWOtJHJjOmMBQL3sz8T+vl0FU93Mu0wyYpItFUSYwJHgWAw65YhTExBBCFTe3YjokilAwYZVNCO7yy6ukfVlzb2rXj1eVRr2Io4RO0RmqIhfdogZ6QE3UQhSN0TN6RW9Wbr1Y79bHonXNKmZO0B9Ynz9h0pLV</latexit> f ( · ; ✓ ) <latexit sha1_base64=\"DPrA95GNP27SFW5vSoLC/hYa644=\">AAAB9XicbVDLSsNAFJ3UV62vqks3g0Wom5KIj4KbghuXFewDmlgmk0k7dJIJMzdKCf0PNy4Uceu/uPNvnLZZaOuBC4dz7uXee/xEcA22/W0VVlbX1jeKm6Wt7Z3dvfL+QVvLVFHWolJI1fWJZoLHrAUcBOsmipHIF6zjj26mfueRKc1lfA/jhHkRGcQ85JSAkR7CqksDCdfYTYb8tF+u2DV7BrxMnJxUUI5mv/zlBpKmEYuBCqJ1z7ET8DKigFPBJiU31SwhdEQGrGdoTCKmvWx29QSfGCXAoVSmYsAz9fdERiKtx5FvOiMCQ73oTcX/vF4KYd3LeJykwGI6XxSmAoPE0whwwBWjIMaGEKq4uRXTIVGEggmqZEJwFl9eJu2zmnNZu7g7rzTqeRxFdISOURU56Ao10C1qohaiSKFn9IrerCfrxXq3PuatBSufOUR/YH3+AFKlkbs=</latexit> f ( · ; \u0000 ) <latexit sha1_base64=\"DPrA95GNP27SFW5vSoLC/hYa644=\">AAAB9XicbVDLSsNAFJ3UV62vqks3g0Wom5KIj4KbghuXFewDmlgmk0k7dJIJMzdKCf0PNy4Uceu/uPNvnLZZaOuBC4dz7uXee/xEcA22/W0VVlbX1jeKm6Wt7Z3dvfL+QVvLVFHWolJI1fWJZoLHrAUcBOsmipHIF6zjj26mfueRKc1lfA/jhHkRGcQ85JSAkR7CqksDCdfYTYb8tF+u2DV7BrxMnJxUUI5mv/zlBpKmEYuBCqJ1z7ET8DKigFPBJiU31SwhdEQGrGdoTCKmvWx29QSfGCXAoVSmYsAz9fdERiKtx5FvOiMCQ73oTcX/vF4KYd3LeJykwGI6XxSmAoPE0whwwBWjIMaGEKq4uRXTIVGEggmqZEJwFl9eJu2zmnNZu7g7rzTqeRxFdISOURU56Ao10C1qohaiSKFn9IrerCfrxXq3PuatBSufOUR/YH3+AFKlkbs=</latexit> f ( · ; \u0000 ) <latexit sha1_base64=\"ipQ+JKlINPDcPjrbUYUkqyyzp40=\">AAAB+nicbVC7TsMwFHXKq5RXCiOLRYXEQpVUvMZKLIxF0IfURpXj3LRWHSeyHVBV+iksDCDEypew8Te4aQZoOZKlo3Puy8dPOFPacb6twsrq2vpGcbO0tb2zu2eX91sqTiWFJo15LDs+UcCZgKZmmkMnkUAin0PbH13P/PYDSMVica/HCXgRGQgWMkq0kfp2+S6bdNqQoCUxQ4K+XXGqTga8TNycVFCORt/+6gUxTSMQmnKiVNd1Eu1NiNSMcpiWeqmChNARGUDXUEEiUN4kO32Kj40S4DCW5gmNM/V3x4RESo0j31RGRA/VojcT//O6qQ6vvAkTSapB0PmiMOVYx3iWAw6YBKr52BBCJTO3YjokklBt0iqZENzFLy+TVq3qXlTPb2uV+lkeRxEdoiN0glx0ieroBjVQE1H0iJ7RK3qznqwX6936mJcWrLznAP2B9fkDSAyT+w==</latexit> Source-Pretrained <latexit sha1_base64=\"ud3dFXm+F2nsLD2/MdusutzkLvU=\">AAAB9HicbVDLSgNBEJyNrxhfUY9eBoPgKeyKr2PAixchgnlAsoTZ2d5kyMzOOjMbDEu+w4sHRbz6Md78GyfJHjSxoKGo6qa7K0g408Z1v53Cyura+kZxs7S1vbO7V94/aGqZKgoNKrlU7YBo4CyGhmGGQztRQETAoRUMb6Z+awRKMxk/mHECviD9mEWMEmMlvysC+ZTdyRD4pNQrV9yqOwNeJl5OKihHvVf+6oaSpgJiQznRuuO5ifEzogyjHCalbqohIXRI+tCxNCYCtJ/Njp7gE6uEOJLKVmzwTP09kRGh9VgEtlMQM9CL3lT8z+ukJrr2MxYnqYGYzhdFKcdG4mkCOGQKqOFjSwhVzN6K6YAoQo3NaRqCt/jyMmmeVb3L6sX9eaV2nsdRREfoGJ0iD12hGrpFddRAFD2iZ/SK3pyR8+K8Ox/z1oKTzxyiP3A+fwCmlpH9</latexit> Model <latexit sha1_base64=\"5LNAmmVR/AN9Lc2T+FRV/is2yz8=\">AAAB8nicbVDLSgNBEJyNrxhfUY9eBoPgKewGX8eACB48RDAP2CxhdjKbDJmdWWZ6lbDkM7x4UMSrX+PNv3GS7EETCxqKqm66u8JEcAOu++0UVlbX1jeKm6Wt7Z3dvfL+QcuoVFPWpEoo3QmJYYJL1gQOgnUSzUgcCtYOR9dTv/3ItOFKPsA4YUFMBpJHnBKwkn+nnvCNBK2Sca9ccavuDHiZeDmpoByNXvmr21c0jZkEKogxvucmEGREA6eCTUrd1LCE0BEZMN9SSWJmgmx28gSfWKWPI6VtScAz9fdERmJjxnFoO2MCQ7PoTcX/PD+F6CrIuExSYJLOF0WpwKDw9H/c55pREGNLCNXc3orpkGhCwaZUsiF4iy8vk1at6l1Uz+9rlfpZHkcRHaFjdIo8dInq6BY1UBNRpNAzekVvDjgvzrvzMW8tOPnMIfoD5/MHKbiRJQ==</latexit> Low Entropy <latexit sha1_base64=\"vLgKkEyV9E/djVdgAkvKuOUQOTU=\">AAAB7nicbVDLSgMxFL1TX7W+qi7dBIvgqswUX8uCG5cV7QPaoWTSTBuaZEKSEcrQj3DjQhG3fo87/8a0nYW2HrhwOOde7r0nUpwZ6/vfXmFtfWNzq7hd2tnd2z8oHx61TJJqQpsk4YnuRNhQziRtWmY57ShNsYg4bUfj25nffqLasEQ+2omiocBDyWJGsHVS+wELxanplyt+1Z8DrZIgJxXI0eiXv3qDhKSCSks4NqYb+MqGGdaWEU6npV5qqMJkjIe066jEgpowm587RWdOGaA40a6kRXP190SGhTETEblOge3ILHsz8T+vm9r4JsyYVKmlkiwWxSlHNkGz39GAaUosnziCiWbuVkRGWGNiXUIlF0Kw/PIqadWqwVX18r5WqV/kcRThBE7hHAK4hjrcQQOaQGAMz/AKb57yXrx372PRWvDymWP4A+/zB19wj48=</latexit> Samples <latexit sha1_base64=\"wuZucU3JbeEJSquG2WgqGdYMCR8=\">AAAB83icbVDLSgMxFL3js9ZX1aWbYBFclZnia1kQocsK9gHtUDJppg3NJCHJCGXob7hxoYhbf8adf2PazkJbD1w4nHMv994TKc6M9f1vb219Y3Nru7BT3N3bPzgsHR23jEw1oU0iudSdCBvKmaBNyyynHaUpTiJO29H4bua3n6g2TIpHO1E0TPBQsJgRbJ3Uq7PhCN0Lq6Wa9Etlv+LPgVZJkJMy5Gj0S1+9gSRpQoUlHBvTDXxlwwxrywin02IvNVRhMsZD2nVU4ISaMJvfPEXnThmgWGpXwqK5+nsiw4kxkyRynQm2I7PszcT/vG5q49swY0KllgqyWBSnHFmJZgGgAdOUWD5xBBPN3K2IjLDGxLqYii6EYPnlVdKqVoLrytVDtVy7zOMowCmcwQUEcAM1qEMDmkBAwTO8wpuXei/eu/exaF3z8pkT+APv8wfIYpF9</latexit> High Entropy <latexit sha1_base64=\"vLgKkEyV9E/djVdgAkvKuOUQOTU=\">AAAB7nicbVDLSgMxFL1TX7W+qi7dBIvgqswUX8uCG5cV7QPaoWTSTBuaZEKSEcrQj3DjQhG3fo87/8a0nYW2HrhwOOde7r0nUpwZ6/vfXmFtfWNzq7hd2tnd2z8oHx61TJJqQpsk4YnuRNhQziRtWmY57ShNsYg4bUfj25nffqLasEQ+2omiocBDyWJGsHVS+wELxanplyt+1Z8DrZIgJxXI0eiXv3qDhKSCSks4NqYb+MqGGdaWEU6npV5qqMJkjIe066jEgpowm587RWdOGaA40a6kRXP190SGhTETEblOge3ILHsz8T+vm9r4JsyYVKmlkiwWxSlHNkGz39GAaUosnziCiWbuVkRGWGNiXUIlF0Kw/PIqadWqwVX18r5WqV/kcRThBE7hHAK4hjrcQQOaQGAMz/AKb57yXrx372PRWvDymWP4A+/zB19wj48=</latexit> Samples <latexit sha1_base64=\"1BO6D/gzkeZNQ7HNIaph5NqELCI=\">AAAB8nicbVDLSgMxFM3UV62vqks3wSK4KjPF17LgRncV7AOmQ8mkd9rQTDIkGaEM/Qw3LhRx69e482/MtLPQ1gOBwzn3kHtPmHCmjet+O6W19Y3NrfJ2ZWd3b/+genjU0TJVFNpUcql6IdHAmYC2YYZDL1FA4pBDN5zc5n73CZRmUjyaaQJBTEaCRYwSYyX/XlAFMQhD+KBac+vuHHiVeAWpoQKtQfWrP5Q0zdOUE619z01MkBFlGOUwq/RTDQmhEzIC31JBYtBBNl95hs+sMsSRVPYJg+fq70RGYq2ncWgnY2LGetnLxf88PzXRTZAxkaQGBF18FKUcG4nz+/GQKaCGTy0hVDG7K6Zjogg1tqWKLcFbPnmVdBp176p++dCoNS+KOsroBJ2ic+Sha9REd6iF2ogiiZ7RK3pzjPPivDsfi9GSU2SO0R84nz9y2ZFU</latexit> Incremental <latexit sha1_base64=\"Jmobmj50NeE6y3ftB4xt5xZD5Eg=\">AAAB8XicbVDLSgNBEOyNrxhfUY9eBoPgKewGX8dALh4jmAcmS5id9CZDZmeXmVkhLP6FFw+KePVvvPk3TpI9aGJBQ1HVTXdXkAiujet+O4W19Y3NreJ2aWd3b/+gfHjU1nGqGLZYLGLVDahGwSW2DDcCu4lCGgUCO8GkMfM7j6g0j+W9mSboR3QkecgZNVZ6aIhUG1Rcjgblilt15yCrxMtJBXI0B+Wv/jBmaYTSMEG17nluYvyMKsOZwKdSP9WYUDahI+xZKmmE2s/mFz+RM6sMSRgrW9KQufp7IqOR1tMosJ0RNWO97M3E/7xeasIbP+MySQ1KtlgUpoKYmMzeJ0OukBkxtYQyxe2thI2posymoEs2BG/55VXSrlW9q+rlXa1Sv8jjKMIJnMI5eHANdbiFJrSAgYRneIU3RzsvzrvzsWgtOPnMMfyB8/kDzgaQ+A==</latexit> Clustering <latexit sha1_base64=\"c4xrXg0yZYBSSDLHCxlf45OWNzg=\">AAAB7nicbVDLSgNBEOz1GeMr6tHLYBA8hd2Aj2PAi8eI5gHJEmYnnWTIzOwyMyuEJR/hxYMiXv0eb/6Nk2QPmljQUFR1090VJYIb6/vf3tr6xubWdmGnuLu3f3BYOjpumjjVDBssFrFuR9Sg4AoblluB7UQjlZHAVjS+nfmtJ9SGx+rRThIMJR0qPuCMWie1HqhMBJpeqexX/DnIKglyUoYc9V7pq9uPWSpRWSaoMZ3AT2yYUW05EzgtdlODCWVjOsSOo4pKNGE2P3dKzp3SJ4NYu1KWzNXfExmVxkxk5DoltSOz7M3E/7xOagc3YcZVklpUbLFokApiYzL7nfS5RmbFxBHKNHe3EjaimjLrEiq6EILll1dJs1oJriqX99VyrZrHUYBTOIMLCOAaanAHdWgAgzE8wyu8eYn34r17H4vWNS+fOYE/8D5/AF7Wj40=</latexit> Samples <latexit sha1_base64=\"eimCpRgfVxBfxhwCehIJdcsMsvY=\">AAAB8XicbVA9SwNBEJ2LXzF+RS1tFoNgFe5SRMtAGssI5gOTI+xt5pIle3vH7p4QjvwLGwtFbP03dv4bN8kVmvhg4PHeDDPzgkRwbVz32ylsbe/s7hX3SweHR8cn5dOzjo5TxbDNYhGrXkA1Ci6xbbgR2EsU0igQ2A2mzYXffUKleSwfzCxBP6JjyUPOqLHSY1Ok2qDicjwsV9yquwTZJF5OKpCjNSx/DUYxSyOUhgmqdd9zE+NnVBnOBM5Lg1RjQtmUjrFvqaQRaj9bXjwnV1YZkTBWtqQhS/X3REYjrWdRYDsjaiZ63VuI/3n91IS3fsZlkhqUbLUoTAUxMVm8T0ZcITNiZgllittbCZtQRZlNQZdsCN76y5ukU6t69Wr9vlZpuHkcRbiAS7gGD26gAXfQgjYwkPAMr/DmaOfFeXc+Vq0FJ585hz9wPn8AzSSQ9Q==</latexit> Clustering <latexit sha1_base64=\"JgGHFC5oztwX6+XjDtZWQo9C1hA=\">AAAB7nicbVA9TwJBEJ3DL8Qv1NJmIzGxIncUaImxscREwAQuZG8ZYMPe7mV3z4Rc+BE2Fhpj6++x89+4wBUKvmSSl/dmMjMvSgQ31ve/vcLG5tb2TnG3tLd/cHhUPj5pG5Vqhi2mhNKPETUouMSW5VbgY6KRxpHATjS5nfudJ9SGK/lgpwmGMR1JPuSMWid1biQbK2365Ypf9Rcg6yTISQVyNPvlr95AsTRGaZmgxnQDP7FhRrXlTOCs1EsNJpRN6Ai7jkoaowmzxbkzcuGUARkq7UpaslB/T2Q0NmYaR64zpnZsVr25+J/XTe3wOsy4TFKLki0XDVNBrCLz38mAa2RWTB2hTHN3K2FjqimzLqGSCyFYfXmdtGvVoF6t39cqDT+PowhncA6XEMAVNOAOmtACBhN4hld48xLvxXv3PpatBS+fOYU/8D5/AFOaj4U=</latexit> Anchors <latexit sha1_base64=\"eimCpRgfVxBfxhwCehIJdcsMsvY=\">AAAB8XicbVA9SwNBEJ2LXzF+RS1tFoNgFe5SRMtAGssI5gOTI+xt5pIle3vH7p4QjvwLGwtFbP03dv4bN8kVmvhg4PHeDDPzgkRwbVz32ylsbe/s7hX3SweHR8cn5dOzjo5TxbDNYhGrXkA1Ci6xbbgR2EsU0igQ2A2mzYXffUKleSwfzCxBP6JjyUPOqLHSY1Ok2qDicjwsV9yquwTZJF5OKpCjNSx/DUYxSyOUhgmqdd9zE+NnVBnOBM5Lg1RjQtmUjrFvqaQRaj9bXjwnV1YZkTBWtqQhS/X3REYjrWdRYDsjaiZ63VuI/3n91IS3fsZlkhqUbLUoTAUxMVm8T0ZcITNiZgllittbCZtQRZlNQZdsCN76y5ukU6t69Wr9vlZpuHkcRbiAS7gGD26gAXfQgjYwkPAMr/DmaOfFeXc+Vq0FJ585hz9wPn8AzSSQ9Q==</latexit> Clustering <latexit sha1_base64=\"JgGHFC5oztwX6+XjDtZWQo9C1hA=\">AAAB7nicbVA9TwJBEJ3DL8Qv1NJmIzGxIncUaImxscREwAQuZG8ZYMPe7mV3z4Rc+BE2Fhpj6++x89+4wBUKvmSSl/dmMjMvSgQ31ve/vcLG5tb2TnG3tLd/cHhUPj5pG5Vqhi2mhNKPETUouMSW5VbgY6KRxpHATjS5nfudJ9SGK/lgpwmGMR1JPuSMWid1biQbK2365Ypf9Rcg6yTISQVyNPvlr95AsTRGaZmgxnQDP7FhRrXlTOCs1EsNJpRN6Ai7jkoaowmzxbkzcuGUARkq7UpaslB/T2Q0NmYaR64zpnZsVr25+J/XTe3wOsy4TFKLki0XDVNBrCLz38mAa2RWTB2hTHN3K2FjqimzLqGSCyFYfXmdtGvVoF6t39cqDT+PowhncA6XEMAVNOAOmtACBhN4hld48xLvxXv3PpatBS+fOYU/8D5/AFOaj4U=</latexit> Anchors <latexit sha1_base64=\"KzBZ8R84UC9mpPFQBWeRHFxcqjw=\">AAAB7XicbVBNS8NAEJ3Ur1q/qh69LBbBU0mKVI8FLx4rmLbQhrLZbNq1m92wuxFK6H/w4kERr/4fb/4bt20O2vpg4PHeDDPzwpQzbVz32yltbG5t75R3K3v7B4dH1eOTjpaZItQnkkvVC7GmnAnqG2Y47aWK4iTktBtObud+94kqzaR4MNOUBgkeCRYzgo2VOn4aYUOH1ZpbdxdA68QrSA0KtIfVr0EkSZZQYQjHWvc9NzVBjpVhhNNZZZBpmmIywSPat1TghOogX1w7QxdWiVAslS1h0EL9PZHjROtpEtrOBJuxXvXm4n9ePzPxTZAzkWaGCrJcFGccGYnmr6OIKUoMn1qCiWL2VkTGWGFibEAVG4K3+vI66TTqXrPevG/UWldFHGU4g3O4BA+uoQV30AYfCDzCM7zCmyOdF+fd+Vi2lpxi5hT+wPn8AYuwjxQ=</latexit> Update <latexit sha1_base64=\"y2NH6tDs2GygUDqZYglGwvR4SpA=\">AAAB+nicbVBNSwMxEJ2tX7V+bfXoJVgEQSi7PVSPFS8eK9oPaEvJptk2NMkuSVYpa3+KFw+KePWXePPfmLZ70NYHA4/3ZpiZF8ScaeN5305ubX1jcyu/XdjZ3ds/cIuHTR0litAGiXik2gHWlDNJG4YZTtuxolgEnLaC8fXMbz1QpVkk780kpj2Bh5KFjGBjpb5bvMMi5lSjc3QlyShSuu+WvLI3B1olfkZKkKHed7+6g4gkgkpDONa643ux6aVYGUY4nRa6iaYxJmM8pB1LJRZU99L56VN0apUBCiNlSxo0V39PpFhoPRGB7RTYjPSyNxP/8zqJCS97KZNxYqgki0VhwpGJ0CwHNGCKEsMnlmCimL0VkRFWmBibVsGG4C+/vEqalbJfLVdvK6Wal8WRh2M4gTPw4QJqcAN1aACBR3iGV3hznpwX5935WLTmnGzmCP7A+fwBUnKTWg==</latexit> Samples + Anchors <latexit sha1_base64=\"u0BDOcH87PXd3DsT+o414+7cHnI=\">AAAB7XicbZC7SgNBFIbPxluMt6ilIINBsAq7FjGdARvLBMwFkhBmZ2eTMbMzy8ysEJaU9jYWitj6Cql8CDufwZdwcik0+sPAx/+fw5xz/JgzbVz308msrK6tb2Q3c1vbO7t7+f2DhpaJIrROJJeq5WNNORO0bpjhtBUriiOf06Y/vJrmzTuqNJPixoxi2o1wX7CQEWys1eiQQBrdyxfcojsT+gveAgqX75Pa1/3xpNrLf3QCSZKICkM41rrtubHpplgZRjgd5zqJpjEmQ9ynbYsCR1R309m0Y3RqnQCFUtknDJq5PztSHGk9inxbGWEz0MvZ1PwvaycmLHdTJuLEUEHmH4UJR0ai6eooYIoSw0cWMFHMzorIACtMjD1Qzh7BW175LzTOi16pWKq5hUoZ5srCEZzAGXhwARW4hirUgcAtPMATPDvSeXRenNd5acZZ9BzCLzlv33Yvk3g=</latexit> ··· <latexit sha1_base64=\"+7L/8ObZcl+JIZaSFhVO3t+lUUE=\">AAAB7XicbVDLSgNBEOyNrxhf8XHzMhiEeAm7ItFjQA8eI5gHJCHMTmaT0dnZZaZXCEv+wYsHRbz6P978GyebHDSxoKGo6qa7y4+lMOi6305uZXVtfSO/Wdja3tndK+4fNE2UaMYbLJKRbvvUcCkUb6BAydux5jT0JW/5j9dTv/XEtRGRusdxzHshHSoRCEbRSs2bvizjWb9YcituBrJMvDkp1Y6CDPV+8as7iFgScoVMUmM6nhtjL6UaBZN8UugmhseUPdIh71iqaMhNL82unZBTqwxIEGlbCkmm/p5IaWjMOPRtZ0hxZBa9qfif10kwuOqlQsUJcsVmi4JEEozI9HUyEJozlGNLKNPC3krYiGrK0AZUsCF4iy8vk+Z5xatWqnc2jQuYIQ/HcAJl8OASanALdWgAgwd4hld4cyLnxXl3PmatOWc+cwh/4Hz+AFjYkTs=</latexit> D l ( t ) <latexit sha1_base64=\"9C0bB8PYImk9DX0HLfGvGd44PFA=\">AAAB7XicbVDLSgNBEOyNrxhf8XHzMhiEeAm7ItFjQA8eI5gHJCHMTmaT0dnZZaZXCEv+wYsHRbz6P978GyebHDSxoKGo6qa7y4+lMOi6305uZXVtfSO/Wdja3tndK+4fNE2UaMYbLJKRbvvUcCkUb6BAydux5jT0JW/5j9dTv/XEtRGRusdxzHshHSoRCEbRSs2b/qiMZ/1iya24Gcgy8eakVDsKMtT7xa/uIGJJyBUySY3peG6MvZRqFEzySaGbGB5T9kiHvGOpoiE3vTS7dkJOrTIgQaRtKSSZ+nsipaEx49C3nSHFkVn0puJ/XifB4KqXChUnyBWbLQoSSTAi09fJQGjOUI4toUwLeythI6opQxtQwYbgLb68TJrnFa9aqd7ZNC5ghjwcwwmUwYNLqMEt1KEBDB7gGV7hzYmcF+fd+Zi15pz5zCH8gfP5A1K8kTc=</latexit> D h ( t ) <latexit sha1_base64=\"eNrtnhPGeU8n4BRDMStm5cjQ4ts=\">AAAB73icbVBNS8NAEJ34WetX1aOXxSJ4KkmR6rHQi8cK9gPaUDbbTbt0s4m7E6GE/gkvHhTx6t/x5r9x2+agrQ8GHu/NMDMvSKQw6Lrfzsbm1vbObmGvuH9weHRcOjltmzjVjLdYLGPdDajhUijeQoGSdxPNaRRI3gkmjbnfeeLaiFg94DThfkRHSoSCUbRS1zNIGlTKQansVtwFyDrxclKGHM1B6as/jFkacYVMUmN6npugn1GNgkk+K/ZTwxPKJnTEe5YqGnHjZ4t7Z+TSKkMSxtqWQrJQf09kNDJmGgW2M6I4NqveXPzP66UY3vqZUEmKXLHlojCVBGMyf54MheYM5dQSyrSwtxI2ppoytBEVbQje6svrpF2teLVK7b5arl/ncRTgHC7gCjy4gTrcQRNawEDCM7zCm/PovDjvzseydcPJZ87gD5zPH1Naj3k=</latexit> 1st Call <latexit sha1_base64=\"mxsL+XuWb2hqFND+pzTctrB1rcY=\">AAAB73icbVBNS8NAEJ34WetX1aOXxSJ4KkmR6rHQi8cK9gPaUDababt0s4m7G6GE/gkvHhTx6t/x5r9x2+agrQ8GHu/NMDMvSATXxnW/nY3Nre2d3cJecf/g8Oi4dHLa1nGqGLZYLGLVDahGwSW2DDcCu4lCGgUCO8GkMfc7T6g0j+WDmSboR3Qk+ZAzaqzUrcqQNKgQg1LZrbgLkHXi5aQMOZqD0lc/jFkaoTRMUK17npsYP6PKcCZwVuynGhPKJnSEPUsljVD72eLeGbm0SkiGsbIlDVmovycyGmk9jQLbGVEz1qveXPzP66VmeOtnXCapQcmWi4apICYm8+dJyBUyI6aWUKa4vZWwMVWUGRtR0Ybgrb68TtrViler1O6r5fp1HkcBzuECrsCDG6jDHTShBQwEPMMrvDmPzovz7nwsWzecfOYM/sD5/AE0o49l</latexit> 2nd Call <latexit sha1_base64=\"oSA1OFmXXL9y3PJtqoVxTIG9mto=\">AAAB8HicbVA9TwJBEJ3DL8Qv1NJmIzGxIncUaElCY2UwkQ8DF7K3zMGGvb3L7p6REH6FjYXG2Ppz7Pw3LnCFgi+Z5OW9mczMCxLBtXHdbye3sbm1vZPfLeztHxweFY9PWjpOFcMmi0WsOgHVKLjEpuFGYCdRSKNAYDsY1+d++xGV5rG8N5ME/YgOJQ85o8ZKD7f4ZEidCtEvltyyuwBZJ15GSpCh0S9+9QYxSyOUhgmqdddzE+NPqTKcCZwVeqnGhLIxHWLXUkkj1P50cfCMXFhlQMJY2ZKGLNTfE1MaaT2JAtsZUTPSq95c/M/rpia89qdcJqlByZaLwlQQE5P592TAFTIjJpZQpri9lbARVZQZm1HBhuCtvrxOWpWyVy1X7yqlWiWLIw9ncA6X4MEV1OAGGtAEBhE8wyu8Ocp5cd6dj2VrzslmTuEPnM8fSFeQCA==</latexit> Next Call Figure 2: Overview of the SimATTA framework. and sample ratio vectors when Dϕ,S(t) is not included, i.e., w′ and λ′ s.t. w′ 0 = λ′ 0 = 0 . If ˆdH∆H(DS, Dϕ,S(t)) < ˆdH∆H(DS, St i=1 Ute(i)), then for any λ ̸= λ′, there exists w s.t. EBS(w, λ, N, t) < EBS(w′, λ′, N, t). (8) Corollary 4 validates that the selected low-entropy samples can mitigate the CF problem under the assumption that these samples are source-like, which is also empirically validated in Fig. 1. Note that our strategy employs entropy minimization in a selective manner, aiming to solve CF rather than the main adaptation issue. While many FTTA works use entropy minimization to adapt across domains without guarantees, our use is more theoretically-sound. 4 A N ATTA ALGORITHM Building on our theoretical findings, we introduce a simple yet effective ATTA method, known as SimATTA, that innovatively integrates incremental clustering and selective entropy minimization techniques, as illustrated in Fig. 2. We start with an overview of our methodology, including the learning framework and the comprehensive sample selection strategies. We then proceed to discuss the details of the incremental clustering technique designed for real-time sample selections. 4.1 A LGORITHM OVERVIEW Let (x, y) be a labeled sample and f(·; θ) be our neural network, where ˆy = f(x; θ) and θ represents the parameters. We have a model pre-trained on source domains with the pre-trained parameters ϕ. We initialize model parameters as θ(0) = ϕ and aim to adapt the model f(·; θ) in real-time. During the test phase, the model continuously predicts labels for streaming-in test data and concurrently gets fine-tuned. We perform sample selection to enable active learning. As discussed in Sec. 3.2, we empirically consider informative high-entropy samples for addressing distribution shifts and source-like low-entropy samples to mitigate CF. As shown in Alg. 1, at each time step t, we first partition unlabeled test samples Ute(t) into high entropy and low entropy datasets, Uh(t) and Ul(t), using an entropy threshold. The source-pretrained model f(·; ϕ) is frozen to predict pseudo labels for low entropy data. We obtain labeled low-entropy data Dl(t) by labeling Ul(t) with f(·; ϕ) and combining it with Dl(t − 1). In contrast, the selection of high-entropy samples for active labeling is less straightforward. Since the complete test dataset is inaccessible for analyzing the target domain distribution, real-time sample selection is required. We design an incremental clustering sample selection technique to reduce sample redundancy and increase distribution coverage, detailed in Sec. 4.2. The incremental clustering algorithm outputs the labeled test samples Dh(t), also referred to as anchors, given Dh(t −1) and Uh(t). After sample selection, the model undergoes test-time training using the labeled test anchors Dh(t) and pseudo-labeled source-like anchors Dl(t). Following the analyses in Sec. 3.1, the training weights and sample numbers should satisfy w(t) ≈ λ(t) for Dh(t) and Dl(t) for optimal results. The analyses and results in Sec. 3.2 further indicate that balancing the source and target ratio is the key to mitigating CF. However, when source-like samples significantly outnumber test samples, the optimal w(t) for test domains can deviate from λ(t) according to Eq. (4). 4.2 I NCREMENTAL CLUSTERING We propose incremental clustering, a novel continual clustering technique designed to select informa- tive samples in unsupervised settings under the ATTA framework. The primary goal of this strategy is to store representative samples for distributions seen so far. Intuitively, we apply clusters to cover all seen distributions while adding new clusters to cover newly seen distributions. During this process with new clusters added, old clusters may be merged due to the limit of the cluster budget. Since 6Published as a conference paper at ICLR 2024 Algorithm 1 SIMATTA: A SIMPLE ATTA ALGORITHM Require: A fixed source pre-trained model f(·; ϕ) and a real-time adapting model f(·; θ(t)) with θ(0) = ϕ. Streaming test data Ute(t) at time step t. Entropy of predictions H(ˆy) = −P c p(ˆyc) logp(ˆyc). Low entropy and high entropy thresholds el and eh. The number of cluster centroid budget NC (t) at time step t. Centroid increase number k. Learning step size η. 1: for t = 1, . . . , Tdo 2: Model inference on Ute(t) using f(·; θ(t − 1)). 3: Dl(t) ← Dl(t − 1) ∪ {(x, f(x; ϕ))|x ∈ Ute(t), H(f(x; ϕ)) < el} 4: Uh(t) ← {x|x ∈ Ute(t), H(f(x; θ)) > eh} 5: Dh(t) ← Dh(t − 1) ∪ {(x, y)|∀x ∈ IC(Dh(t − 1), Uh(t), NC(t)), y= Oracle(x)} 6: λ(t) ← |Dl(t)|/(|Dl(t)| + |Dh(t)|), |Dh(t)|/(|Dl(t)| + |Dh(t)|) 7: w(t) ← GetW(λ(t)) ▷ Generally, GetW(λ(t)) = λ(t) is a fair choice. 8: θ(t) ← θ(t − 1) 9: for (xl, yl) in Dl and (xh, yh) in Dh do 10: θ(t) ← θ(t) − ηw0∇ℓCE (f(xl; θ(t)), yl) − η(1 − w0)∇ℓCE (f(xh; θ(t)), yh) 11: end for 12: NC (t + 1) ← UpdateCentroidNum(NC (t)) ▷ Naive choice: NC (t + 1) ← NC (t) + k. 13: end for clusters cannot be stored efficiently, we store the representative samples of clusters, named anchors, instead. In this work, we adopt weighted K-means (Krishna and Murty, 1999) as our base clustering method due to its popularity and suitability for new setting explorations. When we apply clustering with new samples, a previously selected anchor should not weigh the same as new samples since the anchor is a representation of a cluster,i.e., a representation of many samples. Instead, the anchor should be considered as a barycenter with a weight of the sum of its cluster’s sample weights. For a newly added cluster, its new anchor has the weight of the whole cluster. For clusters containing multiple old anchors, i.e., old clusters, the increased weights are distributed equally among these anchors. These increased weights are contributed by new samples that are close to these old anchors. Intuitively, this process of clustering is analogous to the process of planet formation. Where there are no planets, new planets (anchors) will be formed by the aggregation of the surrounding material (samples). Where there are planets, the matter is absorbed by the surrounding planets. This example is only for better understanding without specific technical meanings. Specifically, we provide the detailed Alg. 2 for incremental clustering. In each iteration, we apply weighted K-Means for previously selected anchors Danc and the new streaming-in unlabeled data Unew. We first extract all sample features using the model from the previous step f(·; θ(t − 1)), and then cluster these weighted features. The initial weights of the new unlabeled samples are 1, while anchors inherit weights from previous iterations. After clustering, clusters including old anchors are old clusters, while clusters only containing new samples are newly formed ones. For each new cluster, we select the centroid-closest sample as the new anchor to store. As shown in line 10 of Alg. 2, for both old and new clusters, we distribute the sample weights in this cluster as its anchors’ weights. With incremental clustering, although we can control the number of clusters in each iteration, we cannot control the number of new clusters/new anchors. This indirect control makes the increase of new anchors adaptive to the change of distributions, but it also leads to indirect budget control. Therefore, in experimental studies, we set the budget limit, but the actual anchor budget will not reach this limit. The overall extra storage requirement is O(B) since the number of saved unlabeled samples is proportional to the number of saved labeled samples (anchors). 5 E XPERIMENTAL STUDIES In this study, we aim to validate the effectiveness of our proposed method, as well as explore the various facets of the ATTA setting. Specifically, we design experiments around the following research questions: RQ1: Can TTA methods address domain distribution shifts? RQ2: Is ATTA as efficient as TTA? RQ3: How do the components of SimATTA perform? RQ4: Can ATTA perform on par with stronger Active Domain Adaptation (ADA) methods? We compare ATTA with three settings, TTA (Tab. 2), enhanced TTA (Tab. 3 and 5), and ADA (Tab. 4). Datasets. To assess the OOD performance of the TTA methods, we benchmark them using datasets from DomainBed (Gulrajani and Lopez-Paz, 2020) and Hendrycks and Dietterich (2019a). We employ PACS (Li et al., 2017), VLCS (Fang et al., 2013), Office-Home (Venkateswara et al., 2017), and Tiny-ImageNet-C datasets for our evaluations. For each dataset, we designate one domain as 7Published as a conference paper at ICLR 2024 Table 2: TTA comparisons on PACS and VLCS.This table includes the two data stream mentioned in the dataset setup and reports performances in accuracy. Results that outperform all TTA baselines are highlighted in bold font. N/A denotes the adaptations are not applied on the source domain. PACS Domain-wise data stream Post-adaptation Random data stream Post-adaptation P →A→ →C→ →S P A C S →1→ →2→ →3→ →4 P A C S BN w/o adapt 99.70 59.38 28.03 42.91 99.70 59.38 28.03 42.91 43.44 43.44 43.44 43.44 99.70 59.38 28.03 42.91BN w/ adapt 98.74 68.07 64.85 54.57 98.74 68.07 64.85 54.57 62.50 62.50 62.50 62.50 98.74 68.07 64.85 54.57 Tent (steps=1) N/A 67.29 64.59 44.67 97.60 66.85 64.08 42.58 56.35 54.09 51.83 48.58 97.19 63.53 60.75 41.56Tent (steps=10) N/A 67.38 57.85 20.23 62.63 34.52 40.57 13.59 47.36 31.01 22.84 20.33 50.78 23.68 20.95 19.62EATA N/A 67.04 64.72 50.27 98.62 66.50 62.46 48.18 57.31 56.06 58.17 59.78 98.62 69.63 65.70 54.26CoTTA N/A 65.48 62.12 53.17 98.62 65.48 63.10 53.78 56.06 54.33 57.16 57.42 98.62 65.97 62.97 54.62SAR (steps=1) N/A 66.75 63.82 49.58 98.32 66.94 62.93 45.74 56.78 56.35 56.68 56.70 98.44 68.16 64.38 52.53SAR (steps=10) N/A 69.38 68.26 49.02 96.47 62.16 56.19 54.62 53.51 51.15 51.78 45.60 94.13 56.64 56.02 36.37 SimATTA (B ≤300) N/A 76.86 70.90 75.39 98.80 84.47 82.25 81.52 69.47 76.49 82.45 82.22 98.98 84.91 83.92 86.00SimATTA (B ≤500) N/A 77.93 76.02 76.30 98.62 88.33 83.49 83.74 68.46 78.22 80.91 85.49 99.16 86.67 84.77 87.71 VLCS Domain-wise data stream Post-adaptation Random data stream Post-adaptation C →L→ →S→ →V C L S V →1→ →2→ →3→ →4 C L S V BN w/o adapt 100.00 33.55 41.10 49.05 100.00 33.55 41.10 49.05 41.23 41.23 41.23 41.23 100.00 33.55 41.10 49.05BN w/ adapt 85.16 37.31 33.27 52.16 85.16 37.31 33.27 52.16 40.91 40.91 40.91 40.91 85.16 37.31 33.27 52.16 Tent (steps=1) N/A 38.55 34.40 53.88 84.73 43.86 33.61 53.11 44.85 44.29 47.38 44.98 85.30 43.49 37.81 53.35Tent (steps=10) N/A 45.41 31.44 32.32 42.54 37.65 27.79 33.12 46.13 42.31 43.51 39.48 52.01 40.32 33.64 40.37EATA N/A 37.24 33.15 52.58 84.10 37.69 32.39 52.49 43.77 42.48 43.34 41.55 83.32 36.67 31.47 52.55CoTTA N/A 37.39 32.54 52.25 82.12 37.65 33.12 52.90 43.69 42.14 43.21 42.32 81.98 37.99 33.52 53.23SAR (steps=1) N/A 36.18 34.43 52.46 83.96 39.72 36.53 52.37 43.64 43.04 44.20 41.93 85.09 40.70 36.44 53.02SAR (steps=10) N/A 35.32 34.10 51.66 82.12 41.49 33.94 53.08 43.56 42.05 42.53 41.16 85.09 37.58 33.12 52.01 SimATTA (B ≤300) N/A 62.61 65.08 74.38 99.93 69.50 66.67 77.34 62.33 69.33 73.20 71.93 99.93 69.43 72.46 80.39SimATTA (B ≤500) N/A 63.52 68.01 76.13 99.51 70.56 73.10 78.35 62.29 70.45 73.50 72.02 99.43 70.29 72.55 80.18 the source domain and arrange the samples from the other domains to form the test data stream. For DomainBed datasets, we adopt two stream order strategies. The first order uses a domain-wise data stream, i.e., we finish streaming samples from one domain before starting streaming another domain. The second order is random, where we shuffle samples from all target domains and partition them into four splits 1, 2, 3, and 4, as shown in Tab. 2. More dataset details are provided in Appx. G.1. Baselines. For baseline models, we start with the common source-only models, which either utilize pre-calculated batch statistics (BN w/o adapt) or test batch statistics (BN w/ adapt). For comparison with other TTA methods, we consider four state-of-the-art TTA methods: Tent (Wang et al., 2021), EATA (Niu et al., 2022), CoTTA (Wang et al., 2022a), and SAR (Niu et al., 2023). The three of them except Tent provide extra design to avoid CF. To compare with ADA methods, we select algorithms that are partially comparable with our method, i.e., they should be efficient (e.g., uncertainty-based) without the requirements of additional networks. Therefore, we adopt random, entropy (Wang and Shang, 2014), k-means (Krishna and Murty, 1999), and CLUE (Prabhu et al., 2021) for comparisons. Settings. For TTA, we compare with general TTA baselines in streaming adaptation using the two aforementioned data streaming orders, domain-wise and random. We choose P in PACS and C in VLCS as source domains. For domain-wise data stream, we use order A → C → S for PACS and L → S → V for VLCS. We report the real-time adaptation accuracy results for each split of the data stream, as well as the accuracy on each domain after all adaptations through the data stream (under “post-adaptation” columns). Enhanced TTA is built on TTA with access to extra random sample labels. TTA baselines are further fine-tuned with these random samples. To further improve enhanced TTA, we use long-term label storage and larger unlabeled sample pools. To its extreme where the model can access the whole test set samples, the setting becomes similar to ADA, thus we also use ADA methods for comparisons. ADA baselines have access to all samples in the pre-collected target datasets but not source domain data, whereas our method can only access the streaming test data. 5.1 T HE FAILURE OF TEST-TIME ADAPTATION The failure of TTA methods on domain distribution shifts is one of the main motivations of the ATTA setting. As shown in Tab. 2, TTA methods cannot consistently outperform eventhe simplest baseline \"BN w/ adapt\" which uses test time batch statistics to make predictions, evidencing that current TTA methods cannot solve domain distribution shifts (RQ1). Additionally, Tent (step=10) exhibits significant CF issues, where \"step=10\" indicates 10 test-time training updates, i.e., 10 gradient backpropagation iterations. This failure of TTA methods necessitates the position of ATTA. In contrast, SimATTA, with a budget B less than 300, outperforms all TTA methods on both source and target domains by substantial margins. Moreover, compared to the source-only baselines, our method improves the target domain performances significantly with negligible source performance loss, showing that ATTA is a more practically effective setting for real-world distribution shifts. 5.2 E FFICIENCY & ENHANCED TTA SETTING COMPARISONS To validate the efficiency of ATTA and broaden the dataset choice, we conduct this study on Tiny- ImageNet-C which, though does not focus on domain shifts, is much larger than PACS and VLCS. we 8Published as a conference paper at ICLR 2024 Table 3: Comparisons with Enhanced TTA on Tiny-ImageNet-C (severity level 5). Tiny-ImageNet-C Time (sec)Noise Blur Weather Digital Gauss. Shot Impul. Defoc. Glass Motion Zoom Snow Frost Fog Contr. Elastic Pixel JPEG Avg. Tent (step=1) 68.83 9.32 11.97 8.86 10.43 7.00 12.20 14.34 13.58 15.46 13.55 3.99 13.31 17.79 18.61 12.17Tent (step=10) 426.90 0.86 0.63 0.52 0.52 0.55 0.54 0.50 0.50 0.50 0.50 0.50 0.50 0.50 0.50 0.54EATA 93.14 3.98 3.33 2.18 4.80 2.37 11.02 11.41 14.06 15.26 9.65 1.36 9.88 14.24 12.12 8.26CoTTA 538.78 5.63 7.12 6.31 8.05 5.74 9.68 10.55 11.75 12.00 11.15 4.17 5.35 7.82 8.90 8.16SAR (step=1) 113.76 8.90 3.11 1.67 1.55 1.47 1.35 1.19 1.03 1.04 0.93 0.83 1.00 0.74 0.77 1.83SAR (step=10) 774.11 2.67 3.26 2.38 1.64 1.85 2.49 3.16 3.81 2.72 3.12 0.81 3.47 4.04 1.76 2.66 SimATTA (step=10) 736.289.68 19.40 12.14 30.28 17.03 42.36 43.10 31.96 40.08 29.243.21 34.56 45.24 45.74 28.86 enhance the TTA setting by fine-tuning baselines on randomly selected labeled samples. Specifically, the classifier of ResNet18-BN is pre-adapted to the brightness corruption (source domain) before test-time adapting. SimATTA’s label budget is around 4,000, while all other TTA methods have budget 4,500 for randomly selected labeled samples. The data stream order is shown in Tab. 3. Time is measured across all corrupted images in the Noise and Blur noise types, and the values represent the average time cost for adapting 10,000 images. The results clearly evidence the efficiency of ATTA (RQ2), while substantially outperforming all enhanced TTA baselines. Simply accessing labeled samples cannot benefit TTA methods to match ATTA. With 10 training updates (step=10) for each batch, FTTA methods would suffer from severe CF problem. In contrast, ATTA covers a statistically significant distribution, achieving stronger performances with 10 training updates or even more steps till approximate convergences. In fact, longer training on Tent (step=10) leads to worse results (compared to step=1), which further motivates the design of the ATTA setting. The reason for higher absolute time cost in Tab. 3 is due to differences in training steps. In this experiment, SimATTA has a training step of 10, and similar time cost as SAR per step. Note that if the enhanced TTA setting is further improved to maintain distributions with a balanced CF mitigation strategy and an incremental clustering design, the design approaches ATTA. Specifically, we compare SimATTA with its variants as the ablation study (RQ3) in Appx. I.2. 5.3 C OMPARISONS TO A STRONGER SETTING : ACTIVE DOMAIN ADAPTATION Table 4: Comparisons to ADA baselines. Source domains are denoted as \"(S)\". Results are average accuracies (with standard deviations). PACS P (S) A C S Random (B= 300) 96.21 (0.80) 81.19 (0.48) 80.75 (1.27) 84.34 (0.18)Entropy (B= 300) 96.31 (0.64)88.00 (1.46)82.48 (1.71) 80.55 (1.01)Kmeans (B= 300) 93.71 (1.50) 79.31 (4.01) 79.64 (1.44) 83.92 (0.65)CLUE (B= 300) 96.69 (0.17)83.97 (0.57)84.77 (0.88) 86.91 (0.26) SimATTA (B ≤300) 98.89 (0.09)84.69 (0.22)83.09 (0.83)83.76 (2.24) VLCS C (S) L S V Random (B= 300) 96.21 (1.65) 66.67 (1.70) 70.72 (0.30) 72.14 (1.71)Entropy (B= 300) 97.74 (1.56) 69.29 (2.26)69.25 (4.77) 75.26 (3.07)Kmeans (B= 300) 98.61 (0.27)67.57 (1.64)70.77 (0.01)74.49 (0.97)CLUE (B= 300) 85.70 (10.09) 65.29 (1.49) 69.42 (2.64) 69.09 (6.05) SimATTA (B ≤300) 99.93 (0.00) 69.47 (0.03)69.57 (2.90)78.87 (1.53) In addtion to the above comparisons with (en- hanced) TTA, which necessitate the requirement of extra information in the ATTA setting, we com- pare ATTA with a stronger setting Active Domain Adaptation (ADA) to demonstrate another supe- riority of ATTA, i.e., weaker requirements for comparable performances (RQ4). ADA baselines are able to choose the global best active samples, while ATTA has to choose samples from a small sample buffer (e.g., a size of 100) and discard the rest. Tab. 4 presents the post-adaptation model per- formance results. All ADA results are averaged from 3 random runs, while ATTA results are the post-adaptation performances averaged from the two data stream orders. As can be observed, despite the lack of a pre-collected target dataset, SimATTA produces better or competitive results against ADA methods. Moreover, without source data access, SimATTA’s design for CF allows it to maintain superior source domain performances over ADA methods. Further experimental studies including the Office-Home dataset are provided in Appx. I. In conclusion, the significant improvement compared to weaker settings (TTA, enhanced TTA) and the comparable performance with the stronger setting, ADA, rendering ATTA a setting that is as efficient as TTA and as effective as ADA. This implies its potential is worthy of future explorations. 6 C ONCLUSION AND DISCUSSION There’s no denying that OOD generalization can be extremely challenging without certain information, often relying on various assumptions easily compromised by different circumstances. Thus, it’s prudent to seek methods to achieve significant improvements with minimal cost, e.g., DG methods leveraging environment partitions and ATTA methods using budgeted annotations. As justified in our theoretical and experimental studies, ATTA stands as a robust approach to achieve real-time OOD generalization. Although SimATTA sets a strong baseline for ATTA, there’s considerable scope for further investigation within the ATTA setting. One potential direction involves developing alternatives to prevent CF in ATTA scenarios. While selective entropy minimization on low-entropy samples has prove to be empirically effective, it relies on the quality of the pre-trained model and training on incorrectly predicted low-entropy samples may reinforce the errors. It might not be cost-effective to expend annotation budgets on low-entropy samples, but correcting them could be a viable alternative solution. We anticipate that our work will spur numerous further explorations in this field. 9Published as a conference paper at ICLR 2024 ACKNOWLEDGMENTS This work was supported in part by National Science Foundation grant IIS-2006861 and National Institutes of Health grant U01AG070112. REFERENCES Hana Ajakan, Pascal Germain, Hugo Larochelle, François Laviolette, and Mario Marchand. Domain- adversarial neural networks. arXiv preprint arXiv:1412.4446, 2014. Lucas Baier, Tim Schlör, Jakob Schöffer, and Niklas Kühl. Detecting concept drift with neural network model uncertainty. In Hawaii International Conference on System Sciences, 2021. URL https://api.semanticscholar.org/CorpusID:235731947. Shai Ben-David, John Blitzer, Koby Crammer, Alex Kulesza, Fernando Pereira, and Jennifer Wortman Vaughan. A theory of learning from different domains. Machine learning, 79:151–175, 2010. Davide Cacciarelli and Murat Kulahci. A survey on online active learning, 2023. Cheng Chen, Quande Liu, Yueming Jin, Qi Dou, and Pheng-Ann Heng. Source-free domain adaptive fundus image segmentation with denoised pseudo-labeling. In Medical Image Computing and Computer Assisted Intervention–MICCAI 2021: 24th International Conference, Strasbourg, France, September 27–October 1, 2021, Proceedings, Part V 24, pages 225–235. Springer, 2021. Li Chen, Tutian Tang, Zhitian Cai, Yang Li, Penghao Wu, Hongyang Li, Jianping Shi, Junchi Yan, and Yu Qiao. Level 2 autonomous driving on a single device: Diving into the devils of openpilot. arXiv preprint arXiv:2206.08176, 2022a. Weijie Chen, Luojun Lin, Shicai Yang, Di Xie, Shiliang Pu, and Yueting Zhuang. Self-supervised noisy label learning for source-free unsupervised domain adaptation. In 2022 IEEE/RSJ In- ternational Conference on Intelligent Robots and Systems (IROS) , pages 10185–10192. IEEE, 2022b. Yining Chen, Colin Wei, Ananya Kumar, and Tengyu Ma. Self-training avoids using spurious features under domain shift. Advances in Neural Information Processing Systems, 33:21061–21071, 2020. David A Cohn, Zoubin Ghahramani, and Michael I Jordan. Active learning with statistical models. Journal of artificial intelligence research, 4:129–145, 1996. Matthias De Lange, Rahaf Aljundi, Marc Masana, Sarah Parisot, Xu Jia, Aleš Leonardis, Gregory Slabaugh, and Tinne Tuytelaars. A continual learning survey: Defying forgetting in classification tasks. IEEE transactions on pattern analysis and machine intelligence, 44(7):3366–3385, 2021. Yuhe Ding, Lijun Sheng, Jian Liang, Aihua Zheng, and Ran He. Proxymix: Proxy-based mixup training with label refinery for source-free domain adaptation. arXiv preprint arXiv:2205.14566, 2022. Cian Eastwood, Ian Mason, Christopher KI Williams, and Bernhard Schölkopf. Source-free adaptation to measurement shift via bottom-up feature restoration. arXiv preprint arXiv:2107.05446, 2021. Jiahao Fan, Hangyu Zhu, Xinyu Jiang, Long Meng, Chen Chen, Cong Fu, Huan Yu, Chenyun Dai, and Wei Chen. Unsupervised domain adaptation by statistics alignment for deep sleep staging networks. IEEE Transactions on Neural Systems and Rehabilitation Engineering, 30:205–216, 2022. Chen Fang, Ye Xu, and Daniel N Rockmore. Unbiased metric learning: On the utilization of multiple datasets and web images for softening bias. In Proceedings of the IEEE International Conference on Computer Vision, pages 1657–1664, 2013. Yuqi Fang, Pew-Thian Yap, Weili Lin, Hongtu Zhu, and Mingxia Liu. Source-free unsupervised domain adaptation: A survey. arXiv preprint arXiv:2301.00265, 2022. Francois Fleuret et al. Uncertainty reduction for model adaptation in semantic segmentation. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 9613–9623, 2021. 10Published as a conference paper at ICLR 2024 Yaroslav Ganin and Victor Lempitsky. Unsupervised domain adaptation by backpropagation. In International conference on machine learning, pages 1180–1189. PMLR, 2015. Yaroslav Ganin, Evgeniya Ustinova, Hana Ajakan, Pascal Germain, Hugo Larochelle, François Laviolette, Mario Marchand, and Victor Lempitsky. Domain-adversarial training of neural networks. The journal of machine learning research, 17(1):2096–2030, 2016. Jakob Gawlikowski, Cedrique Rovile Njieutcheu Tassi, Mohsin Ali, Jongseok Lee, Matthias Humt, Jianxiang Feng, Anna Kruspe, Rudolph Triebel, Peter Jung, Ribana Roscher, et al. A survey of uncertainty in deep neural networks. arXiv preprint arXiv:2107.03342, 2021. Yves Grandvalet and Yoshua Bengio. Semi-supervised learning by entropy minimization. Advances in neural information processing systems, 17, 2004. Shurui Gui, Chaoyue Wang, Qihua Chen, and Dacheng Tao. Featureflow: Robust video interpolation via structure-to-texture generation. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 14004–14013, 2020. Shurui Gui, Xiner Li, Limei Wang, and Shuiwang Ji. GOOD: A graph out-of-distribution benchmark. In Thirty-sixth Conference on Neural Information Processing Systems Datasets and Benchmarks Track, 2022. URL https://openreview.net/forum?id=8hHg-zs_p-h. Ishaan Gulrajani and David Lopez-Paz. In search of lost domain generalization. arXiv preprint arXiv:2007.01434, 2020. Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image recognition. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 770–778, 2016. Dan Hendrycks and Thomas Dietterich. Benchmarking neural network robustness to common corruptions and perturbations. March 2019a. doi: 10.48550/ARXIV .1903.12261. Dan Hendrycks and Thomas Dietterich. Benchmarking neural network robustness to common corruptions and perturbations. arXiv preprint arXiv:1903.12261, 2019b. Steven CH Hoi, Rong Jin, Jianke Zhu, and Michael R Lyu. Semisupervised svm batch mode active learning with applications to image retrieval. ACM Transactions on Information Systems (TOIS), 27(3):1–29, 2009. Yihan Hu, Jiazhi Yang, Li Chen, Keyu Li, Chonghao Sima, Xizhou Zhu, Siqi Chai, Senyao Du, Tianwei Lin, Wenhai Wang, et al. Planning-oriented autonomous driving. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 17853–17862, 2023. Jiaxing Huang, Dayan Guan, Aoran Xiao, and Shijian Lu. Model adaptation: Historical contrastive learning for unsupervised domain adaptation without source data. Advances in Neural Information Processing Systems, 34:3635–3649, 2021. Masato Ishii and Masashi Sugiyama. Source-free domain adaptation via distributional alignment by matching batch normalization statistics. arXiv preprint arXiv:2101.10842, 2021. Yusuke Iwasawa and Yutaka Matsuo. Test-time classifier adjustment module for model-agnostic domain generalization. Advances in Neural Information Processing Systems, 34:2427–2440, 2021. Suyog Dutt Jain and Kristen Grauman. Active image segmentation propagation. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pages 2864–2873, 2016. Guoliang Kang, Lu Jiang, Yi Yang, and Alexander G Hauptmann. Contrastive adaptation network for unsupervised domain adaptation. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 4893–4902, 2019. Ashish Kapoor, Kristen Grauman, Raquel Urtasun, and Trevor Darrell. Active learning with gaussian processes for object categorization. In 2007 IEEE 11th international conference on computer vision, pages 1–8. IEEE, 2007. Neerav Karani, Ertunc Erdil, Krishna Chaitanya, and Ender Konukoglu. Test-time adaptable neural networks for robust medical image segmentation. Medical Image Analysis, 68:101907, 2021. 11Published as a conference paper at ICLR 2024 Ronald Kemker, Marc McClure, Angelina Abitino, Tyler Hayes, and Christopher Kanan. Measuring catastrophic forgetting in neural networks. In Proceedings of the AAAI conference on artificial intelligence, volume 32, 2018. Daniel Kifer, Shai Ben-David, and Johannes Gehrke. Detecting change in data streams. In VLDB, volume 4, pages 180–191. Toronto, Canada, 2004. James Kirkpatrick, Razvan Pascanu, Neil Rabinowitz, Joel Veness, Guillaume Desjardins, Andrei A Rusu, Kieran Milan, John Quan, Tiago Ramalho, Agnieszka Grabska-Barwinska, et al. Overcoming catastrophic forgetting in neural networks. Proceedings of the national academy of sciences, 114 (13):3521–3526, 2017. Pang Wei Koh, Shiori Sagawa, Henrik Marklund, Sang Michael Xie, Marvin Zhang, Akshay Bal- subramani, Weihua Hu, Michihiro Yasunaga, Richard Lanas Phillips, Irena Gao, et al. Wilds: A benchmark of in-the-wild distribution shifts. In International Conference on Machine Learning, pages 5637–5664. PMLR, 2021. Divya Kothandaraman, Sumit Shekhar, Abhilasha Sancheti, Manoj Ghuhan, Tripti Shukla, and Dinesh Manocha. Salad: Source-free active label-agnostic domain adaptation for classification, segmentation and detection. In Proceedings of the IEEE/CVF Winter Conference on Applications of Computer Vision, pages 382–391, 2023. K Krishna and M Narasimha Murty. Genetic k-means algorithm. IEEE Transactions on Systems, Man, and Cybernetics, Part B (Cybernetics), 29(3):433–439, 1999. Alex Krizhevsky, Ilya Sutskever, and Geoffrey E Hinton. Imagenet classification with deep convolu- tional neural networks. Communications of the ACM, 60(6):84–90, 2017. David Krueger, Ethan Caballero, Joern-Henrik Jacobsen, Amy Zhang, Jonathan Binas, Dinghuai Zhang, Remi Le Priol, and Aaron Courville. Out-of-distribution generalization via risk extrap- olation (REx). In International Conference on Machine Learning , pages 5815–5826. PMLR, 2021. Vinod K Kurmi, Venkatesh K Subramanian, and Vinay P Namboodiri. Domain impression: A source data free domain adaptation method. In Proceedings of the IEEE/CVF winter conference on applications of computer vision, pages 615–625, 2021. David D Lewis and Jason Catlett. Heterogeneous uncertainty sampling for supervised learning. In Machine learning proceedings 1994, pages 148–156. Elsevier, 1994. Aodong Li, Alex Boyd, Padhraic Smyth, and Stephan Mandt. Detecting and adapting to irregular distribution shifts in bayesian online learning. Advances in neural information processing systems, 34:6816–6828, 2021a. Da Li, Yongxin Yang, Yi-Zhe Song, and Timothy M Hospedales. Deeper, broader and artier domain generalization. In Proceedings of the IEEE international conference on computer vision, pages 5542–5550, 2017. Rui Li, Qianfen Jiao, Wenming Cao, Hau-San Wong, and Si Wu. Model adaptation: Unsupervised domain adaptation without source data. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 9641–9650, 2020. Xianfeng Li, Weijie Chen, Di Xie, Shicai Yang, Peng Yuan, Shiliang Pu, and Yueting Zhuang. A free lunch for unsupervised domain adaptive object detection without source data. In Proceedings of the AAAI Conference on Artificial Intelligence, volume 35, pages 8474–8481, 2021b. Zhizhong Li and Derek Hoiem. Learning without forgetting. IEEE transactions on pattern analysis and machine intelligence, 40(12):2935–2947, 2017. Jian Liang, Dapeng Hu, Ran He, and Jiashi Feng. Distill and fine-tune: Effective adaptation from a black-box source model. arXiv preprint arXiv:2104.01539, 1(3), 2021. Jian Liang, Dapeng Hu, Jiashi Feng, and Ran He. Dine: Domain adaptation from single and multiple black-box predictors. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 8003–8013, 2022. 12Published as a conference paper at ICLR 2024 Yong Lin, Shengyu Zhu, Lu Tan, and Peng Cui. Zin: When and how to learn invariance without environment partition? Advances in Neural Information Processing Systems, 35:24529–24542, 2022. Xiaofeng Liu, Fangxu Xing, Chao Yang, Georges El Fakhri, and Jonghye Woo. Adapting off-the- shelf source segmenter for target medical image segmentation. In Medical Image Computing and Computer Assisted Intervention–MICCAI 2021: 24th International Conference, Strasbourg, France, September 27–October 1, 2021, Proceedings, Part II 24, pages 549–559. Springer, 2021a. Xinyu Liu and Yixuan Yuan. A source-free domain adaptive polyp detection framework with style diversification flow. IEEE Transactions on Medical Imaging, 41(7):1897–1908, 2022. Yuang Liu, Wei Zhang, Jun Wang, and Jianyong Wang. Data-free knowledge transfer: A survey. arXiv preprint arXiv:2112.15278, 2021b. Yuejiang Liu, Parth Kothari, Bastien Van Delft, Baptiste Bellot-Gurlet, Taylor Mordan, and Alexandre Alahi. Ttt++: When does self-supervised test-time training fail or thrive? Advances in Neural Information Processing Systems, 34:21808–21820, 2021c. Mingsheng Long, Yue Cao, Jianmin Wang, and Michael Jordan. Learning transferable features with deep adaptation networks. In International conference on machine learning, pages 97–105. PMLR, 2015. David Lopez-Paz and Marc’Aurelio Ranzato. Gradient episodic memory for continual learning. Advances in neural information processing systems, 30, 2017. Chaochao Lu, Yuhuai Wu, José Miguel Hernández-Lobato, and Bernhard Schölkopf. Invariant causal representation learning for out-of-distribution generalization. In International Conference on Learning Representations, 2021. Xinhong Ma, Junyu Gao, and Changsheng Xu. Active universal domain adaptation. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 8968–8977, 2021. Haitao Mao, Lun Du, Yujia Zheng, Qiang Fu, Zelin Li, Xu Chen, Shi Han, and Dongmei Zhang. Source free unsupervised graph domain adaptation. arXiv preprint arXiv:2112.00955, 2021. Christoforos Mavrogiannis, Francesca Baldini, Allan Wang, Dapeng Zhao, Pete Trautman, Aaron Steinfeld, and Jean Oh. Core challenges of social robot navigation: A survey. ACM Transactions on Human-Robot Interaction, 12(3):1–39, 2023. Zachary Nado, Shreyas Padhy, D Sculley, Alexander D’Amour, Balaji Lakshminarayanan, and Jasper Snoek. Evaluating prediction-time batch normalization for robustness under covariate shift. arXiv preprint arXiv:2006.10963, 2020. Munan Ning, Donghuan Lu, Dong Wei, Cheng Bian, Chenglang Yuan, Shuang Yu, Kai Ma, and Yefeng Zheng. Multi-anchor active domain adaptation for semantic segmentation. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 9112–9122, 2021. Shuaicheng Niu, Jiaxiang Wu, Yifan Zhang, Yaofo Chen, Shijian Zheng, Peilin Zhao, and Mingkui Tan. Efficient test-time model adaptation without forgetting. In International conference on machine learning, pages 16888–16905. PMLR, 2022. Shuaicheng Niu, Jiaxiang Wu, Yifan Zhang, Zhiquan Wen, Yaofo Chen, Peilin Zhao, and Mingkui Tan. Towards stable test-time adaptation in dynamic wild world. InThe Eleventh International Con- ference on Learning Representations, 2023. URL https://openreview.net/forum?id=g2YraF75Tj. Sinno Jialin Pan, Ivor W Tsang, James T Kwok, and Qiang Yang. Domain adaptation via transfer component analysis. IEEE transactions on neural networks, 22(2):199–210, 2010. Adam Paszke, Sam Gross, Francisco Massa, Adam Lerer, James Bradbury, Gregory Chanan, Trevor Killeen, Zeming Lin, Natalia Gimelshein, Luca Antiga, et al. Pytorch: An imperative style, high-performance deep learning library. Advances in neural information processing systems, 32, 2019. 13Published as a conference paper at ICLR 2024 Vishal M Patel, Raghuraman Gopalan, Ruonan Li, and Rama Chellappa. Visual domain adaptation: A survey of recent advances. IEEE signal processing magazine, 32(3):53–69, 2015. Judea Pearl. Causality. Cambridge university press, 2009. Fabian Pedregosa, Gaël Varoquaux, Alexandre Gramfort, Vincent Michel, Bertrand Thirion, Olivier Grisel, Mathieu Blondel, Peter Prettenhofer, Ron Weiss, Vincent Dubourg, et al. Scikit-learn: Machine learning in python. the Journal of machine Learning research, 12:2825–2830, 2011. Jonas Peters, Peter Bühlmann, and Nicolai Meinshausen. Causal inference by using invariant prediction: identification and confidence intervals. Journal of the Royal Statistical Society: Series B (Statistical Methodology), 78(5):947–1012, 2016. Jonas Peters, Dominik Janzing, and Bernhard Schölkopf. Elements of causal inference: foundations and learning algorithms. The MIT Press, 2017. Viraj Prabhu, Arjun Chandrasekaran, Kate Saenko, and Judy Hoffman. Active domain adaptation via clustering uncertainty-weighted embeddings. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 8505–8514, 2021. Elan Rosenfeld, Pradeep Ravikumar, and Andrej Risteski. The risks of invariant risk minimization. arXiv preprint arXiv:2010.05761, 2020. Shiori Sagawa, Pang Wei Koh, Tatsunori B Hashimoto, and Percy Liang. Distributionally robust neural networks for group shifts: On the importance of regularization for worst-case generalization. arXiv preprint arXiv:1911.08731, 2019. Shibani Santurkar, Dimitris Tsipras, Andrew Ilyas, and Aleksander Madry. How does batch normal- ization help optimization? Advances in neural information processing systems, 31, 2018. Akanksha Saran, Safoora Yousefi, Akshay Krishnamurthy, John Langford, and Jordan T. Ash. Streaming active learning with deep neural networks. In Andreas Krause, Emma Brunskill, Kyunghyun Cho, Barbara Engelhardt, Sivan Sabato, and Jonathan Scarlett, editors, Proceedings of the 40th International Conference on Machine Learning, volume 202 of Proceedings of Machine Learning Research, pages 30005–30021. PMLR, 23–29 Jul 2023. URL https://proceedings.mlr. press/v202/saran23a.html. Harald Schafer, Eder Santana, Andrew Haden, and Riccardo Biasini. A commute in data: The comma2k19 dataset, 2018. Tobias Scheffer, Christian Decomain, and Stefan Wrobel. Active hidden markov models for informa- tion extraction. In Advances in Intelligent Data Analysis: 4th International Conference, IDA 2001 Cascais, Portugal, September 13–15, 2001 Proceedings 4, pages 309–318. Springer, 2001. Steffen Schneider, Evgenia Rusak, Luisa Eck, Oliver Bringmann, Wieland Brendel, and Matthias Bethge. Improving robustness against common corruptions by covariate shift adaptation. Advances in Neural Information Processing Systems, 33:11539–11551, 2020. Burr Settles. Active learning literature survey. 2009. Karen Simonyan and Andrew Zisserman. Very deep convolutional networks for large-scale image recognition. arXiv preprint arXiv:1409.1556, 2014. Jong-Chyi Su, Yi-Hsuan Tsai, Kihyuk Sohn, Buyu Liu, Subhransu Maji, and Manmohan Chandraker. Active adversarial domain adaptation. In Proceedings of the IEEE/CVF Winter Conference on Applications of Computer Vision, pages 739–748, 2020. Baochen Sun and Kate Saenko. Deep coral: Correlation alignment for deep domain adaptation. In European conference on computer vision, pages 443–450. Springer, 2016. Yu Sun, Xiaolong Wang, Zhuang Liu, John Miller, Alexei Efros, and Moritz Hardt. Test-time training with self-supervision for generalization under distribution shifts. In International conference on machine learning, pages 9229–9248. PMLR, 2020. 14Published as a conference paper at ICLR 2024 Yi-Hsuan Tsai, Wei-Chih Hung, Samuel Schulter, Kihyuk Sohn, Ming-Hsuan Yang, and Manmohan Chandraker. Learning to adapt structured output space for semantic segmentation. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 7472–7481, 2018. Eric Tzeng, Judy Hoffman, Trevor Darrell, and Kate Saenko. Simultaneous deep transfer across domains and tasks. In Proceedings of the IEEE international conference on computer vision, pages 4068–4076, 2015. Eric Tzeng, Judy Hoffman, Kate Saenko, and Trevor Darrell. Adversarial discriminative domain adaptation. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 7167–7176, 2017. Hemanth Venkateswara, Jose Eusebio, Shayok Chakraborty, and Sethuraman Panchanathan. Deep hashing network for unsupervised domain adaptation. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 5018–5027, 2017. Sudheendra Vijayanarasimhan and Ashish Kapoor. Visual recognition and detection under bounded computational resources. In 2010 IEEE Computer Society Conference on Computer Vision and Pattern Recognition, pages 1006–1013. IEEE, 2010. Dan Wang and Yi Shang. A new active labeling method for deep learning. In 2014 International joint conference on neural networks (IJCNN), pages 112–119. IEEE, 2014. Dequan Wang, Evan Shelhamer, Shaoteng Liu, Bruno Olshausen, and Trevor Darrell. Tent: Fully test- time adaptation by entropy minimization. InInternational Conference on Learning Representations, 2021. URL https://openreview.net/forum?id=uXl3bZLkr3c. Mei Wang and Weihong Deng. Deep visual domain adaptation: A survey. Neurocomputing, 312: 135–153, 2018. Qin Wang, Olga Fink, Luc Van Gool, and Dengxin Dai. Continual test-time domain adaptation. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 7201–7211, 2022a. Rui Wang, Zuxuan Wu, Zejia Weng, Jingjing Chen, Guo-Jun Qi, and Yu-Gang Jiang. Cross-domain contrastive learning for unsupervised domain adaptation. IEEE Transactions on Multimedia , 2022b. Garrett Wilson and Diane J Cook. A survey of unsupervised deep domain adaptation. ACM Transactions on Intelligent Systems and Technology (TIST), 11(5):1–46, 2020. Binhui Xie, Longhui Yuan, Shuang Li, Chi Harold Liu, Xinjing Cheng, and Guoren Wang. Active learning for domain adaptation: An energy-based approach. InProceedings of the AAAI Conference on Artificial Intelligence, volume 36, pages 8708–8716, 2022. Zhao Xu, Kai Yu, V olker Tresp, Xiaowei Xu, and Jizhi Wang. Representative sampling for text classification using support vector machines. In Advances in Information Retrieval: 25th European Conference on IR Research, ECIR 2003, Pisa, Italy, April 14–16, 2003. Proceedings 25, pages 393–407. Springer, 2003. Baoyao Yang, Hao-Wei Yeh, Tatsuya Harada, and Pong C Yuen. Model-induced generalization error bound for information-theoretic representation learning in source-data-free unsupervised domain adaptation. IEEE Transactions on Image Processing, 31:419–432, 2021a. Guanglei Yang, Hao Tang, Zhun Zhong, Mingli Ding, Ling Shao, Nicu Sebe, and Elisa Ricci. Transformer-based source-free domain adaptation. arXiv preprint arXiv:2105.14138, 2021b. Jianfei Yang, Xiangyu Peng, Kai Wang, Zheng Zhu, Jiashi Feng, Lihua Xie, and Yang You. Divide to adapt: Mitigating confirmation bias for domain adaptation of black-box predictors. arXiv preprint arXiv:2205.14467, 2022. H Yao, Yuhong Guo, and Chunsheng Yang. Source-free unsupervised domain adaptation with surrogate data generation. In Proceedings of NeurIPS 2021 Workshop on Distribution Shifts: Connecting Methods and Applications, 2021. 15Published as a conference paper at ICLR 2024 Hao-Wei Yeh, Baoyao Yang, Pong C Yuen, and Tatsuya Harada. Sofa: Source-data-free feature alignment for unsupervised domain adaptation. InProceedings of the IEEE/CVF Winter Conference on Applications of Computer Vision, pages 474–483, 2021. Fuming You, Jingjing Li, and Zhou Zhao. Test-time batch statistics calibration for covariate shift. arXiv preprint arXiv:2110.04065, 2021. Hu Yu, Jie Huang, Yajing Liu, Qi Zhu, Man Zhou, and Feng Zhao. Source-free domain adaptation for real-world image dehazing. In Proceedings of the 30th ACM International Conference on Multimedia, pages 6645–6654, 2022. Haojian Zhang, Yabin Zhang, Kui Jia, and Lei Zhang. Unsupervised domain adaptation of black-box source models. arXiv preprint arXiv:2101.02839, 2021. Marvin Zhang, Sergey Levine, and Chelsea Finn. Memo: Test time robustness via adaptation and augmentation. Advances in Neural Information Processing Systems, 35:38629–38642, 2022a. Yifan Zhang, Xue Wang, Kexin Jin, Kun Yuan, Zhang Zhang, Liang Wang, Rong Jin, and Tieniu Tan. Adanpc: Exploring non-parametric classifier for test-time adaptation. In International Conference on Machine Learning, pages 41647–41676. PMLR, 2023. Yizhe Zhang, Shubhankar Borse, Hong Cai, and Fatih Porikli. Auxadapt: Stable and efficient test-time adaptation for temporally consistent video semantic segmentation. In Proceedings of the IEEE/CVF Winter Conference on Applications of Computer Vision, pages 2339–2348, 2022b. Bowen Zhao, Chen Chen, and Shu-Tao Xia. Delta: degradation-free fully test-time adaptation. arXiv preprint arXiv:2301.13018, 2023a. Hao Zhao, Yuejiang Liu, Alexandre Alahi, and Tao Lin. On pitfalls of test-time adaptation. In International Conference on Machine Learning (ICML), 2023b. Chunting Zhou, Xuezhe Ma, Paul Michel, and Graham Neubig. Examining and combating spurious features under distribution shift. In International Conference on Machine Learning, pages 12857– 12867. PMLR, 2021. 16Published as a conference paper at ICLR 2024 Active Test-Time Adaptation: Foundational Analyses and An Algorithm Supplementary Material A B ROADER IMPACTS The field of domain generalization primarily concentrates on enhancing a model’s generalization abilities by preparing it thoroughly before deployment. However, it is equally important for deep learning applications to have the capacity for real-time adaptation, as no amount of preparation can account for all possible scenarios. Consequently, domain generalization and test-time adaptation are complementary strategies: the former is more weighty and extensive, while the latter is more agile, lightweight and privacy-friendly. This work delves into the development of a real-time model adaptation strategy that can be applied to any pre-trained models, including large language models, to enhance their adaptive capabilities. Our research does not involve any human subjects or dataset releases, nor does it raise any ethical concerns. Since this work does not directly tie to specific applications, we do not foresee any immediate negative societal impacts. Nonetheless, we acknowledge that any technological advancement may carry potential risks, and we encourage the continued assessment of the broader impacts of real-time adaptation methodologies in various contexts. B FAQ & D ISCUSSIONS To facilitate the reviewing process, we summarize the answers to the questions that arose during the discussion of an earlier version of this paper. The major updates of this version are reorganized theoretical studies, incremental clustering details, experimental reorganization, and additional datasets and settings . We include more related field comparisons to distinguish different settings. We also cover the position of this paper in literature and the main claims of this paper. Finally, we will frankly acknowledge the limitations of this paper, explain and justify the scope of coverage, and provide possible future directions. Q1: What is the relationship between the proposed ATTA protocol and stream based active learning (Saran et al., 2023)? A: We would like to discuss the difference between our work and the referenced work. 1. Real-time Training Distinction: Saran et al. (2023) doesn’t operate in real-time capacity. This is evident from their experiments, where their model is trained only after completing a round. In contrast, our work involves training the model post each batch. This positions Saran et al. (2023)’s work as an intrinsic active learning technique, while our approach leans towards TTA methods. 2. Continual Training Nuance: Following the point above, Saran et al. (2023) stands out of the scope of continual training. As they mentioned ‘each time new data are acquired, the ResNet is reset to the ImageNet pre-trained weights before being updated‘, Saran et al. (2023) starts afresh with each iteration and is out of scope for CF discussions. Contrarily, our model is continuously trained on varying distributions, compelling us to address the CF issue while preserving advantages derived from various stored distributions. 3. Comparative Complexity: Given the aforementioned distinctions, it’s evident that our task presents a greater challenge compared to theirs. In addition, we have included comparisons with stronger active learning settings in Sec. 5.3. Q2: What are the insights from the theoretically foundational analysis? A: 1. It sets a well-defined formulation and grounded theoretical framework for the ATTA setting. 2. While entropy minimizations can cause CF, balancing the learning rate and number of high/low entropy samples is conversely the key solution to both distribution shifts and 17Published as a conference paper at ICLR 2024 CF by corresponding benefits. Though adding low-entropy data is intuitive, it is crucial in that this simple operation can make methods either too conservative or too aggressive without the correct balancing conditions. 3. The studies in Sec. 3.1 directly present a feasible and guaranteed solution for imple- menting ATTA to tackle shifts while avoiding CF. The aligned empirical validations of Sec. 3.2 also instruct the implementation of SimATTA. Q3: In test-time adaptation, one important issue is that the number of testing samples in a batch may be small, which means the sample size m will also be very small. May it affect the theorem and make them become very loose? A: We consider this issue jointly from theoretical and empirical validations. 1. It is true that the theoretical bounds can be loose given a small size of m unlabeled test samples. This situation of the error bound is mathematically ascribed to the quotient between the VC-dimension d of the hypothesis class and m. Under the VC-dimension theory, the ResNet18 model we adopt should have d ≫ m. However, practically we perform fine-tuning on pre-trained models instead of training from scratch, which significantly reduces the scale of parameter update. In this case, an assumption can be established that fine-tuning a model is roughly equivalent to learning a model with a relatively small d (Appx. H). This assumption is potentially underpinned by the empirical alignment of our validation experiments with the theoretical framework (Fig. 1). To this end, experiments indicate thatd and m are practically of similar scale for our settings. This prevents our theoretical bounds from being very loose and meaningless in reality. 2. Regarding cases that our assumption does not apply, this issue would appear inevitable, since it is rigorously inherent in the estimation error of our streaming and varying test distributions. The distribution of a test stream can be hardly monitored when only a limited batch is allowed, which we consider as a limitation of TTA settings. Moreover, this issue directly implies the necessity of using a buffer for unlabeled samples. A good practice is to maintain a relatively comparable sample buffer scale. Q4: What distribution shifts can ATTA solve? A: We would like to follow (but not limited to) the work (Zhao et al., 2023b) to discuss the distribution shifts ATTA can solve. 1. As elucidated in Sec. 3.1 and Sec. 5, ATTA can solve domain generalization shifts. Domain generalization shifts include complex shifts on the joint data distribution P(X, Y), given X as the covariates and Y as the label variable. Since P(X, Y) = P(X)P(Y |X), ATTA can handle covariate shift (P(X)), label shift (P(Y )), and conditional shift (P(Y |X)). The shifts on both covariate and conditional distributions can cover the shift on labels, but they (covariate + conditional shifts) are more complicated than pure label shifts, where only the marginal label distribution changes while the conditional distribution remains. Note that the conditional shifts are generally caused by spurious correlations, where the independent causal mechanism assumption (Pearl, 2009) holds or no concept drifts exist. 2. In our framework, the distribution support of X at different time steps can be different, but we don’t cover the situation where the support of Y changes, i.e., class-incremental problems. Q5: It is unclear how many samples are selected in each minibatch of testing samples. How the total budget is distributed across the whole testing data stream? A: The number of selected samples for each minibatch is decided jointly by the incremental clustering and the cluster centroid number NC (t). Intuitively, this sample selection is a dynamic process, with NC (t) restricting the budget and incremental clustering performing sample selection. For each batch, we increase applicable clustering centroids as a maximum limit, while the exact number of the selected samples is given by the incremental clustering by how many clusters are located in the scope of new distributions. e.g., if the incoming batch does not introduce new data distributions, then we select zero samples even with increased NC (t). In contrast, if the incoming batch contains data located in multiple new distributions, the incremental clustering tends to select more samples than the NC (t) limit, thus forcing to merging of multiple previous clusters into one new cluster. 18Published as a conference paper at ICLR 2024 The incremental clustering is detailed in Sec. 4.2, and NC (t) is naively increased by a constant hyper-parameter k. Therefore, the budget is adaptively distributed according to the data streaming distribution with budgets controlled by k, which is also the reason why we compare methods under a budget limit. Q6: Could compared methods have access to a few ground-truth labels as well? Making other algorithms be able to use the same amount of ground-truth labels randomly will produce fairer comparisons. A: 1. The enhanced TTA setting is exactly the setup we provide to produce fairer comparisons. See Tab. 3 and Tab. 5 for comparison results. 2. ATTA also compares to a stronger setting ADA which can access the whole test datasets multiple times. Table 5: The table demonstrates the comparisons on PACS where all enhanced TTA baselines have 300 budgets to randomly select labeled samples. The training steps of these labeled samples are the same as the original TTA method training steps. For accumulated sample selection, please refer to our ablation studies. Method Domain-wise data stream A VG Random data stream A VG P→ →A→ →C→ →S P A C S 1 2 3 4 P A C S Source onlyBN w/o adapt 99.70 59.38 28.03 42.91 99.70 59.38 28.03 42.91 43.44 43.44 43.44 43.44 99.70 59.38 28.03 42.91BN w/ adapt 98.74 68.07 64.85 54.57 98.74 68.07 64.85 54.57 62.50 62.50 62.50 62.50 98.74 68.07 64.85 54.57 TTA Tent (steps=1) N/A 70.07 68.43 64.42 97.72 74.17 72.61 68.92 61.20 62.36 66.59 67.32 98.14 74.37 70.26 66.07Tent (steps=10) N/A 76.27 63.78 49.35 59.46 38.62 48.46 55.03 56.20 53.22 52.55 55.55 58.32 47.56 60.75 58.00EATA N/A 69.53 66.94 61.42 98.56 69.38 66.60 64.83 60.34 59.81 64.38 65.02 98.68 73.78 68.30 59.74CoTTA N/A 66.55 63.14 59.91 90.12 61.67 66.68 67.68 57.26 57.36 63.46 65.64 92.22 71.53 70.44 62.41SAR (steps=1) N/A 66.60 63.78 50.34 98.38 67.87 64.04 49.48 57.21 56.06 56.78 57.14 98.38 68.80 64.59 53.02SAR (steps=10) N/A 69.09 66.55 49.07 96.23 62.50 59.34 46.53 49.76 52.74 48.51 49.06 95.39 57.13 54.61 38.76 Ours (B ≤300) N/A 76.86 70.90 75.39 98.80 84.47 82.25 81.52 69.47 76.49 82.45 82.22 98.98 84.91 83.92 86.00 Q7: What is the position of ATTA? A: Comparisons with different settings are challenging. In this work, the design of our experiments (Sec. 5) is to overcome this challenge by comparing both weaker settings and stronger settings. While the significant performance over weaker settings renders the necessity of extra information, the comparable performance with stronger settings provides the potential to relax restricted requirements. Intuitively, ATTA is the most cost-effective option in the consideration of both efficiency and effectiveness. We further provide the following ATTA summary: ATTA, which incorporates active learning in FTTA, is the light, real-time, source-free, widely applicable setting to achieve high generalization performances for test-time adaptation. 1. Necessity: From the causality perspective, new information is necessary (Lin et al., 2022; Pearl, 2009; Peters et al., 2017) to attain generalizable over distribution shifts which are insurmountable within the current TTA framework. 2. Effectiveness: Compared to FTTA methods, ATTA produces substantially better perfor- mances, on-par with the costly active domain adaptation (ADA) methods as shown in Table 3 in the paper. 3. Efficiency: Relative to ADA methods, ATTA possesses superior efficiency, similar to general FTTA methods, as shown in Tab. 3. 4. Applicability: ATTA is a model-agnostic setting. (1) Compared to domain generalization methods, ATTA do not require re-training and has the potential to apply to any pre-trained models. One interesting future direction is designing ATTA methods for large language models (LLMs), where re-trainings are extremely expensive and source data may be in- accessible. (2) Compared to FTTA methods, ATTA can protect model parameters from corrupting while learning new distributions by fine-tuning pre-trained models, rendering it more feasible and practical. In comparison with existing works, ATTA is motivated to mitigate the limitations of previous settings: 1. FTTA: Limited generalization performance. 19Published as a conference paper at ICLR 2024 2. TTT: Not source-free; limited generalization performance. 3. ADA & domain adaptation/generalization: Expensive re-trainings; limited applicability to pre-trained models. 4. Online active learning: It does not maintain and protect adaptation performances for multiple distributions in one model and does not consider the CF problem. Q8: What is the potential practical utility of ATTA? A: 1. Empirically, our method can generally finish a round of sample selection/training of 100 frames in 5s, i.e., 20 frames per sec, which is more than enough to handle multiple practical situations. Experiments on time complexity are provided in Tab. 3, where SimATTA has comparable time efficiency. 2. As a case analysis, the autopilot system (Hu et al., 2023; Chen et al., 2022a) presents an application scenario requiring high-speed low-latency adaptations, while these adaptations are largely underexplored. When entering an unknown environment, e.g., a construction section, a system of ATTA setting can require the driver to take over the wheel. During the period of manual operation when the driver is handling the wheel, steering signals are generated, and the in-car system quickly adaptations. The system doesn’t need to record 60 frames per second, since only the key steering operations and the corresponding dash cam frames are necessary, which can be handled by ATTA algorithms processing at 20 frames per sec. In this case, the human annotations are necessary and indirect. ATTA makes use of this information and adapts in the short term instead of collecting videos and having a long-round fine-tuning (Schafer et al., 2018). 3. In addition, many scenarios applicable for ATTA are less speed-demanding than the case above. One example is a personalized chatbot that subtly prompts and gathers user labels during user interaction. In a home decoration setting, applications can request that users scan a few crucial areas to ensure effective adaptation. Social robots (Mavrogiannis et al., 2023), e.g., vacuum robots, often require users to label critical obstacles they’ve encountered. 4. Compared with ADA, ATTA stands out as the tailored solution for the above scenarios. It does not require intensive retraining or server-dependent fine-tuning, offering both speed and computational efficiency. Meanwhile, akin to other TTA methods, ATTA also ensures user privacy. While it might marginally exceed the cost of standard TTA methods, the superior generalization ability makes it a compelling choice and justifies the additional expense. Q9: What can be covered by this paper? A: This paper endeavors to establish the foundational framework for a novel setting referred to as ATTA. We target (1) positioning the ATTA setting, (2) solving the two major and basic challenges of ATTA,i.e., the mitigation of distribution shifts and the avoidance of catastrophic forgetting (CF). We achieve the first goal by building the problem formulation and analyses, and further providing extensive qualitative and well-organized experimental comparisons with TTA, enhanced TTA, and ADA settings. These efforts position ATTA as the most cost-effective option between TTA and ADA, where ATTA inherits the efficiency of TTA and the effectiveness of ADA. With our theoretical analyses and the consistent algorithm design, we validate the success of our second goal through significant empirical performances. Q10: What are not covered by this paper? A: Constructing a new setting involves multifaceted complexities. Although there are various potential applications discussed above including scaling this setting up for large models and datasets, we cannot cover them in this single piece of work. There are three main reasons. First, the topics covered by a single paper are limited. Formally establishing ATTA setting and addressing its major challenges of ATTA takes precedence over exploring practical applications. Secondly, given the interrelations between ATTA and other settings, our experimental investigations are predominantly comparative, utilizing the most representative datasets from TTA and domain adaptation to showcase persuasive results. Thirdly, many practical applications necessitate task-specific configurations, rendering them unsuitable for establishing a universal learning setting. While the current focus is on laying down the foundational aspects of ATTA, the exploration of more specialized applications remains a prospective avenue for future work in the ATTA domain. 20Published as a conference paper at ICLR 2024 C R ELATED WORKS The development of deep learning witnesses various applications (He et al., 2016; Gui et al., 2020). To tackle OOD problem, various domain generalization works emerge (Krueger et al., 2021; Sagawa et al., 2019). C.1 U NSUPERVISED DOMAIN ADAPTATION Unsupervised Domain Adaptation (UDA) (Pan et al., 2010; Patel et al., 2015; Wilson and Cook, 2020; Wang and Deng, 2018) aims at mitigating distribution shifts between a source domain and a target domain, given labeled source domain samples and unlabeled target samples. UDA methods generally rely on feature alignment techniques to eliminate distribution shifts by aligning feature distributions between source and target domains. Typical feature alignment techniques include discrepancy minimization (Long et al., 2015; Sun and Saenko, 2016; Kang et al., 2019) and adversarial training (Ganin and Lempitsky, 2015; Tsai et al., 2018; Ajakan et al., 2014; Ganin et al., 2016; Tzeng et al., 2015; 2017). Nevertheless, alignments are normally not guaranteed to be correct, leading to the alignment distortion problem as noted by Ning et al. (2021). Source-free Unsupervised Domain Adaptation (SFUDA) (Fang et al., 2022; Liu et al., 2021b) algorithms aim to adapt a pre-trained model to unlabeled target domain samples without access to source samples. Based on whether the algorithm can access model parameters, these algorithms are categorized into white-box and black-box methods. White-box SFUDA typically considers data recovery (generation) and fine-tuning methods. The former focuses on recovering source- like data (Ding et al., 2022; Yao et al., 2021), e.g., training a Generative Adversarial Network (GAN) (Kurmi et al., 2021; Li et al., 2020), while the latter employs various techniques (Mao et al., 2021), such as knowledge distillation (Chen et al., 2022b; Liu and Yuan, 2022; Yang et al., 2021b; Yu et al., 2022), statistics-based domain alignment (Ishii and Sugiyama, 2021; Liu et al., 2021a; Fan et al., 2022; Eastwood et al., 2021), contrastive learning (Huang et al., 2021; Wang et al., 2022b), and uncertainty-based adaptation (Gawlikowski et al., 2021; Fleuret et al., 2021; Chen et al., 2021; Li et al., 2021b). Black-box SFUDA cannot access model parameters and often relies on self-supervised knowledge distillation (Liang et al., 2022; 2021), pseudo-label denoising (Zhang et al., 2021; Yang et al., 2022), or generative distribution alignment (Yeh et al., 2021; Yang et al., 2021a). C.2 T EST-TIME ADAPTATION Test-time Adaptation (TTA), especially Fully Test-time Adaptation (FTTA) algorithms (Wang et al., 2021; Iwasawa and Matsuo, 2021; Karani et al., 2021; Nado et al., 2020; Schneider et al., 2020; Wang et al., 2022a; Zhao et al., 2023a; Niu et al., 2022; Zhang et al., 2022a; Niu et al., 2023; You et al., 2021; Zhang et al., 2022b), can be considered as realistic and lightweight methods for domain adaptation. Built upon black-box SFUDA, FTTA algorithms eliminate the requirement of a pre-collected target dataset and the corresponding training phase. Instead, they can only access an unlabeled data stream and apply real-time adaptation and training. In addition to FTTA, Test-time Training (TTT) (Sun et al., 2020; Liu et al., 2021c) often relies on appending the original network with a self-supervised task. TTT methods require retraining on the source dataset to transfer information through the self-supervised task. Although they do not access the source dataset during the test-time adaptation phase, TTT algorithms are not off-the-shelf source-free methods. TTA is a promising and critical direction for real-world applications, but current entropy minimization-based methods can be primarily considered as feature calibrations that require high-quality pseudo-labels. This requirement, however, can be easily violated under larger distribution shifts. Current TTA algorithms, inheriting UDA drawbacks, cannot promise good feature calibration results, which can be detrimental in real-world deployments. For instance, entropy minimization on wrongly predicted target domain samples with relatively low entropy can only exacerbate spurious correla- tions (Chen et al., 2020). Without extra information, this problem may be analogous to applying causal inference without intervened distributions, which is intrinsically unsolvable (Peters et al., 2016; Pearl, 2009). This paper aims to mitigate this issue with minimal labeled target domain samples. To minimize the cost, we tailor active learning techniques for TTA settings. It is worth noting that a recent work AdaNPC (Zhang et al., 2023) is essentially a domain gener- alization method with a TTA phase attached, while our ATTA is built based on the FTTA setting. Specifically, Current FTTA methods and our work cannot access the source domain. In contrast, 21Published as a conference paper at ICLR 2024 AdaNPC accesses source data to build its memory bank, circumventing the catastrophic forgetting problem. Furthermore, AdaNPC requires multiple source domains and training before performing TTA. Thus AdaNPC uses additional information on domain labels and retraining resources for its memory bank, undermining the merits of FTTA. Regarding theoretical bounds, their target domain is bounded by source domain error and model estimations (in big-O expression), while we consider active sample learning and time variables for varying test distributions. C.3 C ONTINUAL DOMAIN ADAPTATION Many domain adaptation methods focus on improving target domain performance, neglecting the performance on the source domain, which leads to the CF problem (Kemker et al., 2018; Kirkpatrick et al., 2017; Li and Hoiem, 2017; Lopez-Paz and Ranzato, 2017; De Lange et al., 2021; Wang et al., 2022a; Niu et al., 2022). This issue arises when a neural network, after being trained on a sequence of domains, experiences a significant degradation in its performance on previously learned domains as it continues to learn new domains. Continual learning, also known as lifelong learning, addresses this problem. Recent continual domain adaptation methods have made significant progress by employing gradient regularization, random parameter restoration, buffer sample mixture, and more. Although the CF problem is proposed in the continual learning field, it can occur in any source-free OOD settings since the degradation caused by CF is attributed to the network’s parameters being updated to optimize performance on new domains, which may interfere with the representations learned for previous domains. C.4 A CTIVE DOMAIN ADAPTATION Active Domain Adaptation (ADA) (Prabhu et al., 2021; Ning et al., 2021; Su et al., 2020; Ma et al., 2021; Xie et al., 2022) extends semi-supervised domain adaptation with active learning strate- gies (Cohn et al., 1996; Settles, 2009), aiming to maximize target domain performance with a limited annotation budget. Therefore, the key challenge of active learning algorithms is selecting the most informative unlabeled data in target domains (Kapoor et al., 2007). Sample selection strategies are of- ten based on uncertainty (Lewis and Catlett, 1994; Scheffer et al., 2001), diversity (Jain and Grauman, 2016; Hoi et al., 2009), representativeness (Xu et al., 2003), expected error minimization (Vijaya- narasimhan and Kapoor, 2010), etc. Among these methods, uncertainty and diversity-based methods are simple and computationally efficient, making them the most suitable choices to tailor for TTA settings. Adapting these strategies is non-trivial because, compared to typical active domain adaptation, our proposed Active Test-time Adaptation (ATTA) setting does not provide access to source data, model parameters, or pre-collected target samples. This requirement demands that our active sample selection algorithm select samples for annotation during data streaming. Consequently, this active sampling selection process is non-regrettable, i.e., we can only meet every sample once in a short period. To avoid possible confusion, compared to the recent Source-free Active Domain Adaptation (SFADA) method SALAD (Kothandaraman et al., 2023), we do not require access to model parameter gradients, training additional neural networks, or pre-collected target datasets. Therefore, our ATTA setting is quite different, much lighter, and more realistic than ADA and SFADA. C.5 A CTIVE ONLINE LEARNING The most related branch of active online learning (AOL) (Cacciarelli and Kulahci, 2023) is active online learning on drifting data stream (Zhou et al., 2021; Baier et al., 2021; Li et al., 2021a). Generally, these methods include two components, namely, detection and adaptation. Compared with ATTA, there are several distinctions. First, this line of studies largely focuses on the distribution shift detection problem, while ATTA focuses on multi-domain adaptations. Second, AOL on drifting data stream aims to detect and adapt to one current distribution in the stream, without considering preserving the adaptation abilities of multiple past distributions by maintaining and fine-tuning the original pre-trained models. In contrast, ATTA’s goal is to achieve the OOD generalization optimums adaptable across multiple source and target distributions, leading to the consideration of CF problems. Third, while AOL requires one-by-one data input and discard, ATTA maintains a buffer for incoming data before selection decisions. This is because ATTA targets maintaining the original model without corrupting and replacing it, such that making statistically meaningful and high-quality decisions is 22Published as a conference paper at ICLR 2024 critical for ATTA. In contrast, AOL allows resetting and retraining new models, whose target is more lean to cost saving and one-by-one manner. D F URTHER THEORETICAL STUDIES In this section, we refine the theoretical studies with supplement analysis and further results. We use the H-divergence and H∆H-distance definitions following (Ben-David et al., 2010). Definition 2 (H-divergence). For a function class H and two distributions D1 and D2 over a domain X, the H-divergence between D1 and D2 is defined as dH(D1, D2) = sup h∈H |Px∼D1 [h(x) = 1] − Px∼D2 [h(x) = 1]|. The H∆H-distance is defined base on H-divergence. We use the H∆H-distance definition follow- ing (Ben-David et al., 2010). Definition 3 (H∆H-distance). For two distributions D1 and D2 over a domain X and a hypothesis class H, the H∆H-distance between D1 and D2 w.r.t. H is defined as dH∆H(D1, D2) = sup h,h′∈H Px∼D1 [h(x) ̸= h′(x)] + Px∼D2 [h(x) ̸= h′(x)]. (9) The H∆H-distance essentially provides a measure to quantify the distribution shift between two distributions. It measures the maximum difference of the disagreement between two hypotheses in H for two distributions, providing a metrics to quantify the distribution shift between D1 and D2. H-divergence and H∆H-distance have the advantage that they can be applied between datasets, i.e., estimated from finite samples. Specifically, let S1, S2 be unlabeled samples of size m sampled from D1 and D2; then we have estimated H∆H-distance ˆdH(S1, S2). This estimation can be bounded based on Theorem 3.4 of Kifer et al. (2004), which we state here for completeness. Theorem 5. Let A be a collection of subsets of some domain measure space, and assume that the VC-dimension is some finite d. Let P1 and P2 be probability distributions over that domain and S1, S2 finite samples of sizes m1, m2 drawn i.i.d. according P1, P2 respectively. Then Pm1+m2 [|ϕA(S1, S2) − ϕA(P1, P2)| > ϵ] ≤ (2m)de−m1ϵ2/16 + (2m)de−m2ϵ2/16, (10) where Pm1+m2 is the m1 + m2’th power of P - the probability that P induces over the choice of samples. Theorem 5 bounds the probability for relativized discrepancy, and its applications in below lemmas and Theorem 1 help us bound the quantified distribution shifts between domains. The probability, according to a distribution D, that an estimated hypothesis h disagrees with the true labeling function g : X → {0, 1} is defined as ϵ(h(t), g) = E(x)∼D[|h(x, t) − g(x)|], which we also refer to as the error or risk ϵ(h(t)). While the source domain dataset is inaccessible under ATTA settings, we consider the existence of the source dataset DS for the purpose of accurate theoretical analysis. Thus, we initialize Dtr(0) as DS, i.e., Dtr(0) = DS. For every time step t, the test and training data can be expressed as Ute(t) and Dtr(t) = DS ∪ Dte(1) ∪ Dte(2) ∪ ··· ∪Dte(t). (11) We use N to denote the total number of samples in Dtr(t) and λ = (λ0, λ1, ··· , λt) to represent the ratio of sample numbers in each component subset. In particular, we have |DS| |Dtr(t)| = λ0, |Dte(1)| |Dtr(t)| = λ1, ··· , |Dte(t)| |Dtr(t)| = λt, (12) where Pt i=0 λi = 1. Therefore, at time step t, the model has been trained on labeled data Dtr(t), which contains t + 1 components consisting of a combination of data from the source domain and multiple test-time domains. For each domain the model encounters, DS, Ute(1), Ute(2), ··· , Ute(t), let ϵj(h(t)) denote the error of hypothesis h at time t on the jth domain. Specifically, ϵ0(h(t)) = ϵS(h(t)) represents the error of h(t) on the source data DS, and ϵj(h(t)) for j ≥ 1 denotes the error of h(t) on test data Ute(j). Our optimization minimizes a convex combination of training error over the labeled samples from all domains. Formally, given the vector w = (w0, w1, ··· , wt) of domain error 23Published as a conference paper at ICLR 2024 weights with Pt j=0 wj = 1 and the sample number from each component Nj = λjN, we minimize the empirical weighted error of h(t) as ˆϵw(h(t)) = tX j=0 wjˆϵj(h(t)) = tX j=0 wj Nj X Nj |h(x, t) − g(x)|. (13) Note that w, λ and N are also functions of t, which we omit for simplicity. We now establish two lemmas as the preliminary for Theorem 1. In the following lemma, we bound the difference between the weighted error ϵw(h(t)) and the domain error ϵj(h(t)). Lemma 6. Let H be a hypothesis space of VC-dimension d. At time step t, let the ATTA data domains be DS, Ute(1), Ute(2), ··· , Ute(t), and Si be unlabeled samples of size m sampled from each of the t + 1 domains respectively. Then for any δ ∈ (0, 1), for every h ∈ Hminimizing ϵw(h(t)) on Dtr(t), we have |ϵw(h(t)) − ϵj(h(t))| ≤ tX i=0,i̸=j wi  1 2 ˆdH∆H(Si, Sj) + 2 s 2d log(2m) + log 2 δ m + γi  , with probability of at least 1 − δ, where γi = minh∈H{ϵi(h(t)) + ϵj(h(t))}. In the following lemma, we provide an upper bound on the difference between the true and empirical weighted errors ϵw(h(t)) and ˆϵw(h(t)). Lemma 7. Let H be a hypothesis class. For Dtr(t) = DS ∪ Dte(1) ∪ ··· ∪Dte(t) at time t, if the total number of samples in Dtr(t) is N, and the ratio of sample numbers in each component is λj, then for any δ ∈ (0, 1) and h ∈ H, with probability of at least 1 − δ, we have P[|ϵw(h(t)) − ˆϵw(h(t))| ≥ϵ] ≤ 2 exp   −2Nϵ2/( tX j=0 w2 j λj ) ! . Thus, as wj deviates from λj, the feasible approximation ˆϵw(h(t)) with a finite number of labeled samples becomes less reliable. The proofs for both lemmas are provided in Appx. E. Building upon the two preceding lemmas, we proceed to derive bounds on the domain errors under the ATTA setting when minimizing the empirical weighted error using the hypothesis h at time t. Lemma 6 bounds the difference between the weighted error ϵw(h(t)) and the domain error ϵj(h(t)), which is majorly influenced by the estimatedH∆H-distance and the quality of discrepancy estimation. During the ATTA process, the streaming test data can form multiple domains and distributions. However, if we consider all data during the test phase as a single test domain,i.e., St i=1 Ute(i), we can simplify Lemma 6 to obtain an upper bound for the test error ϵT as |ϵw(h(t)) − ϵT (h(t))| ≤w0  1 2 ˆdH∆H(S0, ST ) + 2 s 2d log(2m) + log 2 δ m + γ  , (14) where γ = min h∈H{ϵ0(h(t)) + ϵT (h(t))}, and ST is sampled from St i=1 Ute(i). To understand Lamma 7, we need to understand Hoeffding’s Inequality, which we state below as a Proposition for completeness. Proposition 8 (Hoeffding’s Inequality). Let X be a set, D1, . . . , Dt be probability distributions on X, and f1, . . . , ft be real-valued functions on X such that fi : X → [ai, bi] for i = 1, . . . , t. Then for any ϵ >0, P  \f\f\f\f\f 1 t tX i=1 fi(x) − 1 t tX i=1 Ex∼Di[fi(x)] \f\f\f\f\f ≥ ϵ ! ≤ 2 exp   − 2t2ϵ2 Pt i=1(bi − ai)2 ! (15) where E[fi(x)] is the expected value of fi(x). Lamma 7 provides an upper bound on the difference between the true and empirical weighted errors ϵw(h(t)) and ˆϵw(h(t)). Thus, as wj deviates from λj, the feasible approximation ˆϵw(h(t)) with a finite number of labeled samples becomes less reliable. Building upon the two preceding lemmas, we proceed to derive bounds on the domain errors under the ATTA setting when minimizing the empirical weighted error using the hypothesis h at time t. Theorem 1 essentially bounds the performance of ATTA on the source and each test domains. The adaptation performance on a test domain is majorly 24Published as a conference paper at ICLR 2024 bounded by the composition of (labeled) training data, estimated distribution shift, and ideal joint hypothesis performance, which correspond to C, ˆdH∆H(Si, Sj), and γi, respectively. The ideal joint hypothesis error γi gauges the inherent adaptability between domains. If we consider the multiple data distributions during the test phase as a single test domain, i.e., St i=1 Ute(i), Theorem 1 can be reduced into bounds for the source domain error ϵS and test domain error ϵT . With the optimal test/source hypothesis h∗ T (t) = arg min h∈H ϵT (h(t)) and h∗ S(t) = arg minh∈H ϵS(h(t)), |ϵT (ˆh(t)) − ϵT (h∗ T (t))| ≤w0A + s w2 0 λ0 + (1 − w0)2 1 − λ0 B, (16a) |ϵS(ˆh(t)) − ϵS(h∗ S(t))| ≤(1 − w0)A + s w2 0 λ0 + (1 − w0)2 1 − λ0 B, (16b) where the distribution divergence termA = ˆdH∆H(S0, ST )+4 q 2d log(2m)+log 2 δ m +2γ, the empirical gap term B = 2 q d log(2N)−log(δ) 2N , ST is sampled from St i=1 Ute(i), and γ = minh∈H{ϵ0(h(t)) + ϵT (h(t))}. Our learning bounds demonstrates the trade-off between the small amount of budgeted test-time data and the large amount of less relevant source data. Next, we provide an approximation of the condition necessary to achieve optimal adaptation performance, which is calculable from finite samples and can be readily applied in practical ATTA scenarios. Following Eq. (16.a), with approximately B = c1 p d/N, the optimal value w∗ 0 to tighten the test error bound is a function of λ0 and A: w∗ 0 = λ0 − s A2N c2 1d − A2Nλ0(1 − λ0), for λ 0 ≥ 1 − d A2N , (17) where c1 is a constant. Note that λ0 ≥ 1 − d A2N should be the satisfied condition in practical ATTA settings, where the budget is not sufficiently big while the source data amount is relatively large. When the budget is sufficiently large or the source data amount is not sufficiently large compared to the distribution shift A, the optimal w∗ 0 for the test error bound is w∗ 0 = 0, i.e., using no source data since possible error reduction from the data addition is always less than the error increase caused by large divergence between the source data and the test data. Theorem 2 offers a direct theoretical guarantee that ATTA reduces the error bound on test domains in comparison to TTA without the integration of active learning. Following Theorem 1, when no active learning is included during TTA,i.e., w0 = λ0 = 1, the upper boundw0A+ q w2 0 λ0 + (1−w0)2 1−λ0 B ≥ A+B; when enabling ATTA, withw0 = λ0 ̸= 1, we can easily achieve an upper bound w0A + B < A+ B. Therefore, the incorporation of labeled test instances in ATTA theoretically enhances the overall performance across test domains, substantiating the significance of the ATTA setting in addressing distribution shifts. Entropy quantifies the amount of information contained in a probability distribution. In the context of a classification model, lower entropy indicates that the model assigns high probability to one of the classes, suggesting a high level of certainty or confidence in its prediction. When a model assigns low entropy to a sample, this high confidence can be interpreted as the sample being well-aligned or fitting closely with the model’s learned distribution. In other words, the model “recognizes” the sample as being similar to those it was trained on, hence the high confidence in its prediction. While entropy is not a direct measure of distributional distance, it can be used as an indicator of how closely a sample aligns with the model’s learned distribution. This interpretation is more about model confidence and the implied proximity rather than a strict mathematical measure of distributional distance. The pre-trained model is well-trained on abundant source domain data, and thus the model distribution is approximately the source distribution. Selecting low-entropy samples using essentially provides an estimate of sampling from the source dataset. Thus, Dϕ,S(t), based on well-aligned with the model’s learned distribution is an approximation of DS. When we consider the CF problem and feasibly include the source-like dataset Dϕ,S(t) into the ATTA training data in place of the inaccessible DS in Eq. (11), we can also derive bounds on the domain errors under this practical ATTA setting when minimizing the empirical weighted errorϵ′ w(h(t)) using the hypothesis h at time t, similar to Theorem 1. Let H be a hypothesis class of VC-dimension d. At time step t, for ATTA data domainsDϕ,S(t), Ute(1), Ute(2), ··· , Ute(t), Si are unlabeled samples of size m sampled from each of the t + 1 domains respectively. The total number of samples in Dtr(t) is 25Published as a conference paper at ICLR 2024 N and the ratio of sample numbers in each component is λi. If ˆh(t) ∈ Hminimizes the empirical weighted error ˆϵ′ w(h(t)) with the weight vector w on Dtr(t), and h∗ j (t) = arg minh∈H ϵj(h(t)) is the optimal hypothesis on the jth domain, then for any δ ∈ (0, 1), we have ϵj(ˆh(t)) ≤ ϵj(h∗ j (t)) + 2 tX i=0,i̸=j wi  1 2 ˆdH∆H(Si, Sj) + 2 s 2d log(2m) + log 2 δ m + γi   + 2C with probability of at least 1 − δ, where C = r\u0010Pt i=0 w2 i λi \u0011\u0010 d log(2N)−log(δ) 2N \u0011 and γi = minh∈H{ϵi(h(t)) + ϵj(h(t))}. Other derived results following Theorem 1 also apply for this practical ATTA setting. Further empirical validations for our theoretical results are provided in Appx. H. E P ROOFS This section presents comprehensive proofs for all the lemmas, theorems, and corollaries mentioned in this paper, along with the derivation of key intermediate results. Lemma 6. Let H be a hypothesis space of VC-dimension d. At time step t, let the ATTA data domains be DS, Ute(1), Ute(2), ··· , Ute(t), and Si be unlabeled samples of size m sampled from each of the t + 1 domains respectively. Then for any δ ∈ (0, 1), for every h ∈ Hminimizing ϵw(h(t)) on Dtr(t), we have |ϵw(h(t)) − ϵj(h(t))| ≤ tX i=0,i̸=j wi  1 2 ˆdH∆H(Si, Sj) + 2 s 2d log(2m) + log 2 δ m + γi  , with probability of at least 1 − δ, where γi = minh∈H{ϵi(h(t)) + ϵj(h(t))}. Proof. First we prove that given unlabeled samples of size m S1, S2 sampled from two distributions D1 and D2, we have dH∆H(D1, D2) ≤ ˆdH∆H(S1, S2) + 4 s 2d log(2m) + log 2 δ m . (18) We start with Theorem 3.4 of Kifer et al. (2004): Pm1+m2 [|ϕA(S1, S2) − ϕA(P1, P2)| > ϵ] ≤ (2m)de−m1ϵ2/16 + (2m)de−m2ϵ2/16. (19) In Eq. 19, ’d’ is the VC-dimension of a collection of subsets of some domain measure space A, while in our case, d is the VC-dimension of hypothesis space H. Following (Ben-David et al., 2010), the H∆H space is the set of disagreements between every two hypotheses inH, which can be represented as a linear threshold network of depth 2 with 2 hidden units. Therefore, the VC-dimension of H∆H is at most twice the VC-dimension of H, and the VC-dimension of our domain measure space is 2d for Eq. 19 to hold. Given δ ∈ (0, 1), we set the upper bound of the inequality to δ, and solve for ϵ: δ = (2m)2de−m1ϵ2/16 + (2m)2de−m2ϵ2/16. We rewrite the inequality as δ (2m)2d = e−m1ϵ2/16 + e−m2ϵ2/16; taking the logarithm of both sides, we get log δ (2m)2d = −m1 ϵ2 16 + log(1 +e−(m1−m2) ϵ2 16 ). 26Published as a conference paper at ICLR 2024 Assuming m1 = m2 = m and defining a = ϵ2 16 , we have log δ (2m)2d = −ma + log 2; rearranging the equation, we then get ma + log(δ/2) = 2d log(2m). Now, we can solve for a: a = 2d log(2m) + log 2 δ m . Recall that a = ϵ2 16 , so we get: ϵ = 4√a ϵ = 4 s 2d log(2m) + log 2 δ m . With probability of at least 1 − δ, we have |ϕA(S1, S2) − ϕA(P1, P2)| ≤4 s 2d log(2m) + log 2 δ m ; therefore, dH∆H(D1, D2) ≤ ˆdH∆H(S1, S2) + 4 s 2d log(2m) + log 2 δ m . (20) Now we prove Lemma 6. We use the triangle inequality for classification error in the derivation. For the domain error of hypothesis h at time t on the jth domain ϵj(h(t)), given the definition of ϵw(h(t)), |ϵw(h(t)) − ϵj(h(t))| = | tX i=0 wiϵi(h(t)) − ϵj(h(t))| ≤ tX i=0 wi|ϵi(h(t)) − ϵj(h(t))| ≤ tX i=0 wi(|ϵi(h(t)) − ϵi(h(t), h∗ i (t))| + |ϵi(h(t), h∗ i (t)) − ϵj(h(t), h∗ i (t))| + |ϵj(h(t), h∗ i (t)) − ϵj(h(t))|) ≤ tX i=0 wi(ϵi(h∗ i (t)) + |ϵi(h(t), h∗ i (t)) − ϵj(h(t), h∗ i (t))| + ϵj(h∗ i (t))) ≤ tX i=0 wi(γi + |ϵi(h(t), h∗ i (t)) − ϵj(h(t), h∗ i (t))|), where γi = minh∈H{ϵi(h(t)) + ϵj(h(t))}. By the definition of H∆H-distance and our proved Eq. 20, |ϵi(h(t), h∗ i (t)) − ϵj(h(t), h∗ i (t))| ≤sup h,h′∈H |ϵi(h(t), h′(t)) − ϵj(h(t), h′(t))| = sup h,h′∈H Px∼Di[h(x) ̸= h′(x)] + Px∼Dj [h(x) ̸= h′(x)] = 1 2dH∆H(Di, Dj) ≤ 1 2 ˆdH∆H(Si, Sj) + 2 s 2d log(2m) + log 2 δ m , 27Published as a conference paper at ICLR 2024 where Di, Dj denote the ith and jth domain. Therefore, |ϵw(h(t)) − ϵj(h(t))| ≤ tX i=0 wi(γi + |ϵi(h(t), h∗ i (t)) − ϵj(h(t), h∗ i (t))|) ≤ tX i=0 wi(γi + 1 2dH∆H(Di, Dj)) ≤ tX i=0 wi(γi + 1 2 ˆdH∆H(Si, Sj) + 2 s 2d log(2m) + log 2 δ m ). Since ϵi(h(t)) − ϵj(h(t)) = 0 when i = j, we derive |ϵw(h(t)) − ϵj(h(t))| ≤ tX i=0,i̸=j wi  1 2 ˆdH∆H(Si, Sj) + 2 s 2d log(2m) + log 2 δ m + γi  , with probability of at least 1 − δ, where γi = minh∈H{ϵi(h(t)) + ϵj(h(t))}. This completes the proof. Lemma 7. Let H be a hypothesis class. For Dtr(t) = DS ∪ Dte(1) ∪ ··· ∪Dte(t) at time t, if the total number of samples in Dtr(t) is N, and the ratio of sample numbers in each component is λj, then for any δ ∈ (0, 1) and h ∈ H, with probability of at least 1 − δ, we have P[|ϵw(h(t)) − ˆϵw(h(t))| ≥ϵ] ≤ 2 exp   −2Nϵ2/( tX j=0 w2 j λj ) ! . Proof. We apply Hoeffding’s Inequality in our proof: P  \f\f\f\f\f 1 t tX i=1 fi(x) − 1 t tX i=1 Ex∼Di[fi(x)] \f\f\f\f\f ≥ ϵ ! ≤ 2 exp   − 2t2ϵ2 Pt i=1(bi − ai)2 ! . (21) In the jth domain, there are λjN samples. With the true labeling function g(x), for each of the λjN samples x, let there be a real-valued function fi(x) fi(x) = wj λj |h(x, t) − g(x)|, where fi(x) ∈ [0, wj λj ]. Incorporating all the domains, we get ˆϵw(h(t)) = tX j=0 wjˆϵj(h(t)) = tX j=0 wj λjN X λjN |h(x, t) − g(x)| = 1 N tX j=0 λjNX i=1 fi(x), which corresponds to the 1 t Pt i=1 fi(x) part in Hoeffding’s Inequality. Due to the linearity of expectations, we can calculate the sum of expectations as 1 N tX j=0 λjNX i=1 E[fi(x)] = 1 N ( tX j=0 λjN wj λj ϵj(h(t))) = tX j=0 wjϵj(h(t)) = ϵw(h(t)), which corresponds to the 1 t Pt i=1 Ex∼Di[fi(x)] part in Hoeffding’s Inequality. Therefore, we can apply Hoeffding’s Inequality as P[|ϵw(h(t)) − ˆϵw(h(t))| ≥ϵ] ≤ 2 exp   −2N2ϵ2/( NX i=0 range2(fi(x))) ! = 2 exp   −2N2ϵ2/( tX j=0 λjN(wj λj )2) ! = 2 exp   −2Nϵ2/( tX j=0 w2 j λj ) ! . This completes the proof. 28Published as a conference paper at ICLR 2024 Theorem 1. Let H be a hypothesis class of VC-dimension d. At time step t, for ATTA data domains DS, Ute(1), Ute(2), ··· , Ute(t), Si are unlabeled samples of size m sampled from each of the t + 1 domains respectively. The total number of samples in Dtr(t) is N and the ratio of sample numbers in each component is λi. If ˆh(t) ∈ Hminimizes the empirical weighted error ˆϵw(h(t)) with the weight vector w on Dtr(t), and h∗ j (t) = arg minh∈H ϵj(h(t)) is the optimal hypothesis on the jth domain, then for any δ ∈ (0, 1), with probability of at least 1 − δ, we have ϵj(ˆh(t)) ≤ ϵj(h∗ j (t)) + 2 tX i=0,i̸=j wi  1 2 ˆdH∆H(Si, Sj) + 2 s 2d log(2m) + log 2 δ m + γi   + 2C, where C = r\u0010Pt i=0 w2 i λi \u0011\u0010 d log(2N)−log(δ) 2N \u0011 and γi = minh∈H{ϵi(h(t)) + ϵj(h(t))}. For future test domains j = t + k (k >0), assuming k′ = argmink′∈{0,1,...t} dH∆H(D(k′), Ute(t + k)) and min dH∆H (D(k′), Ute(t + k)) ≤ δD, where 0 ≤ δD ≪ +∞, then ∀δ, with probability of at least 1 − δ, we have ϵt+k(ˆh(t)) ≤ ϵt+k(h∗ t+k(t)) + tX i=0 wi  ˆdH∆H(Si, Sk′ ) + 4 s 2d log(2m) + log 2 δ m + δD + 2γi   + 2C. Proof. First we prove that for any δ ∈ (0, 1) and h ∈ H, with probability of at least 1 − δ, we have |ϵw(h(t)) − ˆϵw(h(t))| ≤ vuut  tX i=0 w2 i λi !\u0012d log(2N) − log(δ) 2N \u0013 . (22) We apply Theorem 3.2 of Kifer et al. (2004) and Lemma 7, P[|ϵw(h(t)) − ˆϵw(h(t))| ≥ϵ] ≤ (2N)d exp   −2Nϵ2/( tX j=0 w2 j λj ) ! . Given δ ∈ (0, 1), we set the upper bound of the inequality to δ, and solve for ϵ: δ = (2N)d exp   −2Nϵ2/( tX j=0 w2 j λj ) ! . We rewrite the inequality as δ (2N)d = e −2Nϵ2/(Pt j=0 w2 j λj ) , taking the logarithm of both sides, we get log δ (2N)d = −2Nϵ2/( tX j=0 w2 j λj ). Rearranging the equation, we then get ϵ2 = ( tX j=0 w2 j λj )d log(2N) − log(δ) 2N . Therefore, with probability of at least 1 − δ, we have |ϵw(h(t)) − ˆϵw(h(t))| ≤ vuut  tX i=0 w2 i λi !\u0012d log(2N) − log(δ) 2N \u0013 . (23) 29Published as a conference paper at ICLR 2024 Based on Eq. 23, we now prove Theorem 1. For the empirical domain error of hypothesis h at time t on the jth domain ϵj(ˆh(t)), applying Lemma 6, Eq. 23, and the definition of h∗ j (t), we get ϵj(ˆh(t)) ≤ ϵw(ˆh(t)) + tX i=0,i̸=j wi  1 2 ˆdH∆H(Si, Sj) + 2 s 2d log(2m) + log 2 δ m + γi   ≤ ˆϵw(ˆh(t)) + vuut  tX i=0 w2 i λi !\u0012d log(2N) − log(δ) 2N \u0013 + tX i=0,i̸=j wi  1 2 ˆdH∆H(Si, Sj) + 2 s 2d log(2m) + log 2 δ m + γi   ≤ ˆϵw(h∗ j (t)) + vuut  tX i=0 w2 i λi !\u0012d log(2N) − log(δ) 2N \u0013 + tX i=0,i̸=j wi  1 2 ˆdH∆H(Si, Sj) + 2 s 2d log(2m) + log 2 δ m + γi   ≤ ϵw(h∗ j (t)) + 2 vuut  tX i=0 w2 i λi !\u0012d log(2N) − log(δ) 2N \u0013 + tX i=0,i̸=j wi  1 2 ˆdH∆H(Si, Sj) + 2 s 2d log(2m) + log 2 δ m + γi   ≤ ϵj(h∗ j (t)) + 2 vuut  tX i=0 w2 i λi !\u0012d log(2N) − log(δ) 2N \u0013 + 2 tX i=0,i̸=j wi  1 2 ˆdH∆H(Si, Sj) + 2 s 2d log(2m) + log 2 δ m + γi   = ϵj(h∗ j (t)) + 2 tX i=0,i̸=j wi  1 2 ˆdH∆H(Si, Sj) + 2 s 2d log(2m) + log 2 δ m + γi   + 2C with probability of at least 1 − δ, where C = r\u0010Pt i=0 w2 i λi \u0011\u0010 d log(2N)−log(δ) 2N \u0011 and γi = minh∈H{ϵi(h(t)) + ϵj(h(t))}. For future test domains j = t + k where k > 0, we have the assumption that k′ = argmink′∈{0,1,...t} dH∆H(D(k′), Ute(t + k)) and min dH∆H(D(k′), Ute(t + k)) ≤ δD. Here, we slightly abuse the notation D(k′) to represent Ds if k′ = 0 and Ute(k′) if k′ > 0. Then we get ϵt+k(ˆh(t)) ≤ ϵw(ˆh(t)) + tX i=0 wi  1 2 ˆdH∆H(Si, St+k) + 2 s 2d log(2m) + log 2 δ m + γi   ≤ ϵw(ˆh(t)) + tX i=0 wi  1 2( ˆdH∆H(Si, Sk′ ) + ˆdH∆H(Sk′ , St+k)) + 2 s 2d log(2m) + log 2 δ m + γi   ≤ ϵw(ˆh(t)) + tX i=0 wi  1 2 ˆdH∆H(Si, Sk′ ) + 1 2δD + 2 s 2d log(2m) + log 2 δ m + γi   ≤ ˆϵw(ˆh(t)) + vuut  tX i=0 w2 i λi !\u0012d log(2N) − log(δ) 2N \u0013 + tX i=0 wi  1 2 ˆdH∆H(Si, Sk′ ) + 1 2δD + 2 s 2d log(2m) + log 2 δ m + γi   30Published as a conference paper at ICLR 2024 ≤ ˆϵw(h∗ t+k(t)) + vuut  tX i=0 w2 i λi !\u0012d log(2N) − log(δ) 2N \u0013 + tX i=0 wi  1 2 ˆdH∆H(Si, Sk′ ) + 1 2δD + 2 s 2d log(2m) + log 2 δ m + γi   ≤ ϵw(h∗ t+k(t)) + 2 vuut  tX i=0 w2 i λi !\u0012d log(2N) − log(δ) 2N \u0013 + tX i=0 wi  1 2 ˆdH∆H(Si, Sk′ ) + 1 2δD + 2 s 2d log(2m) + log 2 δ m + γi   ≤ ϵt+k(h∗ t+k(t)) + 2 vuut  tX i=0 w2 i λi !\u0012d log(2N) − log(δ) 2N \u0013 + 2 tX i=0 wi  1 2 ˆdH∆H(Si, Sk′ ) + 1 2δD + 2 s 2d log(2m) + log 2 δ m + γi   = ϵt+k(h∗ t+k(t)) + tX i=0 wi  ˆdH∆H(Si, Sk′ ) + 4 s 2d log(2m) + log 2 δ m + δD + 2γi   + 2C. with probability of at least 1−δ, where C = r\u0010Pt i=0 w2 i λi \u0011\u0010 d log(2N)−log(δ) 2N \u0011 , γi = minh∈H{ϵi(h(t))+ ϵt+k(h(t))}, and 0 ≤ δD ≪ +∞. This completes the proof. Theorem 2. Let H be a hypothesis class of VC-dimension d. For ATTA data domains DS, Ute(1), Ute(2), ··· , Ute(t), considering the test-time data as a single test domain St i=1 Ute(i), if ˆh(t) ∈ H minimizes the empirical weighted error ˆϵw(h(t)) with the weight vector w on Dtr(t), let the test error be upper-bounded with |ϵT (ˆh(t)) − ϵT (h∗ T (t))| ≤EBT (w, λ, N, t). Let w′ and λ′ be the weight and sample ratio vectors when no active learning is included, i.e., w′ and λ′ s.t. w′ 0 = λ′ 0 = 1 and w′ i = λ′ i = 0 for i ≥ 1, then for any λ ̸= λ′, there exists w s.t. EBT (w, λ, N, t) < EBT (w′, λ′, N, t). (24) Proof. From Theorem 1, we can derive the bound for the test error where the test-time data are considered as a single test domain: |ϵT (ˆh(t)) − ϵT (h∗ T (t))| ≤EBT (w, λ, N, t) = w0( ˆdH∆H(S0, ST ) + 4 s 2d log(2m) + log 2 δ m + 2γ) + 2 s w2 0 λ0 + (1 − w0)2 1 − λ0 r d log(2N) − log(δ) 2N ; and we simplify the above equation as |ϵT (ˆh(t)) − ϵT (h∗ T (t))| ≤w0A + s w2 0 λ0 + (1 − w0)2 1 − λ0 B, (25) where the distribution divergence termA = ˆdH∆H(S0, ST )+4 q 2d log(2m)+log 2 δ m +2γ, the empirical gap term B = 2 q d log(2N)−log(δ) 2N , ST is sampled from St i=1 Ute(i), and γ = minh∈H{ϵ0(h(t)) + ϵT (h(t))}. Since we have s w2 0 λ0 + (1 − w0)2 1 − λ0 = s (w0 − λ0)2 λ0(1 − λ0) + 1 ≥ 1, (26) 31Published as a conference paper at ICLR 2024 where Formula 26 obtains the minimum value if and only if w0 = λ0; when enabling ATTA with any λ0 ̸= 1, we can get EBT (w, λ, N, t) = w0A + s w2 0 λ0 + (1 − w0)2 1 − λ0 B ≥ w0A + B, (27) where the minimum value EBT (w, λ, N, t)min = w0A + B can be obtained with condition w0 = λ0 ̸= 1. When no active learning is included, i.e., for weight and sample ratio vectors w′ and λ′, w′ 0 = λ′ 0 = 1 and w′ i = λ′ i = 0 for i ≥ 1, we have EBT (w′, λ′, N, t) = w′ 0A + s w′2 0 λ′ 0 + (1 − w′ 0)2 1 − λ′ 0 B = A + B. (28) Since for EBT (w, λ, N, t)min = w0A + B, w0 < 1 and A, B >0 hold, we derive EBT (w, λ, N, t)min = w0A + B < A+ B = EBT (w′, λ′, N, t). (29) This completes the proof. Corollary 3. At time step t, for ATTA data domains Dϕ,S(t), Ute(1), Ute(2), ··· , Ute(t), Si are unla- beled samples of size m sampled from each of the t + 1 domains respectively, and SS is unlabeled samples of size m sampled from DS. If ˆh(t) ∈ Hminimizes ˆϵ′ w(h(t)) while other conditions remain identical to Theorem 1, then ϵS(ˆh(t)) ≤ ϵS(h∗ S(t)) + tX i=0 wi  ˆdH∆H(Si, SS) + 4 s 2d log(2m) + log 2 δ m + 2γi   + 2C, with probability at least 1 − δ, where C follows Theorem 1 and γi = minh∈H{ϵi(h(t)) + ϵS(h(t))}. Proof. For the empirical source error on DS of hypothesis h at time t, similar to Theorem 1, we apply Lemma 6, Eq. 23 to get ϵS(ˆh(t)) ≤ ϵw(ˆh(t)) + tX i=0 wi  1 2 ˆdH∆H(Si, SS) + 2 s 2d log(2m) + log 2 δ m + γi   ≤ ˆϵw(ˆh(t)) + vuut  tX i=0 w2 i λi !\u0012d log(2N) − log(δ) 2N \u0013 + tX i=0 wi  1 2 ˆdH∆H(Si, SS) + 2 s 2d log(2m) + log 2 δ m + γi   ≤ ˆϵw(h∗ S(t)) + vuut  tX i=0 w2 i λi !\u0012d log(2N) − log(δ) 2N \u0013 + tX i=0 wi  1 2 ˆdH∆H(Si, SS) + 2 s 2d log(2m) + log 2 δ m + γi   ≤ ϵw(h∗ S(t)) + 2 vuut  tX i=0 w2 i λi !\u0012d log(2N) − log(δ) 2N \u0013 + tX i=0 wi  1 2 ˆdH∆H(Si, SS) + 2 s 2d log(2m) + log 2 δ m + γi   ≤ ϵS(h∗ S(t)) + 2 vuut  tX i=0 w2 i λi !\u0012d log(2N) − log(δ) 2N \u0013 + 2 tX i=0 wi  1 2 ˆdH∆H(Si, SS) + 2 s 2d log(2m) + log 2 δ m + γi   32Published as a conference paper at ICLR 2024 = ϵS(h∗ S(t)) + tX i=0 wi  ˆdH∆H(Si, SS) + 4 s 2d log(2m) + log 2 δ m + 2γi   + 2C with probability of at least 1 − δ, where C = r\u0010Pt i=0 w2 i λi \u0011\u0010 d log(2N)−log(δ) 2N \u0011 and γi = minh∈H{ϵi(h(t)) + ϵS(h(t))}. This completes the proof. Corollary 4. At time step t, for ATTA data domains Dϕ,S(t), Ute(1), Ute(2), ··· , Ute(t), suppose that ˆh(t) ∈ Hminimizes ˆϵw′(h(t)) under identical conditions to Theorem 2. Let’s denote the source error upper bound with |ϵS(ˆh(t)) − ϵS(h∗ S(t))| ≤EBS(w, λ, N, t). Let w′ and λ′ be the weight and sample ratio vectors when Dϕ,S(t) is not included, i.e., w′ and λ′ s.t. w′ 0 = λ′ 0 = 0 . If ˆdH∆H(DS, Dϕ,S(t)) < ˆdH∆H(DS, St i=1 Ute(i)), then for any λ ̸= λ′, there exists w s.t. EBS(w, λ, N, t) < EBS(w′, λ′, N, t). (30) Proof. From Theorem 1, considering the test-time data as a single test domain, we can derive the bound for the source error on DS: |ϵS(ˆh(t)) − ϵS(h∗ S(t))| ≤EBS(w, λ, N, t) = w0( ˆdH∆H(S0, SS) + 4 s 2d log(2m) + log 2 δ m + 2γ) + (1 − w0)( ˆdH∆H(SS, ST ) + 4 s 2d log(2m) + log 2 δ m + 2γ′) + 2 s w2 0 λ0 + (1 − w0)2 1 − λ0 r d log(2N) − log(δ) 2N , where ST is sampled fromSt i=1 Ute(i), γ = minh∈H{ϵ0(h(t))+ϵS(h(t))}, and γ′ = minh∈H{ϵT (h(t))+ ϵS(h(t))}. We have s w2 0 λ0 + (1 − w0)2 1 − λ0 = s (w0 − λ0)2 λ0(1 − λ0) + 1 ≥ 1, (31) where the equality and the minimum value are obtained if and only if w0 = λ0. When Dϕ,S(t) is not included,i.e., with the weight and sample ratio vectorsw′ and λ′ s.t. w′ 0 = λ′ 0 = 0, using the empirical gap term B = 2 q d log(2N)−log(δ) 2N , we have EBS(w′, λ′, N, t) = ˆdH∆H(SS, ST ) + 4 s 2d log(2m) + log 2 δ m + 2γ′ + s w2 0 λ0 + (1 − w0)2 1 − λ0 B = ˆdH∆H(SS, ST ) + 4 s 2d log(2m) + log 2 δ m + 2γ′ + B. When Dϕ,S(t) is included with λ0 ̸= 0, EBS(w, λ, N, t) = w0( ˆdH∆H(S0, SS) + 4 s 2d log(2m) + log 2 δ m + 2γ) + (1 − w0)( ˆdH∆H(SS, ST ) + 4 s 2d log(2m) + log 2 δ m + 2γ′) + s w2 0 λ0 + (1 − w0)2 1 − λ0 B ≤ w0( ˆdH∆H(S0, SS) + 4 s 2d log(2m) + log 2 δ m + 2γ) + (1 − w0)( ˆdH∆H(SS, ST ) + 4 s 2d log(2m) + log 2 δ m + 2γ′) + B, 33Published as a conference paper at ICLR 2024 Algorithm 2 INCREMENTAL CLUSTERING (IC) Require: Given previously selected anchors, new unlabeled samples, and the cluster budget as Danc, Unew, and NC . Global anchor weights wanc = (wanc 1 , . . . , wanc |Danc|)⊤. 1: For simplicity, we consider anchor weights wanc as a global vector. 2: function IC(Danc, Unew, NC ) 3: wsp ← Concat(wanc, 1⊤ |Unew|) ▷ Assign all new samples with weight 1. 4: Φ ← Extract the features from the penultimate layer of model f on x ∈ Danc ∪ Unew in order. 5: clusters ← Weighted-K-Means(Φ, wsp, NC) 6: new_clusters ← {clusteri | ∀clusteri ∈ clusters, ∀x ∈ Danc, x /∈ clustersi} 7: Xnew_anchors ← {the closest sample x to the centroid of clusteri | ∀clusteri ∈ new_clusters} 8: Xanchors ← {x ∈ Danc} ∪Xnew_anchors 9: wanc ← Concat(wanc, 0⊤ |Xnew_anchors|) ▷ Initialize new anchor weights. 10: for wanc i ∈ wanc, wanc i ← wanc i + # sample of clusterj # anchor in clusterj , wanc i ∈ clusterj ▷ Weight accumulation. 11: Return Xanchors 12: end function where the minimum value can be obtained with condition w0 = λ0 ̸= 0. In practical learning scenarios, we generally assume adaptation tasks are solvable; therefore, there should be a prediction function that performs well on two distinct domains. In this case, γ and γ′ should be relatively small, so we can assume γ ≈ γ′. If ˆdH∆H(S0, SS) < ˆdH∆H(SS, ST ), then we have EBS(w, λ, N, t)min = w0( ˆdH∆H(S0, SS) + 4 s 2d log(2m) + log 2 δ m + 2γ) + (1 − w0)( ˆdH∆H(SS, ST ) + 4 s 2d log(2m) + log 2 δ m + 2γ′) + B < ˆdH∆H(SS, ST ) + 4 s 2d log(2m) + log 2 δ m + 2γ′ + B = EBS(w′, λ′, N, t). Therefore, we derive EBS(w, λ, N, t)min < EBS(w′, λ′, N, t). (32) This completes the proof. F I NCREMENTAL CLUSTERING F.1 A LGORITHM DETAILS We provide the detailed algorithm for incremental clustering as Alg. 2. F.2 V ISUALIZATION To better illustrate the incremental clustering algorithm, we provide visualization results on PACS to demonstrate the process. As shown in Fig. 3, the initial step of IC is a normal K-Means clustering step, and ten anchors denoted as \"X\" are selected. The weights of all samples in a clusters is aggregated into the corresponding anchor’s weight. Therefore, these ten samples (anchors) are given larger sizes visually (i.e., larger weights) than that of other new test samples in the first IC step (Fig. 4). During the first IC step, several distributions are far away from the existed anchors and form clusters 1,7,9 and 10, which leads to 4 new selected anchors. While the number of cluster centroid is only increased by 1, 4 of the existing anchors are clustered into the same cluster 8 (purple). Thus IC produces 4 new anchors instead of 1. Similarly, in the second IC step (Fig. 5), the new streaming-in test samples introduce a new distribution; IC produces 3 new clusters (4, 8, and 11) and the corresponding number of anchors to cover them. The number of centroid is only increased by 1, which implies that there are two original-cluster-merging events. More IC step visualization results are provided in Fig. 6 and 7. 34Published as a conference paper at ICLR 2024 Figure 3: Initial IC step: normal clustering. Left: Clustering results. Right: Selecting new anchors. Figure 4: The first IC step. Left: Weighted clustering results. Right: Selecting new anchors. Figure 5: The second IC step. Left: Weighted clustering results. Right: Selecting new anchors. 35Published as a conference paper at ICLR 2024 Figure 6: The third IC step. Left: Weighted clustering results. Right: Selecting new anchors. Figure 7: The fourth IC step. Left: Weighted clustering results. Right: Selecting new anchors. 36Published as a conference paper at ICLR 2024 G E XPERIMENT DETAILS In this section, we provide more experimental details including the details of the datasets and training settings. G.1 D ETAILS ABOUT THE DATASETS We adopt datasets PACS, VLCS, and Office-Home from DomainBed (Gulrajani and Lopez-Paz, 2020) with the same domain splits. All available licenses are mentioned below. • PACS (Li et al., 2017) includes four domains: art, cartoons, photos, and sketches. PACS is a 7-class classification dataset with 9,991 images of dimension (3, 224, 224). • VLCS (Fang et al., 2013) contains photographic domains: Caltech101, LabelMe, SUN09, and VOC2007. This dataset includes 10,729 images of dimension (3, 224, 224) with 5 classes. • Office-Home (Venkateswara et al., 2017) is a 65-class dataset, including domains: art, clipart, product, and real. VLCS includes 10,729 images of dimension (3, 224, 244). (License) • Tiny-ImageNet-C is a 200-class dataset, including 15 corrupt types. Tiny-ImageNet-C includes 150,000 images of dimension (3, 224, 244). Since the class number 200 is less than ImageNet (1000), the model’s last layer classifier needs to be adapted. In this work, we use the brightness corruption domain to adapt. In the source pretraining phase, we adopt the most ImageNet-like domain as our source domain. For PACS and Office-Home, we use domains \"photos\" and \"real\" as the source domains, respectively, while for VLCS, Caltech101 is assigned to apply the source pretraining. We freeze the random seeds to generate the sample indices order for the two test data streams, namely, the domain-wise data stream and the random data stream. For PACS, the domain-wise data stream inputs samples from domain art, cartoons, to sketches, while we shuffle all samples from these three domains in the random data stream. For VLCS, we stream the domains in the order: LabelMe, SUN09, and VOC2007, as the domain-wise data stream. For Office-Home, the domain-wise data stream order becomes art, clipart, and product. G.2 T RAINING AND OPTIMIZATION SETTINGS In this section, we extensively discuss the model architectures, optimization settings, and method settings. G.2.1 A RCHITECTURES PACS & VLCS. We adopt ResNet-18 as our model encoder followed by a linear classifier. The initial parameters of ResNet-18 are ImageNet pre-trained weights. In our experiment, we remove the Dropout layer since we empirically found that using the Dropout layer might degrade the optimization process when the sample number is small. The specific implementation of the network is closely aligned with the implementation in DomainBed (Gulrajani and Lopez-Paz, 2020). Office-Home. We employ ResNet-50 as our model encoder for Office-Home. Except for the architecture, the other model settings are aligned with the ResNet-18. Tiny-ImageNet-C ResNet-18 is adapted from ImageNet to Tiny-ImageNet-C by training the last linear layer. G.2.2 T RAINING & OPTIMIZATION In this section, we describe the training configurations for both the source domain pre-training and test-time adaptation procedures. Source domain pre-training. For the PACS and VLCS datasets, models are fine-tuned on the selected source domains for 3,000 iterations. The Adam optimizer is utilized with a learning rate 37Published as a conference paper at ICLR 2024 of 10−4. In contrast, for the Office-Home dataset, the model is fine-tuned for a longer duration of 10,000 iterations with a slightly adjusted learning rate of 5 × 10−5. Test-time adaptation. For test-time adaptation across PACS and VLCS, the pre-trained source model is further fine-tuned using the SGD optimizer with a learning rate of 10−3. While on Office-Home and Tiny-ImageNet-C, a learning rate of 10−4 is adopted. For all TTA baselines, barring specific exceptions, we faithfully adhere to the original implementation settings. A noteworthy exception is the EATA method, which requires a cosine similarity threshold. The default threshold of the original EATA implementation was not suitable for the three datasets used in our study, necessitating an adjustment. We empirically set this threshold to 0.5 for training. Unlike Tent and SAR, which only require the optimization of batch normalization layers (Santurkar et al., 2018), SimATTA allows the training of all parameters in the networks. In experiments, we use a tolerance count (tol) to control the training process. SimATTA will stop updating once the loss does not descrease for more than 5 steps. However, for Tiny-ImageNet-C, SimATTA uses ‘steps=10‘ for time comparisons since other methods apply at most 10 steps. G.2.3 M ETHOD SETTINGS Tent. In our experiments, we apply the official implementation of Tent1. Specifically, we evaluate Tent with 1 test-time training step and 10 steps, respectively. EATA.Our EATA implementation follows its official code2. In our experiments, EATA has 2000 fisher training samples, E0 = 0.4 × log(# class), ϵ <0.5. CoTTA. For CoTTA, we strictly follow all the code and settings from its official implementation3. SAR. With SAR’s official implementation4, we set E0 = 0 .4 × log(# class) and e0 = 0 .1 in our experiments. ADA baselines. For ADA baselines, we follow the architecture of the official implementation of CLUE (Prabhu et al., 2021)5. SimATTA Implementation. Our implementation largely involves straightforward hyperparameter settings. The higher entropy bound eh = 10−2 should exceed the lower entropy bound el, but equal values are acceptable. Empirically, the lower entropy bound el can be set to 10−3 for VLCS and Office-Home, or 10−4 for PACS. The choice of el is largely dependent on the number of source-like samples obtained. A lower el may yield higher-accuracy low-entropy samples, but this could lead to unstable training due to sample scarcity. Though experimentation with different hyperparameters is encouraged, our findings suggest that maintaining a non-trivial number of low-entropy samples and setting an appropriateλ0 are of primary importance. If λ0 < 0.5, CF may ensue, which may negate any potential improvement. Regarding the management of budgets, numerous strategies can be adopted. In our experiments, we utilized a simple hyperparameter k, varying from 1 to 3, to regulate the increasing rate of budget consumption. This strategy is fairly elementary and can be substituted by any adaptive techniques. G.3 S OFTWARE AND HARDWARE We conduct our experiments with PyTorch (Paszke et al., 2019) and scikit-learn (Pedregosa et al., 2011) on Ubuntu 20.04. The Ubuntu server includes 112 Intel(R) Xeon(R) Gold 6258R CPU @2.70GHz, 1.47TB memory, and NVIDIA A100 80GB PCIe graphics cards. The training process costs graphics memory less than 10GB, and it requires CPU computational resources for scikit-learn K-Means clustering calculations. Our implementation also includes a GPU-based PyTorch K-Means method for transferring calculation loads from CPUs to GPUs. However, for consistency, the results of our experiments are obtained with the original scikit-learn K-Means implementation. 1https://github.com/DequanWang/tent 2https://github.com/mr-eggplant/EATA 3https://github.com/qinenergy/cotta 4https://github.com/mr-eggplant/SAR 5https://github.com/virajprabhu/CLUE 38Published as a conference paper at ICLR 2024 Figure 8: Target loss surface on 2000 samples without source pre-training. The red points denote the loss minimum for a fixed λ0. The orange line denote the place where w0 = λ0. Figure 9: Target loss surface on 2000 samples with source pre-training. H E MPIRICAL VALIDATIONS FOR THEORETICAL ANALYSIS In this section, we undertake empirical validation of our learning theory, which encompasses multiple facets awaiting verification. In contemporary computer vision fields, pre-trained models play a pivotal role, and performance would significantly decline without the use of pre-trained features. The learning theory suggests that given the vast VC-dimension of complete ResNets, without substantial data samples, the training error cannot be theoretically tight-bounded. However, we show empirically in the following experiments that fine-tuning pre-trained models is behaviorally akin to training a model with a low VC-dimension. Training on 2000 Samples Without Source Domain Pre-training. For an ImageNet pre-trained ResNet-18 model, we trained it using 2000 samples from the PACS dataset. To ascertain the optimal value w∗ 0 in Equation 4, we trained multiple models for different w0 and λ0 pairings. For each pair, we derived the target domain loss (from art, cartoons, and sketches) post-training and plotted this loss on the z-axis. With w0 and λ0 serving as the xy-axes, we drafted the target domain loss ϵT surface in Figure 8. As the results show, given a λ0, the optimal w∗ 0 typically aligns with the line λ0 = w0, with a slight downward shift, which aligns with Equation 4. 39Published as a conference paper at ICLR 2024 Figure 10: Target loss surface on 500 samples with source pre-training. Figure 11: Source loss surface on 500 samples with source pre-training. 40Published as a conference paper at ICLR 2024 Figure 12: Target and source loss surface on 500 samples with source pre-training. Table 6: TTA comparisons on Office-Home. This table includes the two data stream settings mentioned in the dataset setup and reports performances in accuracy. Results that outperform all TTA baselines are highlighted in bold font. N/A denotes the adaptations are not applied on the source domain. Office-Home Domain-wise data stream Post-adaptation Random data stream Post-adaptation R →A→ →C→ →P R A C P 1 2 3 4 R A C P BN w/o adapt 93.78 42.93 37.62 59.90 93.78 42.93 37.62 59.90 46.82 46.82 46.82 46.82 93.78 42.93 37.62 59.90BN w/ adapt 92.38 49.69 39.43 63.53 92.38 49.69 39.43 63.53 50.88 50.88 50.88 50.88 92.38 49.69 39.43 63.53 Tent (steps=1) N/A 49.61 39.31 63.87 92.47 49.57 39.89 63.89 49.95 50.27 50.23 52.06 92.40 49.24 39.68 63.98Tent (steps=10) N/A 49.61 39.04 61.41 87.08 44.79 38.37 60.49 50.05 49.31 48.74 47.79 85.31 42.85 37.89 58.71EATA N/A 49.65 39.04 63.53 91.60 49.61 38.65 63.48 49.73 50.27 49.45 51.07 91.05 49.11 38.26 62.99CoTTA N/A 49.61 38.76 61.84 87.81 44.95 35.92 59.04 49.84 49.84 48.95 50.43 86.99 43.68 34.73 57.56SAR (steps=1) N/A 49.65 39.24 63.53 92.45 49.73 39.36 63.69 49.84 50.05 49.91 51.67 92.38 49.57 39.50 63.87SAR (steps=10) N/A 49.53 38.81 61.50 88.94 46.15 37.04 59.41 50.09 50.30 49.77 49.22 89.14 46.23 36.31 59.45 SimATTA (B ≤300) N/A 56.20 48.38 71.66 95.75 60.07 52.62 74.70 58.57 60.88 62.91 63.67 95.89 62.01 54.98 74.70SimATTA (B ≤500) N/A 58.71 51.11 74.36 96.03 62.05 57.41 76.98 58.85 62.63 63.41 64.31 95.91 63.78 57.87 77.09 Training on 2000 Samples with Source Domain Pre-training. To further assess the effects of source pre-training, we repeated the same experiment on a source pre-trained ResNet-18. The results are depicted in Figure 9. This experiment provides empirical guidance on selecting w0 in source domain pre-trained situations. The findings suggest that the optimal w∗ 0 non-trivially shifts away from the line λ0 = w0 towards lower-value regions. Considering the source pre-training process as using a greater quantity of source domain samples, it implies that when the number of source samples greatly exceeds target samples, a lower w0 can enhance target domain results. Training on 500 Samples with Source Domain Pre-training. We proceed to fine-tune the source domain pre-trained ResNet-18 using only 500 samples, thereby simulating active TTA settings. We train models with various w0 and λ0 pairings, then graph the target domain losses, source domain losses, and the combined losses. As shown in Figure 10, the target losses still comply with our theoretical deductions where the local minima are close to the line λ0 = w0 and marginally shift towards lower values. Considering the challenge of CF, the source domain results in Figure 11 suggest a reverse trend compared to the target domain, where lower λ0 and w0 values yield superior target domain results but inferior source domain results. Thus, to curb CF, the primary strategy is to maintain a relatively higher λ0. When considering both target and source domains, a balance emerges as depicted in Figure 12. The global minimum is located in the middle region, demonstrating the trade-off between the target domain and source domain performance. I A DDITIONAL EXPERIMENT RESULTS In this section, we provide additional experiment results. The Office-Home results and ablation studies will be presented in a similar way as the main paper. In the full results Sec. I.3, we will post more detailed experimental results with specific budget numbers and intermediate performance during the test-time adaptation. 41Published as a conference paper at ICLR 2024 Table 7: Comparisons to ADA baselines on Office-Home. The source domain is denoted as \"(S)\" in the table. Results are average accuracies with standard deviations). Office-Home R (S) A C P Random (B = 300) 95.04 (0.20) 57.54 (1.16) 53.43 (1.17) 73.46 (0.97) Entropy (B = 300) 94.39 (0.49) 61.21 (0.71) 56.53 (0.71) 72.31 (0.28) Kmeans (B = 300) 95.09 (0.14) 57.37 (0.90) 51.74 (1.34) 71.81 (0.39) CLUE (B = 300) 95.20 (0.23) 60.18 (0.98) 58.05 (0.43) 73.72 (0.70) Ours (B ≤300) 95.82 (0.07) 61.04 (0.97) 53.80 (1.18) 74.70 (0.00) I.1 R ESULTS ON OFFICE -HOME We conduct experiments on Office-Home and get the test-time performances and post-adaptation performances for two data streams. As shown in Tab. 6, SimATTA can outperform all TTA baselines with huge margins. Compared to ADA baselines under the source-free settings, as shown in Tab. 7, SimATTA obtains comparable results. I.2 A BLATION STUDIES Figure 13: Ablation study on PACS and VLCS.\"IC=0\" denotes removing incremental clustering (IC) selection. \"LE=0\" denotes removing the low-entropy (LE) sample training. Domain-wise stream and random stream are applied on first and second rows, respectively. The accuracy values are averaged across all splits/domains. In this section, we explore three variations of our method to examine the individual impacts of its components. The first variant replaces the incremental clustering selection with entropy selection, 42Published as a conference paper at ICLR 2024 where only the samples with the highest entropy are chosen. The second variant eliminates low- entropy sample training. The third variation combines the first and second variants. We perform this ablation study on the PACS and VLCS as outlined in Fig. 13. We denote the use of incremental clustering (IC) and low-entropy training (LE) respectively as IC=1 and LE=1. The experiments essentially reveals the effectiveness of incremental clustering and low-entropy- sample training. As we have detailed in Sec. 3.2, these techniques are designed to to select informative samples, increase distribution coverage, and mitigate catastrophic forgetting. These designs appositely serve the ATTA setting where the oracle has costs and the budget is limited. Therefore, their effectiveness is prominent particularly when the budget is small. As the results show, when the budget B ≤100 or B ≤300, removing the components observably impairs performances. When B gets large, more active samples cover a larger distribution; thus the performance gap from random selection and informative selection gets smaller. In the extreme case where B → ∞, all samples are selected and thus the superiority of our meticulously-designed techniques are not manifested. Specifically, our analysis yields several insights. First, SimATTA (LE=1, IC=1) comprehensively outperforms other variants on both datasets, different streams, and different budgets. Second, variants without low-entropy training (LE=0, IC=0/1) easily fail to produce stable results (e.g., domain-wise stream in VLCS). Third, SimATTA’s performance surpasses this variant on PACS’s domain-wise stream clearly especially when the budgets are low. This indicates these variants fail to retrieve the most informative style shift (PACS’s shifts) samples, which implies the advantage of incremental clustering when the budget is tight. In addition, these results show that IC has its unique advantage on domain-wise streams where distributions change abruptly instead of random streams. Therefore, compared to PACS’s domain- wise stream results, the reason for the smaller performance improvement of SimATTA over the variant (LE=1, IC=0) on VLCS’s domain-wise stream is that images in VLCS are all photos that do not include those severe style shifts in PACS (i.e., art, cartoons, and sketches). That is, when the shift is not severe, we don’t need IC to cover very different distributions, and selecting samples using entropy can produce good results. In brief, IC is extraordinary for severe distribution shifts and quick adaptation. It is worth mentioning that low budget comparison is essential to show the informative sample retrieval ability, since as the budget increases, all AL techniques will tend to perform closely. I.3 C OMPLETE EXPERIMENT RESULTS We provide complete experimental results in this section. As shown in Tab. 8, we present the full results for two data streams. The test-time adaptation accuracies are shown in the \"Current domain\" row, while the \"Budgets\" row denotes the used budget by the end of the domain. The rest four rows denote the four domain test results by the end of the real-time adaptation of the current domain, where the first column results are the test accuracy before the test-time adaptation phase. N/A represents \"do not apply\". Table 8: Tent (steps=1) on PACS. Tent (steps=1) Domain-wise data stream Random data stream P →A→ → C→ → S 1 → → 2→ → 3→ → 4→ Current domain N/A 67.29 64.59 44.67 56.35 54.09 51.83 48.58 Budgets N/A N/A N/A N/A N/A N/A N/A N/A P 99.70 98.68 98.38 97.60 98.56 98.08 97.72 97.19 A 59.38 69.09 68.95 66.85 68.07 67.33 65.58 63.53 C 28.03 64.04 65.19 64.08 64.85 65.19 62.97 60.75 S 42.91 53.65 47.39 42.58 54.57 49.83 44.13 41.56 J C HALLENGES AND PERSPECTIVES Despite advancements, test-time adaptation continues to pose considerable challenges. As previously discussed, without supplementary information and assumptions, the ability to guarantee model generalization capabilities is limited. However, this is not unexpected given that recent progress 43Published as a conference paper at ICLR 2024 Table 9: Tent (steps=10) on PACS. Tent (steps=10) Domain-wise data stream Random data stream P →A→ → C→ → S 1 → → 2→ → 3→ → 4→ Current domain N/A 67.38 57.85 20.23 47.36 31.01 22.84 20.33 Budgets N/A N/A N/A N/A N/A N/A N/A N/A P 99.70 95.45 87.43 62.63 93.83 81.32 65.39 50.78 A 59.38 64.94 55.03 34.52 55.32 40.28 28.27 23.68 C 28.03 55.89 56.70 40.57 54.52 39.68 27.22 20.95 S 42.91 36.96 26.27 13.59 32.25 23.16 20.95 19.62 Table 10: EATA on PACS. EATA Domain-wise data stream Random data stream P →A→ → C→ → S 1 → → 2→ → 3→ → 4→ Current domain N/A 67.04 64.72 50.27 57.31 56.06 58.17 59.78 Budgets N/A N/A N/A N/A N/A N/A N/A N/A P 99.70 98.62 98.50 98.62 98.68 98.62 98.50 98.62 A 59.38 68.90 68.16 66.50 68.65 68.95 69.34 69.63 C 28.03 63.74 65.36 62.46 65.19 66.00 65.57 65.70 S 42.91 54.01 52.89 48.18 55.71 55.64 54.09 54.26 Table 11: CoTTA on PACS. CoTTA Domain-wise data stream Random data stream P →A→ → C→ → S 1 → → 2→ → 3→ → 4→ Current domain N/A 65.48 62.12 53.17 56.06 54.33 57.16 57.42 Budgets N/A N/A N/A N/A N/A N/A N/A N/A P 99.70 98.68 98.62 98.62 98.62 98.62 98.56 98.62 A 59.38 65.82 65.87 65.48 66.02 65.87 66.31 65.97 C 28.03 62.63 63.05 63.10 63.01 62.88 63.01 62.97 S 42.91 53.88 54.03 53.78 54.67 55.31 55.10 54.62 Table 12: SAR (steps=1) on PACS. SAR (steps=1) Domain-wise data stream Random data stream P →A→ → C→ → S 1 → → 2→ → 3→ → 4→ Current domain N/A 66.75 63.82 49.58 56.78 56.35 56.68 56.70 Budgets N/A N/A N/A N/A N/A N/A N/A N/A P 99.70 98.68 98.50 98.32 98.74 98.56 98.50 98.44 A 59.38 68.02 68.07 66.94 67.87 68.65 68.55 68.16 C 28.03 62.84 64.97 62.93 63.82 64.89 64.46 64.38 S 42.91 53.47 52.07 45.74 54.92 55.46 53.68 52.53 Table 13: SAR (steps=10) on PACS. SAR (steps=10) Domain-wise data stream Random data stream P →A→ → C→ → S 1 → → 2→ → 3→ → 4→ Current domain N/A 69.38 68.26 49.02 53.51 51.15 51.78 45.60 Budgets N/A N/A N/A N/A N/A N/A N/A N/A P 99.70 98.20 95.39 96.47 97.13 97.78 97.72 94.13 A 59.38 72.36 66.60 62.16 62.74 64.94 66.11 56.64 C 28.03 63.44 68.30 56.19 59.77 61.73 62.03 56.02 S 42.91 53.37 44.59 54.62 41.00 49.66 48.79 36.37 44Published as a conference paper at ICLR 2024 Table 14: SimATTA (B ≤300) on PACS. SimATTA (B ≤300) Domain-wise data stream Random data stream P →A→ → C→ → S 1 → → 2→ → 3→ → 4→ Current domain N/A 76.86 70.90 75.39 69.47 76.49 82.45 82.22 Budgets N/A 75 145 223 66 142 203 267 P 99.70 98.44 98.86 98.80 97.96 98.68 99.04 98.98 A 59.38 80.71 82.32 84.47 73.97 80.52 81.10 84.91 C 28.03 48.12 82.00 82.25 72.35 81.06 83.36 83.92 S 42.91 32.78 56.25 81.52 79.49 83.10 84.78 86.00 Table 15: SimATTA (B ≤500) on PACS. SimATTA (B ≤500) Domain-wise data stream Random data stream P →A→ → C→ → S 1 → → 2→ → 3→ → 4→ Current domain N/A 77.93 76.02 76.30 68.46 78.22 80.91 85.49 Budgets N/A 121 230 358 102 221 343 425 P 99.70 98.92 98.86 98.62 98.20 99.46 99.10 99.16 A 59.38 87.01 87.60 88.33 73.39 79.20 84.91 86.67 C 28.03 54.78 83.96 83.49 68.43 74.40 84.22 84.77 S 42.91 46.37 63.53 83.74 81.34 81.04 86.66 87.71 Table 16: Tent (steps=1) on VLCS. Tent (steps=1) Domain-wise data stream Random data stream C →L→ → S→ → V 1 → → 2→ → 3→ → 4→ Current domain N/A 38.55 34.40 53.88 44.85 44.29 47.38 44.98 Budgets N/A N/A N/A N/A N/A N/A N/A N/A C 100.00 84.81 85.44 84.73 84.95 85.16 85.80 85.30 L 33.55 40.02 43.11 43.86 39.68 41.98 43.11 43.49 S 41.10 33.39 35.41 33.61 36.29 37.90 38.27 37.81 V 49.08 53.20 54.06 53.11 53.76 54.18 53.76 53.35 Table 17: Tent (steps=10) on VLCS. Tent (steps=10) Domain-wise data stream Random data stream C →L→ → S→ → V 1 → → 2→ → 3→ → 4→ Current domain N/A 45.41 31.44 32.32 46.13 42.31 43.51 39.48 Budgets N/A N/A N/A N/A N/A N/A N/A N/A C 100.00 73.07 48.34 42.54 74.13 62.19 56.54 52.01 L 33.55 46.61 38.44 37.65 44.88 45.93 43.41 40.32 S 41.10 31.75 28.82 27.79 35.37 36.14 35.28 33.64 V 49.08 48.05 40.14 33.12 50.50 44.49 42.48 40.37 Table 18: EATA on VLCS. EATA Domain-wise data stream Random data stream C →L→ → S→ → V 1 → → 2→ → 3→ → 4→ Current domain N/A 37.24 33.15 52.58 43.77 42.48 43.34 41.55 Budgets N/A N/A N/A N/A N/A N/A N/A N/A C 100.00 85.16 85.02 84.10 84.73 84.52 84.10 83.32 L 33.55 37.16 37.24 37.69 37.09 36.78 36.90 36.67 S 41.10 33.39 33.49 32.39 33.33 32.54 31.84 31.47 V 49.08 51.87 52.16 52.49 52.07 52.43 52.64 52.55 45Published as a conference paper at ICLR 2024 Table 19: CoTTA on VLCS. CoTTA Domain-wise data stream Random data stream C →L→ → S→ → V 1 → → 2→ → 3→ → 4→ Current domain N/A 37.39 32.54 52.25 43.69 42.14 43.21 42.32 Budgets N/A N/A N/A N/A N/A N/A N/A N/A C 100.00 81.55 81.98 82.12 82.61 82.47 82.12 81.98 L 33.55 37.20 37.91 37.65 38.48 38.22 38.40 37.99 S 41.10 30.71 32.78 33.12 34.00 33.70 33.97 33.52 V 49.08 52.01 52.64 52.90 53.64 53.14 53.08 53.23 Table 20: SAR (steps=1) on VLCS. SAR (steps=1) Domain-wise data stream Random data stream C →L→ → S→ → V 1 → → 2→ → 3→ → 4→ Current domain N/A 36.18 34.43 52.46 43.64 43.04 44.20 41.93 Budgets N/A N/A N/A N/A N/A N/A N/A N/A C 100.00 84.31 84.17 83.96 85.09 85.23 85.23 85.09 L 33.55 35.62 38.29 39.72 38.55 39.34 40.21 40.70 S 41.10 33.24 36.41 36.53 34.37 35.62 36.29 36.44 V 49.08 51.75 52.61 52.37 52.90 52.75 53.05 53.02 Table 21: SAR (steps=10) on VLCS. SAR (steps=10) Domain-wise data stream Random data stream C →L→ → S→ → V 1 → → 2→ → 3→ → 4→ Current domain N/A 35.32 34.10 51.66 43.56 42.05 42.53 41.16 Budgets N/A N/A N/A N/A N/A N/A N/A N/A C 100.00 83.96 83.04 82.12 84.03 84.24 85.23 85.09 L 33.55 34.07 35.92 41.49 39.53 38.37 37.65 37.58 S 41.10 31.93 34.89 33.94 35.19 32.94 33.88 33.12 V 49.08 51.33 51.51 53.08 52.78 52.34 51.78 52.01 Table 22: SimATTA (B ≤300) on VLCS. SimATTA (B ≤300) Domain-wise data stream Random data stream C →L→ → S→ → V 1 → → 2→ → 3→ → 4→ Current domain N/A 62.61 65.08 74.38 62.33 69.33 73.20 71.93 Budgets N/A 79 175 272 71 135 208 262 C 100.00 99.51 98.52 99.93 99.86 99.79 100.00 99.93 L 33.55 68.11 69.92 69.50 62.61 66.64 68.45 69.43 S 41.10 55.24 68.89 66.67 65.54 69.29 71.79 72.46 V 49.08 66.08 70.94 77.34 73.79 76.87 78.82 80.39 Table 23: SimATTA (B ≤500) on VLCS. SimATTA (B ≤500) Domain-wise data stream Random data stream C →L→ → S→ → V 1 → → 2→ → 3→ → 4→ Current domain N/A 63.52 68.01 76.13 62.29 70.45 73.50 72.02 Budgets N/A 113 266 446 107 203 283 356 C 100.00 99.29 98.59 99.51 99.93 99.86 99.86 99.43 L 33.55 62.95 70.63 70.56 66.57 67.09 67.24 70.29 S 41.10 51.31 73.83 73.10 65.33 71.79 72.91 72.55 V 49.08 59.36 71.65 78.35 73.58 77.84 80.01 80.18 46Published as a conference paper at ICLR 2024 Table 24: Tent (steps=1) on Office-Home. Tent (steps=1) Domain-wise data stream Random data stream R →A→ → C→ → P 1 → → 2→ → 3→ → 4→ Current domain N/A 49.61 39.31 63.87 49.95 50.27 50.23 52.06 Budgets N/A N/A N/A N/A N/A N/A N/A N/A R 96.44 92.33 92.36 92.47 92.38 92.45 92.45 92.40 A 57.07 49.73 49.73 49.57 49.69 49.73 49.57 49.24 C 44.97 39.27 39.54 39.89 39.45 39.68 39.73 39.68 P 73.15 63.60 63.66 63.89 63.60 63.82 63.93 63.98 Table 25: Tent (steps=10) on Office-Home. Tent (steps=10) Domain-wise data stream Random data stream R →A→ → C→ → P 1 → → 2→ → 3→ → 4→ Current domain N/A 49.61 39.04 61.41 50.05 49.31 48.74 47.79 Budgets N/A N/A N/A N/A N/A N/A N/A N/A R 96.44 91.99 89.14 87.08 92.08 90.80 88.59 85.31 A 57.07 49.94 46.77 44.79 49.44 48.21 45.69 42.85 C 44.97 38.58 39.11 38.37 40.18 40.02 38.63 37.89 P 73.15 63.28 61.03 60.49 64.36 63.64 61.12 58.71 Table 26: EATA on Office-Home. EATA Domain-wise data stream Random data stream R →A→ → C→ → P 1 → → 2→ → 3→ → 4→ Current domain N/A 49.65 39.04 63.53 49.73 50.27 49.45 51.07 Budgets N/A N/A N/A N/A N/A N/A N/A N/A R 96.44 92.36 92.17 91.60 92.38 92.22 91.71 91.05 A 57.07 49.57 49.53 49.61 49.69 49.40 49.36 49.11 C 44.97 39.08 39.01 38.65 39.27 39.01 38.42 38.26 P 73.15 63.42 63.42 63.48 63.51 63.37 63.33 62.99 Table 27: CoTTA on Office-Home. CoTTA Domain-wise data stream Random data stream R →A→ → C→ → P 1 → → 2→ → 3→ → 4→ Current domain N/A 49.61 38.76 61.84 49.84 49.84 48.95 50.43 Budgets N/A N/A N/A N/A N/A N/A N/A N/A R 96.44 90.38 88.02 87.81 90.48 89.37 88.00 86.99 A 57.07 48.58 45.53 44.95 47.34 46.35 44.62 43.68 C 44.97 36.66 35.58 35.92 37.55 36.40 35.44 34.73 P 73.15 60.40 57.74 59.04 61.12 59.63 58.35 57.56 Table 28: SAR (steps=1) on Office-Home. SAR (steps=1) Domain-wise data stream Random data stream R →A→ → C→ → P 1 → → 2→ → 3→ → 4→ Current domain N/A 49.65 39.24 63.53 49.84 50.05 49.91 51.67 Budgets N/A N/A N/A N/A N/A N/A N/A N/A R 96.44 92.38 92.31 92.45 92.40 92.36 92.36 92.38 A 57.07 49.65 49.57 49.73 49.69 49.61 49.57 49.57 C 44.97 39.34 39.22 39.36 39.34 39.56 39.47 39.50 P 73.15 63.51 63.51 63.69 63.60 63.71 63.71 63.87 47Published as a conference paper at ICLR 2024 Table 29: SAR (steps=10) on Office-Home. SAR (steps=10) Domain-wise data stream Random data stream R →A→ → C→ → P 1 → → 2→ → 3→ → 4→ Current domain N/A 49.53 38.81 61.50 50.09 50.30 49.77 49.22 Budgets N/A N/A N/A N/A N/A N/A N/A N/A R 96.44 92.20 92.06 88.94 92.40 92.47 91.53 89.14 A 57.07 49.40 49.77 46.15 49.81 50.02 48.91 46.23 C 44.97 39.20 38.63 37.04 39.50 39.29 38.65 36.31 P 73.15 63.53 62.69 59.41 64.18 64.18 62.83 59.45 Table 30: SimATTA (B ≤300) on Office-Home. SimATTA (B ≤300) Domain-wise data stream Random data stream R →A→ → C→ → P 1 → → 2→ → 3→ → 4→ Current domain N/A 56.20 48.38 71.66 58.57 60.88 62.91 63.67 Budgets N/A 75 187 277 79 147 216 278 R 96.44 95.43 95.43 95.75 95.91 95.96 96.01 95.89 A 57.07 57.56 59.50 60.07 58.34 59.91 61.15 62.01 C 44.97 42.25 52.46 52.62 51.66 52.30 54.75 54.98 P 73.15 68.84 70.13 74.70 72.45 73.10 74.50 74.70 Table 31: SimATTA (B ≤500) on Office-Home. SimATTA (B ≤500) Domain-wise data stream Random data stream R →A→ → C→ → P 1 → → 2→ → 3→ → 4→ Current domain N/A 58.71 51.11 74.36 58.85 62.63 63.41 64.31 Budgets N/A 107 284 440 126 248 361 467 R 96.44 95.69 95.71 96.03 96.26 96.19 95.87 95.91 A 57.07 61.43 61.43 62.05 58.18 61.15 61.52 63.78 C 44.97 46.41 57.73 57.41 53.17 55.14 56.79 57.87 P 73.15 70.74 71.98 76.98 73.51 74.18 75.78 77.09 48Published as a conference paper at ICLR 2024 in deep learning heavily relies on large-scale data. Consequently, two promising paths emerge: establishing credible assumptions and leveraging additional information. Firstly, developing credible assumptions can lead to comprehensive comparisons across various stud- ies. Given that theoretical guarantees highlight the inherent differences between methods primarily based on the application limits of their assumptions, comparing these assumptions becomes critical. Without such comparative studies, empirical evaluations may lack precise guidance and explanation. Secondly, while we acknowledge the value of real-world data (observations), discussions surrounding the use of extra information remain pertinent. Considerations include the strategies to acquire this supplementary information and the nature of the additional data needed. Despite the myriad of works on domain generalization, domain adaptation, and test-time adaptation, a comprehensive survey or benchmark encapsulating the aforementioned comparisons remains an unmet need. Moreover, potential future directions for out-of-distribution generalization extend beyond domain generalization and test-time adaptation. One promising avenue is bridging the gap between causal inference and deep learning, for instance, through causal representation learning. In conclusion, our hope is that this work not only offers a novel practical setting and algorithm but also illuminates meaningful future directions and research methodologies that can benefit the broader scientific community. 49",
      "meta_data": {
        "arxiv_id": "2404.05094v1",
        "authors": [
          "Shurui Gui",
          "Xiner Li",
          "Shuiwang Ji"
        ],
        "published_date": "2024-04-07T22:31:34Z",
        "pdf_url": "https://arxiv.org/pdf/2404.05094v1.pdf"
      },
      "llm_extracted_info": {
        "main_contributions": "This paper addresses the limitations of existing Test-Time Adaptation (TTA) methods in handling significant domain distribution shifts by proposing Active Test-Time Adaptation (ATTA). ATTA integrates active learning within the fully TTA setting, leveraging limited labeled test instances to enhance performance with theoretical guarantees. The paper introduces SimATTA, a simple yet effective ATTA algorithm that employs real-time sample selection and sample entropy balancing to avoid catastrophic forgetting. Extensive experiments demonstrate that ATTA significantly improves performance over TTA methods while maintaining efficiency comparable to TTA and effectiveness similar to more demanding Active Domain Adaptation (ADA) methods.",
        "methodology": "The proposed ATTA framework involves continuously selecting informative instances from test batches for labeling and subsequent model learning. A formal definition of ATTA is provided, optimizing a pre-trained model by minimizing a combination of cross-entropy loss (for labeled samples) and an unsupervised learning loss (for unlabeled samples), subject to a labeling budget. Theoretical analysis provides learning bounds to guarantee mitigation of distribution shifts and prevention of catastrophic forgetting (CF). The core algorithm, SimATTA, incorporates balanced sample selections using selective entropy minimization to combat CF (by identifying 'source-like' low-entropy samples) and an incremental clustering technique to select informative high-entropy samples (anchors) from streaming data for active labeling, reducing redundancy and increasing distribution coverage. The model is fine-tuned on these pseudo-labeled low-entropy samples and actively labeled high-entropy anchors.",
        "experimental_setup": "The method's effectiveness was validated on multiple datasets: PACS, VLCS, Office-Home, and Tiny-ImageNet-C, covering various domain shifts and corruptions. For DomainBed datasets (PACS, VLCS, Office-Home), two data stream orders were used: domain-wise (sequential domains) and random (shuffled target domains). Baselines included source-only models (BN w/o adapt, BN w/ adapt), state-of-the-art TTA methods (Tent, EATA, CoTTA, SAR), and Active Domain Adaptation (ADA) methods (Random, Entropy, Kmeans, CLUE). Evaluation was based on accuracy, with comparisons focusing on efficiency and performance metrics. ResNet-18 (for PACS, VLCS, Tiny-ImageNet-C) and ResNet-50 (for Office-Home) models were used, pre-trained on ImageNet or specific source domains. Training parameters included Adam and SGD optimizers with specified learning rates and training iterations. Budget limits for labeled samples were set (e.g., B <= 300, B <= 500). Ablation studies were conducted to assess individual components like incremental clustering and low-entropy sample training.",
        "limitations": "The theoretical bounds can be loose with small batches of unlabeled test samples, as the VC-dimension of the hypothesis class (ResNet models) is large. While fine-tuning reduces this effect, it relies on the assumption that fine-tuning is equivalent to learning with a relatively small VC-dimension. The effectiveness of selective entropy minimization for CF prevention relies on the quality of the pre-trained model; training on incorrectly pseudo-labeled low-entropy samples may reinforce errors. It might not be cost-effective to spend annotation budgets on low-entropy samples, although correcting them could be an alternative. The paper also acknowledges that constructing a new setting involves complexities, and scaling to large models/datasets or specialized task-specific applications is not fully covered.",
        "future_research_directions": "Future research could involve developing alternative methods to prevent catastrophic forgetting in ATTA scenarios, especially since selective entropy minimization on low-entropy samples, while empirically effective, relies on pre-trained model quality and could reinforce errors if pseudo-labels are incorrect. Investigating whether correcting incorrectly predicted low-entropy samples is a viable alternative solution is also suggested. Another promising avenue is designing ATTA methods for large language models (LLMs), where retraining is prohibitively expensive and source data may be inaccessible. The work also generally points to bridging the gap between causal inference and deep learning through causal representation learning for out-of-distribution generalization."
      }
    },
    {
      "title": "EcoTTA: Memory-Efficient Continual Test-Time Adaptation via Self-Distilled Regularization",
      "abstract": "This paper presents a simple yet effective approach that improves continual\ntest-time adaptation (TTA) in a memory-efficient manner. TTA may primarily be\nconducted on edge devices with limited memory, so reducing memory is crucial\nbut has been overlooked in previous TTA studies. In addition, long-term\nadaptation often leads to catastrophic forgetting and error accumulation, which\nhinders applying TTA in real-world deployments. Our approach consists of two\ncomponents to address these issues. First, we present lightweight meta networks\nthat can adapt the frozen original networks to the target domain. This novel\narchitecture minimizes memory consumption by decreasing the size of\nintermediate activations required for backpropagation. Second, our novel\nself-distilled regularization controls the output of the meta networks not to\ndeviate significantly from the output of the frozen original networks, thereby\npreserving well-trained knowledge from the source domain. Without additional\nmemory, this regularization prevents error accumulation and catastrophic\nforgetting, resulting in stable performance even in long-term test-time\nadaptation. We demonstrate that our simple yet effective strategy outperforms\nother state-of-the-art methods on various benchmarks for image classification\nand semantic segmentation tasks. Notably, our proposed method with ResNet-50\nand WideResNet-40 takes 86% and 80% less memory than the recent\nstate-of-the-art method, CoTTA.",
      "full_text": "EcoTTA: Memory-Efficient Continual Test-time Adaptation via Self-distilled Regularization Junha Song1,2* , Jungsoo Lee 1, In So Kweon 2, Sungha Choi 1† 1Qualcomm AI Research‡, 2KAIST Abstract This paper presents a simple yet effective approach that improves continual test-time adaptation (TTA) in a memory- efficient manner. TTA may primarily be conducted on edge devices with limited memory, so reducing memory is cru- cial but has been overlooked in previous TTA studies. In addition, long-term adaptation often leads to catastrophic forgetting and error accumulation, which hinders apply- ing TTA in real-world deployments. Our approach con- sists of two components to address these issues. First, we present lightweight meta networks that can adapt the frozen original networks to the target domain. This novel archi- tecture minimizes memory consumption by decreasing the size of intermediate activations required for backpropaga- tion. Second, our novel self-distilled regularization controls the output of the meta networks not to deviate significantly from the output of the frozen original networks, thereby preserving well-trained knowledge from the source domain. Without additional memory, this regularization prevents er- ror accumulation and catastrophic forgetting, resulting in stable performance even in long-term test-time adaptation. We demonstrate that our simple yet effective strategy out- performs other state-of-the-art methods on various bench- marks for image classification and semantic segmentation tasks. Notably, our proposed method with ResNet-50 and WideResNet-40 takes 86% and 80% less memory than the recent state-of-the-art method, CoTTA. 1. Introduction Despite recent advances in deep learning [15, 24, 23, 22], deep neural networks often suffer from performance degra- dation when the source and target domains differ signifi- cantly [8, 43, 38]. Among several tasks addressing such domain shifts, test-time adaptation (TTA) has recently re- ceived a significant amount of attention due to its practi- cality and wide applicability especially in on-device set- *Work done during an internship at Qualcomm AI Research. †Corresponding author. ‡ Qualcomm AI Research is an initiative of Qualcomm Technologies, Inc. Memory (MB) CIFAR100-C  Error (%) CIFAR10-C  Error (%) ResNet-50 WideResNet-40 CoTTA SWR&NSP TTT++ Con6nual TENT Single domain TENT EATA CoTTA SWR&NSP TTT++ EATA         NOTE Memory (MB)   0 450 900 1350 1800 Param Ac6va6on 86% 72% 0 100 200 300 400 Param Ac6va6on TENT/EATA  CoTTA  Ours 80% 59% (a) (b) Con6nual TENT Single domain TENT             ResNet-50 WideResNet-40 Ours (K=4) Ours (K=5) ���������� ����������� Figure 1. (a) Memory cost comparison between TTA methods. The size of activations, not the parameters, is the primary mem- ory bottleneck during training. (b) CIFAR-C adaptation perfor- mance. We perform the continual online adaptation on CIFAR-C dataset. The x- and y-axis are the average error of all corruptions and the total memory consumption including the parameters and activations, respectively. Our approach, EcoTTA, achieves the best results while consuming the least amount of memory, where K is the model partition factor used in our method. tings [65, 42, 32, 16]. This task focuses on adapting the model to unlabeled online data from the target domain with- out access to the source data. While existing TTA methods show improved TTA per- formances, minimizing the sizes of memory resources have been relatively under-explored, which is crucial considering the applicability of TTA in on-device settings. For example, several studies [66, 42, 9] update entire model parameters 1 arXiv:2303.01904v4  [cs.CV]  23 May 2023TENT𝐷! EATA𝐷! weight regularization: freeze: update: meta networks (Ours) randomrestoration CoTTA Ours (EcoTTA)𝐷!𝐷!transformmovingaverage bnbnbnconv blockmain networksmain networkssource modelmain networksconv blockconv block teacher modelsource model𝐷!:unlabeledonlinetestdata …bn …bn Figure 2. Architecture for test-time adaptation. We illustrate TTA methods: TENT [65], EATA [50], CoTTA [66], and Ours (EcoTTA). TENT and EATA update multiple batch norm layers, in which large activations have to be stored for gradient calculation. In CoTTA, an entire network is trained with additional strategies for continual adaptation that requires a significant amount of both memory and time. In contrast, our approach requires a minimum size of activations by updating onlya few layers. Also, stable long-term adaptation is performed by our proposed regularization, named self-distilled regularization. to achieve large performance improvements, which may be impractical when the available memory sizes are limited. Meanwhile, several TTA approaches update only the batch normalization (BN) parameters [65, 50, 17] to make the optimization efficient and stable However, even updating only BN parameters is not memory efficient enough since the amount of memory required for training models signifi- cantly depends on the size of intermediate activations rather than the learnable parameters [4, 14, 69]. Throughout the paper, activations refer to the intermediate features stored during the forward propagation, which are used for gradi- ent calculations during backpropagation. Fig. 1 (a) demon- strates such an issue. Moreover, a non-trivial number of TTA studies assume a stationary target domain [65, 42, 9, 57], but the target do- main may continuously change in the real world (e.g., con- tinuous changes in weather conditions, illuminations, and location [8] in autonomous driving). Therefore, it is nec- essary to consider long-term TTA in an environment where the target domain constantly varies. However, there exist two challenging issues: 1) catastrophic forgetting [66, 50] and 2) error accumulation. Catastrophic forgetting refers to degraded performance on the source domain due to long- term adaptation to target domains [66, 50]. Such an issue is important since the test samples in the real world may come from diverse domains, including the source and tar- get domains [50]. Also, since target labels are unavailable, TTA relies on noisy unsupervised losses, such as entropy minimization [19], so long-term continual TTA may lead to error accumulation [75, 2]. To address these challenges, we propose memory- Efficient continual Test-Time Adaptation (EcoTTA), a sim- ple yet effective approach for 1) enhancing memory effi- ciency and 2) preventing catastrophic forgetting and error accumulation. First, we present a memory-efficient archi- tecture consisting of frozen original networks and our pro- posed meta networks attached to the original ones. During the test time, we freeze the original networks to discard the intermediate activations that occupy a significant amount of memory. Instead, we only adapt lightweight meta networks to the target domain, composed of only one batch normal- ization and one convolution block. Surprisingly, updating only the meta networks, not the original ones, can result in significant performance improvement as well as consider- able memory savings. Moreover, we propose a self-distilled regularization method to prevent catastrophic forgetting and error accumulation. Our regularization leverages the pre- served source knowledge distilled from the frozen original networks to regularize the meta networks. Specifically, we control the output of the meta networks not to deviate from the one extracted by the original networks significantly. No- tably, our regularization leads to negligible overhead be- cause it requires no extra memory and is performed in par- allel with adaptation loss, such as entropy minimization. Recent TTA studies require access to the source databe- fore model deployments[42, 9, 34, 1, 40, 50]. Similarly, our method uses the source data to warm up the newly attached meta networks for a small number of epochs before model deployment. If the source dataset is publicly available or the owner of the pre-trained model tries to adapt the model to a target domain, access to the source data is feasible [9]. Here, we emphasize that pre-trained original networks are frozen throughout our process, and our method is applicable to any pre-trained model because it is agnostic to the archi- tecture and pre-training method of the original networks. Our paper presents the following contributions: • We present novel meta networks that help the frozen original networks adapt to the target domain. This architecture significantly minimize memory consump- tion up to 86% by reducing the activation sizes of the original networks. • We propose a self-distilled regularization that controls the output of meta networks by leveraging the output of frozen original networks to preserve the source knowl- edge and prevent error accumulation. • We improve both memory efficiency and TTA perfor- mance compared to existing state-of-the-art methods on 1) image classification task ( e.g., CIFAR10/100-C and ImageNet-C) and 2) semantic segmentation task (e.g., Cityscapes with weather corruption) 22. Related Work Mitigating domain shift. One of the fundamental issues of DNNs is the performance degradation due to the domain shift between the train (i.e. source) and test (i.e. target) dis- tributions. Several research fields attempt to address this problem, such as unsupervised domain adaptation [64, 6, 53, 56, 46, 58] and domain generalization [76, 8]. In par- ticular, domain generalization aims to learn invariant rep- resentation so as to cover the possible shifts of test data. They simulate the possible shifts using a single or multiple source dataset [76, 74, 39] or force to minimize the depen- dence on style information [52, 8]. However, it is challeng- ing to handle all potential test shifts using the given source datasets [20]. Thus, instead of enhancing generalization ability during the training time, TTA [65] overcomes the domain shift by directly adapting to the test data. Test-time adaptation. Test-time adaptation allows the model to adapt to the test data ( i.e., target domain) in a source-free and online manner [33, 62, 65]. Existing works improve TTA performance with sophisticated designs of un- supervised loss [48, 72, 42, 9, 45, 57, 5, 16, 1, 3, 12, 59] or enhance the usability of small batch sizes [36, 70, 31, 51, 40] considering streaming test data. They focus on improv- ing the adaptation performance with a stationary target do- main (i.e., single domain TTA setup). In such a setting, the model that finished adaptation to a given target domain is reset to the original model pre-trained with the source do- main in order to adapt to the next target domain. Recently, CoTTA [66] has proposed continual TTA setup to address TTA under a continuously changing target do- main which also involves a long-term adaptation. This setup frequently suffers from error accumulation [75, 2, 63] and catastrophic forgetting [66, 35, 50]. Specifically, perform- ing a long-term adaptation exposes the model to unsuper- vised loss from unlabeled test data for a long time, so er- rors are accumulated significantly. Also, the model focuses on learning new knowledge and forgets about the source knowledge, which becomes problematic when the model needs to correctly classify the test sample as similar to the source distribution. To address such issues, CoTTA [66] randomly restores the updated parameters to the source one, while EATA [50] proposed a weight regularization loss. Efficient on-device learning. Since the edge device is likely to be memory constrained ( e.g., a Raspberry Pi with 512MB and iPhone 13 with 4GB), it is necessary to take account of the memory usage when deploying the models on the device [41]. TinyTL [4], a seminal work in on- device learning, shows that the activation size, not learn- able parameters, bottlenecks the training memory. Follow- ing this, recent on-device learning studies [4, 68, 69] target- ing fine-tuning task attempt to decrease the size of interme- diate activations. In contrast, previous TTA studies [65, 50] have overlooked these facts and instead focused on reduc- ing learnable parameters. This paper, therefore, proposes a method that not only reduces the high activation sizes re- quired for TTA, but also improves adaptation performance. 3. Approach Fig. 3 illustrates our simple yet effective approach which only updates the newly added meta networks on the tar- get domain while regularizing them with the knowledge distilled from the frozen original network. This section describes how such a design promotes memory efficiency and prevents error accumulation and catastrophic forgetting which are frequently observed in long-term adaptation. 3.1. Memory-efficient Architecture Prerequisite. We first formulate the forward and the back- ward propagation. Assume that the ith linear layer in the model consists of weight W and bias b, and the input and output features of this layer are fi and fi+1, respectively. Given that the forward propagation of fi+1 = fiW + b, the backward propagation from the i+1th layer to the ith layer, and the weight gradient are respectively formulated as: ∂L ∂fi = ∂L ∂fi+1 WT , ∂L ∂W = fT i ∂L ∂fi+1 . (1) Eq. (1) means that the learnable layers whose weight W need to be updated must store intermediate activations fi to compute the weight gradient. In contrast, the backward propagation in frozen layers can be accomplished without saving the activations, only requiring its weightW. Further descriptions are provided in Appendix A. TinyTL [4] shows that activations occupy the majority of the memory required for training the model rather than learnable parameters. Due to this fact, updating the entire model (e.g., CoTTA [66]) requires a substantial amount of memory. Also, updating only parameters in batch normal- ization (BN) layers (e.g., TENT [65] and EATA [50]) is not an effective approach enough since they still save the large intermediate activations for multiple BN layers. While pre- vious studies fail to reduce memory by utilizing large ac- tivations, this work proposes a simple yet effective way to reduce a significant amount of memory by discarding them. Before deployment. As illustrated in Fig. 3 (a, b), we first take a pre-trained model using any pre-training method. We divide the encoder of the pre-trained model into K number of parts and attach lightweight meta networks to each part of the original network. The details of how to divide the model into K number of parts are explained in the next sec- tion. One group of meta network composes of one batch normalization layer and one convolution block ( i.e., Conv- BN-Relu). Before the deployment, we pre-train the meta networks on the source dataset Ds for a small number of 3: backpropagation: freeze: update: meta networks (Ours)𝐷!:labeled source data𝐷\":unlabeled online test data Our proposed method (EcoTTA)Anypre-training method(a) partition (K=3)(b) attach and warm up meta networks Any pre-trained model (c) test-time adaptation Deploy=‖𝑥%#-𝑥#‖$ cross entropy 𝐷! K=3 entropy min. input convencoderclassifier 𝑥%#%$ 𝑥# convblockbn 𝑥%# 𝑥%#%$ 𝑥#bn 𝑥%# 𝐷\" self-distilled reg. convblock Figure 3. Overview of our approach. (a) The encoder of the pre-trained model is divided into K parts (i.e., model partition factor K). (b) Before deployment, the meta networks are attached to each part of the original networks and pre-trained with source dataset Ds. (c) After the model is deployed, only the meta networks are updated with unsupervised loss (i.e., entropy minimization) on target data Dt, while the original networks are frozen. To avoid error accumulation and catastrophic forgetting by the long-term adaptation, we regularize the output ˜xk of each group of the meta networks leveraging the output xk of the frozen original network, which preserves the source knowledge. epochs (e.g., 10 epochs for CIFAR dataset) while freezing the original networks. Such a warm-up process is com- pleted before the model deployment, similarly done in sev- eral TTA works [9, 34, 40, 50]. Note that we do not require source dataset Ds during test time. Pre-trained model partition. Previous TTA studies ad- dressing domain shifts [9, 48] indicate that updating shal- low layers is more crucial for improving the adaptation per- formance than updating the deep layers. Inspired by such a finding, given that the encoder of the pre-trained model is split into model partition factor K ( e.g., 4 or 5), we par- tition the shallow parts of the encoder more ( i.e., densely) compared to the deep parts of it. Table 4c shows how per- formance changes as we vary the model partition factor K. After deployment. During the test-time adaptation, we only adapt meta networks to target domains while freezing the original networks. Following EATA [50], we use the entropy minimization H(ˆy) = −P c p(ˆy) logp(ˆy) to the samples achieving entropy less than the pre-defined entropy threshold H0, where ˆy is the prediction output of a test im- age from test dataset Dt and p(·) is the softmax function. Thus, the main task loss for adaptation is defined as Lent = I{H(ˆy)<H0} · H(ˆy), (2) where I{·} is an indicator function. In addition, in order to prevent catastrophic forgetting and error accumulation, we apply our proposed regularization loss Rk, which is de- scribed next in detail. Consequently, the overall loss of our method is formulated as, Ltotal θ = Lent θ + λ KX k Rk θk , (3) where θ and θk denotes parameters of all meta networks and those of k-th group of meta networks, respectively, and λ is used to balance the scale of the two loss functions. Note that our architecture requires less memory than pre- vious works [66, 65] since we use frozen original networks and discard its intermediate activations. To be more spe- cific, our architecture uses 82% and 60% less memory on average than CoTTA and TENT/EATA. 3.2. Self-distilled Regularization The unsupervised loss from unlabeled test data Dt is likely to provide a false signal ( i.e., noise) to the model (ˆy ̸= yt where yt is the ground truth test label). Previ- ous works have verified that long-term adaptation with un- supervised loss causes overfitting due to error accumula- tion [75, 2] and catastrophic forgetting [66, 35]. To prevent the critical issues, we propose a self-distilled regularization utilizing our architecture. As shown in Fig. 3, we regularize the output ˜xk of each k-th group of the meta networks not to deviate from the outputxk of the k-th part of frozen orig- inal networks. Our regularization loss which computes the mean absolute error (i.e., L1 loss) is formulated as follows: Rk θk = ∥˜xk − xk∥1 . (4) Since the original networks are not updated, the output xk,k∼K extracted from them can be considered as contain- ing the knowledge learned from the source domain. Taking advantage of this fact, we let the output of meta networks ˜xk be regularized with knowledge distilled from the origi- nal networks. By preventing the adapted model to not sig- nificantly deviate from the original model, we can prevent 1) catastrophic forgetting by maintaining the source domain knowledge and 2) error accumulation by utilizing the class discriminability of the original model. Remarkably, unlike previous works [66, 50], our regularization does not require saving additional original networks, which accompanies ex- tra memory usage. Moreover, it only needs a negligible 4WideResNet-40 (AugMix) WideResNet-28 ResNet-50Method Avg. err↓ Mem. (MB) Avg. err↓ Mem. (MB) Avg. err↓ Mem. (MB) Source 36.7 11 43.5 58 48.8 91BN Stats Adapt [49] 15.4 11 20.9 58 16.6 91Single do. TENT [65] 12.7 188 19.2 646 15.0 925Continual TENT 13.3 188 20.0 646 15.2 925TTT++ [42] 14.6 391 20.3 1405 16.1 1877SWR&NSP [9] 12.1400 17.2 1551 15.4 1971NOTE [17] 13.4 188 20.2 646 - -EATA [50] 13.0 188 18.6 646 14.2 925CoTTA [66] 14.0 409 17.0 1697 14.4 2066Ours (K=4)12.2 80(80, 58%↓) 16.9404(76, 38%↓) 14.4296(86, 68%↓) Ours (K=5)12.1 92(77, 51%↓) 16.8471(72, 27%↓) 14.1498(76, 46%↓) (a) CIFAR10-C with severity level 5 WideResNet-40 (AugMix) ResNet-50Method Avg. err↓ Mem. (MB) Avg. err↓ Mem. (MB) Source 69.7 11 73.8 91BN Stats Adapt [49] 41.1 11 44.5 91Single do. TENT [65] 36.7 188 40.1 926Continual TENT 38.3 188 45.9 926TTT++ [42] 41.0 391 44.2 1876SWR&NSP [9] 36.6 400 44.1 1970NOTE [17] 42.8 188 - -EATA [50] 37.1 188 39.9 926CoTTA [66] 38.1 409 40.2 2064Ours (K=4)36.4 80(80, 58%↓) 39.5296(86, 68%↓) Ours (K=5)36.3 92(77, 51%↓) 39.3498(76, 46%↓) (b) CIFAR100-C with severity level 5 Table 1. Comparison of error rate ( %) on CIFAR-C. We report an average error of 15 corruptions on continual TTA and a memory requirement including model parameters and activation sizes. The lowest error is in bold, and the second lowest error is underlined. The memory reduction rates compared to CoTTA and TENT are presented sequentially. WideResNet-40 was pre-trained with AugMix [26] that is a data processing to increase the robustness of the model. Source denotes the pre-trained model without adaptation. Single domain (in short, single do.) TENT resets the model when adapting to a new target domain, so the domain labeles are required. ResNet-50 (AugMix) ResNet-50(MB)Total Mem.↓Method Avg. err↓ Avg. err↓ Source 74.36 82.35 91BN Stats Adapt [49] 57.87 72.18 91Continual TENT [65] 56.1 (0.6) 66.2 (1.1) 1486EATA [50] 54.9 (2.3) 63.8 (2.7) 1486CoTTA [66] 54.6(3.9) 62.6(3.1) 3132Ours (K=4) 55.2 (3.0) 64.6 (3.2)438(86, 72%↓) Ours (K=5) 54.4(2.7) 63.4(3.0) 747(75, 51%↓) Table 2. Comparison of error rate ( %) on ImageNet-C with severity level 5. Standard deviation for ten diverse corruption se- quences is denoted by the parentheses values. The total memory refers to the sum of model parameters and activations. Avg. err (%) CIFAR10-C CIFAR100-CMethod Mem. (MB)single do. continual single do. continual BN Stats Adapt [49] 91 16.6 16.6 44.5 44.5TinyTL†[4] 379 15.8 21.9 40.5 77.4RepNet†[69] 508 15.2 20.9 41.5 52.1AuxAdapt†[73] 207 16.0 16.7 44.0 45.8Ours (K=4) 296 14.4 14.4 39.5 39.2 Table 3. Comparison with methods for on-device learning. The backbone is ResNet-50. † denotes our own re-implemented mod- els. single do. indicates the singe domain TTA setup. amount of computational overhead because it is performed in parallel with the entropy minimization loss Lent. 4. Classification Experiments We evaluate our approach to image classification tasks based on the continual test-time adaptation setup with three datasets: CIFAR10-C, CIFAR100-C, and ImageNet-C. Experimental setup. Following CoTTA [66], we conduct most experiments on the continual TTA task, where we continually adapt the deployed model to each corruption type sequentially without resetting the model. This task is more challenging but more realistic than single domain TTA task [65] in which the adapted model is periodically reset to the original pre-trained model after finishing adaptation to each target, so they require additional domain information. Moreover, we evaluate our approach on the long-term TTA setup, which is detailed in Section 4.2. Following the previous TTA studies [65, 66], we eval- uate models with {CIFAR10, CIFAR10-C}, {CIFAR100, CIFAR100-C}, and {ImageNet, ImageNet-C } where the first and the second dataset in each bracket refers to the source and the target domain, respectively. The target do- mains include 15 types of corruptions ( e.g. noise, blur, weather, and digital) with 5 levels of severity, which are widely used in conventional benchmarks [25]. Implementation Details. We evaluate our approach within the frameworks officially provided by previous state-of- the-art methods [66, 50]. For fair comparisons, we use the same pre-trained model, which are WideResNet-28 and WideResNet-40 [71] models from the RobustBench [11], and ResNet-50 [24] model from TTT++ [42, 9]. Before the deployment, we pre-train the meta networks on the source dataset using a cross-entropy loss with SGD optimizer with the learning rate of 5e-2. Since the meta networks contain only a few layers, we pre-train them with a small number of epochs: 10 and 3 epochs for CIFAR and ImageNet, re- spectively. After deployment, similar to EATA [50], we use the same SGD optimizer with the learning rate of 5e-3. In Eq. (2), the entropy threshold H0 is set to 0.4 × ln C where C denotes the number of task classes. The batch size is 64 and 32 for CIFAR and ImageNet, respectively. We set the importance of the regularization λ in Eq. (3) to 0.5 to balance it with the entropy minimization loss. Additional implementation details can be found in Appendix C. Evaluation Metric. For all the experiments, we report error rates calculated during testing and the memory consump- tion, including the model parameter and the activation stor- 5Ours(i) (ii) (iii)(iv)(v)(vi) =‖\t\t\t-\t\t\t‖! convbn convbn bnconv CBAMSE : update conv (a) Visualization of networks variants Avr. errArch CIFAR10-CWRN-28CIFAR10-CWRN-40CIFAR100-CWRN-40(i) 18.1 12.637.2(ii) Ours w\\o BN 18.7 13.7 38.2(iii) Ours w\\o Conv 20.7 14.9 40.1(iv) Conv 60.6 73.3 77.2(v) CBAM [67] 21.4 15.1 40.9(vi) SE [30] 22.3 16.2 40.5Ours 16.812.136.3 (b) Meta network design (K=5) Model #Block Avg. err WRN-28 (12)CIFAR10-C 3,3,3,3 17.34,4,2,2 17.92,2,4,416.9 WRN-40 (18)CIFAR10-C 4,4,5,5 12.86,6,3,3 13.73,3,6,612.2 WRN-40 (18)CIFAR100-C 4,4,5,5 36.96,6,3,3 38.53,3,6,636.4 (c) # of blocks of each partition (K=4) Table 4. Architecture ablation experiments. (a,b) We compare continual TTA performance on several memory-efficient designs. WRN refers to WideResNet [71] backbone. (c) We report the performance based on different designs of partitioning the model. The value next to the backbone’s name denotes the total number of residual blocks of a model. age. We demonstrate the memory efficiency of our work by using the official code provided by TinyTL [4]. 4.1. Comparisons Comparisons with TTA methods. We compare our ap- proach to competing TTA methods on extensive bench- marks and various pre-trained models. The results of CIFAR10/100-C are detailed in Table 1. The model par- tition factor K are set to 4 and 5. Our approach outperforms existing TTA methods with the lowest memory usage in all pre-trained models. Specifically, in WideResNet-40, our method achieves superior performance while requiring 80% and 58% less memory than CoTTA [66] and EATA [50], re- spectively, which are also designed for continual TTA. Ap- proaches targeting single domain TTA [65, 42, 9] show poor performance due to error accumulation and catastrophic for- getting, as observed in CoTTA. The error rates for each cor- ruption type are provided in Appendix F. Table 2 shows the experiment for ImageNet-C. Two ResNet-50 backbones from RobustBench [11] are lever- aged. Following CoTTA, evaluations are conducted on ten diverse corruption-type sequences. We achieve comparable performance to CoTTA while utilizing 86% and 75% less memory with K=4 and 5, respectively. In addition, we ob- serve that our approach shows superior performance when adopting the model pre-trained with strong data augmenta- tion methods (e.g., Augmix [26]). Comparisons with on-device learning methods.We com- pare our approach with methods for memory-efficient on- device learning. TinyTL [4] and RepNet [69] focus on su- pervised on-device learning ( i.e., requiring labeled target data). However, since TTA assumes that we do not have access to the target labels, utilizing such methods to TTA di- rectly is infeasible. Therefore, we experimented by replac- ing supervised loss ( i.e., cross-entropy) with unsupervised loss (i.e., entropy minimization) in TinyTL and RepNet. As shown in Table 3, they suffer from performance degradation in continual TTA, showing inferior performance compared to our proposed approach even in the single domain TTA. Memory (MB) Avg. error (%)  (0.8%)              CIFAR100-C                      WideResNet-40 K=1 K=2 (3.7%) K=3 (4.3%) K=4 (10.8%) K=5 (11.3%) K=6 (12.8%) K=7 (13.3%) ����������� Figure 4. Ablation study of K. We uniformly divide the encoder of the pre-trained model into the model partition factor K. The x- axis indicates the memory size including both model parameter size and activation size while the y-axis indicates the average error rate. The values in parentheses show the rate of increase for the model parameters compared to the original model. Similar to ours, AuxAdapt [73] adds and updates a small network ( i.e., ResNet-18) while freezing the pre-trained model. Unlike our approach, they only modify a prediction output, not intermediate features. While AuxAdapt requires the least memory usage, it fails to improve TTA performan- ce in single domain TTA. Nevertheless, since the original model is frozen, it suffers less from catastrophic forgetting and error accumulation than TinyTL [4] and RepNet [69] in the continual TTA. Through the results, we confirm that our proposed method brings both memory efficiency and a significant performance improvement in both TTA setups. 4.2. Empirical Study Architecture design. An important design of our meta net- works is injecting a single BN layer before the original net- works and utilizing a residual connection with one conv block. Table 4b studies the effectiveness of the proposed design by comparing it with six different variants. From the results, we observe that using only either conv block (ii) or BN (iii) aggravates the performance: error rate increases by 1.4% and 3.8% on CIFAR-100-C with WideResNet-40. In design (i), we enforce both BN parameters and Conv layers in the meta networks to take the output of the origi- nal networks as inputs. Such a design brings performance drop. We speculate that it is because the original network, 6Gaus.ShotImpu.Defo.Glas.Moti.ZoomSnowFros.Fog Brig.Cont.Elas.Pixe.Jpeg 27.5 30.0 32.5 35.0 37.5 40.0 42.5 45.0 25 26 27 28 29 30 31                                Corruption type in 1 round                                Ours (Clean) TENT (Clean) Ours (Corrupt) �������������� (a) Catastrophic forgetting effect Corrup&on Error (%) Round (b) Error accumulation effect Figure 5. Regularization ablation experiments. We conduct experiments with WideResNet-40 on CIFAR100-C. (a) We utilize a test set of the CIFAR-100 dataset to measure clean error after adapting to each corruption. Maintaining clean errors at a stable level indicates that our approach helps the model robust to catastrophic forgetting. (b) We simulate a long-term adaptation scenario by repeating 100 rounds of 15 corruption sequences. In the absence of regularization, error accumulation can lead to overfitting (i.e., the case of the error increases exponentially). However, our approach does not suffer from such an error accumulation. We set K to 5 in the above experiments. Batch size 16 8 4 2 1 Non training Source 69.7 69.7 69.7 69.7 69.7 BN Stats Adapt [49] 41.1 50.2 59.9 81.0 99.1 AdaptBN [55] 39.1 41.2 45.2 49.0 54.0 Training Con. TENT [65] 40.9 47.8 58.6 82.2 99.0 Con. TENT+AdaptBN 38.2 40.2 43.2 47.7 52.2 Ours (K=5) 40.0 45.8 63.4 80.8 99.0 Ours (K=5)+AdaptBN36.9 39.3 42.2 46.5 51.8 Table 5. Experiments with small batch sizes. We evaluate all baselines with WideResNet-40 on CIFAR100-C. Con. TENT is the abbreviation for continual TENT. which is not adapted to the target domain, lacks the ability to extract sufficiently meaningful features from the target image. Also, we observed a significant performance degra- dation after removing the residual connection in design (iv). In addition, since attention mechanisms [67, 30] generally have improved classification accuracy, we study how atten- tion mechanisms can further boost TTA performance of our approach in design (v, vi). The results show that it is diffi- cult for the attention module to train ideally in TTA setup using unsupervised learning, unlike when applying it to su- pervised learning. An ablation study on each element of meta networks can be found in Appendix D. Number of blocks in each partition. ResNet [24] consists of multiple residual blocks (e.g., BasicBlock and Bottleneck in Pytorch [54]). For instance, WideResNet-28 has 12 resid- ual blocks. By varying the number of blocks for each part of the original networks, we analyze TTA performance in Ta- ble 4c. We observe that splitting the shallow parts of the en- coder densely (e.g., 2,2,4,4 blocks, from the shallow to the deep parts sequentially) brings more performance gain than splitting the deep layers densely ( e.g., 4,4,2,2 blocks). We suggest that it is because we modify the lower-level feature more as we split shallow layers densely. Our observation is aligned with the finding of previous TTA works [9, 48], which show that updating the shallow layers more than the deep layers improves TTA performance. Number of model partition K. Fig. 4 shows both memory requirement and adaptation performance according to the model partition factor K. With a small K ( e.g., 1 or 2), the intermediate outputs are barely modified, making it difficult to achieve a reasonable level of performance. We achieve the best TTA performance with K of 4 or 5 as adjusting a greater numver of intermediate features. In the meanwhile, we observe that the average error rate is saturated and re- mains consistent when K is set to large values (e.g. 6,7 or 8) even with the increased amount of activations and learnable parameters. Therefore, we set K to 4 and 5. Catastrophic forgetting. We conduct experiments to con- firm the catastrophic forgetting effect (Fig. 5a). Once fin- ishing adaptation to each corruption, we evaluate the model on clean target data ( i.e., test-set of CIFAR dataset) with- out updating the model. For TENT with no regulariza- tion, the error rates for the clean target data ( i.e., clean er- ror (%)) increase gradually, which can be seen as the phe- nomenon of catastrophic forgetting. In contrast, our ap- proach consistently maintains the error rates for the clean target data, proving that our regularization loss effectively prevents catastrophic forgetting. These results indicate that our method can be reliably utilized in various domains, in- cluding the source and target domains. Error accumulation in long-term adaptation. To evalu- ate the error accumulation effect, we repeat all the corrup- tion sequences for 100 rounds. The results are described in Fig. 5b. For TENT, a gradual increase in error rates is ob- served in later rounds, even with small learning rates. For example, TENT [65] with the learning rate of 1e-5 achieves the error rate of 39.7%, and reached its lowest error rate of 36.5% after 8 rounds. However, it shows increased error rate of 38.6% after 100 rounds due to overfitting. It suggests 7Time t− − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − →Round 1 4 7 10 AllMethod Mem. (MB)Brig. Fog Fros. SnowBrig. Fog Fros. SnowBrig. Fog Fros. SnowBrig. Fog Fros. SnowMean Source 280 60.4 54.3 30.0 4.160.4 54.3 30.0 4.160.4 54.3 30.0 4.160.4 54.3 30.0 4.137.2BN Stats Adapt [49]280 69.1 61.0 44.8 39.169.1 61.0 44.8 39.169.1 61.0 44.8 39.169.1 61.0 44.8 39.153.6Continual TENT [65]2721 70.1 62.1 46.1 40.262.2 53.7 44.4 37.950.0 41.5 31.6 26.639.2 32.6 25.3 22.442.9Ours (K=4) 918(66%↓) 70.262.446.341.970.062.846.542.270.062.846.542.170.162.846.642.255.3 Table 6. Semantic segmentation results in continual test-time adaptation tasks. We conduct experiments on Cityscapes [10] with four weather corruptions [25] applied. The four conditions are repeated ten times to simulate continual domain shifts. All results are evaluated based on DeepLabV3Plus-ResNet-50. that without regularization, TTA methods eventually face overfitting in long-term adaptation [75, 2, 35]. Our method in the absence of regularization (λ = 0) also causes overfit- ting. On the other hand, when self-distilled regularization is involved (λ >0), the performance remains consistent even in the long-term adaptation. Small batch size. We examine the scalability of our ap- proach with a TTA method designed for small batches size, named adapting BN statistics (i.e., AdaptBN [55, 72]). When the number of batches is too small, the estimated statistics can be unreliable [55]. Thus, they calibrate the source and target statistics for the normalization of BN lay- ers so as to alleviate the domain shift and preserve the dis- criminative structures. As shown in Table 5, training mod- els with small batch sizes (e.g., 2 or 1) generally increase the error rates. However, such an issue can be addressed by appying AdaptBN to our method. To be more sepcific, we achieve an absolute improvement of 17.9% and 2.2% from Source and AdaptBN, respectively, in the batch size of 1. Number of the source samples for meta networks. Like previous TTA works [9, 42, 34, 40] including EATA [50], our approach requires access to the source data for pre- training our proposed meta networks before model deploy- ment. In order to cope with the situation where we can only make use of a subset of the source dataset, we study the TTA performance of our method according to the number of ac- cessible source samples. The results are specified in Table 7 where we use WideResNet-40. We observe that our method outperforms the baseline model even with small number of training samples ( e.g., 10% or 20%) while showing com- parable performance with excessively small numbers ( e.g. 5%). Note that we still reduce the memory usage of about 51% compared to EATA. 5. Segmentation Experiments We investigate our approach in semantic segmentation. First, we create Cityscapes-C by applying the weather cor- ruptions (brightness, fog, frost, and snow [25]) to the vali- dation set of Cityscapes [10]. Then, to simulate continual distribution shifts, we repeat the four types of Cityscapes-C ten times. In this scenario, we conduct continual TTA using the publicly-available ResNet-50-based DeepLabV3 + [7], which is pre-trained on Cityscapes for domain generaliza- EATA [50] (188MB) # of source samples Target domain Ours(K=5) (92MB) 10k (20%) 5k (10%) 2.5k (5%) CIFAR10-C 13.0 12.1 12.4 12.9 13.1 CIFAR100-C 37.1 36.3 36.4 36.6 37.2 Table 7. Ablation of # of source samples to warm up the meta networks. Before deployment, we pre-trained the meta networks using only a subset of the source dataset (e.g., 20%, 10%, and 5%). The memory usage (MB) of each method is also presented. tion task [8] in semantic segmentation. For TTA, we use the batch size of 2. More details are specified in Appendix C. Results. We report the results based on mean intersection over union (mIoU) in Table 6. It demonstrates that our ap- proach helps to both minimize memory consumption and performs long-term adaptation stably for semantic segmen- tation. Unlike continual TENT, our method avoids catas- trophic forgetting and error accumulation, allowing us to achieve the highest mIoU score while using 66% less mem- ory usage in a continual TTA setup. Additional experiment results can be found in Appendix B. 6. Conclusion This paper proposed a simple yet effective approach that improves continual TTA performance and saves a signifi- cant amount of memory, which can be applied to edge de- vices with limited memory. First, we presented a memory- efficient architecture that consists of original networks and meta networks. This architecture requires much less mem- ory size than the previous TTA methods by decreasing the intermediate activations used for gradient calculations. Sec- ond, in order to preserve the source knowledge and prevent error accumulation during long-term adaptation with noisy unsupervised loss, we proposed self-distilled regularization that controls the output of meta networks not to deviate sig- nificantly from the output of the original networks. With ex- tensive experiments on diverse datasets and backbone net- works, we verified the memory efficiency and TTA perfor- mance of our approach. In this regard, we hope that our efforts will facilitate a variety of studies that make test-time adaptation for edge devices feasible in practice. Acknowledgments. We would like to thank Kyuwoong Hwang, Simyung Chang, and Byeonggeun Kim for their valuable feedback. We are also grateful for the helpful dis- cussions from Qualcomm AI Research teams. 8References [1] Kazuki Adachi, Shin’ya Yamaguchi, and Atsutoshi Kuma- gai. Covariance-aware feature alignment with pre-computed source statistics for test-time adaptation. arXiv preprint arXiv:2204.13263, 2022. 2, 3 [2] Eric Arazo, Diego Ortego, Paul Albert, Noel E O’Connor, and Kevin McGuinness. Pseudo-labeling and confirmation bias in deep semi-supervised learning. In IJCNN, 2020. 2, 3, 4, 8 [3] Kambiz Azarian, Debasmit Das, Hyojin Park, and Fatih Porikli. Test-time adaptation vs. training-time generaliza- tion: A case study in human instance segmentation using keypoints estimation. In WACV Workshops, 2023. 3 [4] Han Cai, Chuang Gan, Ligeng Zhu, and Song Han. Tinytl: Reduce memory, not parameters for efficient on-device learning. In NeurIPS, 2020. 2, 3, 5, 6, 12, 16 [5] Dian Chen, Dequan Wang, Trevor Darrell, and Sayna Ebrahimi. Contrastive test-time adaptation. In CVPR, 2022. 3, 13 [6] Lin Chen, Huaian Chen, Zhixiang Wei, Xin Jin, Xiao Tan, Yi Jin, and Enhong Chen. Reusing the task-specific classifier as a discriminator: Discriminator-free adversarial domain adap- tation. In CVPR, 2022. 3 [7] Liang-Chieh Chen, Yukun Zhu, George Papandreou, Florian Schroff, and Hartwig Adam. Encoder-decoder with atrous separable convolution for semantic image segmentation. In ECCV, 2018. 8, 14 [8] Sungha Choi, Sanghun Jung, Huiwon Yun, Joanne T Kim, Seungryong Kim, and Jaegul Choo. Robustnet: Improving domain generalization in urban-scene segmentation via in- stance selective whitening. In CVPR, 2021. 1, 2, 3, 8, 14, 15 [9] Sungha Choi, Seunghan Yang, Seokeon Choi, and Sun- grack Yun. Improving test-time adaptation via shift-agnostic weight regularization and nearest source prototypes. In ECCV, 2022. 1, 2, 3, 4, 5, 6, 7, 8, 13, 15, 16, 17 [10] Marius Cordts, Mohamed Omran, Sebastian Ramos, Timo Rehfeld, Markus Enzweiler, Rodrigo Benenson, Uwe Franke, Stefan Roth, and Bernt Schiele. The cityscapes dataset for semantic urban scene understanding. In CVPR, 2016. 8, 14 [11] Francesco Croce, Maksym Andriushchenko, Vikash Se- hwag, Edoardo Debenedetti, Nicolas Flammarion, Mung Chiang, Prateek Mittal, and Matthias Hein. Robustbench: a standardized adversarial robustness benchmark. In NeurIPS Datasets and Benchmarks Track, 2021. 5, 6 [12] Debasmit Das, Shubhankar Borse, Hyojin Park, Kambiz Azarian, Hong Cai, Risheek Garrepalli, and Fatih Porikli. Transadapt: A transformative framework for online test time adaptive semantic segmentation. In ICASSP, 2023. 3 [13] Zhiwei Deng and Olga Russakovsky. Remember the past: Distilling datasets into addressable memories for neural net- works. arXiv preprint arXiv:2206.02916, 2022. 13 [14] Sauptik Dhar, Junyao Guo, Jiayi Liu, Samarth Tripathi, Un- mesh Kurup, and Mohak Shah. A survey of on-device ma- chine learning: An algorithms and learning theory perspec- tive. ACM Transactions on Internet of Things, 2021. 2 [15] Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner, Mostafa Dehghani, Matthias Minderer, Georg Heigold, Syl- vain Gelly, et al. An image is worth 16x16 words: Trans- formers for image recognition at scale. In ICLR, 2021. 1 [16] Yossi Gandelsman, Yu Sun, Xinlei Chen, and Alexei A Efros. Test-time training with masked autoencoders. In NeurIPS, 2022. 1, 3 [17] Taesik Gong, Jongheon Jeong, Taewon Kim, Yewon Kim, Jinwoo Shin, and Sung-Ju Lee. Robust continual test- time adaptation: Instance-aware bn and prediction-balanced memory. In NeurIPS, 2022. 2, 5, 16, 17 [18] Priya Goyal, Piotr Doll ´ar, Ross Girshick, Pieter Noord- huis, Lukasz Wesolowski, Aapo Kyrola, Andrew Tulloch, Yangqing Jia, and Kaiming He. Accurate, large mini- batch sgd: Training imagenet in 1 hour. arXiv preprint arXiv:1706.02677, 2017. 16 [19] Yves Grandvalet and Yoshua Bengio. Semi-supervised learning by entropy minimization. In NeurIPS, 2004. 2, 15 [20] Ishaan Gulrajani and David Lopez-Paz. In search of lost do- main generalization. In ICLR, 2021. 3 [21] Chuan Guo, Geoff Pleiss, Yu Sun, and Kilian Q Weinberger. On calibration of modern neural networks. In ICML, 2017. 13 [22] Kaiming He, Xinlei Chen, Saining Xie, Yanghao Li, Piotr Doll´ar, and Ross Girshick. Masked autoencoders are scalable vision learners. In CVPR, 2022. 1 [23] Kaiming He, Haoqi Fan, Yuxin Wu, Saining Xie, and Ross Girshick. Momentum contrast for unsupervised visual rep- resentation learning. In CVPR, 2020. 1 [24] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image recognition. In CVPR, 2016. 1, 5, 7, 12, 16 [25] Dan Hendrycks and Thomas Dietterich. Benchmarking neu- ral network robustness to common corruptions and perturba- tions. In ICLR, 2019. 5, 8 [26] Dan Hendrycks, Norman Mu, Ekin D. Cubuk, Barret Zoph, Justin Gilmer, and Balaji Lakshminarayanan. AugMix: A simple data processing method to improve robustness and uncertainty. In ICLR, 2020. 5, 6, 14 [27] Geoffrey Hinton, Oriol Vinyals, and Jeff Dean. Distilling the knowledge in a neural network. In NeurIPS, 2014. 13 [28] Neil Houlsby, Andrei Giurgiu, Stanislaw Jastrzebski, Bruna Morrone, Quentin De Laroussilhe, Andrea Gesmundo, Mona Attariyan, and Sylvain Gelly. Parameter-efficient transfer learning for nlp. In ICML, 2019. 13 [29] Edward J Hu, yelong shen, Phillip Wallis, Zeyuan Allen- Zhu, Yuanzhi Li, Shean Wang, Lu Wang, and Weizhu Chen. Lora: Low-rank adaptation of large language models. In ICLR, 2022. 13 [30] Jie Hu, Li Shen, and Gang Sun. Squeeze-and-excitation net- works. In CVPR, 2018. 6, 7 [31] Xuefeng Hu, Gokhan Uzunbas, Sirius Chen, Rui Wang, Ashish Shah, Ram Nevatia, and Ser-Nam Lim. Mixnorm: Test-time adaptation through online normalization estima- tion. arXiv preprint arXiv:2110.11478, 2021. 3 9[32] Yusuke Iwasawa and Yutaka Matsuo. Test-time classifier ad- justment module for model-agnostic domain generalization. In NeurIPS, 2021. 1 [33] Vidit Jain and Erik Learned-Miller. Online domain adapta- tion of a pre-trained cascade of classifiers. In CVPR, 2011. 3 [34] Sanghun Jung, Jungsoo Lee, Nanhee Kim, and Jaegul Choo. Cafa: Class-aware feature alignment for test-time adaptation. arXiv preprint arXiv:2206.00205, 2022. 2, 4, 8 [35] Tommie Kerssies, Joaquin Vanschoren, and Mert Kılıc ¸kaya. Evaluating continual test-time adaptation for contextual and semantic domain shifts. arXiv preprint arXiv:2208.08767 , 2022. 3, 4, 8 [36] Ansh Khurana, Sujoy Paul, Piyush Rai, Soma Biswas, and Gaurav Aggarwal. Sita: Single image test-time adaptation. arXiv preprint arXiv:2112.02355, 2021. 3 [37] Andreas Krause, Pietro Perona, and Ryan Gomes. Discrim- inative clustering by regularized information maximization. In NeurIPS, 2010. 15 [38] Daiqing Li, Junlin Yang, Karsten Kreis, Antonio Torralba, and Sanja Fidler. Semantic segmentation with generative models: Semi-supervised learning and strong out-of-domain generalization. In CVPR, 2021. 1 [39] Xiaotong Li, Yongxing Dai, Yixiao Ge, Jun Liu, Ying Shan, and Ling-Yu Duan. Uncertainty modeling for out-of- distribution generalization. In ICLR, 2022. 3 [40] Hyesu Lim, Byeonggeun Kim, Jaegul Choo, and Sungha Choi. TTN: A domain-shift aware batch normalization in test-time adaptation. In ICLR, 2023. 2, 3, 4, 8, 13, 16 [41] Ji Lin, Wei-Ming Chen, Yujun Lin, Chuang Gan, Song Han, et al. Mcunet: Tiny deep learning on iot devices. InNeurIPS, 2020. 3 [42] Yuejiang Liu, Parth Kothari, Bastien van Delft, Baptiste Bellot-Gurlet, Taylor Mordan, and Alexandre Alahi. Ttt++: When does self-supervised test-time training fail or thrive? In NeurIPS, 2021. 1, 2, 3, 5, 6, 8, 13, 16, 17 [43] Mingsheng Long, Han Zhu, Jianmin Wang, and Michael I Jordan. Unsupervised domain adaptation with residual trans- fer networks. In NeurIPS, 2016. 1 [44] David Lopez-Paz and Marc’Aurelio Ranzato. Gradient episodic memory for continual learning. In NeurIPS, 2017. 13 [45] Robert A Marsden, Mario D ¨obler, and Bin Yang. Gradual test-time adaptation by self-training and style transfer. arXiv preprint arXiv:2208.07736, 2022. 3 [46] Ke Mei, Chuang Zhu, Jiaqi Zou, and Shanghang Zhang. In- stance adaptive self-training for unsupervised domain adap- tation. In ECCV, 2020. 3 [47] Rafael M ¨uller, Simon Kornblith, and Geoffrey E Hinton. When does label smoothing help? In NeurIPS, 2019. 13 [48] Chaithanya Kumar Mummadi, Robin Hutmacher, Kilian Rambach, Evgeny Levinkov, Thomas Brox, and Jan Hendrik Metzen. Test-time adaptation to distribution shift by confi- dence maximization and input transformation.arXiv preprint arXiv:2106.14999, 2021. 3, 4, 7 [49] Zachary Nado, Shreyas Padhy, D Sculley, Alexander D’Amour, Balaji Lakshminarayanan, and Jasper Snoek. Evaluating prediction-time batch normalization for robust- ness under covariate shift. arXiv preprint arXiv:2006.10963, 2020. 5, 7, 8, 16, 17 [50] Shuaicheng Niu, Jiaxiang Wu, Yifan Zhang, Yaofo Chen, Shijian Zheng, Peilin Zhao, and Mingkui Tan. Efficient test- time model adaptation without forgetting. In ICML, 2022. 2, 3, 4, 5, 6, 8, 12, 13, 14, 15, 16, 17 [51] Shuaicheng Niu, Jiaxiang Wu, Yifan Zhang, Zhiquan Wen, Yaofo Chen, Peilin Zhao, and Mingkui Tan. Towards stable test-time adaptation in dynamic wild world. In ICLR, 2023. 3 [52] Xingang Pan, Ping Luo, Jianping Shi, and Xiaoou Tang. Two at once: Enhancing learning and generalization capacities via ibn-net. In ECCV, 2018. 3 [53] Kwanyong Park, Sanghyun Woo, Inkyu Shin, and In So Kweon. Discover, hallucinate, and adapt: Open compound domain adaptation for semantic segmentation. In NeurIPS, 2020. 3 [54] Adam Paszke, Sam Gross, Francisco Massa, Adam Lerer, James Bradbury, Gregory Chanan, Trevor Killeen, and et al. Lin. Pytorch: An imperative style, high-performance deep learning library. In NeurIPS, 2019. 7, 12, 16 [55] Steffen Schneider, Evgenia Rusak, Luisa Eck, Oliver Bring- mann, Wieland Brendel, and Matthias Bethge. Improving robustness against common corruptions by covariate shift adaptation. In NeurIPS, 2020. 7, 8, 16 [56] Inkyu Shin, Dong-Jin Kim, Jae Won Cho, Sanghyun Woo, Kwanyong Park, and In So Kweon. Labor: Labeling only if required for domain adaptive semantic segmentation. In ICCV, 2021. 3 [57] Inkyu Shin, Yi-Hsuan Tsai, Bingbing Zhuang, Samuel Schulter, Buyu Liu, Sparsh Garg, In So Kweon, and Kuk- Jin Yoon. Mm-tta: Multi-modal test-time adaptation for 3d semantic segmentation. In CVPR, 2022. 2, 3 [58] Inkyu Shin, Sanghyun Woo, Fei Pan, and InSo Kweon. Two- phase pseudo label densification for self-training based do- main adaptation. In ECCV, 2020. 3 [59] Junha Song, Kwanyong Park, Inkyu Shin, Sanghyun Woo, Chaoning Zhang, and In So Kweon. Test-time adaptation in the dynamic world with compound domain knowledge man- agement. arXiv preprint arXiv:2212.08356, 2023. 3 [60] Nitish Srivastava, Geoffrey Hinton, Alex Krizhevsky, Ilya Sutskever, and Ruslan Salakhutdinov. Dropout: a simple way to prevent neural networks from overfitting. The journal of machine learning research, 2014. 13 [61] Qianru Sun, Yaoyao Liu, Tat-Seng Chua, and Bernt Schiele. Meta-transfer learning for few-shot learning. InCVPR, 2019. 13 [62] Yu Sun, Xiaolong Wang, Zhuang Liu, John Miller, Alexei A. Efros, and Moritz Hardt. Test-time training with self- supervision for generalization under distribution shifts. In ICML, 2020. 3 [63] Antti Tarvainen and Harri Valpola. Mean teachers are better role models: Weight-averaged consistency targets improve semi-supervised deep learning results. In NeurIPS, 2017. 3 [64] Tuan-Hung Vu, Himalaya Jain, Maxime Bucher, Mathieu Cord, and Patrick P´erez. Advent: Adversarial entropy mini- 10mization for domain adaptation in semantic segmentation. In CVPR, 2019. 3 [65] Dequan Wang, Evan Shelhamer, Shaoteng Liu, Bruno Ol- shausen, and Trevor Darrell. Tent: Fully test-time adaptation by entropy minimization. In ICLR, 2021. 1, 2, 3, 4, 5, 6, 7, 8, 12, 13, 14, 16, 17 [66] Qin Wang, Olga Fink, Luc Van Gool, and Dengxin Dai. Con- tinual test-time domain adaptation. In CVPR, 2022. 1, 2, 3, 4, 5, 6, 12, 13, 14, 16, 17 [67] Sanghyun Woo, Jongchan Park, Joon-Young Lee, and In So Kweon. Cbam: Convolutional block attention module. In ECCV, 2018. 6, 7 [68] Li Yang, Adnan Siraj Rakin, and Deliang Fan. Da3: Dy- namic additive attention adaption for memory-efficient on- device multi-domain learning. In CVPR Workshops, 2022. 3 [69] Li Yang, Adnan Siraj Rakin, and Deliang Fan. Rep-net: Efficient on-device learning via feature reprogramming. In CVPR, 2022. 2, 3, 5, 6, 16 [70] Fuming You, Jingjing Li, and Zhou Zhao. Test-time batch statistics calibration for covariate shift. arXiv preprint arXiv:2110.04065, 2021. 3 [71] Sergey Zagoruyko and Nikos Komodakis. Wide residual net- works. In BMVC, 2016. 5, 6, 12 [72] Marvin Zhang, Sergey Levine, and Chelsea Finn. Memo: Test time robustness via adaptation and augmentation. In NeurIPS, 2021. 3, 8 [73] Yizhe Zhang, Shubhankar Borse, Hong Cai, and Fatih Porikli. Auxadapt: Stable and efficient test-time adaptation for temporally consistent video semantic segmentation. In WACV, 2022. 5, 6, 16 [74] Yabin Zhang, Minghan Li, Ruihuang Li, Kui Jia, and Lei Zhang. Exact feature distribution matching for arbitrary style transfer and domain generalization. In CVPR, 2022. 3 [75] Zixing Zhang, Fabien Ringeval, Bin Dong, Eduardo Coutinho, Erik Marchi, and Bj¨orn Sch¨uller. Enhanced semi- supervised learning for multimodal emotion recognition. In ICASSP, 2016. 2, 3, 4, 8 [76] Kaiyang Zhou, Yongxin Yang, Yu Qiao, and Tao Xiang. Do- main generalization with mixstyle. In ICLR, 2021. 3 11Appendix In this supplementary material, we provide, A. Efficiency for TTA methods B. Discussion and further experiments C. Further implementation details D. Additional ablations E. Baseline details F. Results of all corruptions A. Efficiency for TTA methods Memory efficiency. Existing TTA works [65, 50, 66] up- date model parameters to adapt to the target domain. This process inevitably requires additional memory to store the activations. Fig. 6 describes Eq. (1) of the main paper in more detail. For instance, 1) the backward propagation from the layer (c) to the layer ( b) can be accomplished without saving intermediate activations fi and fi+1, since it only re- quires ∂L ∂fi+1 =∂L ∂LWT i+1 and ∂L ∂fi = ∂L ∂fi+1 WT i =∂L ∂LWT i+1WT i op- erations. 2) During the forward propagation, the learnable layer (a) has to store the intermediate activation fi−1 to cal- culate the weight gradient ∂L ∂Wi−1 =fT i−1 ∂L ∂fi . : freeze: update ℒ𝜕ℒ𝜕ℒ𝜕ℒ𝜕𝑓!\"#𝜕ℒ𝜕𝑓!𝜕ℒ𝜕𝑓!$# 𝒇𝒊$𝟏 𝑓! 𝑓!\"#𝑊!$#,𝑏!$# 𝑊!,𝑏! 𝑊!\"#,𝑏!\"#(𝑎) (𝑏) (𝑐)𝜕ℒ𝜕𝑓!=𝜕ℒ𝜕𝑓!\"#𝑊!' 𝜕ℒ𝜕𝑓!\"#=𝜕ℒ𝜕ℒ𝑊!\"#'𝜕ℒ𝜕𝑓!$#=𝜕ℒ𝜕𝑓!𝑊!$#'𝜕ℒ𝜕𝑊!$#=𝒇𝒊$𝟏𝑻𝜕ℒ𝜕𝑓! 𝑓!\"#=𝑓!𝑊!+𝑏!𝜕ℒ𝜕𝑏!$#=𝜕ℒ𝜕𝑓! Figure 6. Forward and backward propagation. The black and red lines refer to forward and backward propagation, respectively. f and (a, b, c) are the activations and the linear layers, respectively. Computational efficiency. Wall-clock time and floating point operations (FLOPs) are standard measures of com- putational cost. We utilize wall-clock time to compare the computational cost of TTA methods since most libraries computing FLOPs only support inference, not training. Unfortunately, wall-clock time of EATA [50] and our ap- proach can not truly represent its computational efficiency since the current Pytorch version [54] does not support fine-grained implementation [4]. For example, EATA fil- ters samples to improve its computational efficiency. How- ever, its gradient computation is performed on the full mini- batch, so the wall-clock time for backpropagation in EATA is almost the same as that of TENT [65]. In our approach, our implementation follows Algorithm 1 to make each reg- ularization loss Rk θk applied to parameters of k-th group of meta networks θk in Eq. (3). In order to circumvent such an issue, the authors of EATA report the theoretical time, which assumes that PyTorch handles gradient back- propagation at an instance level. Similar to EATA, we also report both theoretical time and wall-clock time in Ta- ble 8. To compute the theoretical time of our approach, we simply subtract the time for re-forward (in Algorithm 1) from wall-clock time. We emphasize that this is mainly an engineering-based issue, and the optimized implementation can further improve computational efficiency. [50]. Using a single NVIDIA 2080Ti GPU, we measure the total time required to adapt to all 15 corruptions, includ- ing the time to load test data and perform TTA. The results in Table 8 show that our proposed method requires neg- ligible overhead compared to CoTTA [66]. For example, CoTTA needs approximately 10 times more training time than Continual TENT [65] with WideResNet-40. Note that meta networks enable our approach to use 80% and 58% less memory than CoTTA and EATA, even with such minor extra operations. B. Discussion and further experiments Comparison on gradually changing setup. In Table 1 and Table 2, we evaluate all methods on the continual TTA task, proposed in CoTTA [66] and EATA [50], where we continually adapt the deployed model to each cor- ruption type sequentially. Additionally, we conduct ex- periments on the gradually changing setup. This grad- WideResNet-40 [71] Avg. err Mem. (MB) Theo. time Wall time Source 69.7 11 - 40s Con. TENT [65] 38.3 188 - 2m 18s CoTTA [66] 38.1 409 - 22m 52s EATA [50] 37.1 188 2m 8s 2m 22s Ours (K=4) 36.4 80(80, 58%↓) 2m 27s 2m 49s Ours (K=5) 36.3 92(77, 51%↓) 2m 31s 2m 52s ResNet-50 [24] Avg. err Mem. (MB) Theo. time Wall time Source 73.8 91 - 1m 8s Con. TENT [65] 45.9 926 - 4m 2s CoTTA [66] 40.2 2064 - 38m 24s EATA [50] 39.9 926 3m 45s 4m 15s Ours (K=4) 39.5 296(86, 68%↓) 4m 16s 4m 41s Ours (K=5) 39.3 498(76, 46%↓) 4m 26s 5m 14s Table 8. Comparison of training time on CIFAR100-C. We re- port both theoretical time (in short, theo. time) and wall-clock time, taking to adapt to all 15 corruption types. Theoretical time is calculated by assuming that the ML frameworks ( e.g., Py- torch [54]) provide fine-grained implementations [4]. Con. TENT refers to continual TENT. 12Algorithm 1: PyTorch-style pseudocode for EcoTTA. # img_t: test image # model: original and meta networks # # ent_min(): Entropy minimization loss # Detach_parts(): Detach the graph connection # between each partition of networks # Attach_parts(): Attach the graph connection # between each partition of networks for img_t in test_loader: # 1. Forward output = model(img_t) # 2. Compute entropy loss loss_ent = ent_min(output) loss_ent.backward() # 3. Re-forward # (This process is not required # in fine-grained ML frameworks.) Detach_parts(model) _ = model(img_t) # 4. Compute regularization loss reg_loss = 0 for k_th_meta in meta_networks: reg_loss += k_th_meta.get_l1_loss() reg_loss.backward() # 5. Update params of meta networks optim.step() optim.zero_grad() Attach_parts(model) ual setup, proposed in CoTTA, represents the sequence by gradually changing severity for the 15 corruption types: . . .2− →1| {z } t-1 and before change − − − − − → type 1− →2− →3− →4− →5− →4− →3− →2− →1| {z } corruption type t, gradually changing severity change − − − − − → type 1− →2. . .| {z } t+1 and on , The results in Table 9 indicate that our approach outper- forms previous TTA methods [65, 50, 66] even with the gradually changing setup. Comparisons with methods for parameter efficient transfer learning. While our framework may be similar to parameter-efficient transfer learning (PETL) [29, 28, 61] in that only partial parameters are updated during training time for PETL or test time for TTA, we utilized meta net- works to minimize intermediate activations, which is crucial for memory-constrained edge devices. We conduct experi- ments by applying a PETL method [28] to the TTA setup. The adapter module is constructed by using 3x3 Conv and ReLU layers as the projection layer and the nonlinearity, re- spectively, and these modules are attached after each resid- ual block of the backbone networks. The Table 10 shows that PETA+SDR needs a 177% increase in memory usage with a 6.1% drop in performance, compared to our method. Comparisons with methods for continual learning. Typ- ical continual learning (CL) and continual TTA assume su- pervised and unsupervised learning, respectively. However, since both are focused on alleviating catastrophic forget- ting, we believe that CL methods can also be applied in continual TTA settings. The methods for addressing catas- trophic forgetting can be divided into regularization- and Method Con. TENT [65] EATA [50] CoTTA [66]Ours (K=4) Avg. err (%) 38.5 31.8 32.5 31.4 Mem. (MB) 188 188 409 80(58, 80%↓) Table 9. Comparision on gradually changing setup. To con- duct experiments, we use WRN-40 backbone on CIFAR100-C. The values in parentheses refer to memory reduction rates com- pared to TENT/EATA and CoTTA, sequentially. Method Con. TENT [65]PETL [28] PETL+SDROurs (K=4) Avg. err (%)38.3 73.3 42.5 36.4Mem. (MB) 188 141 141 80 Table 10. Comparisons with methods for PETL. We com- pare our method with methods [28] for parameter-efficient trans- fer learning (PETL) with WRN-40 on CIFAR100-C. PETL+SDR refers to PETL with our proposed self-distilled regularization. RoundCon. TENTTS DO LS KD Ours (K=4) 1 38.3 37.4 41.0 38.4 39.8 36.410 99.0 96.1 96.3 41.1 40.4 36.3 Table 11. Comparisons with methods for continual learning. We report an average error rate (%) of 15 corruptions using WRN- 40 on CIFAR100-C. In the table, TS: Entropy minimization with temperature scaling [21], DO: Dropout [60], LS: Label smoothing with the pseudo label [47], and KD: Knowledge distillation [27]. replay-based methods. The former can be subdivided into weight regularization ( e.g., CoTTA [66] and EATA [50]) and knowledge distillation [27], while the latter includes GEM [44] and dataset distillation [13]. Suppose dataset dis- tillation is applied to the continual TTA setup; for example, we can periodically replay synthetic samples distilled from the source dataset to prevent the model from forgetting the source knowledge during TTA. Notably, our self-distilled regularization (SDR) is superior to conventional CL meth- ods in terms of the efficiency of TTA in on-device settings. Specifically, unlike previous regularization- or replay-based methods, we do not require storing a copy of the original model or a replay-and-train process. To further compare our SDR with existing regulariza- tion methods, we conduct experiments while keeping our architecture and adaptation loss but replacing SDR with other regularizations, as shown in Table 11. The results demonstrate that our SDR achieves superior performance compared to other regularizations. In addition, Knowledge distillation [27] alleviates the error accumulation effect in long-term adaptation ( e.g., round 10), while showing lim- ited performance for adapting to the target domain. Superiority of our approach compared to existing TTA methods. Our work focuses on proposing an efficient ar- chitecture for continual TTA, which has been overlooked in previous TTA studies [65, 66, 5, 42, 9, 40] by introduc- ing meta networks and self-distilled regularization, rather than adaptation loss such as entropy minimization proposed 13Method Mem. (MB) Round 1 Round 4 Round 7 Round 10 Source 280 37.2 37.2 37.2 37.2Con. TENT 2721 54.6 49.6 37.4 29.9Con. TENT* 2721 56.5 52.7 42.7 36.5CoTTA* 6418 56.7 56.7 56.7 56.7Ours 918(66, 85%↓) 55.2 55.4 55.4 55.4Ours* 918(66, 85%↓) 56.7 56.8 56.9 56.9 Table 12. Further experiments in semantic segmentation. We represent the results based on mean intersection over union (mIoU). * means that the method utilizes the same cross-entropy consistency loss. The values in parentheses refer to memory re- duction rates compared to TENT/EATA and CoTTA, sequentially. #Partitions WRN-28 (12) WRN-40 (18) ResNet-50 (16) K=4 2,2,4,4 3,3,6,6 3,3,5,5 K=5 2,2,2,2,4 3,3,3,3,6 2,2,4,4,4 Table 13. Details of # of blocks of each partition. The list of numbers denotes the number of residual blocks for each part of the original networks, from the shallow to the deep parts sequentially. The values in parentheses are the total number of residual blocks. in TENT [65] and EATA [50]. Thus, our method can be used with various adaptation losses. Moreover, even though our self-distilled regularization can be regarded as a teacher-student distillation from original networks to meta networks, it does not require a large activation size or the storage of an extra source model, unlike CoTTA [66]. In addition to the results in Table 6, we improve the segmentation experiments by comparing our approach with CoTTA [66]. As we aforementioned, our approach has scalability with diverse adaptation loss. Thus, as shown in Table 12, we additionally apply cross-entropy consis- tency loss* with multi-scaling input as proposed in CoTTA, where we use the multi-scale factors of [0.5, 1.0, 1.5, 2.0] and flip. Our method not only achieves comparable per- formance with 85% less memory than CoTTA, but shows consistent performance even for multiple rounds while con- tinual TENT [65] suffers from the error accumulation effect. C. Further implementation details Partition of a pre-trained model. As illustrated in Fig. 3, the given pre-trained model consists of three parts: clas- sifier, encoder, and input conv, where the encoder denotes layer1 to 4 in the case of ResNet. Our method is applied to the encoder and we divide it into K parts. Table 13 describes the details of the number of residual blocks for each part of the encoder. Our method is designed to divide the shallow layers more (i.e., densely) than the deep layers, improving the TTA performance as shown in Table 4c. Convolution layer in meta networks. As the hyperparam- eters of the convolution layer1, we set the bias to false and 1https://pytorch.org/docs/stable/generated/torch.nn.Conv2d.html (K=4) Kernel size= 1, padding=0Kernel size=3, padding=1 Arch Avg. err params↑ Mem. Avg. err params↑ Mem. WRN-28 17.2 0.8% 396 16.9 9.5% 404 WRN-40 12.4 0.6% 80 12.2 6.4% 80 ResNet-50 14.4 11.8% 296 14.2 142.2% 394 Table 14. Kernel size in the conv layer.We report the average er- ror rate (%), the increase rate of the model parameters compared to the original model (%), and the total memory consumption (MB) including the model and activation sizes, based on the kernel size of the conv layer in meta networks. (K=4) Transformations Dataset Arch EATA [50]None +Color +Blur +Gray CIFAR10-C WRN-40 13.0 12.5 12.3 12.3 12.2 CIFAR10-C WRN-28 18.6 17.8 17.4 17.2 16.9 CIFAR100-C WRN-4037.1 36.9 36.7 36.6 36.4 Table 15. Ablation of the combination of transformations. To warm up the meta networks, we use the following transformations in Pytorch: ColorJitter (Color), GaussianBlur (Blur), and Ran- domGrayscale (Gray). We report the average error rate (%). the stride to two if the corresponding part of the encoder in- cludes the stride of two; otherwise, one. As shown in the gray area in Table 14, we conduct experiments by modify- ing the kernel size and padding for each architecture. To be more specific, we obtain better performances by setting the kernel size to three with WideResNet (with 10% additional number of model parameters). On the other hand, utilizing the kernel size of three with ResNet leads to significant in- creases in parameters and memory sizes. Thus, we use one and three as the kernel size with ResNet and WideResNet, respectively. Warming up meta networks. Before the model deploy- ment, we warm up meta networks with the source data by applying the following transformations, which prevent the meta networks from being overfitted to the source domain. Regardless of the pre-trained model’s architecture and pre-training method, we use the same transformations to warm up meta networks. Even for WideResNet-40 pre- trained with AugMix [26], a strong data augmentation tech- nique, the following simple transformations are enough to warm up the meta networks. In addition, we provide the ablation of the combination of transformations in Table 15. from t o r c h v i s i o nimport t r a n s f o r m s a s T TRANSFORMS = t o r c h . nn . S e q u e n t i a l ( RandomApply ( T . C o l o r J i t t e r ( 0 . 4 , 0 . 4 , 0 . 4 , 0 . 1 ) , p = 0 . 4 ) RandomApply ( T . G a u s s i a n B l u r ( ( 3 , 3 ) , p = 0 . 2 ) T . RandomGrayscale ( P = 0 . 1 ) ) Semantic segmentation. For semantic segmentation exper- iments, we utilize ResNet-50-based DeepLabV3+ [7] from RobustNet repository2 [8]. We warm up the meta networks on the train set of Cityscapes [10] with SGD optimizer with the learning rate of 5e-2 and the epoch of 5. Image trans- 2https://github.com/shachoi/RobustNet 14: freeze : update ℒ 𝜕ℒ 𝜕ℒ 𝜕ℒ 𝜕𝑓𝑖+1 𝜕ℒ 𝜕𝑓𝑖 𝜕ℒ 𝜕𝑓𝑖−1 𝑓𝑖−1 𝑓𝑖 𝑓𝑖+1 𝑊1,𝑏1 𝑊2,𝑏2 𝑊3,𝑏3 𝐿1 𝐿2 𝐿3 Appendix entropy min. 𝐷𝑡 ෤𝑥𝑘−1 𝑥𝑘 bn ෤𝑥𝑘 relu bn conv Affine tra. Standard. ① ② ③ ④ ⑤ ⑥ � (a) Visualization of meta networks (K=5) CIFAR10-CWRN-28CIFAR10-CWRN-40CIFAR100-CWRN-40Variants1 2 3 4 5 6 7I ✓ ✓ ✓ ✓ ✓19.9 15.4 39.2II ✓ ✓ ✓ ✓ ✓18.6 13.4 38.0III ✓ ✓ ✓ ✓18.7 13.7 38.2IV ✓ ✓ ✓ ✓ ✓18.6 12.4 36.7V ✓ ✓ ✓ ✓ ✓19.8 12.9 37.2VI ✓ ✓ ✓ ✓ 32.3 14.5 51.8VII✓ ✓ ✓ 20.7 14.9 40.1XIII✓ ✓ ✓ ✓ ✓ ✓18.1 12.6 37.2IX ✓ ✓ ✓ ✓60.6 73.3 77.2Ours✓✓✓✓✓ ✓ 16.8 12.1 36.3 (b) Comparison of average error rate (%) on continual TTA setup (K=5) Table 16. Components of meta networks. We conduct an ablation study on components of meta networks ( i.e., 1⃝ ∼7⃝). Here, 1⃝ and 2⃝ refer to affine transformation and standardization in a BN layer after the original networks. 3⃝∼ 5⃝ and 6⃝∼ 7⃝, respectively, indicate modules in a convolution block and two kinds of inputs of it. In table (b), ✓ means applying the component to meta networks. formations follow the implementation details of [8]. After model deployment, we perform TTA using SGD optimizer with the learning rate of 1e-5, the image size of 1600×800, the batch size of 2, and the importance of regularizationλ of 2. The main loss for adaptation is same as Lent in Eq. (2). D. Additional ablations Main task loss for adaptation. To adapt to the target do- main effectively, selecting the main task loss for adaptation is a non-trivial problem. So, we conduct a comparative experiment on three types of adaptation loss: L1) entropy minimization [19], L2) entropy minimization with mean en- tropy maximization [37], and L3) filtering samples using entropy minimization [50]. With a mini-batch of N test im- ages, the three adaptation losses are formulated as follows: L1 = 1 N NX i=1 H(ˆyi), (5) L2 = λm1 1 N NX i=1 H(ˆyi) − λm2 H(y), (6) L3 = 1 N NX i=1 I{H(ˆyi)<H0} · H(ˆyi), (7) where ˆyi is the logits output of i-th test data, y = 1 N PN i=1 p(ˆyi), H(y) = −P C p(y) logp(y), p( ·) is the softmax function, C is the number of classes, and I{·} is an indicator function. λm1 and λm2 indicate the importance of each term in Eq. (6) which are set to 0.2 and 0.25, respec- tively, following SWR&NSP [9]. The entropy thresholdH0 is set to 0.4 × ln C following EATA [50]. The results are described in Table 17. Particularly, apply- ing any of the three losses, our method achieves comparable performance to EATA. Among them, using L3 of Eq. (7) achieves the lowest error rate in most cases. Therefore, we apply L3 to our approach as mentioned in Section 3.1. Components of meta networks. As shown in Table 16, we (K=5) Ours Dataset Arch EATA[50] L1 L2 L3 CIFAR10-C WRN-28 18.6 17.3 16.9 16.9 WRN-40 13.0 12.2 12.3 12.1 Resnet-50 14.2 15.0 14.3 14.1 CIFAR100-C WRN-40 37.1 36.5 36.4 36.3 Resnet-50 39.9 40.7 38.8 39.4 Table 17. Ablation study of main task loss. We compare the average error rate (%) of three types of adaptation losses. (K=5) Ours Dataset Arch MSE loss (Eq. (8))L1 loss (Eq. (4)) CIFAR10-C WRN-28 16.9 16.9 WRN-40 12.3 12.1 Resnet-50 14.1 14.1 CIFAR100-CWRN-40 36.6 36.3 Resnet-50 39.5 39.4 Table 18. Ablation study of loss function of our regularization. We present the average error (%) according to two types of loss functions for self-distilled regularization. conduct an ablation study on each element of our proposed meta networks. We observe that the affine transformation is more critical than standardization in a BN layer after the original networks. Specifically, removing the standardiza- tion (variant II) causes less performance drop than remov- ing the affine transformation (variant I). In addition, using only a conv layer in conv block (variant VI) also cause per- formance degradation, so it is crucial to use the ReLU and BN layers together in the conv block. Loss function choice of our regularization.As mentioned in Section 3.2, self-distilled regularization loss computes the mean absolute error ( i.e., L1 loss) of Eq. (4). This loss regularizes the output ˜xk of each k-th group of the meta networks not to deviate from the outputxk of each k-th part of frozen original networks. The mean squared error ( i.e., MSE loss) also can be used to get a similar effect which is defined as: MSE = (˜xk − xk)2. (8) 15We compare two kinds of loss functions for our regular- ization in Table 18. By observing a marginal performance difference, our method is robust to the loss function choice. Robustness to the importance of regularization λ. We show that our method is robust to the regularization term λ. We conduct experiments using a wide range of λ as shown in Fig. 5 and the following table. Round\\λ 0 0.1 0.5 1 2 5 10 1 36.31 36.30 36.29 36.56 37.20 38.41 39.58 10 55.47 43.83 36.42 36.14 36.48 37.47 38.95 The experiments are performed with WideResNet-40 on CIFAR100-C. When λ is changed from 0.5 to 1, the per- formance difference was only 0.27% in the first round. We also test λ to be extremely large ( e.g., 5, 8, and 10). Since setting λ to 10 may mean that we hardly adapt the meta net- works to the target domain, the error rate (39.58%) with λ of 10 was close to the one (41.1%) of BN Stats Adapt [49]. E. Baseline details E.1. TTA works We refer to the baselines for which the code was of- ficially released: TENT 3, TTT++4, CoTTA5, EATA6, and NOTE7. We did experiments on their code by adding the needed data loader or pre-trained model loader. In this sec- tion, implementation details of the baselines are provided. BN Stats Adapt [49] is one of the non-training TTA ap- proaches. It can be implemented by setting the model to the train mode8 of Pytorch [54] during TTA. TTT+++ [42] was originally implemented as the offline adaptation, i.e., multi-epoch training. So, we modified their setup to continual TTA. We further tuned the learn- ing rate as 0.005 and 0.00025 for adapting to CIFAR10-C and CIFAR100-C, respectively. NOTE [17] proposed the methods named IABN and PBRS with taking account of temporally correlated target data. However, our experiments were conducted with target data that was independent and identically distributed (i.i.d.). Hence, we adapted NOTE-i.i.d ( i.e., NOTE* in their git repository), which is a combination of TENT [65] and IABN without using PBRS. We fine-tuned the α of their main paper ( i.e., self.k in the code 9) to 8 and the learning rate to 1e-5. Others (e.g., TENT [65], SWR&NSP [9], CoTTA [66], and EATA [50]). We utilized the best hyperparameters specified in their paper and code. In the case where the batch size of 3https://github.com/DequanWang/tent 4https://github.com/vita-epfl/ttt-plus-plus 5https://github.com/qinenergy/cotta 6https://github.com/mr-eggplant/EATA 7https://github.com/TaesikGong/NOTE 8pytorch.org/docs/stable/generated/torch.nn.Module.html#torch.nn.Module.train 9https://github.com/TaesikGong/NOTE/blob/main/utils/iabn.py their works (e.g., 200 and 256) differs from one for our ex- periments (e.g., 64), we decreased the learning rate linearly based on the batch size [18]. AdaptBN [55]. We set the hyperparameter N of their main paper to 8. When AdaptBN is employed alongside TENT or our approach, we set the learning rate to 1e-5 or 5e-6 [40]. E.2. On-device learning works To unify the backbone network as ResNet-50 [24], we reproduced the following works by referencing their paper and published code: TinyTL 10, Rep-Net11, and AuxAdapt. This section presents additional implementation details for reproducing the above three works. TinyTL [4]. We attach the LiteResidualModules 12 to layer1 to 4 in the case of ResNet-50 13. As the hyperpa- rameters of the LiteResidualModules, the hyperparameter expand is set to 4 while the other hyperparameters follow the default values. Rep-Net [69]. We divide the encoder of ResNet-50 into six parts, as each part of the encoder has 2,2,3,3,3,3 resid- ual blocks (e.g., BasicBlock or Bottleneck in Pytorch) from the shallow to the deep parts sequentially. Then, we connect the ProgramModules14 to each corresponding part of the en- coder. For the ProgramModule, we set the hyperparameter expand to 4 while the rest hyperparameters are used as their default values. We copy the input conv of ResNet-50 and make use of it as the input conv of Rep-Net. AuxAdapt [73]. We use ResNet-18 as the AuxNet. We create pseudo labels by fusing the logits output of ResNet- 50 and ResNet-18, and optimize all parameters of ResNet- 18 using the pseudo labels with cross-entropy loss. Warming up the additional modules. Before model de- ployment, we pre-train the additional modules ( i.e., the LiteResidualModule of TinyTL [4], the ProgramModule of Rep-Net [69], and the AuxNet of AuxAdapt [73]) on the source data using the same strategy warming up the meta networks as mentioned in Section C. F. Results of all corruptions We report the error rates (%) of all corruptions on con- tinual TTA and memory consumption (MB) including the model parameters and activations in Table 19 and Table 20. These tables contain additional details to Table 1. 10https://github.com/mit-han-lab/tinyml/tree/master/tinytl 11https://github.com/ASU-ESIC-FAN-Lab/RepNet 12https://github.com/mit-han-lab/tinyml/blob/master/tinytl/tinytl/model/modules.py 13https://github.com/pytorch/vision/blob/main/torchvision/models/resnet.py 14github.com/ASU-ESIC-FAN-Lab/RepNet/blob/master/repnet/model/reprogram.py 16Time t− − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − →Arch Method Gaus. Shot Impu. Defo. Glas. Moti. Zoom Snow Fros. Fog Brig. Cont. Elas. Pixe. JpegAvg. err Mem. WRN-40(AugMix) Source 44.3 37.0 44.8 30.6 43.9 32.6 29.4 23.9 30.1 39.7 12.9 66.4 32.7 58.4 23.5 36.7 11tBN [49] 19.5 17.6 23.8 9.6 23.1 11.1 10.3 13.4 14.2 15.0 8.0 13.9 17.3 16.0 18.8 15.4 11Single do. TENT [65]16.4 13.9 19.1 8.3 19.1 9.3 8.6 10.9 11.3 12.0 6.9 11.6 14.6 12.2 15.6 12.7 188TENT continual [65]16.4 12.2 17.1 9.1 18.7 11.4 10.4 12.7 12.4 14.8 10.1 13.0 17.0 13.3 19.0 13.3 188TTT++ [42] 19.1 16.9 22.2 9.3 21.6 10.8 9.8 12.7 13.1 14.3 7.8 13.9 15.9 14.2 17.2 14.6 391SWRNSP [9] 15.9 13.3 18.2 8.4 18.5 9.5 8.6 11.0 10.2 11.7 7.0 8.1 14.6 11.3 15.1 12.1 400NOTE [17] 19.6 16.4 19.9 9.4 20.3 10.3 10.1 11.6 10.6 13.3 7.9 7.7 15.4 12.0 17.3 13.4 188EATA [50] 15.2 13.1 17.5 9.5 19.9 11.6 9.3 11.4 11.5 12.4 7.8 11.1 16.1 12.2 16.1 13.0 188CoTTA [66] 15.6 13.6 17.3 9.8 19.0 11.0 10.2 13.5 12.6 17.4 7.8 17.3 16.2 12.9 16.0 14.0 409Ours (K=4) 16.1 13.2 18.3 8.0 18.3 9.3 8.6 10.5 10.1 12.2 6.8 11.3 14.5 11.0 14.8 12.2 80Ours (K=5) 15.9 12.6 17.2 8.2 18.4 9.3 8.6 10.6 10.4 12.4 6.7 11.7 14.3 11.3 14.9 12.1 92 WRN-28 Source 72.3 65.7 72.9 46.9 54.3 34.8 42.0 25.1 41.3 26.0 9.3 46.7 26.6 58.5 30.3 43.5 58tBN [49] 28.6 26.8 37.0 13.2 35.4 14.4 12.6 18.0 18.2 16.0 8.6 13.3 24.0 20.3 27.8 20.9 58Single do. TENT [65]25.2 23.8 33.5 12.8 32.3 14.1 11.7 16.4 17.0 14.4 8.4 12.2 22.8 18.0 24.8 19.2 646Continual TENT [65]25.2 20.8 29.8 14.4 31.5 15.4 14.2 18.8 17.5 17.3 10.9 14.9 23.6 20.2 25.6 20.0 646TTT++ [42] 27.9 25.8 35.8 13.0 34.3 14.2 12.2 17.4 17.6 15.5 8.6 13.1 23.1 19.6 26.6 20.3 1405SWRNSP [9] 24.6 20.5 29.3 12.4 31.1 13.0 11.3 15.3 14.7 11.7 7.8 9.3 21.5 15.6 20.3 17.2 1551NOTE [17] 30.4 26.7 34.6 13.6 36.3 13.7 13.9 17.2 15.8 15.2 9.1 7.5 24.1 18.4 25.9 20.2 646EATA [50] 23.8 18.8 27.3 13.9 29.7 16.0 13.3 18.0 16.9 15.7 10.5 12.2 22.9 17.1 23.0 18.6 646CoTTA [66] 24.6 21.6 26.5 12.1 28.0 13.0 10.9 15.3 14.6 13.6 8.1 12.2 20.0 14.9 19.5 17.0 1697Ours (K=4) 23.5 19.0 26.6 11.5 30.6 13.1 10.9 15.2 14.5 13.1 7.8 11.4 20.9 15.4 20.8 16.9 404Ours (K=5) 23.8 18.7 25.7 11.5 29.8 13.3 11.3 15.3 15.0 13.0 7.9 11.3 20.2 15.1 20.5 16.8 471 Resnet-50 Source 65.6 60.7 74.4 28.9 79.9 46.0 25.7 35.0 49.4 54.7 13.0 83.2 41.2 46.7 27.7 48.8 91tBN [49] 18.0 17.2 29.3 10.7 27.2 15.5 8.9 16.7 14.6 21.0 9.3 12.7 20.9 12.4 14.8 16.6 91Single do. TENT [65]16.6 15.7 25.7 10.0 24.8 13.8 8.3 14.9 13.8 17.6 8.7 10.0 19.1 11.5 13.8 15.0 925TENT continual [65]16.6 14.4 22.9 10.4 22.6 13.4 10.3 15.8 14.6 18.0 10.5 11.7 18.4 13.1 15.3 15.2 925TTT++ [42] 18.2 16.9 28.7 10.5 26.5 14.5 8.9 16.5 14.5 20.9 9.0 9.0 20.4 12.3 14.7 16.1 1877SWRNSP [9] 17.3 16.1 26.1 10.6 25.6 14.1 8.7 15.6 13.6 18.6 8.8 10.0 19.3 12.0 14.2 15.4 1971EATA [50] 17.2 14.9 23.6 10.2 23.3 13.2 8.5 14.0 12.5 16.6 8.6 9.4 17.2 11.0 12.7 14.2 925CoTTA [66] 16.2 15.0 21.2 10.4 22.8 13.9 8.4 15.1 12.9 19.8 8.6 11.3 17.5 10.5 12.2 14.4 2066Ours (K=4) 16.5 14.5 24.3 9.7 23.7 13.3 8.8 14.7 12.9 17.0 9.1 9.4 17.6 11.4 13.1 14.4 296Ours (K=5) 16.6 14.4 23.6 9.8 23.4 12.7 8.6 14.5 12.6 16.6 8.7 9.0 17.0 11.3 12.6 14.1 498 Table 19. Comparison of error rate (%) on CIFARC10-C with severity level 5. We conduct experiments on continual TTA setup. Avg. err means the average error rate (%) of all 15 corruptions, and Mem. denotes total memory consumption, including model parameter sizes and activations. WRN refers to WideResNet. The implementation details of the baselines are described in Section E.1. Time t− − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − →Arch Method Gaus. Shot Impu. Defo. Glas. Moti. Zoom Snow Fros. Fog Brig. Cont. Elas. Pixe. JpegAvg. err Mem. WRN-40(AugMix) Source 80.1 77.0 76.4 59.9 77.6 64.2 59.3 64.8 71.3 78.3 48.1 83.4 65.8 80.4 59.2 69.7 11tBN [49] 45.9 45.6 48.2 33.6 47.9 34.5 34.1 40.3 40.4 47.1 31.7 39.7 42.7 39.2 45.6 41.1 11Single do. TENT [65]41.2 40.6 42.2 30.9 43.4 31.8 30.6 35.3 36.2 40.1 28.5 35.5 39.1 33.9 41.7 36.7 188continual TENT [65]41.2 38.2 41.0 32.9 43.9 34.9 33.2 37.7 37.2 41.5 33.2 37.2 41.1 35.9 45.1 38.3 188TTT++ [42] 46.0 45.4 48.2 33.5 47.7 34.4 33.8 39.9 40.2 47.1 31.8 39.7 42.5 38.9 45.5 41.0 391SWRNSP [9] 42.4 40.9 42.7 30.6 43.9 31.7 31.3 36.1 36.2 41.5 28.7 34.1 39.2 33.6 41.3 36.6 400NOTE [17] 50.9 47.4 49.0 37.3 49.6 37.3 37.0 41.3 39.9 47.0 35.2 34.7 45.2 40.9 49.9 42.8 188EATA [50] 41.6 39.9 41.2 31.7 44.0 32.4 31.9 36.2 36.8 39.7 29.1 34.4 39.9 34.2 42.2 37.1 188CoTTA [66] 43.5 41.7 43.7 32.2 43.7 32.8 32.2 38.5 37.6 45.9 29.0 38.1 39.2 33.8 39.4 38.1 409Ours (K=4) 42.7 39.6 42.4 31.4 42.9 31.9 30.8 35.1 34.8 40.7 28.1 35.0 37.5 32.1 40.5 36.4 80Ours (K=5) 41.8 39.0 41.9 31.2 42.7 32.5 31.0 35.0 35.0 39.9 28.8 34.5 37.5 32.8 40.5 36.3 92 Resnet-50 Source 84.7 83.5 93.3 59.6 92.5 71.9 54.8 66.6 77.6 81.8 44.3 91.2 72.2 76.6 56.5 73.8 91tBN [49] 48.1 46.7 60.6 35.1 58.0 41.8 33.2 47.3 43.5 54.9 33.5 35.3 49.8 38.4 40.8 44.5 91Single do. TENT [65]44.1 42.7 53.9 32.6 52.0 37.5 30.5 43.4 40.2 45.7 30.4 31.4 45.1 35.0 37.6 40.1 926continual TENT [65]44.0 40.1 49.9 34.7 50.6 40.0 33.6 47.0 45.7 53.4 42.5 46.2 56.1 51.2 53.3 45.9 926TTT++ [42] 48.1 46.5 60.8 35.1 57.8 41.6 32.9 46.8 43.3 55.0 33.3 34.0 50.0 38.1 40.6 44.2 1876SWRNSP [9] 48.3 46.5 60.5 35.1 57.9 41.7 32.9 47.1 43.5 54.7 33.5 35.1 49.9 38.3 40.7 44.1 1970EATA [50] 44.8 41.9 52.6 33.0 51.1 37.8 30.3 43.0 40.1 45.1 30.1 31.8 45.2 35.2 37.4 39.9 926CoTTA [66] 43.6 42.8 50.4 34.2 51.6 39.2 31.4 43.4 39.6 47.4 31.3 32.2 43.4 35.8 36.7 40.2 2064Ours (K=4) 44.8 40.3 49.2 32.3 50.1 36.3 29.5 41.0 39.9 44.6 31.5 33.7 45.3 36.3 37.7 39.5 296Ours (K=5) 44.9 40.4 48.9 32.7 49.7 36.9 29.3 40.8 39.0 44.4 31.1 33.6 44.0 35.7 37.8 39.3 498 Table 20. Comparison of error rate (%) on CIFARC100-C with severity level 5.We conduct experiments on continual TTA setup. Avg. err means the average error rate (%) of all 15 corruptions, and Mem. denotes total memory consumption, including model parameter sizes and activations. WRN refers to WideResNet. The implementation details of the baselines are described in Section E.1. 17",
      "meta_data": {
        "arxiv_id": "2303.01904v4",
        "authors": [
          "Junha Song",
          "Jungsoo Lee",
          "In So Kweon",
          "Sungha Choi"
        ],
        "published_date": "2023-03-03T13:05:30Z",
        "pdf_url": "https://arxiv.org/pdf/2303.01904v4.pdf"
      },
      "llm_extracted_info": {
        "main_contributions": "This paper introduces EcoTTA, a memory-efficient approach for continual Test-Time Adaptation (TTA). The primary contributions include a novel architecture utilizing lightweight meta networks that adapt frozen original networks to target domains, significantly minimizing memory consumption by reducing intermediate activation sizes (up to 86% less memory than CoTTA). It also proposes a self-distilled regularization technique that prevents catastrophic forgetting and error accumulation during long-term adaptation by controlling meta network outputs to align with frozen original network outputs, preserving source knowledge without additional memory. The method demonstrates superior TTA performance and memory efficiency across image classification (CIFAR-C, ImageNet-C) and semantic segmentation (Cityscapes-C) tasks.",
        "methodology": "EcoTTA consists of two main components. First, a memory-efficient architecture uses frozen original networks to discard intermediate activations, attaching lightweight meta networks (one batch normalization and one convolution block) to K partitioned parts of the original network. These meta networks are warmed up on source data before deployment. Shallow layers of the encoder are partitioned more densely. Second, a self-distilled regularization method is introduced, employing an L1 loss (Rk = ||~xk - xk||1) to prevent the meta network's output (~xk) from significantly deviating from the frozen original network's output (xk). This preserves source knowledge and prevents error accumulation. The overall loss combines entropy minimization (Lent) with this regularization loss (Ltotal = Lent + λ * sum(Rk)).",
        "experimental_setup": "Experiments were conducted on continual TTA tasks using image classification and semantic segmentation. For classification, CIFAR10-C, CIFAR100-C, and ImageNet-C datasets were used, featuring 15 corruption types with 5 severity levels. WideResNet-28, WideResNet-40, and ResNet-50 were used as backbone models. For semantic segmentation, Cityscapes-C (with brightness, fog, frost, and snow corruptions) was used with a DeepLabV3+-ResNet-50 model. The continual TTA setup involves adapting the model to each corruption type sequentially without resetting. Evaluation metrics include error rates (for classification), mean Intersection over Union (mIoU for segmentation), and total memory consumption (model parameters + activations). Meta networks were warmed up using SGD (LR 5e-2) for 10 epochs (CIFAR) or 3 epochs (ImageNet). Test-time adaptation used SGD (LR 5e-3), batch sizes of 64 (CIFAR) and 32 (ImageNet), an entropy threshold of 0.4*ln C, and a regularization importance (λ) of 0.5.",
        "limitations": "The proposed method requires access to the source dataset for a pre-deployment warm-up phase of the meta networks, which might not always be universally available in all TTA scenarios. Although the paper argues this is feasible, it is a dependency. Additionally, the paper notes an engineering-based issue where current PyTorch implementations do not fully support fine-grained gradient computations, meaning the theoretical computational efficiency of EcoTTA is higher than its measured wall-clock time. Performance gain from increasing the model partition factor K tends to saturate beyond K=4 or K=5, indicating diminishing returns for further partitioning.",
        "future_research_directions": "The paper concludes with a broad statement that the efforts made will \"facilitate a variety of studies that make test-time adaptation for edge devices feasible in practice.\" This suggests a general direction towards further practical deployment and optimization of TTA on resource-constrained edge devices. Specific avenues could involve further integration with advanced small-batch size adaptation techniques beyond AdaptBN, or exploring more complex adaptation losses while maintaining memory efficiency."
      }
    },
    {
      "title": "Robust Test-Time Adaptation in Dynamic Scenarios",
      "abstract": "Test-time adaptation (TTA) intends to adapt the pretrained model to test\ndistributions with only unlabeled test data streams. Most of the previous TTA\nmethods have achieved great success on simple test data streams such as\nindependently sampled data from single or multiple distributions. However,\nthese attempts may fail in dynamic scenarios of real-world applications like\nautonomous driving, where the environments gradually change and the test data\nis sampled correlatively over time. In this work, we explore such practical\ntest data streams to deploy the model on the fly, namely practical test-time\nadaptation (PTTA). To do so, we elaborate a Robust Test-Time Adaptation (RoTTA)\nmethod against the complex data stream in PTTA. More specifically, we present a\nrobust batch normalization scheme to estimate the normalization statistics.\nMeanwhile, a memory bank is utilized to sample category-balanced data with\nconsideration of timeliness and uncertainty. Further, to stabilize the training\nprocedure, we develop a time-aware reweighting strategy with a teacher-student\nmodel. Extensive experiments prove that RoTTA enables continual testtime\nadaptation on the correlatively sampled data streams. Our method is easy to\nimplement, making it a good choice for rapid deployment. The code is publicly\navailable at https://github.com/BIT-DA/RoTTA",
      "full_text": "Robust Test-Time Adaptation in Dynamic Scenarios Longhui Yuan Binhui Xie Shuang Li \f School of Computer Science and Technology, Beijing Institute of Technology {longhuiyuan,binhuixie,shuangli}@bit.edu.cn Abstract Test-time adaptation (TTA) intends to adapt the pre- trained model to test distributions with only unlabeled test data streams. Most of the previous TTA methods have achieved great success on simple test data streams such as independently sampled data from single or multiple distri- butions. However, these attempts may fail in dynamic sce- narios of real-world applications like autonomous driving, where the environments gradually change and the test data is sampled correlatively over time. In this work, we ex- plore such practical test data streams to deploy the model on the fly, namely practical test-time adaptation (PTTA). To do so, we elaborate a Robust Test-Time Adaptation (RoTTA) method against the complex data stream in PTTA. More specifically, we present a robust batch normalization scheme to estimate the normalization statistics. Meanwhile, a memory bank is utilized to sample category-balanced data with consideration of timeliness and uncertainty. Further, to stabilize the training procedure, we develop a time-aware reweighting strategy with a teacher-student model. Exten- sive experiments prove that RoTTA enables continual test- time adaptation on the correlatively sampled data streams. Our method is easy to implement, making it a good choice for rapid deployment. The code is publicly available at https://github.com/BIT-DA/RoTTA 1. Introduction In recent years, many machine learning problems have made considerable headway with the success of deep neu- ral networks [13, 22, 33, 38]. Unfortunately, the perfor- mance of deep models drops significantly when training data and testing data come from different distributions [59], which limits their utility in real-world applications. To re- duce the distribution shift, a handful of works focus on transfer learning field [56], in particular, domain adapta- tion (DA) [17, 42, 45, 48, 69, 72] or domain generalization (DG) [40, 41, 52, 71, 83], in which one or more different but \fCorresponding author Test data stream Continual TTANon-i.i.d.TTAPractical  TTACategoryDistribution Fully TTA Correlation samplingDistributionchanging Figure 1. We consider the practical test-time adaptation (TTA) setup and compare it with related ones. First, Fully TTA [70] adapts models on a fixed test distribution with an independently sampled test stream. Then, on this basis, Continual TTA [73] takes the continually changing distributions into consideration. Next, Non-i.i.d. TTA [19] tries to tackle the correlatively sampled test streams on a single test distribution, where the label distribution among a batch of data deviates from that of the test distribution. To be more practical, Practical TTA strives to connect both worlds: distribution changing and correlation sampling. related labeled datasets (a.k.a. source domain) are collected to help the model generalize well to unlabeled or unseen samples in new datasets (a.k.a. target domain). While both DA and DG have extensively studied the problem of distribution shifts, they typically assume acces- sibility to the raw source data. However, in many practical scenarios like personal consumption records, the raw data should not be publicly available due to data protection reg- ulations. Further, existing methods have to perform heavy backward computation, resulting in unbearable training costs. Test-time adaptation (TTA) [3,11,16,24,26,54,65,81] attempts to address the distribution shift online at test time with only unlabeled test data streams. Unequivocally, TTA has drawn widespread attention in a variety of applications, e.g., 2D/3D visual recognition [2, 29, 49, 65, 82], multi- modality [63, 64] and document understanding [15]. Prior TTA studies [7, 20, 70, 73] mostly concentrate on a simple adaptation scenario, where test samples are inde- pendently sampled from a fixed target domain. To name a few, Sun et al. [65] adapt to online test samples drawn from a constant or smoothly changing distribution with an auxil- iary self-supervised task. Wang et al. [70] adapt to a fixed arXiv:2303.13899v1  [cs.CV]  24 Mar 2023Table 1. Comparison between our proposed practical test-time adaptation (PTTA) and related adaptation settings. Setting Adaptation StageAvailable Data Test Data Stream Train Test Source Target Distribution Sampling Protocol Domain Adaptation ! % ! ! - - Domain Generalization ! % ! % - - Test-Time Training [65] ! ! ! ! stationary independently Fully Test-Time Adaptation [70] % ! % ! stationary independently Continual Test-Time Adaptation [73]% ! % ! continually changing independently Non-i.i.d. Test-Time Adaptation [5, 19]% ! % ! stationary correlatively Practical Test-Time Adaptation (Ours)% ! % ! continually changing correlatively target distribution by performing entropy minimization on- line. However, such an assumption is violated when the test environments change frequently [73]. Later on, Boudiaf et al. [5] and Gonget al. [19] consider the temporal correlation ship within test samples. For example, in autonomous driv- ing, test samples are highly correlated over time as the car will follow more vehicles on the highway or will encounter more pedestrians in the streets. More realistically, the data distribution changes as the surrounding environment alerts in weather, location, or other factors. In a word, distribution change and data correlation occur simultaneously in reality. Confronting continually changing distributions, tradi- tional algorithms like pseudo labeling or entropy minimiza- tion become more unreliable as the error gradients cumu- late. Moreover, the high correlation among test samples re- sults in the erroneous estimation of statistics for batch nor- malization and collapse of the model. Driven by this analy- sis, adapting to such data streams will encounter two major obstacles: 1) incorrect estimation in the batch normaliza- tion statistics leads to erroneous predictions of test samples, consequently resulting in invalid adaptation; 2) the model will easily or quickly overfit to the distribution caused by the correlative sampling. Thus, such dynamic scenarios are pressing for a new TTA paradigm to realize robust adapta- tion. In this work, we launch a more realistic TTA setting, where distribution changing and correlative sampling oc- cur simultaneously at the test phase. We call this Practical Test-Time Adaptation, or briefly,PTTA. To understand more clearly the similarities and differences between PTTA and the previous setups, we visualize them in Figure 1 and sum- marize them in Table 1. To conquer this challenging prob- lem, we propose a Robust Test-Time Adaptation (RoTTA) method, which consists of three parts: 1) robust statistics es- timation, 2) category-balanced sampling considering time- liness and uncertainty and 3) time-aware robust training. More concretely, we first replace the erroneous statistics of the current batch with global ones maintained by the expo- nential moving average. It is a more stable manner to esti- mate the statistics in BatchNorm layers. Then, we simulate a batch of independent-like data in memory with category- balanced sampling while considering the timeliness and un- certainty of the buffered samples. That is, samples that are newer and less uncertain are kept in memory with higher priority. With this batch of category-balanced, timely and confident samples, we can obtain a snapshot of the current distribution. Finally, we introduce a time-aware reweight- ing strategy that considers the timeliness of the samples in the memory bank, with a teacher-student model to perform robust adaptation. With extensive experiments, we demon- strate that RoTTA can robustly adapt in the practical setup, i.e., PTTA. In a nutshell, our contributions can be summarized as: • We propose a new test-time adaptation setup that is more suitable for real-world applications, namely practical test-time adaptation (PTTA). PTTA considers both distribution changing and correlation sampling. • We benchmark the performance of prior methods in PTTA and uncover that they only consider one aspect of the problem, resulting in ineffective adaptation. • We propose a robust test-time adaptation method (RoTTA), which has a more comprehensive considera- tion of PTTA challenges. Ease of implementation and effectiveness make it a practical deployment option. • We extensively demonstrate the practicality of PTTA and the effectiveness of RoTTA on common TTA benchmarks [23], i.e., CIFAR-10-C and CIFAR-100- C and a large-scale DomainNet [58] dataset. RoTTA obtains state-of-the-art results, outperforming the best baseline by a large margin (reducing the averaged classification error by over 5.9%, 5.5% and 2.2% on CIFAR-10-C, CIFAR-100-C and DomainNet, respec- tively). 2. Related Work Domain adaptation (DA) studies the problem of transfer- ring the knowledge learned from a labeled source dataset to an unlabeled target dataset [8, 17, 43, 51, 67, 68]. Represen- tative techniques include latent distribution alignment [48, 77], adversarial training [17, 62], or self-training [75, 85]. The limitation of this setting, however, is that an unlabeled test dataset (target domain) is needed at training time, in addition to a labeled training dataset (source domain). Ac- cordingly, it might fail to handle more practical scenariosFeature 𝐹Robust batch normalization (RBN)Update𝜇௚, 𝜎௚ଶNormalizeFeature𝐹′Update bank with current sample  Training lossℒ௥in Eq. (7) Teacher StudentAdaptation with RBNMemorybankEMA 𝑡A stream of online dataUpdateTest timeCorrelationsamplingStrong & weakaugmentation flowDistributionsCategoryTeacherMajor classhas highest ℋin majorRemoveAddWhen ℋ>ℋSamples to beadded& removed Figure 2. Framework overview. Firstly, we replace the batch normalization layer with RBN which robustly normalizes the feature map. During the inference of the online test stream of PTTA, we utilize the predictions of samples to maintain a memory bank by category- balanced sampling with timeliness and uncertainty. Finally, we use the category-balanced, timely and confident data in the memory bank combined with a robust loss to adapt the model at test time. like test-time adaptation. Our practical test-time adaptation setting can be viewed as performing correlatively sample adaptation on the fly. It is worth noting that standard domain adaptation techniques might collapse when only continual data streams from multiple target domains are accessible. Domain generalization (DG) assumes that multiple source domains are available for model training and tries to learn models that can generalize well to any unseen domains [4, 26,40,41,52,84]. A broad spectrum of methodologies based on data augmentation [78, 84], meta-learning [14, 40], or domain alignment [50,52] has made great progress. In con- trast, this work instead aims to improve the performance of source pre-trained models at the test time by using unla- beled online data streams from multiple continually chang- ing target domains. Continual learning (CL) (also known as incremental learning, life-long learning) addresses the problem of learn- ing a model for many tasks sequentially without forgetting knowledge obtained from the preceding tasks. [1, 6, 31, 37, 60]. CL methods can often be categorized into replay- based [60, 66] and regularization-based [31, 44] methods. Ideas from continual learning are also adopted for continu- ous domain adaptation approaches [34, 74] In our work, we share the same motivation as CL and point out that prac- tical test-time adaptation (PTTA) also suffers catastrophic forgetting (i.e., performance degradation on new test sam- ples due to correlation sampling), which makes test-time adaptation approaches are unstable to deploy. Test-time adaptation (TTA) focus on more challenging settings where only source model and unlabeled target data are available [9, 18, 27, 28, 35, 46, 61]. A similar paradigm is source-free domain adaptation (SFDA) [10, 36, 47, 79], which also requires no access to the training (source) data. To name a few, Liang et al . [45] fit the source hypoth- esis by exploiting the information maximization and self- supervised pseudo-labeling. Kundu et al. [35] formalize a unified solution that explores SFDA without any category- gap knowledge. To fully utilize any arbitrary pre-trained model, Sun et al. [65] propose conducting adaptation on the fly with an auxiliary self-supervised task. Later on, Wanget al. [70] take a source pre-trained model and adapt it to the test data by updating a few trainable parameters in Batch- Norm layers [25] using entropy minimization [21]. While standard TTA has been widely studied in many tasks [2, 20, 63, 64, 70, 82], the fact remains that both dis- tribution changing [73] and data correlation sampling [19] has only been considered in isolation. For example, Gong et al. [19] propose instance-aware batch normalization and prediction-balanced reservoir sampling to address the chal- lenges of correlatively sampled test streams, however, it does not consider unstable adaptation resulting from long- term adaptation on continually changing distributions. On the other hand, Wang et al. [73] assume that the target test data is streamed from a continually changing environment and continually adapt an off-the-shelf source pre-trained model to the current test data. In this work, we launch PTTA, a more practical TTA setting to connect both worlds: distribution changing and correlation sampling. 3. Method 3.1. Problem Definition and Motivation Given a model fθ0 with parameter θ0 pre-trained on source domain DS = {(xS, yS)}, the proposed practical test-time adaptation (PTTA) aims to adapt fθ0 to a stream of online unlabeled samples X0, X1, ...,XT , where Xt is a batch of highly correlated samples from the distribution Ptest that changes with time t continually. More specifi- cally, at test time, with time going on, the test distribution Ptest changes continually as P0, P1, ...,P∞. At time step t, we will receive a batch of unlabeled and correlated samplesmotion distribution changing snow time  Distributions and Labels of PTTA T est Stream uniform 10 1 0.1 0.01 0.001 Dirichlet Parameter  Figure 3. Illustration of the labels and distributions of the test stream of CIFAR10-C under the setup PTTA. And we adopt Dirichlet distribution to simulate the process of correlative sam- pling. It is clear that as the concentration parameter δ decreases, the correlation among sampled data increases, which is reflected in the increasing aggregation of categories. Xt from Ptest. Next, Xt is fed into the model fθt and the model needs to adapt itself to the current test data streams and make predictions fθt (Xt) on the fly. As a matter of fact, this setup is largely driven the prac- tical demands of deploying models in dynamic scenarios. Taking for example the case of autonomous driving men- tioned in § 1, test samples are highly correlated and the data distribution changes continually with the weather or loca- tion. Another example is the situation of intelligent moni- toring, the camera will continuously capture more people at certain times, such as after work, but fewer of them during work time. Meanwhile, the light condition changes con- tinually from day to night. The deployed model should be robustly adapted in such dynamic scenarios. In a word, dis- tribution change and data correlation often happen simul- taneously in the real world. For this reason, existing TTA methods [7,9,19,28,70,73,81] might become unstable when the test stream is sampled from such dynamic scenarios. To obtain the test stream of PTTA, we adopt Dirich- let Distribution with parameter δ to simulate the correla- tion among test samples. We present the test data streams corresponding to different values of δ on the CIFAR10-C dataset in Figure 3. We can observe that the smaller δ is, the higher the correlation will be. For the sake of unity, we set δ = 0.1 as the default for all experiments. In the follow- ing, we present a robust test-time adaptation framework for the practical test-time adaptation setup defined above. An overview of our RoTTA is illustrated in Figure 2. 3.2. Robust Test-Time Adaptation Motivated by the fact that the statistics of current batch data, which are commonly used in previous TTA meth- ods [7, 20, 65, 70, 73], become unreliable when they en- counter correlative test data streams, we first turn to the global robust statistics for normalization. Then, to effec- tively adapt to the current distribution, we maintain a mem- ory bank by category-balanced sampling with considering timeliness and uncertainty, which captures a more stable snapshot of the distribution. Finally, we utilize the teacher- student model and design a timeliness-based reweighting strategy to train the model robustly. Robust batch normalization (RBN). Batch Normaliza- tion (BN) [25] is a widely-used training technique as it can accelerate the training and convergence speed of networks and stabilize the training process by reducing the risk of gradient explosion and vanishing. Given the feature map F ∈ RB×C×H×W as the input for a BN layer when train- ing, the channel-wise mean µ ∈ RC and variance σ2 ∈ RC are calculated as follows: µc = 1 BHW BX b=1 HX h=1 WX w=1 F(b,c,h,w) , (1) σ2 c = 1 BHW BX b=1 HX h=1 WX w=1 (F(b,c,h,w) − µc)2 . (2) Then the feature map is normalized and refined in a channel-wise manner as BN (F(b,c,h,w); µ, σ2) =γc F(b,c,h,w) − µc √σ2c + ϵ + βc , (3) where γ, β∈ RC are learnable parameters in the layer and ϵ > 0 is a constant for numerical stability. Meanwhile, during training, the BN layer maintains a group of global running mean and running variance (µs, σ2 s) for inference. Due to the domain shift at test time, the global statis- tics (µs, σ2 s) normalize test features inaccurately, causing significant performance degradation. To tackle the prob- lem above, some methods [55, 70, 73] use the statistics of the current batch to perform normalization. Unfortunately, when the test samples have a high correlation under PTTA setup, the statistics of the current batch also fail to correctly normalize the feature map, as demonstrated in Figure 4c. Specifically, the performance of BN [53] decreases rapidly as the data correlation increases. Based on the analysis above, we propose a robust batch normalization (RBN) module, which maintains a group of global statistics (µg, σ2 g) to normalize the feature map ro- bustly. Before the whole test-time adaptation, (µg, σ2 g) is initialized as the running mean and variance (µs, σ2 s) of the pre-trained model. When adapting the model, we update the global statistics first by exponential moving average as µg = (1− α)µg + αµ , (4) σ2 g = (1− α)σ2 g + ασ2 , (5) where (µ, σ2) is the statistics of the buffered samples in the memory bank. Then we normalize and affine the feature as Eq. (3) with (µg, σ2 g). When inferring for test samples, we directly utilize (µg, σ2 g) to calculate the output as Eq (3). Al- though simple, RBN is effective enough to tackle the prob- lem of normalization on test streams of PTTA.Category-balanced sampling with timeliness and uncer- tainty (CSTU). In the PTTA setup, the correlation among test samples Xt at time t leads to a deviation between the observed distribution bPtest and the test distribution Ptest. Specifically, the marginal label distribution p(y|t) tends to differ from p(y). Continuously learning with Xt over time t can lead to model adaptation to an unreliable distribution bPtest, resulting in ineffective adaptation and an increased risk of model collapse. To address this issue, we propose a category-balanced memory bank M with a capacity of N, which takes into account the timeliness and uncertainty of samples when up- dating. In particular, we adopt the predictions of test sam- ples as pseudo labels to guide the update ofM. Meanwhile, to guarantee the balance among categories, we distribute the capacity of M equally to each category, and samples of the major categories will be replaced first (refer to lines 5-9 in Algorithm 1). Furthermore, due to the continually changing test distribution, old samples in M are limited in value, and could even impair the ability of the model to adapt to the current distribution. Additionally, samples of high uncer- tainty always produce erroneous gradient information that can hinder model adaptation, as suggested by [55]. With this in mind, we attach each sample in M with a group of heuristics (A, U), where A, initialized as 0 and in- creasing with time t, is the age of the sample, and U the un- certainty calculated as the entropy of the prediction. Next, we combine the timeliness and uncertainty to calculate a heuristic score, i.e., category-balanced sampling with time- liness and uncertainty (CSTU), as follows: H = λt 1 1 + exp(−A/N) + λu U log C , (6) where λt and λu make the trade-off between timeliness and uncertainty, and for simplicity, λt and λu are set to 1.0 for all experiments, andC is the number of categories. We sum- marize our sampling algorithm in Algorithm 1. With CSTU, we can obtain a robust snapshot of the current test distribu- tion Ptest, and effectively adapt the model to it. Robust training with timeliness. Actually, after replacing BN layers with our RBN and obtaining the memory bank selected via CSTU, we can directly adopt the widely used techniques like pseudo labeling or entropy minimization to perform test-time adaptation. However, we notice that too old or unreliable instances still have the opportunity to stay in M since keeping the category balance is assigned the top priority. In addition, too aggressive updates of the model will make the category balance ofM unreliable, resulting in unstable adaptation. Meanwhile, error accumulation caused by the distribution change also makes the aforementioned approaches unworkable. To further reduce the risk of error gradients information from old and unreliable instances and stabilize the adapta- tion, we turn to the robust unsupervised learning method Algorithm 1: CSTU for one test sample. 1 Input: a test sample x and the teacher model fθT . 2 Define: memory bank M and its capacity N, number of classes C, per class occupation O ∈RC, total occupation Ω, classes to pop instance D. 3 Infer as p(y|x) =Softmax(fθT (x)). 4 Calculate the predicted category of x as ˆy = arg maxc p(c|x), the uncertainty as Ux = −PC c=1 p(c|x) log(p(c|x)), the age as Ax = 0, and the heuristic score Hx of x with Eq (6) 5 if Oˆy < N C then 6 if Ω <N: Search range D = ∅. 7 else: Search range D = {j|j = arg maxc Oc} 8 else 9 Search range D = {ˆy} 10 if D is ∅ then 11 Add (x, ˆy, Hx, Ux) into M. 12 else 13 Find the instance (ˆx, yˆx, Aˆx, Uˆx) with the highest value in Eq (6) Hˆx among D. 14 if Hx < Hˆx then 15 Remove (ˆx, yˆx, Aˆx, Uˆx) from M. 16 Add (x, ˆy, Hx, Ux) into M. 17 else 18 Discard x. 19 Increase the age of all instances in M. teacher-student model and propose a timeliness reweight- ing strategy. In addition, for the sake of time efficiency and stability, only affine parameters in RBN are trained during adaptation. At time step t, after inferring for the correlated data Xt with the teacher model fθT t and updating the memory bank M with Xt, we begin updating the student model fθS t and the teacher model fθT t . Firstly, we update parameters of stu- dent model θS t → θS t+1 by minimizing the following loss: Lr = 1 Ω ΩX i=1 L(xM i , Ai; θT t , θS t ) , (7) where Ω = |M| is the total occupation of the memory bank, and xM i and Ai(i = 1, ..., Ω) are instances in the memory bank and their age respectively. Subsequently, the teacher model is updated by exponential moving average as θT t+1 = (1− ν)θT t + νθS t+1 . (8) To calculate the loss value of an instancexM i from the mem- ory bank, the timeliness reweighting term is computed as E(Ai) = exp(−Ai/N) 1 + exp(−Ai/N) , (9)where Ai is the age of xM i , and N is the capacity of the bank. And then we calculate the cross entropy between the soft-max prediction pS(y|x′′ i ) of the strong-augmented view x′′ i from the student model and that pT (y|x′ i) of the weak- augmented view 1 x′ i from the teacher model as follows: ℓ(x′ i, x′′ i ) =−1 C CX c=1 pT (c|x′ i) logpS(c|x′′ i ) . (10) Finally, equipped with Eq. (9) and Eq. (10), the right-hand side of Eq. (7) reduces to L(xM i , Ai; θT t , θS t ) =E(Ai)ℓ(x′ i, x′′ i ) . (11) To sum up, equipped with RBN, CSTU, and robust training with timeliness, our RoTTA is capable of effectively adapt- ing any pre-trained models in dynamic scenarios. 4. Experiments 4.1. Setup Datasets. CIFAR10-C and CIFAR100-C [23] are the com- monly used TTA benchmarks to testify the robustness un- der corruptions. Both of them are obtained by applying 15 kinds of corruption with 5 different degrees of severity on their clean test images of original datasets CIFAR10 and CIFAR100 respectively. CIFAR10/CIFAR100 [32] have 50,000/10,000 training/test images, all of which fall into 10/100 categories. DomainNet [58] is the largest and hard- est dataset to date for domain adaptation and consists of about 0.6 million images with 345 classes. It consists of six different domains including Clipart (clp), Infograph (inf), Painting (pnt), Quickdraw (qdr), Real (rel), and Sketch (skt). We first pre-train a source model on the train set in one of six domains and testify all baseline methods on the test set of the remaining five domains. Implementation details. All experiments are conducted with PyTorch [57] framework. In the case of robustness to corruption, following the previous methods [55, 70, 73], we obtain the pre-trained model from RobustBench bench- mark [12], including the WildResNet-28 [80] for CIFAR10 → CIFAR10-C, and the ResNeXt-29 [76] for CIFAR100 → CIFAR100-C. Then, we change the test corruption at the highest severity 5 one by one to simulate that the test distri- bution continually changes with time in PTTA. And in the case of generalization under the huge domain gap, we train a ResNet-101 [22] by standard classification loss for each domain in DomainNet and adapt them continually to differ- ent domains except the source domain. Meanwhile, we uti- lize the Dirichlet distribution to simulate the correlatively sampled test stream for all datasets. For optimization, we adopt Adam [30] optimizer with learning rate 1.0 × 10−3, 1Weak augmentation is ReSize+CenterCrop. Strong augmentation is a combination nine operations like Clip, ColorJitter, and RandomAffine. β = 0.9. For a fair comparison, we set the batch size for all methods as 64 and the capacity of the memory bank of RoTTA as N = 64. Concerning the hyperparameters, we adopt a unified set of values for RoTTA across all experi- ments including α = 0.05, ν = 0.001, λt = 1.0, λu = 1.0, and δ = 0.1. More details are provided in the appendix. 4.2. Comparisons with the State-of-the-arts Robustness under corruptions. The classification error on CIFAR10→CIFAR10-C and CIFAR100→CIFAR100-C are shown in Table 2 and Table 3 respectively. We change the type of the current corruption at the highest severity 5 as time goes on, and sample data correlatively for infer- ence and adaptation simultaneously. The same test stream is shared across all compared methods. From Table 2 and Table 3, we can see that RoTTA achieves the best performance compared to previous meth- ods. Moreover, RoTTA has a significant performance gain to the second-best method that 5.9% improvement on CIFAR10 →CIFAR10-C and 5.5% improvement on CIFAR100→CIFAR100-C respectively, verifying the effec- tiveness of RoTTA to adapt the model under PTTA. In more detail, we can observe that BN [53], PL [39], TENT [70] and CoTTA [73] negatively adapt the model to the test streams of both datasets compared to Source (−6.5 ∼ −46.4%). This is attributed to the fact that these methods overlook the issues posed by correlation sampling, which can result in highly correlated data within a batch. As a consequence, traditional normalization statistics may be ineffective in appropriately normalizing the feature maps. Equipped with RBN and CSTU, RoTTA no longer suffers from this issue. Meanwhile, in Table 3, if focus on the adaptation procedure, we can see that the performance of PL [39], TENT [70] and NOTE [19] becomes worse and worse, and eventually, the model even collapses (error rate > 97%). This reveals that the impact of error accumula- tion on long-term adaptation can be catastrophic. To tackle this problem, RoTTA turns to robustly adapt the model with timeliness reweighting and confident samples in the mem- ory bank, and superior performance throughout the adapta- tion process demonstrates its effectiveness. In addition, we find that although LAME [5] never tunes the parameters of the model, it is still a competi- tive baseline for example it achieves the second-best result on CIFAR100→CIFAR100-C. However, its performance is very dependent on the performance of the pre-trained model e.g. negligible improvement on difficult corruptions (shot, gaussian, pixelate). On the contrary, our RoTTA is more flexible and achieves better and more robust results. Generalization under domain shift. We also evalu- ate RoTTA under a more challenging dataset DomainNet, where we continually adapt a source pre-trained model to correlatively sampled test streams of the rest domains. AsTable 2. Average classification error of the task CIFAR10→ CIFAR10-C while continually adapting to different corruptions at the highest severity 5 with correlatively sampled test stream under the proposed setup PTTA. Time t− − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − → Method motionsnow fog shot defocuscontrastzoom brightnessfrost elasticglass gaussianpixelatejpeg impulse Avg. Source 34.8 25.1 26.0 65.7 46.9 46.7 42.0 9.3 41.3 26.6 54.3 72.3 58.5 30.3 72.9 43.5BN [53] 73.2 73.4 72.7 77.2 73.7 72.5 72.9 71.0 74.1 77.7 80.0 76.9 75.5 78.3 79.0 75.2PL [39] 73.9 75.0 75.6 81.0 79.9 80.6 82.0 83.2 85.3 87.3 88.3 87.5 87.5 87.5 88.2 82.9TENT [70] 74.3 77.4 80.1 86.2 86.7 87.3 87.9 87.4 88.2 89.0 89.2 89.0 88.3 89.7 89.2 86.0LAME [5] 29.5 19.0 20.3 65.3 42.4 43.4 36.8 5.4 37.2 18.6 51.2 73.2 57.0 22.6 71.3 39.5CoTTA [73]77.1 80.6 83.1 84.4 83.9 84.2 83.1 82.6 84.4 84.2 84.5 84.6 82.7 83.8 84.9 83.2NOTE [19] 18.0 22.1 20.6 35.6 26.9 13.6 26.5 17.3 27.2 37.0 48.3 38.8 42.6 41.9 49.7 31.1 RoTTA 18.1 21.3 18.8 33.6 23.6 16.5 15.1 11.2 21.9 30.7 39.6 26.8 33.7 27.8 39.5 25.2(+5.9) Table 3. Average classification error of the task CIFAR100 → CIFAR100-C while continually adapting to different corruptions at the highest severity 5 with correlatively sampled test stream under the proposed setup PTTA. Time t− − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − → Method motionsnow fog shot defocuscontrastzoom brightnessfrost elasticglass gaussianpixelatejpeg impulse Avg. Source 30.8 39.5 50.3 68.0 29.3 55.1 28.8 29.5 45.8 37.2 54.1 73.0 74.7 41.2 39.4 46.4BN [53] 48.5 54.0 58.9 56.2 46.4 48.0 47.0 45.4 52.9 53.4 57.1 58.2 51.7 57.1 58.8 52.9PL [39] 50.6 62.1 73.9 87.8 90.8 96.0 94.8 96.4 97.4 97.2 97.4 97.4 97.3 97.4 97.4 88.9TENT [70] 53.3 77.6 93.0 96.5 96.7 97.5 97.1 97.5 97.3 97.2 97.1 97.7 97.6 98.0 98.3 92.8LAME [5] 22.4 30.4 43.9 66.3 21.3 51.7 20.6 21.8 39.6 28.0 48.7 72.8 74.6 33.1 32.3 40.5CoTTA [73]49.2 52.7 56.8 53.0 48.7 51.7 49.4 48.7 52.5 52.2 54.3 54.9 49.6 53.4 56.2 52.2NOTE [19] 45.7 53.0 58.2 65.6 54.2 52.0 59.8 63.5 74.8 91.8 98.1 98.3 96.8 97.0 98.2 73.8 RoTTA 31.8 36.7 40.9 42.1 30.0 33.6 27.9 25.4 32.3 34.0 38.8 38.7 31.3 38.0 42.9 35.0(+5.5) Table 4. Average classification error of DomainNet while continually adapting to different domains with correlatively sampled test stream. Timet− − − − − − − − − − − − − − − − − − →Timet− − − − − − − − − − − − − − − − − − →Timet− − − − − − − − − − − − − − − − − − →Timet− − − − − − − − − − − − − − − − − − →Sourceclp inf pnt qdr rel sktAvg. BN clp inf pnt qdr rel sktAvg. PL clp inf pnt qdr rel sktAvg.TENTclp inf pnt qdr rel sktAvg. clp N/A 83.9 65.4 88.6 48.0 59.1 69.0clp N/A 88.6 70.7 90.5 65.4 67.0 76.5clp N/A 94.5 98.9 99.5 99.7 99.7 98.5clp N/A 87.5 71.9 94.2 96.2 98.9 89.7inf 61.8 N/A 66.9 96.0 50.0 70.6 69.1inf 68.6 N/A 74.2 96.2 69.9 76.8 77.1inf 82.6 N/A 99.2 99.6 99.7 99.3 96.1inf 68.6 N/A 75.0 97.3 95.9 98.7 87.1pnt 56.5 83.7 N/A 94.2 42.6 63.4 68.1pnt 60.8 87.9 N/A 94.3 62.3 68.7 74.8pnt 78.6 99.4 N/A 99.7 99.6 99.7 95.4pnt 61.7 87.1 N/A 96.4 95.3 98.8 87.8qdr 89.2 99.0 98.6 N/A 95.0 92.3 94.8qdr 80.3 97.7 92.6 N/A 88.7 88.1 89.5qdr 81.7 99.5 99.6 N/A 99.7 99.8 96.1qdr 78.9 97.1 91.6 N/A 89.2 88.7 89.1rel 49.4 80.4 51.5 93.4 N/A 63.3 67.6rel 57.9 87.1 63.1 94.3 N/A 70.8 74.6rel 73.5 99.4 99.2 99.6 N/A 99.7 94.3rel 57.8 86.4 68.1 96.9 N/A 96.7 81.2skt 47.5 88.2 62.9 87.1 51.8 N/A 67.5skt 50.4 87.6 64.6 89.6 63.1 N/A 71.1skt 64.8 99.2 99.4 99.7 99.7 N/A 92.6skt 51.9 87.2 69.1 95.3 97.3 N/A 80.1Avg.60.9 87.0 69.1 91.9 57.5 69.7 72.7Avg.63.6 89.8 73.0 93.0 69.9 74.3 77.3Avg.76.2 98.4 99.3 99.6 99.7 99.6 95.5Avg.63.8 89.0 75.1 96.0 94.8 96.4 85.8 Timet− − − − − − − − − − − − − − − − − − →Timet− − − − − − − − − − − − − − − − − − →Timet− − − − − − − − − − − − − − − − − − →Timet− − − − − − − − − − − − − − − − − − →LAMEclp inf pnt qdr rel sktAvg.COTTAclp inf pnt qdr rel sktAvg.NOTEclp inf pnt qdr rel sktAvg.RoTTAclp inf pnt qdr rel sktAvg. clp N/A 82.2 64.5 87.7 46.9 58.9 68.0clp N/A 90.6 77.9 89.3 76.3 72.7 81.4clp N/A 89.2 73.0 94.8 98.4 99.4 91.0clp N/A 85.5 62.0 82.0 49.3 59.8 67.7inf 60.1 N/A 65.7 95.4 48.5 69.4 67.8inf 74.5 N/A 82.0 95.7 80.2 81.5 82.8inf 75.4 N/A 78.7 98.7 98.1 99.5 90.1inf 61.8 N/A 63.7 91.5 52.5 67.6 67.4pnt 55.8 81.5 N/A 93.3 41.3 62.1 66.8pnt 66.3 89.8 N/A 93.4 74.0 75.4 79.8pnt 64.7 89.8 N/A 97.8 98.4 99.2 90.0pnt 53.3 84.1 N/A 89.1 47.3 61.4 67.0qdr 88.3 99.1 99.0 N/A 94.9 92.2 94.7qdr 82.3 98.2 94.6 N/A 92.5 90.1 91.5qdr 74.7 97.2 92.2 N/A 93.5 99.6 91.4qdr 77.5 97.0 89.8 N/A 80.3 82.2 85.3rel 48.0 79.3 50.1 91.6 N/A 60.2 65.8rel 64.0 90.3 73.2 93.5 N/A 77.6 79.7rel 61.3 89.2 68.9 98.8 N/A 99.2 83.5rel 49.1 82.3 50.3 88.0 N/A 61.1 66.2skt 45.6 87.1 59.5 83.9 49.9 N/A 65.2skt 56.1 89.2 71.9 89.2 73.5 N/A 76.0skt 55.2 89.7 70.1 96.9 98.3 N/A 82.0skt 42.6 83.7 54.4 80.9 47.5 N/A 61.8Avg.59.6 85.8 67.8 90.4 56.3 68.6 71.4Avg.68.6 91.6 79.9 92.2 79.3 79.5 81.9Avg.66.3 91.0 76.6 97.4 97.3 99.4 88.0Avg.56.8 86.5 64.0 86.3 55.4 66.469.2(+2.2) shown in Table 4, consistent with the previous analysis, most of the methods include BN [53], PL [39], TENT [70], CoTTA [73] and NOTE [19] even perform worse than the Source model ( −4.6 ∼ −22.8%). RoTTA consistently achieves the best performance and has 2.2% gain than the second method LAME [5], demonstrating RoTTA’s effec- tiveness again. 4.3. Ablation Study Effect of each component. To further investigate the effi- cacy of each component, we replace each part with the nor- mally used solutions to obtain three variants: (1) RoTTA w/o RBN, replace RBN with test-time BN in TENT [70]; (2) RoTTA w/o CSTU, directly adapt the model on test stream; (3) RoTTA w/o robust training (RT), directly adapt the model only with entropy minimization. As shown in Table 5, we can observe that significant performance degra- dation occurs for all variants, proving that every part of our proposed method is valid for PTTA. Take one com- ponent for a detailed example, without RBN robustly nor- malizing feature maps, the performance of RoTTA drops 50.2% and 16.3% on CIFAR10-C and CIFAR100-C respec- tively, proving that RBN is robust enough to tackle the prob- lem of normalization of correlatively sampled data streams. CSTU enables RoTTA to adapt to a more stable distribu- tion by maintaining a timely and confident snapshot of the test distribution. Meanwhile, robust training with timeliness greatly reduces the accumulation of errors. Every compo- nent behaves significantly to enable effective adaptation un- der PTTA. Effect of the distribution changing order. To exclude the effect of a fixed order of distribution changing, we con- ducted experiments on ten different sequences of changes on CIFAR10-C and CIFAR100-C with independently andBN PL TENT LAME CoTTA NOTE RoTTA0 10 20 30 40 50 60 70 80Classification error (%) Source CIFAR-10  CIFAR-10-C Independent Correlative (a) CIFAR10-C. BN PL TENT LAME CoTTA NOTE RoTTA0 20 40 60 80Classification error (%) Source CIFAR-100  CIFAR-100-C Independent Correlative (b) CIFAR100-C. uniform 10 1 0.1 0.01 0.001 30 40 50 60 70 80 90 100Classification error (%) Source BN PL TENT LAME CoTTA NOTE RoTTA (c) δ. 16 32 64 128 256 512 40 50 60 70 80 90 100Classification error (%) Source BN PL TENT LAME CoTTA NOTE RoTTA (d) Batch size. Figure 4. (a) & (b) we adapt the model continually to different corruptions of 10 different orders with independently and correlatively sampled test streams on CIFAR10-C and CFAR100-C respectively and report their average classification error. (c) & (d) we verify the effect of δ and batch size to different methods on CIFAR100-C respectively. Table 5. Classification error of different variants of our RoTTA. Variant CIFAR10-C CIFAR100-C Avg. RoTTA w/o RBN 75.4 51.3 63.4 RoTTA w/o CSTU 47.1 46.3 46.7 RoTTA w/o RT 78.2 95.0 81.6 RoTTA 25.2 35.0 30.1 correlatively sampled test streams respectively. As shown in Figure 4a and 4b, no matter what kind of setup, RoTTA can achieve excellent results. The detailed results on the correlatively sampled test streams are shown in Table 6, RoTTA achieves 4.3% and 4.7% progress on CIFAR10- C and CIFAR100-C respectively. This shows that RoTTA can adapt the model robustly and effectively in long-term scenarios where distribution continually changes and test streams are sampled either independently or correlatively, making it a good choice for model deployment. Effect of Dirichlet concentration parameter δ. We vary the value of δ on CIFAR100-C and compare RoTTA with other approaches in Figure 4c. As the value of δ increases, the performance of BN [53], PL [39], TENT [70] and CoTTA [73] drops quickly, because they never consider the increasing correlation among test samples. NOTE [19] is stable to correlatively sampled test streams but does not consider the distribution changing, causing ineffective adaptation. Meanwhile, the higher correlation between test samples will make the propagation of labels more accurate, which is why the result of LAME [5] slightly improves. Fi- nally, excellent and stable results once again prove the sta- bility and effectiveness of RoTTA. Effect of batch size. In real scenarios, considering deploy- ment environments may use different test batch sizes, we conduct experiments with different values of test batch sizes and results are shown in Figure 4d. For a fair comparison, we control the frequency of updating the model of RoTTA so that the number of samples involved in back-propagation is the same. As the batch size increases, we can see that all of the compared methods have a significant improvement except for lame which has a slight decrease. This is be- cause the number of categories in a batch increases with the Table 6. Average classification error of tasks CIFAR10 → CIFAR10-C and CIFAR100 → CIFAR100-C while continually adapting to different corruptions of 10 different orders at the high- est severity 5 with correlatively sampled test stream. Method CIFAR10-C CIFAR100-C Avg. Source 43.5 46.4 46.9 BN [53] 75.2 52.9 64.1 PL [39] 75.2 52.9 60.1 TENT [70] 82.3 93.2 87.8 LAME [5] 39.5 40.6 40.1 NOTE [19] 30.5 76.1 53.3 CoTTA [73] 83.1 52.8 67.9 RoTTA 26.2(+4.3) 35.9(+4.7) 31.1(+9.0) increasing batch size, causing the overall correlation to be- come lower but the propagation of labels to become more difficult. Most significantly, RoTTA achieves the best re- sults across different batch sizes, demonstrating its robust- ness in dynamic scenarios once again. 5. Conclusion This work proposes a more realistic TTA setting where distribution changing and correlative sampling occur si- multaneously at the test phase, namely Practical Test-Time Adaptation (PTTA). To tackle the problems of PTTA, we propose Robust Test-Time Adaptation (RoTTA) method against the complex data stream. More specifically, a group of robust statistics for the normalization of feature maps is estimated by robust batch normalization. Meanwhile, a memory bank is adopted to capture a snapshot of the test distribution by category-balanced sampling with consider- ing timeliness and uncertainty. Further, we develop a time- aware reweighting strategy with a teacher-student model to stabilize the adaptation process. Extensive experiments and ablation studies are conducted to verify the robustness and effectiveness of the proposed method. We believe this work will pave the way for thinking about adapting models into real-world applications by test-time adaptation algorithm. Acknowledgements. This paper was supported by National Key R&D Program of China (No. 2021YFB3301503), and also supported by the National Natural Science Foundation of China under Grant No. 61902028.References [1] Rahaf Aljundi, Min Lin, Baptiste Goujaud, and Yoshua Ben- gio. Gradient based sample selection for online continual learning. In NeurIPS, pages 11816–11825, 2019. 3 [2] Fatemeh Azimi, Sebastian Palacio, Federico Raue, J ¨orn Hees, Luca Bertinetto, and Andreas Dengel. Self-supervised test-time adaptation on video data. In WACV, pages 2603– 2612, 2022. 1, 3 [3] Mathilde Bateson, Herve Lombaert, and Ismail Ben Ayed. Test-time adaptation with shape moments for image segmen- tation. In MICCAI, pages 736–745, 2022. 1 [4] Gilles Blanchard, Gyemin Lee, and Clayton Scott. General- izing from several related classification tasks to a new unla- beled sample. In NeurIPS, pages 2178–2186, 2011. 3 [5] Malik Boudiaf, Romain Mueller, Ismail Ben Ayed, and Luca Bertinetto. Parameter-free online test-time adaptation. In CVPR, pages 8344–8353, 2022. 2, 6, 7, 8, 13, 14, 15, 16, 17 [6] Francisco M Castro, Manuel J Mar ´ın-Jim´enez, Nicol´as Guil, Cordelia Schmid, and Karteek Alahari. End-to-end incre- mental learning. In ECCV, pages 233–248, 2018. 3 [7] Dian Chen, Dequan Wang, Trevor Darrell, and Sayna Ebrahimi. Contrastive test-time adaptation. In CVPR, pages 295–305, 2022. 1, 4 [8] Yuhua Chen, Wen Li, Christos Sakaridis, Dengxin Dai, and Luc Van Gool. Domain adaptive faster r-cnn for object de- tection in the wild. In CVPR, pages 3339–3348, 2018. 2 [9] Zhixiang Chi, Yang Wang, Yuanhao Yu, and Jin Tang. Test- time fast adaptation for dynamic scene deblurring via meta- auxiliary learning. In CVPR, pages 9137–9146, 2021. 3, 4 [10] Boris Chidlovskii, St ´ephane Clinchant, and Gabriela Csurka. Domain adaptation in the absence of source domain data. In KDD, pages 451–460, 2016. 3 [11] Sungha Choi, Seunghan Yang, Seokeon Choi, and Sun- grack Yun. Improving test-time adaptation via shift-agnostic weight regularization and nearest source prototypes. In ECCV, pages 440–458, 2022. 1 [12] Francesco Croce, Maksym Andriushchenko, Vikash Se- hwag, Edoardo Debenedetti, Nicolas Flammarion, Mung Chiang, Prateek Mittal, and Matthias Hein. Robustbench: a standardized adversarial robustness benchmark. In Neurips, 2021. 6 [13] Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner, Mostafa Dehghani, Matthias Minderer, Georg Heigold, Syl- vain Gelly, Jakob Uszkoreit, and Neil Houlsby. An image is worth 16x16 words: Transformers for image recognition at scale. In ICLR, 2021. 1 [14] Ying-Jun Du, Jun Xu, Huan Xiong, Qiang Qiu, Xiantong Zhen, Cees G. M. Snoek, and Ling Shao. Learning to learn with variational information bottleneck for domain general- ization. In ECCV, pages 200–216, 2020. 3 [15] Sayna Ebrahimi, Sercan ¨O. Arik, and Tomas Pfister. Test- time adaptation for visual document understanding. CoRR, abs/2206.07240, 2022. 1 [16] Yossi Gandelsman, Yu Sun, Xinlei Chen, and Alexei A Efros. Test-time training with masked autoencoders. In NeurIPS, 2022. 1 [17] Yaroslav Ganin, Evgeniya Ustinova, Hana Ajakan, Pas- cal Germain, Hugo Larochelle, Franc ¸ois Laviolette, Mario Marchand, and Victor S. Lempitsky. Domain-adversarial training of neural networks. J. Mach. Learn. Res., 17:59:1– 59:35, 2016. 1, 2 [18] Yunhe Gao, Xingjian Shi, Yi Zhu, Hao Wang, Zhiqiang Tang, Xiong Zhou, Mu Li, and Dimitris N. Metaxas. Vi- sual prompt tuning for test-time domain adaptation. CoRR, abs/2210.04831, 2022. 3 [19] Taesik Gong, Jongheon Jeong, Taewon Kim, Yewon Kim, Jinwoo Shin, and Sung-Ju Lee. Robust continual test- time adaptation: Instance-aware BN and prediction-balanced memory. In NeurIPS, 2022. 1, 2, 3, 4, 6, 7, 8, 13, 14, 15, 16, 17 [20] Sachin Goyal, Mingjie Sun, Aditi Raghunathan, and J Zico Kolter. Test time adaptation via conjugate pseudo-labels. In NeurIPS, 2022. 1, 3, 4 [21] Yves Grandvalet and Yoshua Bengio. Semi-supervised learning by entropy minimization. In NeurIPS, pages 529– 536, 2004. 3 [22] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image recognition. In CVPR, pages 770–778, 2016. 1, 6 [23] Dan Hendrycks and Thomas G. Dietterich. Benchmarking neural network robustness to common corruptions and per- turbations. In ICLR, 2019. 2, 6 [24] Hengguan Huang, Xiangming Gu, Hao Wang, Chang Xiao, Hongfu Liu, and Ye Wang. Extrapolative continuous-time bayesian neural network for fast training-free test-time adap- tation. In NeurIPS, 2022. 1 [25] Sergey Ioffe and Christian Szegedy. Batch normalization: Accelerating deep network training by reducing internal co- variate shift. In ICML, pages 448–456, 2015. 3, 4 [26] Yusuke Iwasawa and Yutaka Matsuo. Test-time classifier ad- justment module for model-agnostic domain generalization. In NeurIPS, pages 2427–2440, 2021. 1, 3 [27] Vidit Jain and Erik Learned-Miller. Online domain adapta- tion of a pre-trained cascade of classifiers. In CVPR, pages 577–584, 2011. 3 [28] Minguk Jang and Sae-Young Chung. Test-time adaptation via self-training with nearest neighbor information. CoRR, abs/2207.10792, 2022. 3, 4 [29] Junho Kim, Inwoo Hwang, and Young Min Kim. Ev-tta: Test-time adaptation for event-based object recognition. In CVPR, pages 17724–17733, 2022. 1 [30] Diederik P. Kingma and Jimmy Ba. Adam: A method for stochastic optimization. In ICLR, 2015. 6 [31] James Kirkpatrick, Razvan Pascanu, Neil C. Rabinowitz, Joel Veness, Guillaume Desjardins, Andrei A. Rusu, Kieran Milan, John Quan, Tiago Ramalho, Agnieszka Grabska- Barwinska, Demis Hassabis, Claudia Clopath, Dharshan Ku- maran, and Raia Hadsell. Overcoming catastrophic forget- ting in neural networks. CoRR, abs/1612.00796, 2016. 3 [32] Alex Krizhevsky, Geoffrey Hinton, et al. Learning multiple layers of features from tiny images. 2009. 6[33] Alex Krizhevsky, Ilya Sutskever, and Geoffrey E. Hinton. Imagenet classification with deep convolutional neural net- works. In NeurIPS, pages 1097–1105, 2012. 1 [34] Ananya Kumar, Tengyu Ma, and Percy Liang. Understand- ing self-training for gradual domain adaptation. In ICML, pages 5468–5479, 2020. 3 [35] Jogendra Nath Kundu, Naveen Venkat, Rahul M. V ., and R. Venkatesh Babu. Universal source-free domain adapta- tion. In CVPR, pages 4543–4552, 2020. 3 [36] Vinod K Kurmi, Venkatesh K Subramanian, and Vinay P Namboodiri. Domain impression: A source data free do- main adaptation method. In WACV, pages 615–625, 2021. 3 [37] Matthias De Lange, Rahaf Aljundi, Marc Masana, Sarah Parisot, Xu Jia, Ales Leonardis, Gregory G. Slabaugh, and Tinne Tuytelaars. A continual learning survey: Defying for- getting in classification tasks. IEEE Trans. Pattern Anal. Mach. Intell., 44(7):3366–3385, 2022. 3 [38] Yann LeCun, Yoshua Bengio, and Geoffrey E. Hinton. Deep learning. Nat., 521(7553):436–444, 2015. 1 [39] Dong-Hyun Lee et al. Pseudo-label: The simple and effi- cient semi-supervised learning method for deep neural net- works. In Workshop on challenges in representation learn- ing, ICML, volume 3, page 896, 2013. 6, 7, 8, 12, 14, 15, 16, 17 [40] Da Li, Yongxin Yang, Yi-Zhe Song, and Timothy M. Hospedales. Learning to generalize: Meta-learning for do- main generalization. In AAAI, pages 3490–3497, 2018. 1, 3 [41] Haoliang Li, Sinno Jialin Pan, Shiqi Wang, and Alex C. Kot. Domain generalization with adversarial feature learning. In CVPR, pages 5400–5409, 2018. 1, 3 [42] Shuang Li, Binhui Xie, Qiuxia Lin, Chi Harold Liu, Gao Huang, and Guoren Wang. Generalized domain conditioned adaptation network. IEEE Trans. Pattern Anal. Mach. Intell., 44(8):4093–4109, 2022. 1 [43] Shuang Li, Mixue Xie, Kaixiong Gong, Chi Harold Liu, Yulin Wang, and Wei Li. Transferable semantic augmen- tation for domain adaptation. In CVPR, pages 11516–11525, 2021. 2 [44] Zhizhong Li and Derek Hoiem. Learning without forgetting. IEEE Trans. Pattern Anal. Mach. Intell., 40(12):2935–2947, 2018. 3 [45] Jian Liang, Dapeng Hu, and Jiashi Feng. Do we really need to access the source data? source hypothesis transfer for un- supervised domain adaptation. In ICML, pages 6028–6039, 2020. 1, 3 [46] Yuejiang Liu, Parth Kothari, Bastien van Delft, Baptiste Bellot-Gurlet, Taylor Mordan, and Alexandre Alahi. TTT++: when does self-supervised test-time training fail or thrive? In NeurIPS, pages 21808–21820, 2021. 3 [47] Yuang Liu, Wei Zhang, and Jun Wang. Source-free do- main adaptation for semantic segmentation. In CVPR, pages 1215–1224, 2021. 3 [48] Mingsheng Long, Yue Cao, Zhangjie Cao, Jianmin Wang, and Michael I. Jordan. Transferable representation learning with deep adaptation networks. IEEE Trans. Pattern Anal. Mach. Intell., 41(12):3071–3085, 2019. 1, 2 [49] Wenao Ma, Cheng Chen, Shuang Zheng, Jing Qin, Huimao Zhang, and Qi Dou. Test-time adaptation with calibration of medical image classification nets for label distribution shift. In MICCAI, pages 313–323, 2022. 1 [50] Divyat Mahajan, Shruti Tople, and Amit Sharma. Domain generalization using causal matching. In ICML, pages 7313– 7324, 2021. 3 [51] Yishay Mansour, Mehryar Mohri, and Afshin Rostamizadeh. Domain adaptation: Learning bounds and algorithms. In COLT, 2009. 2 [52] Krikamol Muandet, David Balduzzi, and Bernhard Sch¨olkopf. Domain generalization via invariant fea- ture representation. In ICML, pages 10–18, 2013. 1, 3 [53] Zachary Nado, Shreyas Padhy, D. Sculley, Alexander D’Amour, Balaji Lakshminarayanan, and Jasper Snoek. Evaluating prediction-time batch normalization for robust- ness under covariate shift. CoRR, abs/2006.10963, 2020. 4, 6, 7, 8, 12, 14, 15, 16, 17 [54] Shuaicheng Niu, Jiaxiang Wu, Yifan Zhang, Yaofo Chen, Shijian Zheng, Peilin Zhao, and Mingkui Tan. Efficient test- time model adaptation without forgetting. In ICML, pages 16888–16905, 2022. 1 [55] Shuaicheng Niu, Jiaxiang Wu, Yifan Zhang, Yaofo Chen, Shijian Zheng, Peilin Zhao, and Mingkui Tan. Efficient test- time model adaptation without forgetting. In ICML, volume 162, pages 16888–16905, 2022. 4, 5, 6 [56] Sinno Jialin Pan and Qiang Yang. A survey on transfer learning. IEEE Trans. Knowl. Data Eng., 22(10):1345–1359, 2010. 1 [57] Adam Paszke, Sam Gross, Francisco Massa, Adam Lerer, James Bradbury, Gregory Chanan, Trevor Killeen, Zeming Lin, Natalia Gimelshein, Luca Antiga, et al. Pytorch: An imperative style, high-performance deep learning library. In NeurIPS, pages 8024–8035, 2019. 6 [58] Xingchao Peng, Qinxun Bai, Xide Xia, Zijun Huang, Kate Saenko, and Bo Wang. Moment matching for multi-source domain adaptation. In ICCV, pages 1406–1415, 2019. 2, 6 [59] Joaquin Quinonero-Candela, Masashi Sugiyama, Anton Schwaighofer, and Neil D Lawrence. Dataset shift in ma- chine learning. 2008. 1 [60] Sylvestre-Alvise Rebuffi, Alexander Kolesnikov, Georg Sperl, and Christoph H. Lampert. icarl: Incremental classi- fier and representation learning. InCVPR, pages 5533–5542, 2017. 3 [61] Amelie Royer and Christoph H Lampert. Classifier adapta- tion at prediction time. In CVPR, pages 1401–1409, 2015. 3 [62] Kuniaki Saito, Kohei Watanabe, Yoshitaka Ushiku, and Tat- suya Harada. Maximum classifier discrepancy for unsuper- vised domain adaptation. In CVPR, pages 3723–3732, 2018. 2 [63] Inkyu Shin, Yi-Hsuan Tsai, Bingbing Zhuang, Samuel Schulter, Buyu Liu, Sparsh Garg, In So Kweon, and Kuk- Jin Yoon. MM-TTA: multi-modal test-time adaptation for 3d semantic segmentation. In CVPR, pages 16907–16916, 2022. 1, 3[64] Manli Shu, Weili Nie, De-An Huang, Zhiding Yu, Tom Goldstein, Anima Anandkumar, and Chaowei Xiao. Test- time prompt tuning for zero-shot generalization in vision- language models. In NeurIPS, 2022. 1, 3 [65] Yu Sun, Xiaolong Wang, Zhuang Liu, John Miller, Alexei Efros, and Moritz Hardt. Test-time training with self- supervision for generalization under distribution shifts. In ICML, pages 9229–9248, 2020. 1, 2, 3, 4 [66] Rishabh Tiwari, KrishnaTeja Killamsetty, Rishabh K. Iyer, and Pradeep Shenoy. GCR: gradient coreset based replay buffer selection for continual learning. In CVPR, pages 99– 108, 2022. 3 [67] Yi-Hsuan Tsai, Wei-Chih Hung, Samuel Schulter, Ki- hyuk Sohn, Ming-Hsuan Yang, and Manmohan Chandraker. Learning to adapt structured output space for semantic seg- mentation. In CVPR, pages 7472–7481, 2018. 2 [68] Eric Tzeng, Judy Hoffman, Trevor Darrell, and Kate Saenko. Simultaneous deep transfer across domains and tasks. In ICCV, pages 4068–4076, 2015. 2 [69] Eric Tzeng, Judy Hoffman, Kate Saenko, and Trevor Darrell. Adversarial discriminative domain adaptation. In CVPR, pages 2962–2971, 2017. 1 [70] Dequan Wang, Evan Shelhamer, Shaoteng Liu, Bruno A. Ol- shausen, and Trevor Darrell. Tent: Fully test-time adaptation by entropy minimization. In ICLR, 2021. 1, 2, 3, 4, 6, 7, 8, 12, 13, 14, 15, 16, 17 [71] Jindong Wang, Cuiling Lan, Chang Liu, Yidong Ouyang, Tao Qin, Wang Lu, Yiqiang Chen, Wenjun Zeng, and Philip Yu. Generalizing to unseen domains: A survey on domain generalization. IEEE Trans. Knowl. Data Eng., 2022. 1 [72] Mei Wang and Weihong Deng. Deep visual domain adapta- tion: A survey. Neurocomputing, 312:135–153, 2018. 1 [73] Qin Wang, Olga Fink, Luc Van Gool, and Dengxin Dai. Con- tinual test-time domain adaptation. In CVPR, pages 7191– 7201, 2022. 1, 2, 3, 4, 6, 7, 8, 13, 14, 15, 16, 17 [74] Markus Wulfmeier, Alex Bewley, and Ingmar Posner. Incre- mental adversarial domain adaptation for continually chang- ing environments. In ICRA, pages 4489–4495, 2018. 3 [75] Binhui Xie, Shuang Li, Mingjia Li, Chi Harold Liu, Gao Huang, and Guoren Wang. Sepico: Semantic-guided pixel contrast for domain adaptive semantic segmentation. IEEE Trans. Pattern Anal. Mach. Intell., pages 1–17, 2023. 2 [76] Saining Xie, Ross Girshick, Piotr Doll ´ar, Zhuowen Tu, and Kaiming He. Aggregated residual transformations for deep neural networks. In CVPR, pages 5987–5995, 2017. 6 [77] Ruijia Xu, Guanbin Li, Jihan Yang, and Liang Lin. Larger norm more transferable: An adaptive feature norm approach for unsupervised domain adaptation. In ICCV, pages 1426– 1435, 2019. 2 [78] Zhenlin Xu, Deyi Liu, Junlin Yang, Colin Raffel, and Marc Niethammer. Robust and generalizable visual representation learning via random convolutions. In ICLR, 2021. 3 [79] Shiqi Yang, Yaxing Wang, Joost van de Weijer, Luis Herranz, and Shangling Jui. Generalized source-free domain adapta- tion. In ICCV, pages 8978–8987, 2021. 3 [80] Sergey Zagoruyko and Nikos Komodakis. Wide residual net- works. In BMVC, 2016. 6 [81] Marvin Mengxin Zhang, Sergey Levine, and Chelsea Finn. MEMO: Test time robustness via adaptation and augmenta- tion. In NeurIPS, 2022. 1, 4 [82] Yizhe Zhang, Shubhankar Borse, Hong Cai, and Fatih Porikli. Auxadapt: Stable and efficient test-time adaptation for temporally consistent video semantic segmentation. In WACV, pages 2633–2642, 2022. 1, 3 [83] Kaiyang Zhou, Ziwei Liu, Yu Qiao, Tao Xiang, and Chen Change Loy. Domain generalization: A survey. IEEE Trans. Pattern Anal. Mach. Intell., 2022. 1 [84] Kaiyang Zhou, Yongxin Yang, Yu Qiao, and Tao Xiang. Do- main generalization with mixstyle. In ICLR, 2021. 3 [85] Yang Zou, Zhiding Yu, BVK Vijaya Kumar, and Jinsong Wang. Unsupervised domain adaptation for semantic seg- mentation via class-balanced self-training. In ECCV, pages 289–305, 2018. 26. Appendix 6.1. Discussion Societal impact. RoTTA enables adapting pre-trained models on continually changing distributions with correl- atively sampled test streams without any more raw data or label requirements. Thus, our work may have a positive im- pact on communities to effectively deploy and adapt models in various real-world scenarios, which is economically and environmentally friendly. And since no training data is re- quired, this protects data privacy and has potential commer- cial value. We carry out experiments on benchmark datasets and do not notice any societal issues. It does not involve sensitive attributes. Future work. Our work suggests a few promising direc- tions for future work. Firstly, the proposed RoTTA is a preliminary attempt to perform test-time adaptation for the more realistic test stream under the setup PTTA. One could experiment to improve the algorithm by replacing some parts of RoTTA. More importantly, we hope that with this work, we can open a path to the original goal of test-time adaptation, which is performing test-time adaptation in real- world scenarios. Thus, one could improve PTTA to make it more realistic. Limitations. RoTTA achieves excellent performance on various tasks under the setup PTTA as demonstrated in Sec- tion 4 in the main paper, but we still find some limitations of it. Firstly, the adopted robust batch normalization (RBN) is a naive solution to the normalization of the correlatively sampled batch of data. This requires careful design of the value of α in RBN. Secondly, we observe that during the adaptation procedure of some methods like PL [39] and TENT [70], the model collapse finally. Although we de- sign many strategies to stabilize the adaptation and model collapse never occurs in the experiments of RoTTA, we are still missing a way to recover the model from the collapse state as a remedy. Thirdly, category similarity is only one kind of correlation. Although we conduct experiments on different datasets with Dirichlet distribution to simulate cor- relatively sampled test streams, we still need to validate our approach in some real-world scenarios. 6.2. Sensitivity to different hyper-parameters In this section, we conduct a detailed sensitivity analy- sis of the hyperparameters involved in RoTTA. All experi- ments are conducted on CIFAR100→CIFAR100-C, and the corruptions changes as motion, snow, fog, shot, defocus, contrast, zoom, brightness, frost, elastic, glass, gaussian, pixelate, jpeg, and impulse, and test streams are sampled correlatively with the Dirichlet parameter δ = 0.1. When we investigate the sensitivity to a specific hyperparameter, other hyperparameters are fixed to the default values, i.e., λt = 1.0, λu = 1.0, α = 0.05, and ν = 0.001, for all experiments. Table 7. Classification error with different value of λt/λu. λt/λu 0.0/2.0 0.5/1.5 1.0/1.0 1.5/ 0.5 2.0/ 0.0 CIFAR100-C 57.5 36.9 35.0 35.9 38.9 Trade-off between timeliness and uncertainty. When updating the memory bank, we take the timeliness and uncertainty of samples into account simultaneously, and λt and λu will make a trade-off between them. In Table 7, we show the results of RoTTA with varying λt/λu, i.e., λt/λu ∈ {0.0/2.0, 0.5/1.5, 1.0/1.0, 1.5/0.5, 2.0/0.0}. When we consider both of them, the results are relatively stable (35.0-36.9%). When we only think about one side, the performance drops significantly. For example, when we set λt/λu = 0.0/2.0 which means only considering uncer- tainty, the performance drops 22.5%. That’s because some confident samples get stuck in the memory bank, making it not work the way we design it. Table 8. Classification error with varying α α 0.5 0.1 0.05 0.01 0.005 0.001 CIFAR100-C 39.0 36.0 35.0 36.0 38.1 41.5 Sensitivity to α. We show the results of RoTTA with vary- ing α, i.e., α ∈ {0.5, 0.1, 0.05, 0.01, 0.005, 0.001} in Ta- ble 8. A larger value of α means updating the global statis- tics faster and vice versa. We can see that RoTTA achieves competitive results (35.0 − 36.0%) at appropriate values of α, i.e., α ∈ {0.1, 0.05, 0.01}. Updating too aggressively or too gently can lead to unreliable estimates of statistics. Table 9. Classification error with varying ν ν 0.05 0.01 0.005 0.001 0.0005 0.0001 CIFAR100-C 44.8 39.1 37.1 35.0 37.6 43.6 Sensitivity to ν. We show the results of RoTTA with vary- ing ν, i.e., ν ∈ {0.05, 0.01, 0.005, 0.001, 0.0005, 0.0001} in Table 9. As we can see, the best performance is achieved at ν = 0.001. Updating the teacher model too quickly or too slowly can cause performance degradation. 6.3. Additional experiment details and results 6.3.1 Compared methods BN [53] utilizes statistics of the current batch of data to nor- malize their feature maps without tuning any parameters. PL [39] is based on BN [53], and adopts pseudo labels to train the affine parameters in BN layers.TENT [70] is the first to propose fully test-time adaptation. It adopts test-time batch normalization and utilizes entropy minimization to train the affine parameters of BN layers. We reimplement it following the released code https:// github.com/DequanWang/tent. LAME [5] adapts the output of the pre-trained model by optimizing a group of latent variables without tuning any in- ner parts of the model. We reimplement it following the re- leased code https://github.com/fiveai/LAME. CoTTA [73] considers performing test-time adapta- tion on continually changing distributions and pro- pose augmentation-averaged pseudo-labels and stochastic restoration to address error accumulation and catastrophic forgetting. We reimplement it following the released code https://github.com/qinenergy/cotta. NOTE [19] proposes instance-aware normalization and prediction-balanced reservoir sampling to stable the adapta- tion on temporally correlated test streams. We reimplement it following the released code https://github.com/ TaesikGong/NOTE. 6.3.2 Simulate correlatively sampling As we described in the scenarios of autonomous driving that the car will follow more vehicles on the highway or will en- counter more pedestrians on the sidewalk, so we use the same category to simulate correlation. From a macro point of view, the test distribution Ptest changes continually as P0, P1, ...,P∞. During the period when Ptest = Pt, we adopt Dirichlet distribution to simulate correlatively sam- pled test stream. More specifically, we consider dividing samples of C classes into T slots. Firstly, we utilize Dirich- let distribution with parameter γ to generate the partition criterion q ∈ RC×T . Then for each class c, we split samples into T parts according to qc and assign each part to each slot respectively. Finally, we concatenate all slots to sim- ulate the correlatively sampled test stream for Ptest = Pt. And as Ptest changes, we use the above method again to generate the test stream. 6.3.3 Detailed results of different orders We report the average classification error of ten different distribution changing orders in Table 6 of the main pa- per. And then we present the specific results here, includ- ing Table 10, 11, 12, 13, 14, 15, 16, 17, 18, and 19 for CIFAR10→CIFAR10-C and Table 20, 21, 22, 23, 24, 25, 26, 27, 28, and 29 for CIFAR100 →CIFAR100-C. We can see consistently superior performance of RoTTA. One thing to mention is that on DomainNet we use alphabetical order to determine the order of domain changes.Table 10. Average classification error of the task CIFAR10→ CIFAR10-C while continually adapting to different corruptions at the highest severity 5 with correlatively sampled test stream under the proposed setup PTTA. Time t− − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − → Method brightnesspixelategaussianmotionzoom glass impulsejpeg defocuselasticshot frost snow fog contrast Avg. Source 9.3 58.5 72.3 34.8 42.0 54.3 72.9 30.3 46.9 26.6 65.7 41.3 25.1 26.0 46.7 43.5BN [53] 71.1 75.2 76.8 74.2 73.7 80.1 79.3 77.5 73.8 77.7 77.2 73.3 73.8 72.7 71.7 75.2PL [39] 71.7 75.9 80.2 78.4 80.2 85.2 85.3 85.4 85.1 86.7 87.9 87.9 88.1 88.3 87.9 83.6TENT [70] 71.6 75.9 81.3 80.5 82.3 85.6 87.1 87.0 87.1 88.1 88.2 87.8 87.9 88.3 88.2 84.4LAME [5] 5.4 56.8 73.1 29.1 37.0 50.5 71.4 22.3 42.8 18.6 65.5 37.3 18.8 20.4 43.6 39.5CoTTA [73] 75.0 79.8 83.1 83.4 83.2 84.0 84.5 83.2 83.5 83.3 83.6 83.0 83.0 83.4 83.7 82.6NOTE [19] 10.1 29.9 47.1 23.4 28.4 48.4 46.1 41.8 26.9 36.1 37.5 25.0 25.0 23.2 14.2 30.9 RoTTA 10.4 26.6 37.5 23.9 17.0 40.9 39.7 30.1 18.0 29.9 30.1 23.6 21.7 17.6 19.0 25.7(+5.2) Table 11. Average classification error of the task CIFAR10→ CIFAR10-C while continually adapting to different corruptions at the highest severity 5 with correlatively sampled test stream under the proposed setup PTTA. Time t− − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − → Method jpeg shot zoom frost contrastfog defocuselasticgaussianbrightnessglass impulsepixelatesnow motion Avg. Source 30.3 65.7 42.0 41.3 46.7 26.0 46.9 26.6 72.3 9.3 54.3 72.9 58.5 25.1 34.8 43.5BN [53] 77.6 75.8 73.4 74.1 73.1 72.5 72.9 77.1 77.2 72.2 79.9 79.9 75.5 74.6 72.9 75.2PL [39] 77.6 77.1 76.6 78.3 77.5 79.8 82.0 84.8 86.1 83.5 87.8 87.1 86.5 85.6 85.7 82.4TENT [70] 78.5 78.2 79.2 81.8 84.8 84.8 86.4 87.3 87.9 86.7 87.3 87.8 87.2 87.5 87.1 84.8LAME [5] 22.5 65.2 37.0 37.1 44.0 20.3 41.7 18.7 72.8 5.2 51.2 71.5 57.0 19.0 29.4 39.5CoTTA [73]78.5 81.0 82.8 84.1 84.9 83.4 83.5 83.5 84.5 83.3 84.7 84.6 83.0 84.4 83.4 83.3NOTE [19]35.4 36.1 22.1 21.3 11.6 24.8 24.5 36.0 37.7 18.4 49.0 47.4 43.9 30.4 29.2 31.2 RoTTA 33.2 33.3 19.8 24.1 24.9 20.5 16.2 31.7 28.4 11.8 43.1 36.9 32.5 20.7 20.6 26.5(+4.7) Table 12. Average classification error of the task CIFAR10→ CIFAR10-C while continually adapting to different corruptions at the highest severity 5 with correlatively sampled test stream under the proposed setup PTTA. Time t− − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − → Method contrastdefocusgaussianshot snow frost glass zoom elasticjpeg pixelatebrightnessimpulsemotion fog Avg. Source 46.7 46.9 72.3 65.7 25.1 41.3 54.3 42.0 26.6 30.3 58.5 9.3 72.9 34.8 26.0 43.5BN [53] 72.3 72.6 76.9 77.1 74.8 73.5 80.0 73.2 77.4 78.6 76.4 71.0 79.1 73.9 71.5 75.2PL [39] 72.4 75.3 80.7 82.6 83.3 83.5 86.6 85.7 86.6 88.4 87.5 86.6 88.3 88.2 86.8 84.1TENT [70] 73.5 77.9 85.5 86.9 87.6 87.8 88.3 87.7 88.6 89.2 88.5 88.5 89.3 88.6 88.6 86.4LAME [5] 43.5 42.3 73.1 65.3 19.2 37.3 51.1 36.8 18.5 22.5 56.9 5.5 71.1 29.1 20.5 39.5CoTTA [73]79.4 80.3 83.8 83.9 83.9 83.4 85.0 83.2 85.1 84.3 83.9 83.3 84.7 83.9 82.5 83.4NOTE [19] 9.6 21.8 40.1 31.0 25.5 22.6 44.8 22.8 33.2 39.4 33.2 18.1 50.0 28.3 29.8 30.0 RoTTA 18.4 17.9 38.4 31.9 23.3 19.8 40.7 17.4 31.4 29.8 27.8 11.3 43.8 19.7 18.8 26.0(+4.0) Table 13. Average classification error of the task CIFAR10→ CIFAR10-C while continually adapting to different corruptions at the highest severity 5 with correlatively sampled test stream under the proposed setup PTTA. Time t− − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − → Method shot fog glass pixelatesnow elasticbrightnessimpulsedefocusfrost contrastgaussianmotionjpeg zoom Avg. Source 65.7 26.0 54.3 58.5 25.1 26.6 9.3 72.9 46.9 41.3 46.7 72.3 34.8 30.3 42.0 43.5BN [53] 76.4 72.0 80.4 76.2 74.8 77.0 71.1 79.6 73.8 74.4 73.0 77.0 72.5 78.3 72.5 75.3PL [39] 77.0 73.3 82.4 79.8 81.0 82.3 79.5 84.4 82.7 83.5 83.5 85.5 84.8 87.0 84.5 82.1TENT [70]76.9 74.6 82.3 81.7 82.0 84.9 84.8 87.3 86.6 87.3 87.6 89.2 88.3 88.9 87.3 84.6LAME [5] 65.3 20.6 50.9 56.7 19.2 18.8 5.4 71.8 42.8 37.2 43.3 73.2 29.4 22.6 36.9 39.6CoTTA [73]77.4 77.6 83.8 81.9 82.2 82.6 80.4 83.3 82.3 81.5 82.7 82.6 81.1 82.9 81.0 81.6NOTE [19]34.0 20.9 43.1 36.6 24.0 36.4 12.1 48.0 25.9 23.9 13.4 38.1 25.0 43.2 24.2 29.9 RoTTA 35.0 21.1 43.9 29.2 22.1 29.7 10.8 44.6 25.3 22.7 24.6 29.4 26.9 34.4 16.1 27.7(+2.2) Table 14. Average classification error of the task CIFAR10→ CIFAR10-C while continually adapting to different corruptions at the highest severity 5 with correlatively sampled test stream under the proposed setup PTTA. Time t− − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − → Method pixelateglass zoomsnow fog impulsebrightnessmotionfrost jpeg gaussianshot contrastdefocus elastic Avg. Source 58.5 54.3 42.0 25.1 26.0 72.9 9.3 34.8 41.3 30.3 72.3 65.7 46.7 46.9 26.6 43.5BN [53] 76.0 79.6 73.3 75.2 72.9 79.8 71.1 73.5 74.1 78.6 77.4 76.1 72.0 73.8 76.4 75.3PL [39] 76.7 81.3 77.4 80.3 81.2 86.3 83.3 85.9 86.2 87.7 88.1 88.4 87.4 87.6 87.7 84.4TENT [70] 76.4 80.2 77.8 81.2 83.0 87.1 85.6 87.2 87.6 88.7 88.6 88.9 88.5 88.6 88.2 85.2LAME [5] 56.9 50.7 37.0 19.0 20.3 71.5 5.4 29.2 37.2 22.5 73.0 65.3 43.8 42.4 18.7 39.5CoTTA [73]77.1 83.6 84.1 84.8 84.4 85.2 84.0 84.3 84.9 84.9 85.0 84.7 85.3 84.4 84.3 84.1NOTE [19] 27.8 52.2 24.5 22.3 21.6 44.5 14.5 21.3 25.9 42.5 38.8 36.0 16.7 28.1 40.6 30.5 RoTTA 25.9 43.3 17.7 22.1 20.2 41.5 12.2 22.9 22.5 31.2 33.8 26.0 31.4 17.7 27.6 26.4(+4.1)Table 15. Average classification error of the task CIFAR10→ CIFAR10-C while continually adapting to different corruptions at the highest severity 5 with correlatively sampled test stream under the proposed setup PTTA. Time t− − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − → Method motionsnow fog shot defocuscontrastzoom brightnessfrost elasticglass gaussianpixelatejpeg impulse Avg. Source 34.8 25.1 26.0 65.7 46.9 46.7 42.0 9.3 41.3 26.6 54.3 72.3 58.5 30.3 72.9 43.5BN [53] 73.2 73.4 72.7 77.2 73.7 72.5 72.9 71.0 74.1 77.7 80.0 76.9 75.5 78.3 79.0 75.2PL [39] 73.9 75.0 75.6 81.0 79.9 80.6 82.0 83.2 85.3 87.3 88.3 87.5 87.5 87.5 88.2 82.9TENT [70] 74.3 77.4 80.1 86.2 86.7 87.3 87.9 87.4 88.2 89.0 89.2 89.0 88.3 89.7 89.2 86.0LAME [5] 29.5 19.0 20.3 65.3 42.4 43.4 36.8 5.4 37.2 18.6 51.2 73.2 57.0 22.6 71.3 39.5CoTTA [73]77.1 80.6 83.1 84.4 83.9 84.2 83.1 82.6 84.4 84.2 84.5 84.6 82.7 83.8 84.9 83.2NOTE [19] 18.0 22.1 20.6 35.6 26.9 13.6 26.5 17.3 27.2 37.0 48.3 38.8 42.6 41.9 49.7 31.1 RoTTA 18.1 21.3 18.8 33.6 23.6 16.5 15.1 11.2 21.9 30.7 39.6 26.8 33.7 27.8 39.5 25.2(+5.9) Table 16. Average classification error of the task CIFAR10→ CIFAR10-C while continually adapting to different corruptions at the highest severity 5 with correlatively sampled test stream under the proposed setup PTTA. Time t− − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − → Method frost impulsejpeg contrastzoom glass pixelatesnow defocusmotionbrightnesselasticshot fog gaussian Avg. Source 41.3 72.9 30.3 46.7 42.0 54.3 58.5 25.1 46.9 34.8 9.3 26.6 65.7 26.0 72.3 43.5BN [53] 73.8 79.1 77.9 73.0 73.7 80.1 75.7 74.4 73.7 74.0 71.7 77.0 75.9 72.8 76.2 75.3PL [39] 74.2 80.9 80.4 79.5 81.8 85.9 83.9 85.1 84.7 85.9 85.9 86.7 87.2 87.0 87.8 83.8TENT [70]73.9 80.3 81.8 81.6 83.6 86.3 85.6 85.7 86.4 87.7 87.4 88.8 88.8 88.5 88.4 85.0LAME [5] 37.4 71.8 22.4 43.5 37.0 50.5 57.0 19.0 42.8 29.1 5.4 18.7 65.2 20.4 72.9 39.5CoTTA [73]76.5 82.2 82.8 85.0 82.9 85.0 83.0 82.9 83.5 83.4 82.6 83.7 83.2 83.3 83.6 82.9NOTE [19]21.1 41.4 36.3 10.2 21.7 46.7 37.5 26.4 26.1 21.4 14.3 37.9 38.5 24.4 40.7 29.6 RoTTA 22.2 44.9 35.2 18.8 19.7 41.5 28.5 23.2 21.2 18.6 12.4 30.0 27.4 20.0 31.2 26.3(+3.3) Table 17. Average classification error of the task CIFAR10→ CIFAR10-C while continually adapting to different corruptions at the highest severity 5 with correlatively sampled test stream under the proposed setup PTTA. Time t− − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − → Method defocusmotionzoom shot gaussianglass jpeg fog contrastpixelatefrost snow brightnesselastic impulse Avg. Source 46.9 34.8 42.0 65.7 72.3 54.3 30.3 26.0 46.7 58.5 41.3 25.1 9.3 26.6 72.9 43.5BN [53] 72.8 72.7 73.3 77.2 77.3 80.0 77.6 72.6 73.3 76.6 73.8 74.1 70.3 77.5 79.0 75.2PL [39] 73.2 74.6 76.5 81.7 82.8 84.6 85.1 84.6 86.2 86.4 86.1 87.1 86.8 88.4 88.1 83.5TENT [70] 73.7 74.3 77.1 82.5 84.3 86.9 87.4 86.6 88.0 88.5 88.1 88.5 88.4 89.4 88.9 84.8LAME [5] 42.5 29.3 37.0 65.3 73.2 50.5 22.5 20.5 43.5 56.9 37.1 18.9 5.4 18.5 71.3 39.5CoTTA [73]76.3 79.8 82.4 83.3 83.8 84.5 83.1 82.7 84.7 82.9 83.0 83.3 81.4 83.8 83.8 82.6NOTE [19] 18.5 18.8 23.6 36.5 33.7 47.8 38.6 22.8 13.0 40.0 29.2 26.3 17.5 44.0 52.9 30.9 RoTTA 17.0 17.5 16.5 33.8 33.3 42.7 29.4 18.0 19.6 29.5 20.7 22.1 11.5 29.5 38.1 25.3(+5.6) Table 18. Average classification error of the task CIFAR10→ CIFAR10-C while continually adapting to different corruptions at the highest severity 5 with correlatively sampled test stream under the proposed setup PTTA. Time t− − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − → Method glass zoom impulsefog snow jpeg gaussianfrost shot brightnesscontrastmotionpixelatedefocus elastic Avg. Source 54.3 42.0 72.9 26.0 25.1 30.3 72.3 41.3 65.7 9.3 46.7 34.8 58.5 46.9 26.6 43.5BN [53] 79.7 72.3 79.8 73.2 74.7 77.7 76.6 73.2 77.1 72.2 73.0 73.3 75.5 73.8 76.4 75.2PL [39] 79.6 73.2 81.3 77.3 79.1 83.0 83.2 83.0 85.5 84.3 87.0 86.9 86.4 86.5 87.6 82.9TENT [70] 79.5 74.1 84.2 82.2 84.5 86.5 86.7 85.9 87.2 86.6 86.8 87.3 86.9 87.4 87.3 84.9LAME [5] 50.8 36.9 71.3 20.6 19.2 22.4 72.5 37.2 65.4 5.2 43.3 29.1 57.0 42.4 18.7 39.5CoTTA [73]81.5 79.4 85.2 84.1 84.5 84.2 84.8 84.0 84.8 83.2 85.2 83.8 83.2 84.6 83.6 83.7NOTE [19]45.0 21.2 42.3 21.0 21.6 38.4 36.4 21.4 33.1 16.7 14.6 25.4 43.5 29.1 38.5 29.9 RoTTA 42.6 17.6 48.1 23.9 21.9 32.6 32.1 20.7 30.2 12.0 21.9 20.0 33.7 16.4 28.1 26.8(+3.1) Table 19. Average classification error of the task CIFAR10→ CIFAR10-C while continually adapting to different corruptions at the highest severity 5 with correlatively sampled test stream under the proposed setup PTTA. Time t− − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − → Method contrastgaussiandefocuszoom frost glass jpeg fog pixelateelasticshot impulsesnow motion brightness Avg. Source 46.7 72.3 46.9 42.0 41.3 54.3 30.3 26.0 58.5 26.6 65.7 72.9 25.1 34.8 9.3 43.5BN [53] 72.4 76.2 73.2 73.7 73.6 80.0 77.6 72.6 76.4 77.7 77.2 79.9 73.8 73.9 70.0 75.2PL [39] 73.0 78.2 76.7 79.7 81.6 85.6 86.0 85.3 87.2 88.2 88.3 88.9 88.5 89.2 88.2 84.3TENT [70] 73.6 80.9 83.1 85.6 87.1 88.5 88.8 88.4 89.2 89.3 89.0 89.0 89.3 89.9 89.1 86.7LAME [5] 43.5 73.2 42.3 37.0 37.2 50.5 22.5 20.5 57.0 18.6 65.5 71.5 18.8 29.1 5.6 39.5CoTTA [73]79.5 81.4 83.4 83.6 83.9 85.0 84.0 82.8 84.8 84.8 84.5 84.7 84.1 84.4 82.8 83.6NOTE [19] 9.6 43.6 26.5 24.8 23.9 46.9 38.0 23.4 34.0 41.2 41.5 45.0 27.6 25.8 19.0 31.4 RoTTA 18.4 36.0 21.1 15.6 23.0 41.7 30.8 19.1 34.1 31.1 31.3 39.9 26.0 18.8 12.8 26.6(+4.8)Table 20. Average classification error of the task CIFAR100 → CIFAR100-C while continually adapting to different corruptions at the highest severity 5 with correlatively sampled test stream under the proposed setup PTTA. Time t− − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − → Method brightnesspixelategaussianmotionzoom glass impulsejpeg defocuselasticshot frost snow fog contrast Avg. Source 29.5 74.7 73.0 30.8 28.8 54.1 39.4 41.2 29.3 37.2 68.0 45.8 39.5 50.3 55.1 46.4BN [53] 46.5 52.0 58.6 47.4 47.4 57.6 58.2 56.9 47.0 53.4 56.0 52.5 53.1 57.7 49.1 52.9PL [39] 48.5 60.7 77.1 85.9 91.5 95.5 95.8 96.6 96.8 96.9 97.3 97.5 97.6 97.7 97.9 88.9TENT [70] 49.8 69.4 92.2 96.0 96.7 97.3 97.5 97.9 97.5 97.9 98.0 98.2 98.2 98.2 98.2 92.2LAME [5] 21.7 75.1 72.7 22.9 20.6 49.0 32.1 33.3 21.2 28.0 66.8 40.0 30.6 43.9 51.3 40.6CoTTA [73] 46.8 48.4 54.7 48.7 48.6 53.5 55.4 52.8 49.8 51.8 53.5 52.9 54.1 56.7 53.6 52.1NOTE [19] 42.6 53.0 69.9 52.1 53.3 70.4 73.1 76.7 80.8 96.0 97.7 97.1 96.6 97.2 95.8 76.8 RoTTA 28.4 37.3 44.6 31.9 28.3 41.8 43.6 39.9 28.0 35.2 38.2 33.7 33.0 39.5 31.0 35.6(+5.0) Table 21. Average classification error of the task CIFAR100 → CIFAR100-C while continually adapting to different corruptions at the highest severity 5 with correlatively sampled test stream under the proposed setup PTTA. Time t− − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − → Method jpeg shot zoom frost contrastfog defocuselasticgaussianbrightnessglass impulsepixelatesnow motion Avg. Source 41.2 68.0 28.8 45.8 55.1 50.3 29.3 37.2 73.0 29.5 54.1 39.4 74.7 39.5 30.8 46.4BN [53] 58.3 56.8 47.8 51.8 48.9 57.3 46.8 53.5 57.8 45.5 57.1 58.5 51.7 53.3 48.8 52.9PL [39] 59.4 66.3 74.9 87.5 94.2 95.5 96.2 97.1 97.4 97.2 97.5 97.7 98.0 98.2 98.2 90.4TENT [70] 62.0 79.3 91.7 95.8 96.9 97.0 97.4 97.7 97.6 97.7 97.9 97.9 98.0 97.9 97.9 93.5LAME [5] 33.6 66.7 21.1 39.9 50.6 43.9 21.0 28.6 72.5 21.6 48.6 32.5 74.5 30.6 22.5 40.6CoTTA [73]54.6 54.1 49.6 52.1 52.7 58.0 50.3 53.3 55.0 49.1 55.4 55.7 51.0 54.6 52.1 53.2NOTE [19]60.4 63.0 49.9 55.7 47.0 65.2 59.4 76.6 90.9 87.2 96.8 97.0 97.3 96.7 96.8 76.0 RoTTA 43.9 45.3 31.0 37.3 35.7 41.2 27.7 34.8 39.7 26.6 39.5 41.9 32.0 33.0 30.5 36.0(+4.6) Table 22. Average classification error of the task CIFAR100 → CIFAR100-C while continually adapting to different corruptions at the highest severity 5 with correlatively sampled test stream under the proposed setup PTTA. Time t− − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − → Method contrastdefocusgaussianshot snow frost glass zoom elasticjpeg pixelatebrightnessimpulsemotion fog Avg. Source 55.1 29.3 73.0 68.0 39.5 45.8 54.1 28.8 37.2 41.2 74.7 29.5 39.4 30.8 50.3 46.4BN [53] 49.4 47.2 58.6 56.2 52.7 52.0 57.9 46.1 54.4 57.7 50.5 46.2 58.2 47.6 58.5 52.9PL [39] 54.8 64.2 83.3 92.4 95.5 96.5 96.9 96.4 97.2 97.4 97.8 97.8 97.9 97.7 98.0 90.9TENT [70] 60.2 83.1 95.2 96.5 96.9 97.3 97.0 97.3 97.8 97.8 97.6 97.9 97.8 97.9 98.1 93.9LAME [5] 51.3 21.3 72.7 66.3 30.2 40.0 48.6 20.9 27.7 33.3 75.0 21.5 32.2 22.5 43.8 40.5CoTTA [73]52.1 48.6 55.1 52.7 53.4 51.9 55.9 49.2 53.2 52.8 49.2 49.7 56.2 50.7 58.1 52.6NOTE [19] 39.5 45.9 68.8 61.8 57.4 58.5 71.4 66.5 80.8 90.9 94.2 94.9 97.0 95.5 96.6 74.6 RoTTA 41.7 30.5 44.9 40.5 35.4 34.1 40.5 28.2 34.5 39.5 31.1 26.7 43.3 31.4 38.8 36.1(+4.4) Table 23. Average classification error of the task CIFAR100 → CIFAR100-C while continually adapting to different corruptions at the highest severity 5 with correlatively sampled test stream under the proposed setup PTTA. Time t− − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − → Method shot fog glass pixelatesnow elasticbrightnessimpulsedefocusfrost contrastgaussianmotionjpeg zoom Avg. Source 68.0 50.3 54.1 74.7 39.5 37.2 29.5 39.4 29.3 45.8 55.1 73.0 30.8 41.2 28.8 46.4BN [53] 57.5 58.6 58.5 50.5 52.7 53.1 45.9 57.9 47.0 51.5 47.8 58.2 48.2 57.1 47.7 52.8PL [39] 59.5 72.9 85.1 89.6 94.5 96.8 97.1 97.9 97.8 98.0 98.3 98.2 98.0 98.0 98.2 92.0TENT [70]60.3 81.4 95.0 96.6 97.0 97.3 97.3 97.7 97.7 97.7 97.8 97.7 97.6 97.6 97.9 93.8LAME [5] 66.4 43.2 49.0 75.2 30.2 28.5 21.6 32.5 21.2 39.5 52.0 72.8 22.3 33.1 20.5 40.5CoTTA [73]54.5 58.4 55.6 50.0 53.9 53.4 50.3 56.7 51.3 53.2 53.7 56.1 52.0 54.5 51.5 53.7NOTE [19]61.8 60.2 63.4 55.6 59.8 65.9 58.6 75.1 77.8 93.8 94.2 97.0 95.0 95.5 94.4 76.5 RoTTA 45.5 44.5 43.5 35.6 35.1 35.7 26.2 44.0 29.7 34.2 32.0 40.7 31.4 39.4 27.7 36.3(+4.2) Table 24. Average classification error of the task CIFAR100 → CIFAR100-C while continually adapting to different corruptions at the highest severity 5 with correlatively sampled test stream under the proposed setup PTTA. Time t− − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − → Method pixelateglass zoomsnow fog impulsebrightnessmotionfrost jpeg gaussianshot contrastdefocus elastic Avg. Source 74.7 54.1 28.8 39.5 50.3 39.4 29.5 30.8 45.8 41.2 73.0 68.0 55.1 29.3 37.2 46.4BN [53] 51.7 58.6 47.8 52.9 57.1 58.2 45.9 47.6 52.9 57.8 57.5 56.7 49.5 46.1 54.0 52.9PL [39] 52.4 68.0 73.4 87.9 93.7 96.1 95.7 96.0 96.5 96.7 97.5 97.7 97.7 97.3 97.7 89.6TENT [70] 53.5 77.8 91.1 96.0 97.0 97.6 97.4 97.6 97.9 98.1 98.1 98.0 98.1 97.9 98.1 92.9LAME [5] 74.8 48.2 21.1 30.6 43.4 32.5 21.6 23.0 39.6 33.3 72.7 66.5 51.5 20.7 27.5 40.5CoTTA [73]49.3 55.1 49.1 52.9 56.8 55.7 49.5 50.0 53.6 53.4 54.9 53.9 53.8 50.1 53.5 52.8NOTE [19] 52.2 64.9 47.5 57.0 61.9 67.3 60.4 67.8 77.4 90.6 97.1 96.8 92.8 95.9 96.6 75.1 RoTTA 36.4 44.4 29.7 36.5 41.0 44.1 26.8 29.5 33.0 40.3 40.3 38.2 33.9 28.5 34.9 35.8(+4.7)Table 25. Average classification error of the task CIFAR100 → CIFAR100-C while continually adapting to different corruptions at the highest severity 5 with correlatively sampled test stream under the proposed setup PTTA. Time t− − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − → Method motionsnow fog shot defocuscontrastzoom brightnessfrost elasticglass gaussianpixelatejpeg impulse Avg. Source 30.8 39.5 50.3 68.0 29.3 55.1 28.8 29.5 45.8 37.2 54.1 73.0 74.7 41.2 39.4 46.4BN [53] 48.5 54.0 58.9 56.2 46.4 48.0 47.0 45.4 52.9 53.4 57.1 58.2 51.7 57.1 58.8 52.9PL [39] 50.6 62.1 73.9 87.8 90.8 96.0 94.8 96.4 97.4 97.2 97.4 97.4 97.3 97.4 97.4 88.9TENT [70] 53.3 77.6 93.0 96.5 96.7 97.5 97.1 97.5 97.3 97.2 97.1 97.7 97.6 98.0 98.3 92.8LAME [5] 22.4 30.4 43.9 66.3 21.3 51.7 20.6 21.8 39.6 28.0 48.7 72.8 74.6 33.1 32.3 40.5CoTTA [73]49.2 52.7 56.8 53.0 48.7 51.7 49.4 48.7 52.5 52.2 54.3 54.9 49.6 53.4 56.2 52.2NOTE [19] 45.7 53.0 58.2 65.6 54.2 52.0 59.8 63.5 74.8 91.8 98.1 98.3 96.8 97.0 98.2 73.8 RoTTA 31.8 36.7 40.9 42.1 30.0 33.6 27.9 25.4 32.3 34.0 38.8 38.7 31.3 38.0 42.9 35.0(+5.5) Table 26. Average classification error of the task CIFAR100 → CIFAR100-C while continually adapting to different corruptions at the highest severity 5 with correlatively sampled test stream under the proposed setup PTTA. Time t− − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − → Method frost impulsejpeg contrastzoom glass pixelatesnow defocusmotionbrightnesselasticshot fog gaussian Avg. Source 45.8 39.4 41.2 55.1 28.8 54.1 74.7 39.5 29.3 30.8 29.5 37.2 68.0 50.3 73.0 46.4BN [53] 52.9 58.8 57.6 48.2 47.4 57.6 50.9 52.4 47.0 47.2 45.1 54.0 56.4 57.7 58.2 52.8PL [39] 56.9 73.3 86.7 94.4 95.8 97.3 97.2 97.4 97.6 97.4 97.7 97.6 97.8 98.3 98.1 92.2TENT [70]60.1 84.2 95.7 97.2 97.4 97.9 97.8 98.0 98.1 98.2 98.3 98.4 98.4 98.4 98.4 94.4LAME [5] 39.9 32.4 33.4 51.4 20.6 49.0 74.4 31.3 21.2 22.6 21.9 28.1 66.9 43.9 72.5 40.6CoTTA [73]51.5 55.3 54.3 51.8 49.4 55.3 50.7 54.2 51.4 50.6 49.5 53.6 55.0 57.1 55.8 53.0NOTE [19]51.6 60.9 60.3 45.4 54.3 70.8 68.8 75.0 75.7 87.1 94.7 95.6 96.7 96.4 97.2 75.4 RoTTA 40.0 46.3 42.8 36.4 29.2 42.3 33.2 34.4 28.4 29.2 26.4 34.5 38.5 39.8 39.3 36.0(+4.6) Table 27. Average classification error of the task CIFAR100 → CIFAR100-C while continually adapting to different corruptions at the highest severity 5 with correlatively sampled test stream under the proposed setup PTTA. Time t− − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − → Method defocusmotionzoom shot gaussianglass jpeg fog contrastpixelatefrost snow brightnesselastic impulse Avg. Source 29.3 30.8 28.8 68.0 73.0 54.1 41.2 50.3 55.1 74.7 45.8 39.5 29.5 37.2 39.4 46.4BN [53] 47.1 48.6 47.8 56.2 57.6 57.6 57.6 57.5 48.7 50.6 51.8 53.2 46.9 53.5 58.8 52.9PL [39] 48.8 58.7 69.9 88.0 95.1 96.6 96.7 96.9 97.4 97.4 98.2 98.2 98.2 98.3 98.5 89.1TENT [70] 51.0 67.6 85.8 95.9 97.2 97.5 97.2 97.7 98.1 97.9 97.7 97.7 98.0 98.0 98.2 91.7LAME [5] 21.2 22.8 21.1 66.3 72.8 49.0 33.3 44.8 51.7 74.9 39.8 31.2 21.3 27.3 32.3 40.6CoTTA [73]48.4 48.8 48.2 52.9 54.0 53.8 52.7 57.2 52.6 48.6 51.8 53.9 49.4 52.3 56.0 52.0NOTE [19] 45.1 46.7 49.1 67.3 65.5 69.4 75.5 80.3 83.8 96.0 97.6 97.1 96.1 97.9 98.7 77.7 RoTTA 29.6 31.3 28.8 43.9 41.5 41.3 40.9 39.8 32.1 32.6 33.1 33.0 26.5 34.5 42.9 35.4(+5.2) Table 28. Average classification error of the task CIFAR100 → CIFAR100-C while continually adapting to different corruptions at the highest severity 5 with correlatively sampled test stream under the proposed setup PTTA. Time t− − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − → Method glass zoom impulsefog snow jpeg gaussianfrost shot brightnesscontrastmotionpixelatedefocus elastic Avg. Source 54.1 28.8 39.4 50.3 39.5 41.2 73.0 45.8 68.0 29.5 55.1 30.8 74.7 29.3 37.2 46.4BN [53] 58.8 47.7 59.2 57.6 52.7 56.9 58.2 52.0 56.7 45.5 47.8 48.2 51.7 46.1 54.0 52.9PL [39] 60.1 59.5 75.1 85.7 91.5 94.6 96.5 97.1 97.4 97.3 98.0 97.7 97.9 97.8 97.7 89.6TENT [70] 61.6 71.5 91.0 95.9 96.6 97.1 96.9 97.3 97.4 97.2 97.9 98.0 98.1 97.9 97.8 92.8LAME [5] 48.6 20.6 32.3 44.4 30.2 33.6 72.4 40.0 66.3 21.6 52.0 22.8 74.6 20.7 27.5 40.5CoTTA [73]56.4 48.9 56.1 57.8 54.1 54.2 56.2 53.6 55.4 50.0 53.6 51.6 51.2 50.7 54.4 53.6NOTE [19]62.5 46.3 61.5 61.1 58.6 68.4 76.1 78.3 92.0 93.4 96.1 95.4 96.2 95.8 96.4 78.5 RoTTA 45.5 30.0 45.9 42.6 35.3 41.8 42.2 34.5 40.2 27.3 31.3 30.2 32.7 28.1 34.9 36.2(+4.3) Table 29. Average classification error of the task CIFAR100 → CIFAR100-C while continually adapting to different corruptions at the highest severity 5 with correlatively sampled test stream under the proposed setup PTTA. Time t− − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − → Method contrastgaussiandefocuszoom frost glass jpeg fog pixelateelasticshot impulsesnow motion brightness Avg. Source 55.1 73.0 29.3 28.8 45.8 54.1 41.2 50.3 74.7 37.2 68.0 39.4 39.5 30.8 29.5 46.4BN [53] 49.5 58.8 47.0 46.5 52.2 57.6 57.6 57.6 51.7 53.5 56.0 58.5 53.1 47.6 46.3 52.9PL [39] 53.6 70.4 76.0 85.1 91.2 95.2 96.0 97.0 96.9 97.3 97.3 97.6 97.5 97.6 97.7 89.8TENT [70] 60.2 89.1 95.0 96.2 96.9 97.0 96.5 97.0 97.0 97.2 97.6 97.8 97.5 97.9 97.7 94.0LAME [5] 51.3 72.5 21.5 21.0 39.6 49.0 33.3 44.8 74.8 28.0 66.8 32.5 30.6 22.5 21.4 40.6CoTTA [73]52.3 55.3 49.5 48.1 52.1 54.8 52.7 56.9 50.6 52.6 53.7 55.8 54.6 50.6 50.5 52.7NOTE [19] 39.1 64.7 48.9 50.6 59.1 70.1 71.7 75.0 85.2 95.7 96.9 98.4 96.0 95.9 94.9 76.1 RoTTA 41.4 46.2 30.5 28.5 36.0 40.9 40.5 39.6 33.0 35.0 38.2 43.1 33.9 30.7 27.1 36.3(+4.3)",
      "meta_data": {
        "arxiv_id": "2303.13899v1",
        "authors": [
          "Longhui Yuan",
          "Binhui Xie",
          "Shuang Li"
        ],
        "published_date": "2023-03-24T10:19:14Z",
        "pdf_url": "https://arxiv.org/pdf/2303.13899v1.pdf"
      },
      "llm_extracted_info": {
        "main_contributions": "The paper introduces a new, more realistic Test-Time Adaptation (TTA) setting called Practical Test-Time Adaptation (PTTA), which simultaneously considers continually changing distributions and correlative sampling in test data streams. To address the challenges of PTTA, it proposes Robust Test-Time Adaptation (RoTTA), a method designed for robust adaptation. RoTTA is shown to be effective and easy to implement, making it a practical deployment option. Extensive experiments demonstrate that RoTTA achieves state-of-the-art results on CIFAR-10-C, CIFAR-100-C, and DomainNet, significantly outperforming existing baselines.",
        "methodology": "RoTTA consists of three main components: (1) Robust Batch Normalization (RBN): It replaces standard BatchNorm statistics with global statistics (running mean µg and variance σ²g) maintained via Exponential Moving Average (EMA). These global statistics are initialized from the pre-trained model's source domain statistics and updated using EMA with statistics from the buffered samples in the memory bank, providing a more stable normalization for correlatively sampled data. (2) Category-Balanced Sampling with Timeliness and Uncertainty (CSTU): A memory bank stores samples, updated based on category balance, timeliness (age of the sample), and uncertainty (entropy of prediction). Newer and less uncertain samples are prioritized, and capacity is equally distributed per category, with major categories replaced first. (3) Robust Training with Timeliness: A teacher-student model architecture is employed. The student model's affine parameters in RBN layers are updated by minimizing a loss (Lr) over samples from the memory bank. This loss is a cross-entropy between strongly-augmented student predictions and weakly-augmented teacher predictions, reweighted by a timeliness term (E(Ai) = exp(−Ai/N) / (1 + exp(−Ai/N))). The teacher model is updated via EMA of the student's parameters.",
        "experimental_setup": "The method was evaluated on CIFAR-10-C, CIFAR-100-C for robustness under corruptions, and DomainNet for generalization under large domain shifts. For CIFAR-C datasets, WildResNet-28 and ResNeXt-29 were used, pre-trained from RobustBench. For DomainNet, a ResNet-101 was pre-trained for each source domain. The PTTA setup was simulated by continually changing corruption types (severity 5) for CIFAR-C and adapting to different target domains for DomainNet. Correlative sampling was simulated using a Dirichlet distribution with parameter δ=0.1. Experiments used the Adam optimizer with a learning rate of 1.0 × 10⁻³, batch size of 64, and a memory bank capacity of N=64. Unified hyperparameters for RoTTA were α=0.05, ν=0.001, λt=1.0, and λu=1.0. Performance was measured by average classification error, and ablation studies confirmed the efficacy of each component, as well as sensitivity analyses to hyper-parameters, batch size, and distribution changing order.",
        "limitations": "The adopted Robust Batch Normalization (RBN) is a naive solution and its effectiveness is sensitive to the design of the α value. While RoTTA is designed to prevent model collapse, the paper notes that an explicit mechanism to recover a model from a collapsed state, as observed in some baselines (e.g., PL, TENT), is still missing. The simulation of correlative sampling primarily focuses on category similarity using Dirichlet distribution, and the approach needs further validation in more diverse real-world scenarios beyond this specific simulation.",
        "future_research_directions": "Future work could focus on improving the RoTTA algorithm by replacing or refining some of its current components. More importantly, the authors hope this work paves the way for further research into Practical Test-Time Adaptation (PTTA) to make it even more realistic and applicable for deploying models in real-world dynamic scenarios."
      }
    },
    {
      "title": "Improved Test-Time Adaptation for Domain Generalization",
      "abstract": "The main challenge in domain generalization (DG) is to handle the\ndistribution shift problem that lies between the training and test data. Recent\nstudies suggest that test-time training (TTT), which adapts the learned model\nwith test data, might be a promising solution to the problem. Generally, a TTT\nstrategy hinges its performance on two main factors: selecting an appropriate\nauxiliary TTT task for updating and identifying reliable parameters to update\nduring the test phase. Both previous arts and our experiments indicate that TTT\nmay not improve but be detrimental to the learned model if those two factors\nare not properly considered. This work addresses those two factors by proposing\nan Improved Test-Time Adaptation (ITTA) method. First, instead of heuristically\ndefining an auxiliary objective, we propose a learnable consistency loss for\nthe TTT task, which contains learnable parameters that can be adjusted toward\nbetter alignment between our TTT task and the main prediction task. Second, we\nintroduce additional adaptive parameters for the trained model, and we suggest\nonly updating the adaptive parameters during the test phase. Through extensive\nexperiments, we show that the proposed two strategies are beneficial for the\nlearned model (see Figure 1), and ITTA could achieve superior performance to\nthe current state-of-the-art methods on several DG benchmarks. Code is\navailable at https://github.com/liangchen527/ITTA.",
      "full_text": "Improved Test-Time Adaptation for Domain Generalization Liang Chen1 Yong Zhang2* Yibing Song3 Ying Shan2 Lingqiao Liu1∗ 1 The University of Adelaide 2 Tencent AI Lab 3 AI3 Institute, Fudan University {liangchen527, zhangyong201303, yibingsong.cv}@gmail.com yingsshan@tencent.com lingqiao.liu@adelaide.edu.au Abstract The main challenge in domain generalization (DG) is to handle the distribution shift problem that lies between the training and test data. Recent studies suggest that test-time training (TTT), which adapts the learned model with test data, might be a promising solution to the problem. Gen- erally, a TTT strategy hinges its performance on two main factors: selecting an appropriate auxiliary TTT task for up- dating and identifying reliable parameters to update during the test phase. Both previous arts and our experiments in- dicate that TTT may not improve but be detrimental to the learned model if those two factors are not properly consid- ered. This work addresses those two factors by proposing an Improved Test-Time Adaptation (ITTA) method. First, in- stead of heuristically defining an auxiliary objective, we pro- pose a learnable consistency loss for the TTT task, which con- tains learnable parameters that can be adjusted toward bet- ter alignment between our TTT task and the main prediction task. Second, we introduce additional adaptive parameters for the trained model, and we suggest only updating the adap- tive parameters during the test phase. Through extensive ex- periments, we show that the proposed two strategies are ben- eficial for the learned model (see Figure 1), and ITTA could achieve superior performance to the current state-of-the-art methods on several DG benchmarks. Code is available at https://github.com/liangchen527/ITTA. 1. Introduction Recent years have witnessed the rapid development of deep learning models, which often assume the training and test data are from the same domain and follow the same distribution. However, this assumption does not always hold in real-world scenarios. Distribution shift among the source and target domains is ubiquitous in related areas [35], such as autonomous driving or object recognition tasks, resulting *Corresponding authors. This work is done when L. Chen is an intern in Tencent AI Lab. 0.5 1.1 0.5 1.2 0.5 0.5 0.5 1.4 0.4 0.4 0.4 0.3 art cartoon photo sketch 79.9 75.4 94.4 75.8 83.3 76.0 94.4 76.7 84.7 78.0 94.5 78.2 Figure 1. Performance improvements from the proposed two strate- gies (i.e. introducing a learnable consistency loss and including additional adaptive parameters to improve TTT) for the baseline model (i.e. ResNet18 [30] with existing augmentation strategy [75]). Experiments are conducted on the PACS dataset [37] with the leave- one-out setting. Following [27], we use 60 sets of random seeds and hyper-parameters for each target domain. The reported average accuracy and error bars verify the effectiveness of our method. in poor performances for delicately designed models and hindering the further application of deep learning techniques. Domain generalization (DG) [2,8,16,23,24,31,38 –40,40, 44, 47, 51, 52, 69], designed to generalize a learned model to unseen target domains, has attracted a great deal of attention in the research community. The problem can be traced back to a decade ago [7], and various approaches have been pro- posed to push the DG boundary ever since. Those efforts in- clude invariant representation learning [28,47,49,58], adver- sarial learning [23,40,44,69], augmentation [9,41,42,66,75], or meta-learning [2, 16, 38, 39]. Despite successes on certain occasions, a recent study [27] shows that, under a rigorous evaluation protocol, most of these arts are inferior to the baseline empirical risk minimization (ERM) method [61]. This finding is not surprising, as most current arts strive to decrease the distribution shift only through the training data while overlooking the contributions from test samples. Recently, the test-time training (TTT) technique [60] has been gaining momentum for easing the distribution shift problem. TTT lies its success in enabling dynamic tuning of the pretrained model with the test samples via an auxil- iary TTT task, which seems to be a promising effort when arXiv:2304.04494v2  [cs.CV]  16 Apr 2023confronting data from different domains. However, TTT is not guaranteed to improve the performance. Previous arts [46, 63] indicate that selecting an appropriate auxiliary TTT task is crucial, and an inappropriate one that does not align with the main loss may deteriorate instead of improv- ing the performance. Meanwhile, it is pointed out in [63] that identifying reliable parameters to update is also essential for generalization, which is in line with our experimental findings in Sec. 5.3. Both of these two tasks are non-trivial, and there are limited efforts made to address them. This paper aims to improve the TTT strategy for better DG. First, different from previous works that empirically define auxiliary objectives and assume they are aligned with the main task, our work does not make such assumptions. Instead, we suggest learning an appropriate auxiliary loss for test-time updating. Specifically, encouraged by recent successes in multi-view consistency learning [13,26,29], we propose to augment the consistency loss by adding learn- able parameters based on the original implementation, where the parameters can be adjusted to assure our TTT task can be more aligned with the main task and are updated by en- forcing the two tasks share the same optimization direction. Second, considering that identifying reliable parameters to update is an everlasting job given the growing size of current deep models, we suggest introducing new adaptive param- eters after each block during the test phase, and we only tune the new parameters by the learned consistency loss while leaving the original parameters unchanged. Through extensive evaluations on the current benchmark [27], we illustrate that the learnable consistency loss performs more effectively than the self-supervised TTT tasks adopted in previous arts [60, 63], and by tuning only the new adaptive parameters, our method is superior to existing strategies that update all the parameters or part of them. This work aims to ease the distribution shift problem by improving TTT, and the main contributions are three-fold: • We introduce a learnable consistency loss for test-time adaptation, which can be enforced to be more aligned with the main loss by tuning its learnable parameters. • We introduce new adaptive parameters for the trained model and only update them during the test phase. • We conduct experiments on various DG benchmarks and illustrate that our ITTA performs competitively against current arts under the rigorous setting [27] for both the multi-source and single-source DG tasks. 2. Related Works 2.1. Domain Generalization. Being able to generalize to new environments while de- ploying is a challenging and practical requirement for cur- rent deep models. Existing DG approaches can be roughly categorized into three types. (1) Invariant representation learning: The pioneering work [5] theoretically proves that if the features remain invariant across different domains, then they are general and transferable to different domains. Guided by this finding, [47] uses maximum mean discrep- ancy (MMD) to align the learned features, and [25] proposes to use a multi-domain reconstruction auto-encoder to obtain invariant features. More recently, [58] suggests maximiz- ing the inner product of gradients from different domains to enforce invariance, and a similar idea is proposed in [52] where these gradients are expected to be similar to their mean values. (2) Optimization algorithms: Among the different optimization techniques adopted in DG, prevail- ing approaches resort to adversarial learning [23, 40, 44, 69] and meta-learning [2, 16, 38, 39]. Adversarial training is often used to enforce the learned features to be agnostic about the domain information. In [23], a domain-adversarial neural network (DANN) is implemented by asking the main- stream feature to maximize the domain classification loss. This idea is also adopted in [44], where adversarial training and an MMD constraint are employed to update an auto- encoder. Meanwhile, the meta-learning technique is used to simulate the distribution shifts between seen and unseen environments [2, 16, 38, 39], and most of these works are developed based on the MAML framework [20]. (3) Aug- mentation: Most augmentation skills applied in the general- ization tasks are operated in the feature level [34, 41, 48, 75] except for [11,66,68] which mix images [68] or its phase [66] to synthesize new data. To enable contrastive learning, we incorporate an existing augmentation strategy [75] in our framework. This method originated from AdaIN [32], which synthesizes new domain information by mixing the statistics of the features. Similar ideas can be found in [42, 48]. 2.2. Test-Time Training and Adaptation Test-Time Training (TTT) is first introduced in [60]. The basic paradigm is to employ a test-time task besides the main task during the training phase and update the pre- trained model using the test data with only the test-time objective before the final prediction step. The idea is empir- ically proved effective [60] and further developed in other related areas [3, 10, 12, 14, 21, 22, 43, 56, 63, 65, 73, 74]. Most current works focus on finding auxiliary tasks for updat- ing during the test phase, and the efforts derive from self- supervion [3, 10, 21, 22, 43, 60], meta-learning [65, 73, 74], information entropy [63], pseudo-labeling [12, 14], to name a few. However, not all empirically selected test-time tasks are effective. A recent study [46] indicates that only when the auxiliary loss aligns with the main loss can TTT improve the trained model. Inspired by that, we propose a learnable consistency loss and enforce alignment between the two ob- jectives. Results show that our strategy can be beneficial for the trained model (see Figure 1).subtract Figure 2. Training process of ITTA. We use x from the source domain as input for the feature extractor fθ(·) to obtain the repre- sentation z and its augmented version z′, where the augmentation skill from [75] is applied. The classifier fϕ(·) and weight subnet- work fw(·) are used to compute the main loss Lmain and learnable consistency loss Lwcont. Please refer to our text for details. Meanwhile, [63] suggests that auxiliary loss is not the only factor that affects the performance. Selecting reliable parameters to update is also crucial within the TTT frame- work. Given the large size of current models, correctly iden- tifying these parameters may require tremendous amounts of effort. To this end, instead of heuristically selecting candi- dates, we propose to include new adaptive parameters for up- dating during the test phase. Experimental results show that the proposed method can obtain comparable performances against existing skills. 3. Methodology In the task of DG, we are often given access to data from S (S ≥ 1) source domains Ds = {D1, D2, ..., DS} and expect a model to make good prediction on unseen target domains Dt = {D1, D2, ..., DT } (T ≥ 1). Our method aims to improve the test-time training (TTT) strategy for better DG. The improvements are two-fold. First, we pro- pose a learnable consistency loss for the TTT task, which could be enforced to align with the main objective by tuning its learnable weights. Second, we suggest including addi- tional adaptive parameters and only updating these adaptive parameters during the test phase. 3.1. A Learnable Consistency Loss for TTT The TTT strategies have shown promising performances when dealing with distribution shift problems [43, 63]. How- ever, their successes are depended on the empirically selected auxiliary TTT tasks, which may deteriorate the performances if chosen improperly. Motivated by the recent successes in multi-view consistency learning [13, 26, 29], we suggest adopting a consistency loss in our TTT task. Note that the naive consistency loss is still not guaranteed to be effective as prior art [46] indicates that only when the auxiliary loss aligns with the main loss, can TTT improves the perfor- mance. To this end, we propose to augment the auxiliary loss with learnable parameters that could be adjusted toward a better alignment between the TTT and main tasks. In our case, we make the adopted consistency loss learnable by introducing a weight subnetwork that allows flexible ways Algorithm 1 Pseudo code of the training phase of ITTA in a PyTorch-like style. # fθ, fϕ, fw: feature extractor, classifier, weight subnetwork # α, 0: weight paramter, all zero tensor # training process for x, yin training loader: # load a minibatch with N samples def forward process(x, y): z, z′ = fθ.forward(x) # computing losses Lmain = CrossEntropyLoss(fϕ.forward(z), y) Lmain+ =CrossEntropyLoss(fϕ.forward(z′), y) Lwcont = MSELoss(fw.forward(z − z′), 0) return Lmain, Lwcont # SGD update: feature extractor and classifier Lmain, Lwcont = forward process(x, y) ([fθ.params, fϕ.params]).zero grad() (Lmain + αLwcont).backward() update( \u0002 fθ.params, fϕ.params \u0003 ) # compute objectives for updating weight subnetwork Lmain, Lwcont = forward process(x, y) Lmain.backward() ˆgmain = fθ.params.grad.clone().normalize() fθ.params.zero grad() Lwcont.backward() ˆgwcont = fθ.params.grad.clone().normalize() # SGD update: weight subnetwork MSELoss(ˆgmain, ˆgwcont).backward() fw.params.zero grad() update(fw.params) to measure the consistency between two views of the same instance. We first introduce the pipeline of our training framework. Given the D dimensional representation z ∈ RD1 and its corresponding augmented version z′ that are obtained from a feature extractor (i.e. {z, z′} = fθ(x), where x is an input image from Ds, and fθ(·) is the feature extractor parame- terized by θ. In our implementation, we use the existing augmentation method [75] to obtain z′ by modifying the intermediate activation in fθ(x). We show in our supplemen- tary material that our framework can also thrive with other augmentation strategies), our learnable consistency loss is given by, Lwcont = ∥fw(z − z′)∥, (1) where ∥ · ∥denotes the L2 norm; fw(·) is the weight sub- network parameterized by w. To make the training process more stable and potentially achieve better performance, we apply a dimension-wise nonlinear function to map each di- mension of z − z′ before calculating the L2 norm. That is, ∀h ∈ RD, fw(h) is implemented by stacking layers of a nonlinear function: ReLU(a ∗ h + b), where a ∈ RD and b ∈ RD are the weight and bias from the nonlinear function, 1We omit the batch dimensions of the variables for simplicity.… … subtract Figure 3. Test adaptation process of ITTA. Different from that in the training stage, we include additional adaptive parameters fΘ after each block of the feature extractor fθ. For each test sample x, the intermediate representations zi and z′i obtained from fi θ are passed to fi Θ before going to the next block fi+1 θ . We use the learnable consistency loss Lwcont as the objective to update fΘ. Please refer to our text for details. and different layers of a, bform the parameter w in fw. In effect, this creates a piecewise-linear mapping function for h: depending on the value of h, the output could be 0, a constant, or a scaling-and-shifted version of h. More studies about the design of fw are provided in our supplementary material. Compared to the naive consistency learning with- out fw, our Lwcont can be more flexible with an adjustable fw, which we show in the following is the key for learning an appropriate loss in the improved TTT framework. Combining Lwcont with the main loss Lmain which applies the cross-entropy loss (CE) for both the origi- nal and augmented inputs ( i.e. Lmain = CE(fϕ(z), y) + CE(fϕ(z′), y), where fϕ is the classifier parameterized by ϕ, and y is the corresponding label), the objective for the feature extractor and classifier can be formulated into, min{θ,ϕ} Lmain + αLwcont, (2) where α is the weight parameter that balances the contri- butions from the two terms. A simple illustration of the workflow is shown in Figure 2. From Eq. (2), the expected gradients for the feature ex- tractor from Lmain and Lwcont can be represented as, \u001a gmain = ∇θ(CE(fϕ(z), y) + CE(fϕ(z′), y)), (3) gwcont = ∇θ∥fw(z − z′)∥. (4) We observe that the direction of gwcont is also determined by the weight subnetwork fw(·), which should be close with gmain to ensure alignment between Lmain and Lwcont [46, 60]. To this end, we propose a straightforward solution by enforcing equality between the normalized versions of gmain and gwcont, and we use this term as the objective for updating fw(·), which gives, min w Lalign, s.t. Lalign = ∥ˆgmain − ˆgwcont∥, (5) where ˆgmain = gmain−Egmain σgmain , and similar for ˆgwcont. In our implementation, we update {θ, ϕ} and w in an alternative manner. Pseudo code of the training process are shown in Algorithm 1. Algorithm 2 Pseudo code of the test phase of ITTA in a PyTorch-like style. # fθ, fϕ: feature extractor, classifier # fw, fΘ: weight subnetwork, additional adaptive blocks # m, 0: total number of blocks in fθ, all zero tensor # test process for x in test loader: # load a test batch def forward process(x): z1, z′1 = f1 Θ.forward((f1 θ .forward(x))) # first blocks for i in range(2, m + 1): # the following m − 1 blocks zi, z′i = fi θ.forward(zi−1), fi θ.forward(z′i−1) zi, z′i = fi Θ.forward(zi), fi Θ.forward(z′i) return zi, z′i # test adaptation phase: SGD update additional adaptive parameters z, z′ = forward process(x) Lwcont = MSELoss(fw.forward(z − z′), 0) fΘ.params.zero grad() Lwcont.backward() update(fΘ.params) # final prediction z, = forward process(x) result = fϕ.forward(z) 3.2. Including Additional Adaptive Parameters Selecting expressive and reliable parameters to update during the test phase is also essential in the TTT frame- work [63]. Some strategies decide to update all the parame- ters from the feature extractor [3, 43], while others use only the parameters from the specific layers for updating [63, 71]. Given the fact that the sizes of current deep models are often very large and still growing, exhaustively trying different combinations among the millions of candidates seems to be an everlasting job. As there are no consensuses on which parameter should be updated, we suggest another easy alter- native in this work. Specifically, assuming there are a total of m blocks in the pretrained feature extractor fθ(·), and the i-th block can be denoted as fi θ(·). Then the intermediate representation zi from fi θ(·) can be formulated as, zi = fi θ(zi−1), s.t. z1 = f1 θ (x). (6) We propose to include additional adaptive blockfΘ that is parameterized by Θ after each block of fθ during the test- time adaptation phase, which reformulates Eq. (6) into, zi = fi Θ(fi θ(zi−1)), s.t. z1 = f1 Θ(f1 θ (x)), (7) where fΘ(·) does not change the dimension and sizes of the intermediate representations. In our work, we use a structure similar to fw to implement fΘ. Note zm is simplified as z in this phase, and the same process is applied for obtaining z′. Then, in the test-time adaptation phase, we suggest only updating the new adaptive parameters via the learned con- sistency loss. The optimization process can be written as,Table 1. Multi sources domain generalization. Experiments are conducted on the DomainBed benchmark [27]. All methods are examined for 60 trials in each unseen domain. Top5 accumulates the number of datasets where a method achieves the top 5 performances. The score here accumulates the numbers of the dataset where a specific art obtains larger accuracy than ERM on account of the variance. Best results are colored as red. Among the 22 methods compared, less than a quarter outperforms ERM in most datasets (Score ≥ 3). PACS VLCS OfficeHome TerraInc DomainNet Avg. Top5↑ Score↑ MMD [40] 81.3 ± 0.8 74.9 ± 0.5 59.9 ± 0.4 42.0 ± 1.0 7.9 ± 6.2 53.2 1 2 RSC [33] 80.5 ± 0.2 75.4 ± 0.3 58.4 ± 0.6 39.4 ± 1.3 27.9 ± 2.0 56.3 0 1 IRM [1] 80.9 ± 0.5 75.1 ± 0.1 58.0 ± 0.1 38.4 ± 0.9 30.4 ± 1.0 56.6 0 1 ARM [72] 80.6 ± 0.5 75.9 ± 0.3 59.6 ± 0.3 37.4 ± 1.9 29.9 ± 0.1 56.7 0 0 DANN [23] 79.2 ± 0.3 76.3 ± 0.2 59.5 ± 0.5 37.9 ± 0.9 31.5 ± 0.1 56.9 1 1 GroupGRO [55] 80.7 ± 0.4 75.4 ± 1.0 60.6 ± 0.3 41.5 ± 2.0 27.5 ± 0.1 57.1 0 1 CDANN [44] 80.3 ± 0.5 76.0 ± 0.5 59.3 ± 0.4 38.6 ± 2.3 31.8 ± 0.2 57.2 0 0 VREx [36] 80.2 ± 0.5 75.3 ± 0.6 59.5 ± 0.1 43.2 ± 0.3 28.1 ± 1.0 57.3 1 1 CAD [53] 81.9 ± 0.3 75.2 ± 0.6 60.5 ± 0.3 40.5 ± 0.4 31.0 ± 0.8 57.8 1 2 CondCAD [53] 80.8 ± 0.5 76.1 ± 0.3 61.0 ± 0.4 39.7 ± 0.4 31.9 ± 0.7 57.9 0 1 MTL [6] 80.1 ± 0.8 75.2 ± 0.3 59.9 ± 0.5 40.4 ± 1.0 35.0 ± 0.0 58.1 0 0 ERM [61] 79.8 ± 0.4 75.8 ± 0.2 60.6 ± 0.2 38.8 ± 1.0 35.3 ± 0.1 58.1 1 - MixStyle [75] 82.6 ± 0.4 75.2 ± 0.7 59.6 ± 0.8 40.9 ± 1.1 33.9 ± 0.1 58.4 1 1 MLDG [38] 81.3 ± 0.2 75.2 ± 0.3 60.9 ± 0.2 40.1 ± 0.9 35.4 ± 0.0 58.6 1 1 Mixup [68] 79.2 ± 0.9 76.2 ± 0.3 61.7 ± 0.5 42.1 ± 0.7 34.0 ± 0.0 58.6 2 2 Fishr [52] 81.3 ± 0.3 76.2 ± 0.3 60.9 ± 0.3 42.6 ± 1.0 34.2 ± 0.3 59.0 2 2 SagNet [48] 81.7 ± 0.6 75.4 ± 0.8 62.5 ± 0.3 40.6 ± 1.5 35.3 ± 0.1 59.1 1 2 SelfReg [34] 81.8 ± 0.3 76.4 ± 0.7 62.4 ± 0.1 41.3 ± 0.3 34.7 ± 0.2 59.3 2 3 Fish [58] 82.0 ± 0.3 76.9 ± 0.2 62.0 ± 0.6 40.2 ± 0.6 35.5 ± 0.0 59.3 3 4 CORAL [59] 81.7 ± 0.0 75.5 ± 0.4 62.4 ± 0.4 41.4 ± 1.8 36.1 ± 0.2 59.4 2 3 SD [51] 81.9 ± 0.3 75.5 ± 0.4 62.9 ± 0.2 42.0 ± 1.0 36.3 ± 0.2 59.7 4 4 Ours 83.8 ± 0.3 76.9 ± 0.6 62.0 ± 0.2 43.2 ± 0.5 34.9 ± 0.1 60.2 4 4 min Θ ∥fw(z − z′)∥, s.t. {z, z′} = fΘ(fθ(x)). (8) Note that different from the training phase, x in this stage is from the target domain Dt, and we use the online setting in [60] for updating. A simple illustration of the test adaptation pipeline is shown in Figure 3. For the final step, we use the original representation ob- tained from the pretrained feature extractor and the adapted adaptive parameters for prediction. Pseudo code of the test stage are shown in Algorithm 2. 4. Experiments 4.1. Settings Datasets. We evalute ITTA on five benchmark datasets: PACS [37] which consists of 9,991 images from 7 cate- gories. This dataset is probably the most widely-used DG benchmark owing to its large distributional shift across 4 do- mains including art painting, cartoon, photo, and sketch; VLCS [18] contains 10,729 images of 5 classes from 4 different datasets (i.e. domains) including PASCAL VOC 2007 [17], LabelMe [54], Caltech [19], and Sun [64] where each dataset is considered a domain in DG;OfficeHome [62] is composed of 15,588 images from 65 classes in office and home environments, and those images can be categorized into 4 domains (i.e. artistic, clipart, product, and real world); TerraInc [4] has 24,788 images from 10 classes. Those images are wild animals taken from 4 different locations (i.e. domains) including L100, L38, L43, and L46; Domain- Net [50] which contains 586,575 images from 345 classes, and the images in it can be depicted in 6 styles (i.e. clipart, infograph, painting, quickdraw, real, and sketch). Implementation details. For all the experiments, we use the ImageNet [15] pretrained ResNet18 [30] backbone that with 4 blocks as the feature extractor fθ, which could en- large the gaps in DG compared to larger models [70]. Corre- spondingly, we also include 4 blocks of additional adaptive parameters (i.e. fΘ), and each block is implemented with 5 layers of learnable parameters with weight initialized as all ones and bias initialized as all zeros. For the weight subnet- work fw, we use 10 layers of learnable parameters with the initialization skill similar to that of fΘ. The classifier fϕ is an MLP layer provided by the Domainbed benchmark [27]. For the weight parameter α in Eq. (2), we set it to be 1 for all experiments (please refer to our supplementary material for analysis). The random seeds, learning rates, batch size, and augmentation skills are all dynamically set for all the compared arts according to [27].Table 2. Single source domain generalization. Experiments are conducted on the PACS dataset [37]. Here A, C, P, and S are the art, cartoon, photo, and sketch domains in PACS. A→C represents models trained on the art domain and tested on the cartoon domain, and similar for others. All methods are examined for 60 trials in each unseen domain. Best results are colored as red. A→C A →P A →S C →A C →P C →S P →A P →C P →S S →A S →C S →P Avg. RSC 66.3 ±1.3 88.2±0.6 57.2±3.1 65.8±1.5 82.4±0.6 68.7±2.5 60.5±2.0 41.3±6.0 53.1±2.8 53.8±1.6 65.9±0.7 48.4±1.9 62.6 Fish 67.1 ±0.5 89.2±1.8 57.0±0.2 66.7±1.0 85.6±0.4 64.5±3.6 55.1±2.1 33.9±2.3 51.2±4.2 59.1±3.2 67.1±0.9 58.4±1.2 62.9 CDANN 66.5±1.7 92.2±0.6 65.0±0.9 70.6±0.1 82.9±1.4 67.7±3.0 60.6±0.3 42.2±6.4 46.9±9.9 51.4±2.3 60.7±1.2 51.9±0.4 63.2 SelfReg 63.9±1.9 90.1±1.0 56.8±2.2 70.2±2.3 85.4±0.3 70.2±2.2 60.9±2.6 38.8±4.0 50.5±3.2 54.5±4.7 66.2±1.2 51.7±4.1 63.3 DANN 67.5 ±1.6 91.2±1.3 67.5±1.3 70.6±1.0 81.4±0.4 66.6±1.1 54.1±2.3 33.5±2.7 52.8±2.3 53.8±1.7 64.4±0.7 58.9±0.8 63.5 CAD 67.1 ±1.5 89.6±0.4 60.2±0.2 67.7±3.1 83.7±1.4 70.2±2.6 60.6±2.6 38.3±3.7 53.8±3.2 50.7±1.6 65.8±1.3 54.4±1.7 63.5 GroupGRO66.5±1.2 90.5±1.5 58.9±2.5 70.8±0.9 85.7±1.2 69.7±1.8 62.3±2.1 41.1±2.7 48.2±4.1 54.8±0.5 65.2±1.6 53.9±1.4 64.0 MTL 67.3 ±1.0 90.1±1.0 58.9±0.7 70.2±1.8 84.2±2.2 71.9±0.7 58.3±2.7 38.5±2.7 52.8±1.5 55.4±3.1 66.1±1.3 55.2±2.6 64.1 IRM 67.5 ±1.8 93.0±0.5 62.9±4.7 67.6±1.3 83.8±0.4 68.9±0.8 63.7±1.8 39.9±3.7 49.0±5.4 54.9±1.4 63.1±2.1 54.9±1.4 64.1 ARM 66.0 ±2.4 91.2±0.7 58.7±6.9 70.6±0.8 84.2±1.0 69.1±0.9 59.2±1.8 42.1±5.6 52.1±3.0 60.0±0.6 62.9±3.3 53.8±2.0 64.2 Mixup 65.5 ±0.8 87.8±0.3 57.2±1.0 71.4±1.1 83.1±1.8 68.0±3.0 59.6±1.7 37.2±2.7 56.5±3.8 55.0±2.2 66.2±1.5 62.7±4.2 64.2 CORAL 66.8±0.5 90.3±0.7 61.5±1.9 67.9±2.1 85.4±0.3 70.4±1.3 55.9±2.9 40.4±4.9 49.8±8.5 55.8±2.1 67.6±0.9 58.9±3.8 64.2 SD 67.1 ±1.3 91.7±1.2 63.7±4.1 70.3±0.9 84.4±0.7 69.4±2.3 57.5±2.5 42.6±0.8 47.7±1.7 55.9±2.4 65.7±0.8 55.8±2.1 64.3 MMD 67.1 ±1.4 88.0±0.8 63.6±1.6 70.0±1.1 83.6±0.2 70.2±1.0 58.8±2.6 40.3±1.0 52.3±2.4 57.4±1.9 68.7±0.9 52.7±3.7 64.4 MLDG 67.3±2.0 90.8±0.5 64.4±0.9 70.8±1.0 84.2±0.3 69.7±1.8 61.6±1.0 41.3±5.1 50.4±0.2 49.9±2.5 66.8±0.4 58.7±3.4 64.7 CondCAD66.9±1.4 92.3±0.7 60.8±4.5 71.0±0.6 84.7±1.1 72.6±0.5 61.2±1.5 40.7±3.6 55.7±1.6 52.3±1.7 64.2±0.4 55.3±1.2 64.8 ERM 67.3 ±0.7 91.7±0.9 60.1±4.7 70.4±0.6 82.3±2.7 68.1±0.9 59.6±1.8 44.7±2.8 56.5±2.7 52.8±2.3 68.1±0.7 58.4±0.9 65.0 VREx 67.1 ±1.5 91.0±1.0 62.6±3.5 71.1±2.4 84.1±0.9 71.7±1.3 62.4±3.1 37.7±3.3 53.6±2.3 60.6±1.6 66.7±0.8 57.5±1.4 65.5 Fishr 67.9 ±1.9 92.7±0.3 62.4±4.7 71.2±0.5 83.4±0.6 70.2±1.1 60.0±2.3 42.7±3.2 57.1±3.9 55.7±3.7 68.4±1.0 62.0±3.1 66.1 SagNet 67.6±1.4 92.3±0.5 59.5±1.7 71.8±0.3 82.8±0.6 69.9±1.8 62.5±2.5 45.2±2.5 64.1±2.0 55.8±1.1 65.7±1.4 55.9±3.5 66.1 MixStyle 68.5±2.0 91.2±1.6 65.1±0.7 73.2±1.3 85.0±0.8 71.7±1.5 63.6±1.7 46.3±1.1 51.6±3.7 54.2±1.5 67.0±3.4 58.3±1.4 66.3 Ours 68.9 ±0.6 92.4±0.1 62.5±0.6 75.3±0.4 85.9±0.3 70.2±1.4 66.5±1.1 52.2±2.7 63.8±1.1 57.6±3.7 68.0±1.3 57.9±2.0 68.4 Training and evaluation details. For all the compared methods, we conduct 60 trials on each source domain, and each with 5,000 iteration steps. During the training stage, we split the examples from training domains to 8:2 (train:val) where the training and validation samples are dynamically selected among different training trials. During test, we select the model that performs the best in the validation samples and test it on the target domains. The strategy is referred to as the “training-domain validate set” model selec- tion method in [27]. For each domain in different datasets, the final performance is the average accuracy from the 60 trials. 4.2. Multi-Source Generalization In these experiments, all five benchmark datasets afore- mentioned are used for evaluation, and the leave-one-out strategy is adopted for training (i.e. with S = |Ds ∪Dt|2 −1, and T = 1). Results are shown in Table 1. We note that ERM method obtains favorable performance against existing arts. In fact, as a strong baseline, ERM is superior to half of the methods in the term of average accuracy, and only 5 arts (i.e. SelfReg [34], Fish [58], CORAL [59], SD [51], and ours) among the compared 22 methods outperforms ERM in most datasets (i.e. with Score ≥ 3). In comparison, the proposed ITTA is more effective than all other models on average. In particular, ITTA achieves the best performances in 3 out of the 5 benchmarks (i.e. PACS, VLCS, and TerraInc datasets) and 4 in the top 5. Note that although our method does not obtain the best performances in the OfficeHome and DomainNet benchmarks, it still outperforms more than half 2We use | · |to denote the number of domains in the environment. of the existing models. The results validate the effectiveness of our method when tested in the multi-source setting. We present results of average accuracy in each domain from different datasets in the supplementary material. Please refer to it for details. 4.3. Single-Source Generalization In these experiments, we adopt the widely-used PACS [37] benchmark for evaluation, and the models are trained on one domain while tested on the remaining three (i.e. with S = 1, and T = 3). Although some approaches, such as MLDG [38] and Fishr [52], may require more than one domain information for their trainings, we can simu- late multi-domain information using only the source domain, and thus the experimental settings are still feasible for them. Compared to the multi-source generalization task, the single- source generalization is considered more difficult due to the limited domain information during the training phase. Evalu- ation results are presented in Table 2. We note that the ERM method outperforms most state-of-the-art models, and only 5 models, including VREx [36], Fishr [52], SagNet [48], MixStyle [75], and the proposed ITTA, can obtain better re- sults than ERM in the term of average accuracy. Meanwhile, our method achieves the best performances when trained in 5 out of the 12 source domain, and it obtains the best perfor- mance on average, leading more than 2% than the second best (i.e. MixStyle [75]) and 3% the ERM method. In line with the findings in [27], we notice that the naive ERM method [61] can indeed perform favorably against most existing models under rigorous evaluation protocol. As a matter of fact, the proposed method is the only one that consistently outperforms ERM in both the multi-sourceTable 3. Evaluations of different TTT-based models in the unseen domain from PACS [37]. The reported accuracies (%) and standard deviations are computed from 60 trials in each target domain. Model Target domain Avg.Art Cartoon Photo Sketch Baseline 79.9 ±0.5 75.4±1.1 94.4±0.5 75.8±1.2 81.4±0.5 TTT [60] 81.5±0.8 77.6±0.6 94.3±0.2 78.4±0.7 83.0±0.2 MT3 [3] 82.0 ±1.0 76.5±1.0 94.1±0.2 77.7±1.3 82.6±0.6 TENT [63] 80.2±0.9 77.2±0.8 94.4±0.2 77.4±0.1 82.3±0.5 Ours 84.7 ±0.4 78.0±0.4 94.5±0.4 78.2±0.3 83.8±0.3 and single-source settings. These results indicate that DG remains challenging for current efforts that aim to ease the distribution shift only through training data, and using the proposed improved TTT strategy may be a promising direc- tion for solving DG. 5. Analysis All experiments in this section are conducted on the widely-used PACS benchmark [37] with the leave-one-out strategy. The experimental settings are the same as that illus- trated in Sec. 4.1. Please refer to our supplementary material for more analysis. 5.1. Compared with Other TTT-Based Models Using test-time adaptation to ease the distribution shift problem has been explored in previous works, such as the original TTT method [60] and MT3 [3]. Their differences lie in that TTT uses a rotation estimation task for the test-time objective, and MT3 adopts a contrastive loss for the task and implements the overall framework using MAML [20]. There is also a recently proposed TENT [63] that aims to minimize the entropy of the final results by tuning the parameters from the batch normalization (BN) layers. To analyze the overall effectiveness of our method, we compare ITTA with these arts using the same baseline (i.e. ResNet18 [30] backbone with the existing augmentation skill [75]). Results are shown in Table 3. We observe that all the com- pared TTT-based methods can improve the baseline model in almost all target domains except for the “Photo” domain, which might be due to the ImageNet pretraining [67]. This phenomenon demonstrates that the TTT strategy may be a promising effort for easing the distribution shift problem. Meanwhile, we observe that the proposed ITTA is superior to all other approaches in most target domains and leads in the term of average accuracy. The main reason is that compared to the empirically designed TTT tasks adopted in previous works, the proposed learnable consistency loss is enforced to be more aligned with the main loss, thus more suitable for the test-time adaptation task [46]. Meanwhile, compared to the strategies that update the original param- eters from the trained model, the adaptation of the newly included parameters is also more effective for the overall (a) Input (b) Ours w/o fw (c) Ours (d) Main Figure 4. Grad-CAM [57] visualizations from different loss terms. We use images with varying class labels from the four target do- mains of PACS [37] as inputs (i.e. art, cartoon, photo, and sketch domains from top to bottom). Ours w/o fw is the naive consis- tency loss with fw disabled in Eq. (1). The proposed learnable consistency loss can align well with the main classification task. TTT framework. In the following, we provide more analysis to support these claims. 5.2. Effectiveness of the Learnable Consistency Loss To examine the effectiveness of our learnable consistency loss, we conduct ablation studies by comparing our method with the following variants. (1) Ours w/o fw: we disable fw when computing the learnable consistency loss in Eq. (1), which uses the naive consistency loss for the auxiliary TTT task. (2) Ours w/ Ent.: after training the model using the baseline settings (i.e. ResNet18 with the augmentation strat- egy [75]), we use the entropy minimization task in [63] for the TTT task. (3) Ours w/ Rot.: we use the rotation estimation task in [60] for the TTT task. To ensure fair com- parisons, we use the same baseline settings and include the same additional adaptive parameters for all the variants. Results are shown in the 4th to 6th rows Table 4. We find that the results from the naive consistency loss ( i.e. Ours w/o fw) are slightly better than that from the other two specially-designed objectives (i.e. Ours w/ Ent. and Ours w/ Rot.) on average. Besides the possibility of deteriorating the performance [46], our results indicate that empirically select- ing a TTT task may also be far from optimal. Meanwhile, we observe that when enabling fw, the proposed learnable consistency loss is superior to that withoutfw in all target do-Table 4. Comparison between different TTT tasks and parameter selecting strategies in the unseen domain from the PACS benchmark [37]. Here the “Ent.”, “Rot.”, and “Lwcont” denotes the entropy minimization task in [63], the rotation estimation task in [60], and the proposed learnable consistency objective, the “All”, “BN”, and “Ada.” are the strategies that update all the parameters, parameters from the batch normalization layer, and the proposed strategy that updates only the new additional adaptive parameters. The reported accuracies (%) and standard deviations are computed from 60 trials in each target domain. Model TTT tasks Param selectings Target domain Avg.Ent. Rot. Lwcont All BN Ada. Art Cartoon Photo Sketch Ours − − ✓ − − ✓ 84.7±0.4 78.0 ±0.4 94.5 ±0.4 78.2 ±0.3 83.8 ±0.3 Ours w/ofw − − − − − ✓ 83.1±0.4 74.6 ±0.6 94.0 ±0.5 78.0 ±0.8 82.5 ±0.1 Ours w/ Ent. ✓ − − − − ✓ 79.9±2.4 77.3 ±0.3 94.8 ±0.8 77.6 ±0.4 82.4 ±0.8 Ours w/ Rot. − ✓ − − − ✓ 81.1±1.0 75.2 ±0.5 94.9 ±0.3 77.3 ±0.6 82.1 ±0.3 Ours w/o TTT − − ✓ − − − 83.3±0.5 76.0 ±0.5 94.4 ±0.5 76.7 ±1.4 82.8 ±0.3 Ours w/ All − − ✓ ✓ − − 83.0±0.7 77.0 ±1.4 94.5 ±0.7 77.4 ±0.9 83.0 ±0.2 Ours w/ BN − − ✓ − ✓ − 81.8±0.5 75.6 ±0.3 94.4 ±0.3 77.9 ±1.1 82.4 ±0.5 mains, and it leads in the term of average accuracy among the variants compared, illustrating its advantage against other adopted TTT tasks. These results are not surprising. By comparing the Grad-CAM [57] visualizations from the main classification task with the learnable and naive consistency losses in Figure 4, we find that the proposed learnable objec- tive can well align with the main loss when fw is enabled as the hot zones activated by these two tasks are similar, which guarantees the improvement for the test-time adapta- tion [46, 60]. Please refer to our supplementary material for more visualizations. 5.3. Effectiveness of the Adaptive Parameters We compare ITTA with three variants to demonstrate the effectiveness of the proposed additional adaptive parameters. (1) Ours w/o TTT: we do not update any parameters during the test phase. This variant is used to verify whether TTT can improve the pretrained model. (2) Ours w/ ALL: similar to the updating strategy in the original TTT method [60], we update all the parameters from the feature extractor during the test phase. (3) Ours w/ BN: following the suggestion from TENT [63], only parameters from the BN layers of the feature extractor are updated. Note the same pretrained model is shared for all variants in these experiments, and the objectives during the test adaptation phase are to minimize the same learned consistency loss. We list the results in the last three rows in Table 4. We observe that when only updating parameters from the BN layers, the performance is inferior to the strategy without test-time adaptation, and updating all the parameters does not ensure improvements in all target domains. The observations are in line with the findings in [63] that selecting reliable parameters to update is essential in the TTT system and may also interact with the choice of the TTT task. In comparison, when including additional adaptive parameters for updating, the pretrained model can be boosted in all environments. The results validate that our adaptive parameters are more effective than that selected with existing strategies [60, 63] when applied with the proposed learnable test-time objective. 5.4. Limitation Although the proposed learned loss can bring satisfaction improvements, we are aware that the lunch is not free. When the weight subnetwork fw is disabled, updating the joint loss in Eq. (2) only costs 1 forward and 1 backward. However, in order to update fw, we have to compute the second-order derivative in Eq. (5), which will require 1 more forward and 3 more backward processes, bringing extra burden to the system. Our future efforts aim to simplify the overall optimization process and reduce the cost for ITTA. 6. Conclusion In this paper, we aim to improve the current TTT strategy for alleviating the distribution shift problem in DG. First, given that the auxiliary TTT task plays a vital role in the over- all framework, and an empirically selecting one that does not align with the main task may potentially deteriorate instead of improving the performance, we propose a learnable con- sistency loss that can be enforced to be more aligned with the main loss by adjusting its learnable parameters. This strategy is ensured to improve the model and shows favorable perfor- mance against some specially-designed objectives. Second, considering that selecting reliable and effective parameters to update during the test phase is also essential while exhaus- tively trying different combinations may require tremendous effort, we propose a new alternative by including new ad- ditional adaptive parameters for adaptation during the test phase. This alternative is shown to outperform some pre- vious parameter selecting strategies via our experimental findings. By conducting extensive experiments under a rig- orous evaluation protocol, we show that our method can achieve superior performance against existing arts in both the multi-source and single-source DG tasks. Acknowledgements. Liang Chen is supported by the ChinaScholarship Council (CSC Student ID 202008440331). References [1] Martin Arjovsky, L´eon Bottou, Ishaan Gulrajani, and David Lopez-Paz. Invariant risk minimization. arXiv preprint arXiv:1907.02893, 2019. 5, 15, 16, 17 [2] Yogesh Balaji, Swami Sankaranarayanan, and Rama Chel- lappa. Metareg: Towards domain generalization using meta- regularization. In NeurIPS, 2018. 1, 2, 14, 15 [3] Alexander Bartler, Andre B¨uhler, Felix Wiewel, Mario D¨obler, and Bin Yang. Mt3: Meta test-time training for self- supervised test-time adaption. In AISTATS, 2022. 2, 4, 7 [4] Sara Beery, Grant Van Horn, and Pietro Perona. Recognition in terra incognita. In ECCV, 2018. 5, 17 [5] Shai Ben-David, John Blitzer, Koby Crammer, and Fernando Pereira. Analysis of representations for domain adaptation. In NeurIPS, 2006. 2 [6] Gilles Blanchard, Aniket Anand Deshmukh, Urun Dogan, Gyemin Lee, and Clayton Scott. Domain generalization by marginal transfer learning. arXiv preprint arXiv:1711.07910, 2017. 5, 15, 16, 17 [7] Gilles Blanchard, Gyemin Lee, and Clayton Scott. Generaliz- ing from several related classification tasks to a new unlabeled sample. In NeurIPS, 2011. 1 [8] Chaoqi Chen, Jiongcheng Li, Xiaoguang Han, Xiaoqing Liu, and Yizhou Yu. Compound domain generalization via meta- knowledge encoding. In CVPR, 2022. 1 [9] Chaoqi Chen, Luyao Tang, Feng Liu, Gangming Zhao, Yue Huang, and Yizhou Yu. Mix and reason: Reasoning over se- mantic topology with data mixing for domain generalization. In NeurIPS, 2022. 1 [10] Dian Chen, Dequan Wang, Trevor Darrell, and Sayna Ebrahimi. Contrastive test-time adaptation. In CVPR, 2022. 2 [11] Liang Chen, Yong Zhang, Yibing Song, Lingqiao Liu, and Jue Wang. Self-supervised learning of adversarial example: Towards good generalizations for deepfake detection. In CVPR, 2022. 2 [12] Liang Chen, Yong Zhang, Yibing Song, Jue Wang, and Lingqiao Liu. Ost: Improving generalization of deepfake detection via one-shot test-time training. In NeurIPS, 2022. 2, 12 [13] Ting Chen, Simon Kornblith, Mohammad Norouzi, and Geof- frey Hinton. A simple framework for contrastive learning of visual representations. In ICML, 2020. 2, 3 [14] Sungha Choi, Seunghan Yang, Seokeon Choi, and Sungrack Yun. Improving test-time adaptation via shift-agnostic weight regularization and nearest source prototypes. In ECCV, 2022. 2 [15] Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li, and Li Fei-Fei. Imagenet: A large-scale hierarchical image database. In CVPR, 2009. 5 [16] Qi Dou, Daniel Coelho de Castro, Konstantinos Kamnitsas, and Ben Glocker. Domain generalization via model-agnostic learning of semantic features. In NeurIPS, 2019. 1, 2 [17] Mark Everingham, Luc Van Gool, Christopher KI Williams, John Winn, and Andrew Zisserman. The pascal visual object classes (voc) challenge. IJCV, 88(2):303–338, 2010. 5 [18] Chen Fang, Ye Xu, and Daniel N Rockmore. Unbiased metric learning: On the utilization of multiple datasets and web images for softening bias. In ICCV, 2013. 5, 16 [19] Li Fei-Fei, Rob Fergus, and Pietro Perona. Learning gener- ative visual models from few training examples: An incre- mental bayesian approach tested on 101 object categories. In CVPR worksho, 2004. 5 [20] Chelsea Finn, Pieter Abbeel, and Sergey Levine. Model- agnostic meta-learning for fast adaptation of deep networks. In ICML, 2017. 2, 7 [21] Francois Fleuret et al. Uncertainty reduction for model adap- tation in semantic segmentation. In CVPR, 2021. 2 [22] Yossi Gandelsman, Yu Sun, Xinlei Chen, and Alexei A Efros. Test-time training with masked autoencoders. In NeurIPS, 2022. 2 [23] Yaroslav Ganin, Evgeniya Ustinova, Hana Ajakan, Pascal Germain, Hugo Larochelle, Franc ¸ois Laviolette, Mario Marc- hand, and Victor Lempitsky. Domain-adversarial training of neural networks. JMLR, 17(1):2096–2030, 2016. 1, 2, 5, 15, 16, 17 [24] Muhammad Ghifary, David Balduzzi, W Bastiaan Kleijn, and Mengjie Zhang. Scatter component analysis: A unified framework for domain adaptation and domain generalization. IEEE TPAMI, 39(7):1414–1430, 2016. 1 [25] Muhammad Ghifary, W Bastiaan Kleijn, Mengjie Zhang, and David Balduzzi. Domain generalization for object recognition with multi-task autoencoders. In ICCV, 2015. 2 [26] Jean-Bastien Grill, Florian Strub, Florent Altch ´e, Corentin Tallec, Pierre Richemond, Elena Buchatskaya, Carl Doer- sch, Bernardo Avila Pires, Zhaohan Guo, Mohammad Ghesh- laghi Azar, et al. Bootstrap your own latent-a new approach to self-supervised learning. In NeurIPS, 2020. 2, 3 [27] Ishaan Gulrajani and David Lopez-Paz. In search of lost domain generalization. In ICLR, 2021. 1, 2, 5, 6, 14, 15, 16, 17 [28] Sivan Harary, Eli Schwartz, Assaf Arbelle, Peter Staar, Shady Abu-Hussein, Elad Amrani, Roei Herzig, Amit Alfassy, Raja Giryes, Hilde Kuehne, et al. Unsupervised domain general- ization by learning a bridge across domains. In CVPR, 2022. 1 [29] Kaiming He, Haoqi Fan, Yuxin Wu, Saining Xie, and Ross Girshick. Momentum contrast for unsupervised visual repre- sentation learning. In CVPR, 2020. 2, 3 [30] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image recognition. In CVPR, 2016. 1, 5, 7, 14 [31] Shoubo Hu, Kun Zhang, Zhitang Chen, and Laiwan Chan. Domain generalization via multidomain discriminant analysis. In UAI, 2020. 1 [32] Xun Huang and Serge Belongie. Arbitrary style transfer in real-time with adaptive instance normalization. In ICCV, 2017. 2 [33] Zeyi Huang, Haohan Wang, Eric P Xing, and Dong Huang. Self-challenging improves cross-domain generalization. In ECCV, 2020. 5, 15, 16, 17[34] Daehee Kim, Youngjun Yoo, Seunghyun Park, Jinkyu Kim, and Jaekoo Lee. Selfreg: Self-supervised contrastive regular- ization for domain generalization. In ICCV, 2021. 2, 5, 6, 15, 16, 17 [35] Pang Wei Koh, Shiori Sagawa, Henrik Marklund, Sang Michael Xie, Marvin Zhang, Akshay Balsubramani, Weihua Hu, Michihiro Yasunaga, Richard Lanas Phillips, Irena Gao, et al. Wilds: A benchmark of in-the-wild distribu- tion shifts. In ICML, 2021. 1 [36] David Krueger, Ethan Caballero, Joern-Henrik Jacobsen, Amy Zhang, Jonathan Binas, Dinghuai Zhang, Remi Le Priol, and Aaron Courville. Out-of-distribution generalization via risk extrapolation (rex). In ICML, 2021. 5, 6, 15, 16, 17 [37] Da Li, Yongxin Yang, Yi-Zhe Song, and Timothy M Hospedales. Deeper, broader and artier domain generalization. In ICCV, 2017. 1, 5, 6, 7, 8, 12, 13, 14, 15 [38] Da Li, Yongxin Yang, Yi-Zhe Song, and Timothy M Hospedales. Learning to generalize: Meta-learning for do- main generalization. In AAAI, 2018. 1, 2, 5, 6, 15, 16, 17 [39] Da Li, Jianshu Zhang, Yongxin Yang, Cong Liu, Yi-Zhe Song, and Timothy M Hospedales. Episodic training for domain generalization. In ICCV, 2019. 1, 2 [40] Haoliang Li, Sinno Jialin Pan, Shiqi Wang, and Alex C Kot. Domain generalization with adversarial feature learning. In CVPR, 2018. 1, 2, 5, 15, 16, 17 [41] Pan Li, Da Li, Wei Li, Shaogang Gong, Yanwei Fu, and Timothy M Hospedales. A simple feature augmentation for domain generalization. In ICCV, 2021. 1, 2, 12, 14 [42] Xiaotong Li, Yongxing Dai, Yixiao Ge, Jun Liu, Ying Shan, and Ling-Yu Duan. Uncertainty modeling for out- of-distribution generalization. In ICLR, 2022. 1, 2 [43] Yizhuo Li, Miao Hao, Zonglin Di, Nitesh Bharadwaj Gun- davarapu, and Xiaolong Wang. Test-time personalization with a transformer for human pose estimation. In NeurIPS, 2021. 2, 3, 4 [44] Ya Li, Xinmei Tian, Mingming Gong, Yajing Liu, Tongliang Liu, Kun Zhang, and Dacheng Tao. Deep domain generaliza- tion via conditional invariant adversarial networks. In ECCV, 2018. 1, 2, 5, 15, 16, 17 [45] Yiying Li, Yongxin Yang, Wei Zhou, and Timothy Hospedales. Feature-critic networks for heterogeneous do- main generalization. In ICML, 2019. 14, 15 [46] Yuejiang Liu, Parth Kothari, Bastien van Delft, Baptiste Bellot-Gurlet, Taylor Mordan, and Alexandre Alahi. Ttt++: When does self-supervised test-time training fail or thrive? In NeurIPS, 2021. 2, 3, 4, 7, 8, 12, 14, 15 [47] Krikamol Muandet, David Balduzzi, and Bernhard Sch¨olkopf. Domain generalization via invariant feature representation. In ICML, 2013. 1, 2 [48] Hyeonseob Nam, HyunJae Lee, Jongchan Park, Wonjun Yoon, and Donggeun Yoo. Reducing domain gap by reducing style bias. In CVPR, 2021. 2, 5, 6, 15, 16, 17 [49] Prashant Pandey, Mrigank Raman, Sumanth Varambally, and Prathosh Ap. Generalization on unseen domains via inference- time label-preserving target projections. In CVPR, 2021. 1 [50] Xingchao Peng, Qinxun Bai, Xide Xia, Zijun Huang, Kate Saenko, and Bo Wang. Moment matching for multi-source domain adaptation. In ICCV, 2019. 5, 17 [51] Mohammad Pezeshki, Oumar Kaba, Yoshua Bengio, Aaron C Courville, Doina Precup, and Guillaume Lajoie. Gradient star- vation: A learning proclivity in neural networks. In NeurIPS, 2021. 1, 5, 6, 15, 16, 17 [52] Alexandre Rame, Corentin Dancette, and Matthieu Cord. Fishr: Invariant gradient variances for out-of-distribution gen- eralization. In ICML, 2022. 1, 2, 5, 6, 15, 16, 17 [53] Yangjun Ruan, Yann Dubois, and Chris J Maddison. Optimal representations for covariate shift. In ICLR, 2022. 5, 15, 16, 17 [54] Bryan C Russell, Antonio Torralba, Kevin P Murphy, and William T Freeman. Labelme: a database and web-based tool for image annotation. IJCV, 77(1):157–173, 2008. 5 [55] Shiori Sagawa, Pang Wei Koh, Tatsunori B Hashimoto, and Percy Liang. Distributionally robust neural networks for group shifts: On the importance of regularization for worst- case generalization. In ICLR, 2020. 5, 15, 16, 17 [56] Steffen Schneider, Evgenia Rusak, Luisa Eck, Oliver Bring- mann, Wieland Brendel, and Matthias Bethge. Improving robustness against common corruptions by covariate shift adaptation. In NeurIPS, 2020. 2 [57] Ramprasaath R Selvaraju, Michael Cogswell, Abhishek Das, Ramakrishna Vedantam, Devi Parikh, and Dhruv Batra. Grad- cam: Visual explanations from deep networks via gradient- based localization. In ICCV, 2017. 7, 8, 11, 13 [58] Yuge Shi, Jeffrey Seely, Philip HS Torr, N Siddharth, Awni Hannun, Nicolas Usunier, and Gabriel Synnaeve. Gradient matching for domain generalization. In ICLR, 2021. 1, 2, 5, 6, 15, 16, 17 [59] Baochen Sun and Kate Saenko. Deep coral: Correlation alignment for deep domain adaptation. In ECCV, 2016. 5, 6, 15, 16, 17 [60] Yu Sun, Xiaolong Wang, Zhuang Liu, John Miller, Alexei Efros, and Moritz Hardt. Test-time training with self- supervision for generalization under distribution shifts. In ICML, 2020. 1, 2, 4, 5, 7, 8, 11, 12, 13 [61] Vladimir Vapnik. The nature of statistical learning theory . Springer science & business media, 1999. 1, 5, 6, 15, 16, 17 [62] Hemanth Venkateswara, Jose Eusebio, Shayok Chakraborty, and Sethuraman Panchanathan. Deep hashing network for unsupervised domain adaptation. In CVPR, 2017. 5, 16 [63] Dequan Wang, Evan Shelhamer, Shaoteng Liu, Bruno Ol- shausen, and Trevor Darrell. Tent: Fully test-time adaptation by entropy minimization. In ICLR, 2021. 2, 3, 4, 7, 8, 11, 12, 13 [64] Jianxiong Xiao, James Hays, Krista A Ehinger, Aude Oliva, and Antonio Torralba. Sun database: Large-scale scene recog- nition from abbey to zoo. In CVPR, 2010. 5 [65] Zehao Xiao, Xiantong Zhen, Ling Shao, and Cees GM Snoek. Learning to generalize across domains on single test samples. In ICLR, 2022. 2 [66] Qinwei Xu, Ruipeng Zhang, Ya Zhang, Yanfeng Wang, and Qi Tian. A fourier-based framework for domain generaliza- tion. In CVPR, 2021. 1, 2 [67] Zhenlin Xu, Deyi Liu, Junlin Yang, Colin Raffel, and Marc Niethammer. Robust and generalizable visual representation learning via random convolutions. In ICLR, 2021. 7[68] Shen Yan, Huan Song, Nanxiang Li, Lincan Zou, and Liu Ren. Improve unsupervised domain adaptation with mixup training. arXiv preprint arXiv:2001.00677, 2020. 2, 5, 15, 16, 17 [69] Fu-En Yang, Yuan-Chia Cheng, Zu-Yun Shiau, and Yu- Chiang Frank Wang. Adversarial teacher-student representa- tion learning for domain generalization. In NeurIPS, 2021. 1, 2 [70] Nanyang Ye, Kaican Li, Haoyue Bai, Runpeng Yu, Lanqing Hong, Fengwei Zhou, Zhenguo Li, and Jun Zhu. Ood-bench: Quantifying and understanding two dimensions of out-of- distribution generalization. In CVPR, 2022. 5 [71] Fuming You, Jingjing Li, and Zhou Zhao. Test-time batch statistics calibration for covariate shift. arXiv preprint arXiv:2110.04065, 2021. 4 [72] Marvin Zhang, Henrik Marklund, Nikita Dhawan, Abhishek Gupta, Sergey Levine, and Chelsea Finn. Adaptive risk mini- mization: A meta-learning approach for tackling group distri- bution shift. arXiv preprint arXiv:2007.02931, 2020. 5, 15, 16, 17 [73] Marvin Zhang, Henrik Marklund, Nikita Dhawan, Abhishek Gupta, Sergey Levine, and Chelsea Finn. Adaptive risk mini- mization: Learning to adapt to domain shift. NeurIPS, 2021. 2 [74] Tao Zhong, Zhixiang Chi, Li Gu, Yang Wang, Yuanhao Yu, and Jin Tang. Meta-dmoe: Adapting to domain shift by meta- distillation from mixture-of-experts. In NeurIPS, 2022. 2 [75] Kaiyang Zhou, Yongxin Yang, Yu Qiao, and Tao Xiang. Do- main generalization with mixstyle. In ICLR, 2021. 1, 2, 3, 5, 6, 7, 12, 15, 16, 17 Appendix In this supplementary material, we provide, 1. Resource usage for ITTA in Section 7. 2. Grad-CAM visualizations of different loss terms in Section 8. 3. Parameter analysis of ITTA in Section 9; 4. Using a different augmentation skill for ITTA in Sec- tion 10. 5. Using different updating steps or a strategy for ITTA during the test phase in Section 11. 6. Using different network structures for the learnable consistency loss and adaptive parameters in Section 12. 7. Comparisons with other related methods in Section 13. 8. Detailed experimental results in the DomainBed bench- mark in Section 14. 7. Resource Usage Comparisons Between ITTA and the Baseline Model Requiring extra resources for our ITTA is a common lim- itation for existing test-time-based arts. To further evaluate our method, in this section, we compare FLOPS, model size, and inference time in Table 5. We compare only with ERM as most existing methods utilize the same network during in- ferences. We note that compare to the baseline model, ITTA requires extra Flops and processing time, this is because the adaptation process uses extra forward and backward steps during the test phase. While the parameters between the two models are similar because the newly included adaptive blocks are much smaller in size compared to the original model. Table 5. Resource comparisons during testing. Here inc. and exc. columns in ITTA indicate to include and exclude the TTA phase. Model Flops (G) Params (M) Time (s) Baseline 1.82 11.18 0.004 ITTA (inc.| exc.) 6.12 | 1.83 14.95 | 14.94 0.021 | 0.005 8. Grad-CAM Visualizations of Different Self- Supervised Objectives In Section 5 of the manuscript, we provide Grad-CAM [57] visualizations of our learnable consistency and the main losses to illustrate their alignment. To further show the differences between several TTT tasks [60, 63], we present more visual examples in this section. Results are shown in Figure 5. We observe that the entropy minimization [63] and rotation estimation [60] objectives do not activate the same regions as the main loss. As shown in the first row, for the class label of giraffe, both the main loss and our learned loss can correctly locate the two giraffes in the image, while the rotation estimation task can only locate one target, the same observation can be found when the learned weightsare disabled in our loss term. Meanwhile, although the two objects can be found for the entropy minimization task, the corresponding hot region does not align with that of the main loss. Similar phenomena can be observed in other samples. These visual examples demonstrate that our learned objective can better align with the main task than the TTT tasks adopted in previous works [60, 63], explaining why using the proposed learnable consistency loss can better improve TTT. 9. Parameter Analysis In this section, we analyze the hyper-parameter used in ITTA. We use the weight parameterα to balance the contri- butions from the main loss and weighted consistency loss (i.e. Lmain + αLwcont in Eq. (2) of our manuscript). To analyze the sensitivity of ITTA regarding different values of α, we conduct ablation studies in the PACS benchmark [37]. Results are listed in Table 6. We observe that the proposed ITTA can obtain favorable performances when α is in the range of 0.1 to 10, and it performs the best on average when setting as 1. We thus fix the parameter as 1 in all experi- ments. 10. A Different Augmentation Skill for ITTA In our manuscript, we use the existing augmentation strat- egy from [75] to obtain the augmented feature. In this sec- tion, we replace this implementation with that from [41] to further verify if our ITTA can still thrive with another aug- mentation skill. Different from [75] that mixes the statics of the feature to synthesize new information, [41] uses an affine transformation to create new features, where the weight for the transformation is sampled from a normal distribution with the mean value of one and standard value of zero, and the bias for the transformation is sampled from a normal distribution with the mean and standard values both zero. Experiments are conducted on the PACS benchmark [37] with the leave-one-out strategy. We compare ITTA with several different variants. (1) Ours w/o fw & TTT: this variant is the baseline model which uses the naive consistency loss for training and does not include TTT during the test phase. (2) Ours w/o fw: we disable the fw in our consistency loss, which uses the naive consistency loss for the test-time updating. (3) Ours w/o TTT: we do not update any parameters during the test phase. This variant is used to verify whether TTT can improve the pretrained model when replacing the augmentation strategy. We also compare these variants with the ERM method to show their effectivenesses. Results are listed in Table 7. We observe that ERM per- forms favorably against the baseline model, indicating that this augmentation strategy may not be beneficial for the training process. Meanwhile, we observe that when fw is disabled, the performances seem to decrease in 3 out of 4 target domains, and the average accuracy is also inferior to the baseline (i.e. Ours w/o fw & TTT). This result is in line with the finding in [46] that an inappropriate TTT task may deteriorate the performance. In comparison, we note that the performances are both improved when fw is enabled (i.e. Ours w/o TTT and Ours), which once again demonstrates that the proposed learnable consistency loss can improve the trained model. Moreover, we can also observe that when combining fw and TTT, our model is superior to other vari- ants and the ERM method. These results demonstrate that the proposed two strategies can improve the current TTT framework despite a less effective augmentation strategy. 11. Different Updating Steps or Strategies for ITTA In the manuscript, we use one TTT step for ITTA before during the testing step. In this section, we conduct experi- ments to evaluate the performances of ITTA with different TTT steps. Experiments are conducted on the PACS bench- mark [37] with the leave-one-out strategy, and each target domain is examined with 60 sets of random seeds and hyper- parameter settings. Results are listed in Table 8. We observe that the average accuracies of using more TTT steps are not improved greatly while the computational times are propor- tional to the TTT steps. To this end, we use one TTT step for ITTA as a compromise between accuracy and efficiency. We use the online setting from TTT [60] for all arts, which assumes test samples arrive sequentially and updates the adaptive blocks based on the states optimized from a previous sample. In this section, we also test ITTA in an episodic manner (i.e. Epi) [12]. Results in Table 8 suggest that while the episodic updating strategy performs slightly worse than the current scheme, and it still outperforms the baseline. 12. Different Network Structures for the Learnable Consistency Loss and Adaptive Parameters In our implementation, we use 10 layers of learnable pa- rameters for fw, and we use 5 layers of learnable parameters for fΘ after each block. In this section, we evaluate our ITTA with different network structures for these two mod- ules. Specifically, we compare the original implementation with the variants that use 1, 5, and 15 layers for fw and 1, 10, and 15 layers for fΘ to evaluate the performances of dif- ferent structures. Similarly, we conduct experiments on the PACS benchmark [37] with the leave-one-out strategy, and each target domain is examined with 60 sets of random seeds and hyper-parameter settings. Evaluation results are listed in Table 9. We observe that their differences in the average accuracy are rather subtle on account of the variances. To(a) Input (b) Entropy (c) Rotation (d) Ours w/o fw (e) Ours (f) Main Figure 5. Grad-CAM [57] visualizations from different loss terms. We use images with varying class labels (i.e. giraffe, elephant, house, and horse from top to bottom) from the four target domains of PACS [37] as inputs (i.e. art, cartoon, photo, and sketch domains from top to bottom). “Entropy” and “Rotation” here denote the entropy minimization and rotation estimation tasks in [63] and [60]. Ours w/o fw is the learnable consistency loss in Eq. (1) in the manuscript (i.e. ∥fw(z − z′)∥) when fw is disabled. The proposed learnable consistency loss can align well with the main classification task. Table 6. Sensitivity analysis of ITTA regarding different values ofα in the unseen domain from PACS [37]. The reported accuracies (%) and standard deviations are computed from 60 trials in each target domain. Values Target domain Avg.Art Cartoon Photo Sketch α = 0.1 83.9 ± 0.7 76.2 ± 1.1 94.8 ± 0.2 78.8 ± 0.8 83.4 ± 0.2 α = 1 (Ours) 84.7 ± 0.4 78.0 ± 0.4 94.5 ± 0.4 78.2 ± 0.3 83.8 ± 0.3 α = 10 83.9 ± 0.5 77.4 ± 0.6 94.2 ± 0.7 77.3 ± 0.8 83.2 ± 0.3 α = 100 81.5 ± 1.2 77.0 ± 0.6 92.6 ± 0.7 78.9 ± 2.1 82.5 ± 0.9 this end, we use the original implementation with 10 layers of learnable parameters for fw and 5 layers of learnable pa- rameters for fΘ, which performs relatively better than other variants. Since the adaptive blocks fΘ are attached after each layer of the network, one may wonder how the varying locations of the adaptive blocks affect the performance of ITTA. To answer this question, we further conduct experiments by adding the adaptive blocks after different layers of the orig- inal network. Denoting as Loc = lan given the n layers in the original network, we note that the model performs less effectively when the adaptive block is placed after the 1st layer of the network, and using all four adaptive blocks (i.e. ours) is more effective than other alternatives. 13. Comparisons with Other Related Methods Apart from the proposed ITTA, some other works also propose to include learnable parameters in their auxiliaryTable 7. Performances of our method with another augmentation strategy from [41] in the unseen domain from PACS [37]. The reported accuracies (%) and standard deviations are computed from 60 trials in each target domain. Model Target domain Avg.Art Cartoon Photo Sketch ERM 78.0 ± 1.3 73.4 ± 0.8 94.1 ± 0.4 73.6 ± 2.2 79.8 ± 0.4 Ours w/o fw & TTT 74.9 ± 0.4 74.1 ± 0.8 90.6 ± 0.3 79.7 ± 0.7 79.8 ± 0.4 Ours w/o fw 77.1 ± 1.0 73.6 ± 1.1 89.9 ± 0.4 78.4 ± 0.8 79.7 ± 0.2 Ours w/o TTT 77.5 ± 0.3 73.2 ± 0.6 92.4 ± 0.4 78.0 ± 1.0 80.3 ± 0.3 Ours (w/ fw & TTT) 79.2 ± 0.8 74.9 ± 1.1 92.2 ± 0.3 76.9 ± 0.7 80.8 ± 0.4 Table 8. Evaluations of ITTA in the unseen domain from PACS [37] with different TTT steps and updating strategies during the testing phase. The reported accuracies (%) and standard deviations are computed from 60 trials in each target domain. The time consumption (TC) is computed using one image with the size of 224 × 224. Epi. denotes updating ITTA in an episodic manner. Steps Target domain Avg. TCArt Cartoon Photo Sketch 1 step (Ours) 84.7 ± 0.4 78.0 ± 0.4 94.5 ± 0.4 78.2 ± 0.3 83.8 ± 0.3 2.4 ms 2 step 84.2 ± 0.9 77.5 ± 0.6 94.4 ± 0.4 79.1 ± 1.0 83.8 ± 0.1 4.2 ms 3 step 84.5 ± 1.2 77.6 ± 0.6 94.0 ± 0.6 79.3 ± 0.1 83.9 ± 0.3 6.1 ms Epi. 83.6 ± 0.7 77.9 ± 0.5 95.2 ± 0.1 76.6 ± 0.5 83.3 ± 0.4 losses. Examples include MetaReg [2] and Feature-Critic [45] which both suggest using meta-learning to produce more general models. The main difference between these arts and ITTA is that parameters in the auxiliary loss from [2,45] are gradually refined by episode training, and they are updated via a gradient alignment step in ITTA (see Sec. 3.1 in the manuscript), which is much simpler. In this sec- tion, we compare ITTA with these two arts in the PACS dataset [37] using the same settings aforementioned. Be- cause MetaReg [2] does not release codes, we thus directly cite the data from their paper in the comparison. Different from others, the results in [2] are averaged by 5 trials accord- ing to their paper, which is much less than our experimental settings. Meanwhile, we also compare with TTT++ [46] which suggests storing the momentum of the features from the source domain and enforcing the similarity between mo- mentums of features from the source and target domains. We use the same setting in Section 5.1 from the manuscript to evaluate TTT++. Results are listed in Table 10. We observe that our method consistently outperforms that from [2,45,46] for both the cases with and without TTT, indicating that the proposed learnable consistency loss and updating method is not only simpler but also more effective than the losses in [2, 45]. 14. Detailed Results in the DomainBed Bench- mark [27] this section presents the average accuracy in each domain from different datasets. As shown in Table 11, 12, 13, 14, and 15, these results are detailed illustrations of the results in Table 2 in our manuscript. For all the experiments, we use the “training-domain validate set” as the model selection method. A total of 22 methods are examined for 60 trials in each unseen domain, and all methods are trained with the leave-one-out strategy using the ResNet18 [30] backbones.Table 9. Performances of our method with different network structures for the consistency loss (i.e. fw) and adaptive parameters (i.e. fΘ) in the unseen domain from PACS [37]. Here ‘Loc=lan’ locates the adaptive block after the n-th layer of the model (‘la4’ is the last layer). The reported accuracies (%) and standard deviations are computed from 60 trials in each target domain. Structures Target domain Avg.Art Cartoon Photo Sketch Structures offw 1 layer 83.5 ±1.2 76.0 ±1.0 95.3 ±0.2 78.7 ±1.5 83.4 ±0.4 5 layers 83.7 ±0.6 76.8 ±0.9 94.6 ±0.3 78.8 ±0.3 83.5 ±0.3 10 layers (Ours) 84.7 ±0.4 78.0 ±0.4 94.5 ±0.4 78.2 ±0.3 83.8 ±0.3 15 layers 84.1 ±0.4 75.8 ±0.2 94.3 ±0.3 79.5 ±0.4 83.4 ±0.2 Structures offΘ 1 layer 84.0 ±0.6 77.4 ±0.5 94.4 ±0.5 78.3 ±0.4 83.5 ±0.3 5 layers (Ours) 84.7 ±0.4 78.0 ±0.4 94.5 ±0.4 78.2 ±0.3 83.8 ±0.3 10 layers 84.8 ±0.3 76.0 ±0.6 94.1 ±0.5 78.3 ±0.1 83.3 ±0.3 15 layers 83.9 ±0.8 76.0 ±0.5 93.8 ±0.4 78.7 ±1.4 83.1 ±0.6 Locations offΘ Loc=la1 83.4±0.7 76.8 ±0.3 94.4 ±0.3 77.8 ±0.3 83.1 ±0.3 Loc=la2 83.4±0.6 77.7 ±0.6 94.2 ±0.5 78.0 ±0.5 83.3 ±0.3 Loc=la3 84.0±0.4 77.5 ±0.3 94.4 ±0.1 77.8 ±0.1 83.4 ±0.2 Loc=la4 84.1±0.7 77.8 ±0.5 94.8 ±0.2 76.9 ±1.5 83.4 ±0.4 Table 10. Compare with learnable losses in [2, 45] in the unseen domain from PACS [37]. The reported accuracies ( %) and standard deviations are computed from 60 trials in each target domain except for [2] where the numbers are directly cited from their paper. Model Target domain Avg.Art Cartoon Photo Sketch MetaReg [2] 83.7 ± 0.2 77.2 ± 0.3 95.5 ± 0.2 70.3 ± 0.3 81.7 Feture-Critic [45] 78.4 ± 1.6 75.4 ± 1.2 92.6 ± 0.5 73.3 ± 1.4 80.0 ± 0.3 TTT++ [46] 84.3 ± 0.1 78.4 ± 0.5 93.8 ± 1.3 73.2 ± 3.2 82.4 ± 1.1 Ours w/o TTT 83.3 ± 0.5 76.0 ± 0.5 94.4 ± 0.5 76.7 ± 1.4 82.8 ± 0.3 Ours 84.7 ± 0.4 78.0 ± 0.4 94.5 ± 0.4 78.2 ± 0.3 83.8 ± 0.3 Table 11. Average accuracies on the PACS [37] datasets using the default hyper-parameter settings in DomainBed [27]. art cartoon photo sketch Average ERM [61] 78.0 ± 1.3 73.4 ± 0.8 94.1 ± 0.4 73.6 ± 2.2 79.8 ± 0.4 IRM [1] 76.9 ± 2.6 75.1 ± 0.7 94.3 ± 0.4 77.4 ± 0.4 80.9 ± 0.5 GroupGRO [55] 77.7 ± 2.6 76.4 ± 0.3 94.0 ± 0.3 74.8 ± 1.3 80.7 ± 0.4 Mixup [68] 79.3 ± 1.1 74.2 ± 0.3 94.9 ± 0.3 68.3 ± 2.7 79.2 ± 0.9 MLDG [38] 78.4 ± 0.7 75.1 ± 0.5 94.8 ± 0.4 76.7 ± 0.8 81.3 ± 0.2 CORAL [59] 81.5 ± 0.5 75.4 ± 0.7 95.2 ± 0.5 74.8 ± 0.4 81.7 ± 0.0 MMD [40] 81.3 ± 0.6 75.5 ± 1.0 94.0 ± 0.5 74.3 ± 1.5 81.3 ± 0.8 DANN [23] 79.0 ± 0.6 72.5 ± 0.7 94.4 ± 0.5 70.8 ± 3.0 79.2 ± 0.3 CDANN [44] 80.4 ± 0.8 73.7 ± 0.3 93.1 ± 0.6 74.2 ± 1.7 80.3 ± 0.5 MTL [6] 78.7 ± 0.6 73.4 ± 1.0 94.1 ± 0.6 74.4 ± 3.0 80.1 ± 0.8 SagNet [48] 82.9 ± 0.4 73.2 ± 1.1 94.6 ± 0.5 76.1 ± 1.8 81.7 ± 0.6 ARM [72] 79.4 ± 0.6 75.0 ± 0.7 94.3 ± 0.6 73.8 ± 0.6 80.6 ± 0.5 VREx [36] 74.4 ± 0.7 75.0 ± 0.4 93.3 ± 0.3 78.1 ± 0.9 80.2 ± 0.5 RSC [33] 78.5 ± 1.1 73.3 ± 0.9 93.6 ± 0.6 76.5 ± 1.4 80.5 ± 0.2 SelfReg [34] 82.5 ± 0.8 74.4 ± 1.5 95.4 ± 0.5 74.9 ± 1.3 81.8 ± 0.3 MixStyle [75] 82.6 ± 1.2 76.3 ± 0.4 94.2 ± 0.3 77.5 ± 1.3 82.6 ± 0.4 Fish [58] 80.9 ± 1.0 75.9 ± 0.4 95.0 ± 0.4 76.2 ± 1.0 82.0 ± 0.3 SD [51] 83.2 ± 0.6 74.6 ± 0.3 94.6 ± 0.1 75.1 ± 1.6 81.9 ± 0.3 CAD [53] 83.9 ± 0.8 74.2 ± 0.4 94.6 ± 0.4 75.0 ± 1.2 81.9 ± 0.3 CondCAD [53] 79.7 ± 1.0 74.2 ± 0.9 94.6 ± 0.4 74.8 ± 1.4 80.8 ± 0.5 Fishr [52] 81.2 ± 0.4 75.8 ± 0.8 94.3 ± 0.3 73.8 ± 0.6 81.3 ± 0.3 Ours 84.7 ± 0.4 78.0 ± 0.4 94.5 ± 0.4 78.2 ± 0.3 83.8 ± 0.3Table 12. Average accuracies on the VLCS [18] datasets using the default hyper-parameter settings in DomainBed [27]. Caltech LabelMe Sun VOC Average ERM [61] 97.7 ± 0.3 62.1 ± 0.9 70.3 ± 0.9 73.2 ± 0.7 75.8 ± 0.2 IRM [1] 96.1 ± 0.8 62.5 ± 0.3 69.9 ± 0.7 72.0 ± 1.4 75.1 ± 0.1 GroupGRO [55] 96.7 ± 0.6 61.7 ± 1.5 70.2 ± 1.8 72.9 ± 0.6 75.4 ± 1.0 Mixup [68] 95.6 ± 1.5 62.7 ± 0.4 71.3 ± 0.3 75.4 ± 0.2 76.2 ± 0.3 MLDG [38] 95.8 ± 0.5 63.3 ± 0.8 68.5 ± 0.5 73.1 ± 0.8 75.2 ± 0.3 CORAL [59] 96.5 ± 0.3 62.8 ± 0.1 69.1 ± 0.6 73.8 ± 1.0 75.5 ± 0.4 MMD [40] 96.0 ± 0.8 64.3 ± 0.6 68.5 ± 0.6 70.8 ± 0.1 74.9 ± 0.5 DANN [23] 97.2 ± 0.1 63.3 ± 0.6 70.2 ± 0.9 74.4 ± 0.2 76.3 ± 0.2 CDANN [44] 95.4 ± 1.2 62.6 ± 0.6 69.9 ± 1.3 76.2 ± 0.5 76.0 ± 0.5 MTL [6] 94.4 ± 2.3 65.0 ± 0.6 69.6 ± 0.6 71.7 ± 1.3 75.2 ± 0.3 SagNet [48] 94.9 ± 0.7 61.9 ± 0.7 69.6 ± 1.3 75.2 ± 0.6 75.4 ± 0.8 ARM [72] 96.9 ± 0.5 61.9 ± 0.4 71.6 ± 0.1 73.3 ± 0.4 75.9 ± 0.3 VREx [36] 96.2 ± 0.0 62.5 ± 1.3 69.3 ± 0.9 73.1 ± 1.2 75.3 ± 0.6 RSC [33] 96.2 ± 0.0 63.6 ± 1.3 69.8 ± 1.0 72.0 ± 0.4 75.4 ± 0.3 SelfReg [34] 95.8 ± 0.6 63.4 ± 1.1 71.1 ± 0.6 75.3 ± 0.6 76.4 ± 0.7 MixStyle [75] 97.3 ± 0.3 61.6 ± 0.1 70.4 ± 0.7 71.3 ± 1.9 75.2 ± 0.7 Fish [58] 97.4 ± 0.2 63.4 ± 0.1 71.5 ± 0.4 75.2 ± 0.7 76.9 ± 0.2 SD [51] 96.5 ± 0.4 62.2 ± 0.0 69.7 ± 0.9 73.6 ± 0.4 75.5 ± 0.4 CAD [53] 94.5 ± 0.9 63.5 ± 0.6 70.4 ± 1.2 72.4 ± 1.3 75.2 ± 0.6 CondCAD [53] 96.5 ± 0.8 62.6 ± 0.4 69.1 ± 0.2 76.0 ± 0.2 76.1 ± 0.3 Fishr [52] 97.2 ± 0.6 63.3 ± 0.7 70.4 ± 0.6 74.0 ± 0.8 76.2 ± 0.3 Ours 96.9 ± 1.2 63.7 ± 1.1 72.0 ± 0.3 74.9 ± 0.8 76.9 ± 0.6 Table 13. Average accuracies on the OfficeHome [62] datasets using the default hyper-parameter settings in DomainBed [27]. art clipart product real Average ERM [61] 52.2 ± 0.2 48.7 ± 0.5 69.9 ± 0.5 71.7 ± 0.5 60.6 ± 0.2 IRM [1] 49.7 ± 0.2 46.8 ± 0.5 67.5 ± 0.4 68.1 ± 0.6 58.0 ± 0.1 GroupGRO [55] 52.6 ± 1.1 48.2 ± 0.9 69.9 ± 0.4 71.5 ± 0.8 60.6 ± 0.3 Mixup [68] 54.0 ± 0.7 49.3 ± 0.7 70.7 ± 0.7 72.6 ± 0.3 61.7 ± 0.5 MLDG [38] 53.1 ± 0.3 48.4 ± 0.3 70.5 ± 0.7 71.7 ± 0.4 60.9 ± 0.2 CORAL [59] 55.1 ± 0.7 49.7 ± 0.9 71.8 ± 0.2 73.1 ± 0.5 62.4 ± 0.4 MMD [40] 50.9 ± 1.0 48.7 ± 0.3 69.3 ± 0.7 70.7 ± 1.3 59.9 ± 0.4 DANN [23] 51.8 ± 0.5 47.1 ± 0.1 69.1 ± 0.7 70.2 ± 0.7 59.5 ± 0.5 CDANN [44] 51.4 ± 0.5 46.9 ± 0.6 68.4 ± 0.5 70.4 ± 0.4 59.3 ± 0.4 MTL [6] 51.6 ± 1.5 47.7 ± 0.5 69.1 ± 0.3 71.0 ± 0.6 59.9 ± 0.5 SagNet [48] 55.3 ± 0.4 49.6 ± 0.2 72.1 ± 0.4 73.2 ± 0.4 62.5 ± 0.3 ARM [72] 51.3 ± 0.9 48.5 ± 0.4 68.0 ± 0.3 70.6 ± 0.1 59.6 ± 0.3 VREx [36] 51.1 ± 0.3 47.4 ± 0.6 69.0 ± 0.4 70.5 ± 0.4 59.5 ± 0.1 RSC [33] 49.0 ± 0.1 46.2 ± 1.5 67.8 ± 0.7 70.6 ± 0.3 58.4 ± 0.6 SelfReg [34] 55.1 ± 0.8 49.2 ± 0.6 72.2 ± 0.3 73.0 ± 0.3 62.4 ± 0.1 MixStyle [75] 50.8 ± 0.6 51.4 ± 1.1 67.6 ± 1.3 68.8 ± 0.5 59.6 ± 0.8 Fish [58] 54.6 ± 1.0 49.6 ± 1.0 71.3 ± 0.6 72.4 ± 0.2 62.0 ± 0.6 SD [51] 55.0 ± 0.4 51.3 ± 0.5 72.5 ± 0.2 72.7 ± 0.3 62.9 ± 0.2 CAD [53] 52.1 ± 0.6 48.3 ± 0.5 69.7 ± 0.3 71.9 ± 0.4 60.5 ± 0.3 CondCAD [53] 53.3 ± 0.6 48.4 ± 0.2 69.8 ± 0.9 72.6 ± 0.1 61.0 ± 0.4 Fishr [52] 52.6 ± 0.9 48.6 ± 0.3 69.9 ± 0.6 72.4 ± 0.4 60.9 ± 0.3 Ours 54.4 ± 0.2 52.3 ± 0.8 69.5 ± 0.3 71.7 ± 0.2 62.0 ± 0.2Table 14. Average accuracies on the TerraInc [4] datasets using the default hyper-parameter settings in DomainBed [27]. L100 L38 L43 L46 Average ERM [61] 42.1 ± 2.5 30.1 ± 1.2 48.9 ± 0.6 34.0 ± 1.1 38.8 ± 1.0 IRM [1] 41.8 ± 1.8 29.0 ± 3.6 49.6 ± 2.1 33.1 ± 1.5 38.4 ± 0.9 GroupGRO [55] 45.3 ± 4.6 36.1 ± 4.4 51.0 ± 0.8 33.7 ± 0.9 41.5 ± 2.0 Mixup [68] 49.4 ± 2.0 35.9 ± 1.8 53.0 ± 0.7 30.0 ± 0.9 42.1 ± 0.7 MLDG [38] 39.6 ± 2.3 33.2 ± 2.7 52.4 ± 0.5 35.1 ± 1.5 40.1 ± 0.9 CORAL [59] 46.7 ± 3.2 36.9 ± 4.3 49.5 ± 1.9 32.5 ± 0.7 41.4 ± 1.8 MMD [40] 49.1 ± 1.2 36.4 ± 4.8 50.4 ± 2.1 32.3 ± 1.5 42.0 ± 1.0 DANN [23] 44.3 ± 3.6 28.0 ± 1.5 47.9 ± 1.0 31.3 ± 0.6 37.9 ± 0.9 CDANN [44] 36.9 ± 6.4 32.7 ± 6.2 51.1 ± 1.3 33.5 ± 0.5 38.6 ± 2.3 MTL [6] 45.2 ± 2.6 31.0 ± 1.6 50.6 ± 1.1 34.9 ± 0.4 40.4 ± 1.0 SagNet [48] 36.3 ± 4.7 40.3 ± 2.0 52.5 ± 0.6 33.3 ± 1.3 40.6 ± 1.5 ARM [72] 41.5 ± 4.5 27.7 ± 2.4 50.9 ± 1.0 29.6 ± 1.5 37.4 ± 1.9 VREx [36] 48.0 ± 1.7 41.1 ± 1.5 51.8 ± 1.5 32.0 ± 1.2 43.2 ± 0.3 RSC [33] 42.8 ± 2.4 32.2 ± 3.8 49.6 ± 0.9 32.9 ± 1.2 39.4 ± 1.3 SelfReg [34] 46.1 ± 1.5 34.5 ± 1.6 49.8 ± 0.3 34.7 ± 1.5 41.3 ± 0.3 MixStyle [75] 50.6 ± 1.9 28.0 ± 4.5 52.1 ± 0.7 33.0 ± 0.2 40.9 ± 1.1 Fish [58] 46.3 ± 3.0 29.0 ± 1.1 52.7 ± 1.2 32.8 ± 1.0 40.2 ± 0.6 SD [51] 45.5 ± 1.9 33.2 ± 3.1 52.9 ± 0.7 36.4 ± 0.8 42.0 ± 1.0 CAD [53] 43.1 ± 2.6 31.1 ± 1.9 53.1 ± 1.6 34.7 ± 1.3 40.5 ± 0.4 CondCAD [53] 44.4 ± 2.9 32.9 ± 2.5 50.5 ± 1.3 30.8 ± 0.5 39.7 ± 0.4 Fishr [52] 49.9 ± 3.3 36.6 ± 0.9 49.8 ± 0.2 34.2 ± 1.3 42.6 ± 1.0 Ours 51.7 ± 2.4 37.6 ± 0.6 49.9 ± 0.6 33.6 ± 0.6 43.2 ± 0.5 Table 15. Average accuracies on the DomainNet [50] datasets using the default hyper-parameter settings in DomainBed [27]. clip info paint quick real sketch Average ERM [61] 50.4 ± 0.2 14.0 ± 0.2 40.3 ± 0.5 11.7 ± 0.2 52.0 ± 0.2 43.2 ± 0.3 35.3 ± 0.1 IRM [1] 43.2 ± 0.9 12.6 ± 0.3 35.0 ± 1.4 9.9 ± 0.4 43.4 ± 3.0 38.4 ± 0.4 30.4 ± 1.0 GroupGRO [55] 38.2 ± 0.5 13.0 ± 0.3 28.7 ± 0.3 8.2 ± 0.1 43.4 ± 0.5 33.7 ± 0.0 27.5 ± 0.1 Mixup [68] 48.9 ± 0.3 13.6 ± 0.3 39.5 ± 0.5 10.9 ± 0.4 49.9 ± 0.2 41.2 ± 0.2 34.0 ± 0.0 MLDG [38] 51.1 ± 0.3 14.1 ± 0.3 40.7 ± 0.3 11.7 ± 0.1 52.3 ± 0.3 42.7 ± 0.2 35.4 ± 0.0 CORAL [59] 51.2 ± 0.2 15.4 ± 0.2 42.0 ± 0.2 12.7 ± 0.1 52.0 ± 0.3 43.4 ± 0.0 36.1 ± 0.2 MMD [40] 16.6 ± 13.3 0.3 ± 0.0 12.8 ± 10.4 0.3 ± 0.0 17.1 ± 13.7 0.4 ± 0.0 7.9 ± 6.2 DANN [23] 45.0 ± 0.2 12.8 ± 0.2 36.0 ± 0.2 10.4 ± 0.3 46.7 ± 0.3 38.0 ± 0.3 31.5 ± 0.1 CDANN [44] 45.3 ± 0.2 12.6 ± 0.2 36.6 ± 0.2 10.3 ± 0.4 47.5 ± 0.1 38.9 ± 0.4 31.8 ± 0.2 MTL [6] 50.6 ± 0.2 14.0 ± 0.4 39.6 ± 0.3 12.0 ± 0.3 52.1 ± 0.1 41.5 ± 0.0 35.0 ± 0.0 SagNet [48] 51.0 ± 0.1 14.6 ± 0.1 40.2 ± 0.2 12.1 ± 0.2 51.5 ± 0.3 42.4 ± 0.1 35.3 ± 0.1 ARM [72] 43.0 ± 0.2 11.7 ± 0.2 34.6 ± 0.1 9.8 ± 0.4 43.2 ± 0.3 37.0 ± 0.3 29.9 ± 0.1 VREx [36] 39.2 ± 1.6 11.9 ± 0.4 31.2 ± 1.3 10.2 ± 0.4 41.5 ± 1.8 34.8 ± 0.8 28.1 ± 1.0 RSC [33] 39.5 ± 3.7 11.4 ± 0.8 30.5 ± 3.1 10.2 ± 0.8 41.0 ± 1.4 34.7 ± 2.6 27.9 ± 2.0 SelfReg [34] 47.9 ± 0.3 15.1 ± 0.3 41.2 ± 0.2 11.7 ± 0.3 48.8 ± 0.0 43.8 ± 0.3 34.7 ± 0.2 MixStyle [75] 49.1 ± 0.4 13.4 ± 0.0 39.3 ± 0.0 11.4 ± 0.4 47.7 ± 0.3 42.7 ± 0.1 33.9 ± 0.1 Fish [58] 51.5 ± 0.3 14.5 ± 0.2 40.4 ± 0.3 11.7 ± 0.5 52.6 ± 0.2 42.1 ± 0.1 35.5 ± 0.0 SD [51] 51.3 ± 0.3 15.5 ± 0.1 41.5 ± 0.3 12.6 ± 0.2 52.9 ± 0.2 44.0 ± 0.4 36.3 ± 0.2 CAD [53] 45.4 ± 1.0 12.1 ± 0.5 34.9 ± 1.1 10.2 ± 0.6 45.1 ± 1.6 38.5 ± 0.6 31.0 ± 0.8 CondCAD [53] 46.1 ± 1.0 13.3 ± 0.4 36.1 ± 1.4 10.7 ± 0.2 46.8 ± 1.3 38.7 ± 0.7 31.9 ± 0.7 Fishr [52] 47.8 ± 0.7 14.6 ± 0.2 40.0 ± 0.3 11.9 ± 0.2 49.2 ± 0.7 41.7 ± 0.1 34.2 ± 0.3 Ours 50.7 ± 0.7 13.9 ± 0.4 39.4 ± 0.5 11.9 ± 0.2 50.2 ± 0.3 43.5 ± 0.1 34.9 ± 0.1",
      "meta_data": {
        "arxiv_id": "2304.04494v2",
        "authors": [
          "Liang Chen",
          "Yong Zhang",
          "Yibing Song",
          "Ying Shan",
          "Lingqiao Liu"
        ],
        "published_date": "2023-04-10T10:12:38Z",
        "pdf_url": "https://arxiv.org/pdf/2304.04494v2.pdf"
      },
      "llm_extracted_info": {
        "main_contributions": "The paper addresses the domain generalization (DG) problem by improving test-time training (TTT) strategies, specifically tackling the challenges of selecting an appropriate auxiliary TTT task and identifying reliable parameters to update. The Improved Test-Time Adaptation (ITTA) method proposes a learnable consistency loss for the TTT task, which contains adjustable parameters for better alignment with the main prediction task. Additionally, it introduces new adaptive parameters to the trained model, suggesting that only these adaptive parameters should be updated during the test phase. Extensive experiments demonstrate that ITTA achieves superior performance compared to state-of-the-art methods on various DG benchmarks for both multi-source and single-source DG tasks, validating the effectiveness of the proposed strategies.",
        "methodology": "The ITTA method improves test-time training for domain generalization through two main components. First, it introduces a **learnable consistency loss (Lwcont)** for the TTT task. Unlike heuristically defined objectives, Lwcont is made learnable by augmenting it with a weight subnetwork (fw), which measures consistency between original and augmented representations (z, z') from a feature extractor fθ(x). The training objective for the feature extractor (fθ) and classifier (fϕ) combines the main cross-entropy loss (Lmain) with Lwcont, weighted by α. To ensure alignment between Lmain and Lwcont, the weight subnetwork (fw) is updated by minimizing the L2 norm of the difference between the normalized gradients of Lmain and Lwcont with respect to fθ's parameters. Second, the method introduces **additional adaptive parameters (fΘ)** as new blocks placed after each block of the pretrained feature extractor (fθ). During the test-time adaptation phase, only these new adaptive parameters (fΘ) are updated by minimizing the learned consistency loss, while the original model parameters (fθ, fϕ) remain fixed. The training process for fθ, fϕ, and fw, and the test-time adaptation process for fΘ are performed in an alternative and online manner, respectively.",
        "experimental_setup": "ITTA was evaluated on five benchmark datasets: PACS, VLCS, OfficeHome, TerraInc, and DomainNet. The model used an ImageNet-pretrained ResNet18 backbone with 4 blocks as the feature extractor (fθ). Four blocks of additional adaptive parameters (fΘ) were included, each implemented with 5 layers of learnable parameters. The weight subnetwork (fw) consisted of 10 layers of learnable parameters. The classifier (fϕ) was an MLP layer provided by the DomainBed benchmark. The weight parameter (α) for balancing loss terms was set to 1. Experiments followed the rigorous evaluation protocol of DomainBed, conducting 60 trials for each unseen domain, with 5,000 iteration steps per trial. Model selection was based on the 'training-domain validate set' method, picking the best-performing model on validation samples. Multi-source generalization used a leave-one-out strategy, while single-source generalization focused on PACS, training on one domain and testing on the other three. The method was compared against 21 existing domain generalization methods (e.g., ERM, MMD, MixStyle) and three TTT-based methods (TTT, MT3, TENT). Ablation studies were conducted to analyze the effectiveness of the learnable consistency loss (vs. naive consistency, entropy, rotation tasks), adaptive parameters (vs. no TTT, updating all, updating BN layers), α sensitivity, different augmentation skills, varying TTT steps and updating strategies, and different network structures and locations for fw and fΘ.",
        "limitations": "The proposed learnable consistency loss introduces a computational burden. Specifically, updating the weight subnetwork (fw) requires computing second-order derivatives, which entails an additional forward pass and three backward passes compared to a simpler approach without fw (which requires one forward and one backward pass). This adds extra computational cost to the system.",
        "future_research_directions": "Future research efforts will focus on simplifying the overall optimization process to reduce the computational cost associated with ITTA, particularly the overhead introduced by updating the learnable weight subnetwork."
      }
    },
    {
      "title": "Persistent Test-time Adaptation in Recurring Testing Scenarios",
      "abstract": "Current test-time adaptation (TTA) approaches aim to adapt a machine learning\nmodel to environments that change continuously. Yet, it is unclear whether TTA\nmethods can maintain their adaptability over prolonged periods. To answer this\nquestion, we introduce a diagnostic setting - recurring TTA where environments\nnot only change but also recur over time, creating an extensive data stream.\nThis setting allows us to examine the error accumulation of TTA models, in the\nmost basic scenario, when they are regularly exposed to previous testing\nenvironments. Furthermore, we simulate a TTA process on a simple yet\nrepresentative $\\epsilon$-perturbed Gaussian Mixture Model Classifier, deriving\ntheoretical insights into the dataset- and algorithm-dependent factors\ncontributing to gradual performance degradation. Our investigation leads us to\npropose persistent TTA (PeTTA), which senses when the model is diverging\ntowards collapse and adjusts the adaptation strategy, striking a balance\nbetween the dual objectives of adaptation and model collapse prevention. The\nsupreme stability of PeTTA over existing approaches, in the face of lifelong\nTTA scenarios, has been demonstrated over comprehensive experiments on various\nbenchmarks. Our project page is available at https://hthieu166.github.io/petta.",
      "full_text": "Persistent Test-time Adaptation in Recurring Testing Scenarios Trung-Hieu Hoang1 Duc Minh Vo2 Minh N. Do1,3 1Department of Electrical and Computer Engineering, University of Illinois at Urbana-Champaign 2The University of Tokyo 3VinUni-Illinois Smart Health Center, VinUniversity {hthieu, minhdo}@illinois.edu vmduc@nlab.ci.i.u-tokyo.ac.jp Abstract Current test-time adaptation (TTA) approaches aim to adapt a machine learn- ing model to environments that change continuously. Yet, it is unclear whether TTA methods can maintain their adaptability over prolonged periods. To answer this question, we introduce a diagnostic setting - recurring TTA where envi- ronments not only change but also recur over time, creating an extensive data stream. This setting allows us to examine the error accumulation of TTA models, in the most basic scenario, when they are regularly exposed to previous testing environments. Furthermore, we simulate a TTA process on a simple yet repre- sentative ϵ-perturbed Gaussian Mixture Model Classifier, deriving theoretical insights into the dataset- and algorithm-dependent factors contributing to gradual performance degradation. Our investigation leads us to propose persistent TTA (PeTTA), which senses when the model is diverging towards collapse and adjusts the adaptation strategy, striking a balance between the dual objectives of adaptation and model collapse prevention. The supreme stability of PeTTA over existing approaches, in the face of lifelong TTA scenarios, has been demonstrated over comprehensive experiments on various benchmarks. Our project page is available at https://hthieu166.github.io/petta. 1 Introduction Machine learning (ML) models have demonstrated significant achievements in various areas [18, 38, 47, 23]. Still, they are inherently susceptible to distribution-shift [46, 13, 48, 21, 6] (also known as the divergence between the training and testing environments), leading to a significant degradation in model performance. The ability to deviate from the conventional testing setting appears as a crucial aspect in boosting ML models’ adaptability when confronted with a new testing environment that has been investigated [ 30, 53, 14]. Among common domain generalization methods [ 58, 24, 1], test-time adaptation (TTA) takes the most challenging yet rewarding path that leverages unlabeled data available at test time for self-supervised adaptation prior to the final inference [57, 39, 8, 41, 59]. Early TTA studies have concentrated on a simply ideal adaptation scenario where the test samples come from a fixed single domain [57, 39, 41]. As a result, such an assumption is far from the ever- changing and complex testing environments. To confront continually changing environments [59, 12], Yuan et al. [61] proposed a practical TTA scenario where distribution changing and correlative sampling occur [15] simultaneously. Though practical TTA is more realistic than what the previous assumptions have made, it still assumes that any environment only appears once in the data stream, a condition which does not hold true. Taking a surveillance camera as an example, it might accom- modate varying lighting conditions recurringly day after day (Fig. 1-left). Based on this reality, we hypothesize that the recurring of those conditions may reveal the error accumulation phenomenon in TTA, resulting in performance degradation over a long period. To verify our hypothesis, we simulate a 38th Conference on Neural Information Processing Systems (NeurIPS 2024). arXiv:2311.18193v4  [cs.CV]  2 Nov 2024Testing Error Time Day 1 Illumination Condition Day 2 Day 3 0 50 100 150 200 250 300 0 0.2 0.4 0.6 0.8 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 201 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 Test-time adaptation step Testing Error No TTA RoTTA PeTTA (ours) Figure 1: Recurring Test-time Adaption (TTA). (left) Testing environments may change recurringly and preserving adaptability when visiting the same testing condition is not guaranteed. (right) The testing error of RoTTA [61] progressively raises (performance degradation) and exceeds the error of the source model (no TTA) while our PeTTA demonstrates its stability when adapting to the test set of CIFAR-10-C [19] 20 times. The bold lines denote the running mean and the shaded lines in the background represent the testing error on each domain (excluding the source model, for clarity). recurring testing environment and observe the increasing error rate by recurringly adapting to the test set of CIFAR-10-C [19] multiple times. We showcase the testing error of RoTTA [61] after 20 cycles of adaptation in Fig. 1-right. As expected, RoTTA can successfully adapt and deliver encouraging outcomes within the first few passes. However, this advantage is short-lived as our study uncovers a significant issue: TTA approaches in this setting may experience severe and persistent degradation in performance. Consequently, the testing error of RoTTA gradually escalates over time and quickly surpasses the model without adaptation. This result confirms the risk of TTA deployment in our illustrative scenario, as an algorithm might work well in the first place and gradually degenerate. Therefore, ensuring sustainable quality is crucial for real-world applications, especially given the recurring nature of testing environments. This study examines whether the adaptability of a TTA algorithm persists over an extended testing stream. Specifically, in the most basic scenario, where the model returns to a previously encountered testing environment after undergoing various adjustments. We thus propose a more general testing scenario than the practical TTA [61], namely recurring TTA, where the environments not only change gradually but also recur in a correlated manner over time. We first analyze a simulation using the ϵ−perturbed Gaussian Mixture Model Classifier (ϵ−GMMC) on a synthesized dataset and derive a theoretical analysis to confirm our findings, offering insights to tackle similar issues in deep neural networks. The analysis provides hints for reasoning the success of many recent robust continual TTA approaches [61, 12, 59, 15] and leading us to propose a simple yet effective baseline to avoid performance degradation, namely Persistent TTA (PeTTA). PeTTA continuously monitors the chance of collapsing and adjusts the adaptation strategy on the fly, striking a balance between the two objectives: adaptation and collapse prevention. Our contributions can be summarized as follows: • First, this work proposes a testing scenario - recurring TTA, a simple yet sufficient setup for diagnosing the overlooked gradual performance degradation phenomenon of TTA. • Second, we formally define the phenomenon of TTA collapsing and undertake a theoretical analysis on an ϵ-GMMC, shedding light on dataset-dependent and algorithm-dependent factors that contribute to the error accumulation during TTA processes. • Third, we introduce persistent TTA (PeTTA)- a simple yet effective adaptation scheme that surpasses all baseline models and demonstrates a persisting performance. For more context on related work, readers are directed to visit our discussions in Appdx. A. 2 Background Test-time Adaptation (TTA). A TTA algorithm operates on an ML classifier ft : X → Ywith parameter θt ∈ Θ (parameter space) gradually changing over time (t ∈ T) that maps an input image x ∈ Xto a category (label) y ∈ Y. Let the capital letters (Xt, Yt) ∈ X × Ydenote a pair of random variables with the joint distribution Pt(x, y) ∈ Pd, t∈ T. Here, Pd belongs to collection of D sets of testing scenarios (domains) {Pd}D d=1. The covariate shift [46] is assumed: Pt(x) and Pt′(x) 2could be different but Pt(y|x) = Pt′(y|x) holds ∀t ̸= t′. At t = 0, θ0 is initialized by a supervised model trained on P0 ∈ P0 (source dataset). The model then explores an online stream of testing data. For each t >0, it receives Xt (typically in form of a batch of Nt testing samples) for adapting itself ft−1 → ft before making the final prediction ft (Xt). TTA with Mean Teacher Update. To achieve a stable optimization process, the main (teacher) model ft are updated indirectly through a student model with parameters θ′ t [57, 61, 12, 15, 55]. At first, the teacher model in the previous step introduces a pseudo label [28] ˆYt for each Xt: ˆYt = ft−1(Xt). (1) With a classification loss LCLS (e.g., cross-entropy [16]), and a model parameters regularizer R, the student model is first updated with a generic optimization operatorOptim, followed by an exponential moving average (EMA) update of the teacher model parameter θt−1: θ′ t = Optim θ′∈Θ EPt h LCLS \u0010 ˆYt, Xt; θ′ \u0011i + λR(θ′), (2) θt = (1 − α)θt−1 + αθ′ t, (3) with α ∈ (0, 1) - the update rate of EMA, andλ ∈ R+ - the weighting coefficient of the regularization term, are the two hyper-parameters. Practical TTA. In practical TTA [61], two characteristics of the aforementioned distribution of data stream are noticeable. Firstly, Pt’s can be partitioned by td’s in which {Pt}td t=td−1 ⊂ Pd. Here, each partition of consecutive steps follows the same underlying distribution which will change continually through D domains [59] (P1 → P2 ··· → PD). Secondly, the category distribution in each testing batch is temporally correlated [15]. This means within a batch, a small subset of categories is dominant over others, making the marginal distribution Pt(y) = 0, ∀y ̸∈ Yt ⊂ Yeven though the category distribution over all batches are balanced. Optimizing under this low intra-batch diversity (|Yt| ≪ |Y|) situation can slowly degenerate the model [7]. 3 Recurring TTA and Theoretical Analysis This section conducts a theoretical analysis on a concrete failure case of a simple TTA model. The results presented at the end of Sec. 3.2 will elucidate the factors contributing to the collapse (Sec. 3.1), explaining existing good practices (Sec. 3.3) and give insights into potential solutions (Sec. 4). 3.1 Recurring TTA and Model Collapse Recurring TTA.To study the gradual performance degradation (or model collapse), we propose anew testing scenario based on practical TTA [61]. Conducting a single pass through D distributions, as done in earlier studies [61, 59], may not effectively identify the degradation. To promote consistency, our recurring TTA performs revisiting the previous distributions K times to compare the incremental error versus the previous visits. For example, a sequence with K = 2 could be P1 → P2 → ··· → PD → P1 → P2 → ··· → PD. Appdx. D extends our justifications on constructing recurring TTA. Definition 1 (Model Collapse). A model is said to be collapsed from step τ ∈ T, τ <∞ if there exists a non-empty subset of categories ˜Y ⊂ Ysuch that Pr{Yt ∈ ˜Y} > 0 but the marginal Pr{ˆYt ∈ ˜Y} converges to zero in probability: lim t→τ Pr{ˆYt ∈ ˜Y} = 0. Here, upon collapsing, a model tends to ignore almost categories in ˜Y. As it is irrecoverable once collapsed, the only remedy would be resetting all parameters back to θ0. 3.2 Simulation of Failure and Theoretical Analysis Collapsing behavior varies across datasets and the adaptation processes. Formally studying this phenomenon on a particular real dataset and a TTA algorithm is challenging. Therefore, we propose a theoretical analysis on ϵ-perturbed binary Gaussian Mixture Model Classifier (ϵ-GMMC) that shares the typical characteristics by construction and demonstrates the same collapsing pattern in action (Sec. 5.1) as observed on real continual TTA processes (Sec. 5.3). 3Pseudo-label Predictor ˆYt = argmax y∈Y Pr(Xt|y;θt−1) Xt Mean-teacher Update θ′ t = Optim θ′∈Θ EPt h LCLS \u0010ˆYt, Xt;θ′\u0011i θt = (1−α)θt−1 +αθ′ t ϵt ··· θt−1 θt ··· Figure 2: ϵ-perturbed binary Gaussian Mix- ture Model Classifier, imitating a continual TTA algorithm for theoretical analysis. Two main components include a pseudo-label predictor (Eq. 1), and a mean teacher up- date (Eqs. 2, 3). The predictor is perturbed for retaining a false negative rate of ϵt to simulate an undesirable TTA testing stream. Simulated Testing Stream. Observing a testing stream with (Xt, Yt) ∈ X × Y= R × {0, 1} and the underlying joint distribution Pt(x, y) = py,t · N(x; µy, σ2 y). The main task is predicting Xt was sampled from cluster 0 or 1 (negative or positive). Conveniently, let py,t ∆ = Pt(y) = Pr(Yt = y) and ˆpy,t ∆ = Pr( ˆYt = y) be the marginal distribution of the true label Yt and pseudo label ˆYt. GMMC and TTA. GMMC first implies an equal prior distribution by construction which is desirable for the actual TTA algorithms (e.g., category-balanced sampling strategies in [ 61, 15]). Thus, it simplifies ft into a maximum likelihood estimation ft(x) = argmaxy∈Y Pr(x|y; θt) with Pr(x|y; θt) = N(x; ˆµy,t, ˆσ2 y,t). The goal is estimating a set of parameters θt = {ˆµy,t, ˆσ2 y,t}y∈Y. A perfect classifier θ0 = {µy, σ2 y}y∈Y is initialized at t = 0. For the consecutive steps, the simplicity of GMMC allows solving the Optim (for finding θ′ t, Eq. 2) perfectly by computing the empirical mean and variance of new samples, approximating EPt. The mean teacher update (Eq. 3) for GMMC is: ˆµy,t = ( (1 − α)ˆµy,t−1 + αEPt h Xt|ˆYt i if ˆYt = y ˆµy,t−1 otherwise . (4) The update of ˆσ2 y,t is similar. ˆYt = ft−1(Xt) can be interpreted as a pseudo label (Eq. 1). ϵ-GMMC. Severe distribution shifts or low intra-batch category diversity of recurring TTA/practical TTA both result in an increase in the error rate of the predictor . Instead of directly modeling the dynamic changes of py,t (which can be complicated depending on the dataset), we study an ϵ−pertubed GMMC (ϵ−GMMC), where py,t is assumed to be static (defined below) and the pseudo- label predictor of this model is perturbed to simulate undesirable effects of the testing stream on the predictor. Two kinds of errors appear in a binary classifier [4]. Let ϵt = Pr{Yt = 1|ˆYt = 0} (5) be the false negative rate (FNR) of the model at step t. Without loss of generality, we study the increasing type II collapse of ϵ-GMMC. By intentionally flipping the true positive pseudo labels in simulation, an FNR of ϵt is maintained (Fig. 2). Assumption 1 (Static Data Stream). The marginal distribution of the true label follows the same Bernoulli distribution Ber(p0): p0,t = p0, (p1,t = p1 = 1 − p0), ∀t ∈ T. Lemma 1 (Increasing FNR). Under Assumption 1, a binary ϵ-GMMC would collapsed (Def. 1) with lim t→τ ˆp1,t = 0 (or lim t→τ ˆp0,t = 1, equivalently) if and only if lim t→τ ϵt = p1. Lemma 1 states the negative correlation between ˆp1,t and ϵt. Unsurprisingly, towards the collapsing point where all predictions are zeros, the FNR also increases at every step and eventually reaches the highest possible FNR of p1. Lemma 2 (ϵ-GMMC After Collapsing ). For a binary ϵ-GMMC model, with Assumption 1, if lim t→τ ˆp1,t = 0 (collapsing), the cluster 0 in GMMC converges in distribution to a single-cluster GMMC with parameters: N(ˆµ0,t, ˆσ2 0,t) d. → N(p0µ0 + p1µ1, p0σ2 0 + p1σ2 1 + p0p1(µ0 − µ1)2). Lemma 2 states the resulting ϵ−GMMC after collapsing. Cluster 0 now covers the whole data distribution (and assigning label 0 for all samples). Furthermore, collapsing happens when ˆµ0,t moves toward µ1. We next investigate the factors and conditions for this undesirable convergence. 4Theorem 1 (Convergence of ϵ−GMMC). For a binary ϵ-GMMC model, with Assumption 1, let the distance from ˆµ0,t toward µ1 is d0→1 t = |EPt [ˆµ0,t] − µ1|, then: d0→1 t − d0→1 t−1 ≤ α · p0 · \u0012 |µ0 − µ1| −d0→1 t−1 1 − ϵt \u0013 . From Thm. 1, we observe that the distance d0→1 t ’s converges (also indicating the convergence to the distribution in Lemma 2) if d0→1 t < d0→1 t−1 . The model collapse happens when this condition holds for a sufficiently long period. Corollary 1 (A Condition forϵ−GMMC Collapse). With fixedp0, α, µ0, µ1, ϵ−GMMC is collapsed if there exists a sequence of {ϵt}τ τ−∆τ (τ ≥ ∆τ > 0) such that: p1 ≥ ϵt > 1 − d0→1 t−1 |µ0 − µ1|, t ∈ [τ − ∆τ , τ]. Corollary 1 introduces a condition ϵ-GMMC collapse. Here, ϵt’s are non-decreasing, lim t→τ ϵt = p1. Remarks. Thm. 1 concludes two sets of factors contributing to collapse: (i) data-dependent factors: the prior data distribution (p0), the nature difference between two categories (|µ0 − µ1|); and (ii) algorithm-dependent factors: the update rate (α), the FNR at each step (ϵt). ϵ-GMMC analysis sheds light on explaining model collapse on real datasets (Sec. 5.3), reasons the existing approaches (Sec. 3.3) and motivates the development of our baseline (Sec. 4). 3.3 Connection to Existing Solutions Prior TTA algorithms have already incorporated implicit mechanisms to mitigate model collapse. The theoretical results in the previous section explain the rationale behind these effective strategies. Regularization Term for θt. Knowing that f0 is always well-behaved, an attempt is restricting the divergence of θt from θ0, e.g. using R(θt) ∆ = ∥θ0 − θt∥2 2 regularization [40]. The key idea is introducing a penalty term to avoid an extreme divergence as happening in Thm. 1. Memory Bank for Harmonizing Pt(x). Upon receiving Xt, samples in this batch are selectively updated to a memory bank M (which already contains a subset of some instances ofXt′, t′ < tin the previous steps). By keeping a balanced number of samples from each category, distribution PM t (y) of samples in M is expected to have less zero entries than Pt(y), making the optimization step over PM t more desirable. From Thm. 1, M moderates the extreme value of the category distribution (p0 term) which typically appears on batches with low intra-batch category diversity. 4 Persistent Test-time Adaptation (PeTTA) Now we introduce our Persistent TTA (PeTTA) approach. Further inspecting Thm. 1, while ϵt (Eq. 5) is not computable without knowing the true labels, the measure of divergence from the initial distribution (analogously to d0→1 t−1 term) can provide hints to fine-tune the adaptation process. Key Idea. A proper adjustment toward the TTA algorithm can break the chain of monotonically increasing ϵt’s in Corollary 1 to prevent the model collapse. In the mean teacher update, the larger value of λ (Eq. 2) prioritizes the task of preventing collapse on one hand but also limits its adaptability to the new testing environment. Meanwhile, α (Eq. 3) controls the weight on preserving versus changing the model from the previous step. Drawing inspiration from the exploration-exploitation tradeoff [49, 25] encountered in reinforcement learning [54], we introduce a mechanism for adjusting λ and α on the fly, balancing between the two primary objectives: adaptation and preventing model collapse. Our strategy is prioritizing collapse prevention (increasing λ) and preserving the model from previous steps (decreasing α) when there is a significant deviation from θ0. In [40, 61, 59], λ and α were fixed through hyper-parameter tuning. This is suboptimal due to varying TTA environments and the lack of validation set [62]. Furthermore, Thm. 1 suggests the convergence rate quickly escalates when ϵt increases, making constant λ, αinsufficient to prevent collapse. Sensing the Divergence of θt. We first equip PeTTA with a mechanism for measuring its divergence from θ0. Since ft(x) = argmax y∈Y Pr(y|x; θt), we can decompose Pr(y|x; θt) = [h (ϕθt(x))]y, with ϕθt(·) is a θt-parameterized deep feature extractor followed by a fixed classification head (a linear and softmax layer) h(·). The operator [·]y extracts the yth component of a vector. 5Since h(·) remains unchanged, instead of comparing the divergence in the parameter space (Θ) or between the output probability Pr(y|x; θt) and Pr(y|x; θ0), we suggest an inspection over the feature embedding space that preserves a maximum amount of information in our case (data processing inequality [9]). Inspired by [31] and under Gaussian assumption, the Mahalanobis distance of the first moment of the feature embedding vectors is compared. Let z = ϕθt(x), we keep track of a collection of the running mean of feature vector z: {ˆµy t }y∈Y in which ˆµy t is EMA updated with vector z if ft(x) = y. The divergence of θt at step t, evaluated on class y is defined as: γy t = 1 − exp \u0010 −(ˆµy t − µy 0)T (Σy 0)−1 (ˆµy t − µy 0) \u0011 , (6) where µy 0 and Σy 0 are the pre-computed empirical mean and covariant matrix of feature vectors in the source dataset (P0). The covariant matrix here is diagonal for simplicity. In practice, without directly accessing the training set, we assume a small set of unlabeled samples can be drawn from the source distribution for empirically computing these values (visit Appdx. E.4 for further details). Here, we implicitly expect the independence of each entry in z and TTA approaches learn to align feature vectors of new domains back to the source domain (P0). Therefore, the accumulated statistics of these feature vectors at each step should be concentrated near the vectors of the initial model. The value of γy t ∈ [0, 1] is close to 0 when θt = θ0 and increases exponentially as ˆµy t diverging from µy 0. Adaptive Regularization and Model Update. With α0, λ0 are initial values, utilizing γy t derived in Eq. 6, a pair of (λt, αt) is adaptively chosen at each step: ¯γt = 1 | ˆYt| X y∈ ˆYt γy t , ˆYt = n ˆY (i) t |i = 1, ··· , Nt o ; λt = ¯γt · λ0, α t = (1 − ¯γt) · α0, (7) ˆYt is a set of unique pseudo labels in a testing batch ( ˆY (i) t is the ith realization of ˆYt). Anchor Loss. Penalizing the divergence with regular vector norms in high-dimensional space (Θ) is insufficient (curse of dimensionality [5, 51]), especially with a large model and limited samples. Anchor loss LAL can nail down the similarity between ft and f0 in the probability space [32, 12]: LAL(Xt; θ) = − X y∈Y Pr(y|Xt; θ0) log Pr(y|Xt; θ), (8) which is equivalent to minimizing the KL divergence DKL (Pr(y|Xt; θ0)∥Pr(y|Xt; θ)). Persistent TTA.Having all the ingredients, we design our approach, PeTTA, following the convention setup of the mean teacher update, with the category-balanced memory bank and the robust batch normalization layer from [61]. Appdx. E.1 introduces the pseudo code of PeTTA. ForLCLS, either the self-training scheme [12] or the regular cross-entropy [16] is adopted. With R(θ), cosine similarity or L2 distance are both valid metrics for measuring the distance between θ and θ0 in the parameter space. Fisher regularizer coefficient [ 40, 27] can also be used, optionally. To sum up, the teacher model update of PeTTA is an elaborated version of EMA with λt, αt (Eq. 7) and LAL (Eq. 8): θ′ t = Optim θ′∈Θ EPt h LCLS \u0010 ˆYt, Xt; θ′ \u0011 + LAL (Xt; θ′) i + λtR(θ′), θt = (1 − αt)θt−1 + αtθ′ t. 5 Experimental Results 5.1 ϵ−MMC Simulation Result Simulation Setup. A total of 6000 samples from two Gaussian distributions: N(µ0 = 0, σ2 0 = 1) and N(µ1 = 2, σ2 1 = 1) with p0 = p1 = 1 2 are synthesized and gradually released in a batch of B = 10 samples. For evaluation, an independent set of 2000 samples following the same distribution is used for computing the prediction frequency, and the false negative rate (FNR). ϵ−GMMC update follows Eq. 4 with α = 5e−2. To simulate model collapse, the predictor is intercepted and 10% of the true-postive pseudo labels at each testing step are randomly flipped (Corollary 1). Simulation Result. In action, both the likelihood of predicting class 0 (Fig. 3a-left) and theϵt (Eq. 5) (Fig. 3c-right, solid line) gradually increases over time as expected (Lemma 1). After collapsing, 60 120 240 360 480 6000 0.2 0.4 0.6 0.8 1 Testing Step (t) 0 120 240 360 480 6000 0.2 0.4 0.6 0.8 1 Testing Step (t) −4 −2 0 2 4x−4 −2 0 2 40 0.2 0.4 0.6 0.8 1 x Probability density N(µ0, σ0) N(µ1, σ1) N(ˆµ0, ˆσ0) N(ˆµ1, ˆσ1) 0 100 200 300 400 500 600 0.8 1.2 1.6 2.0 Testing step (t) |ˆµ0,t −µ1| Numerical Simulation Theoretical Result 0 100 200 300 400 500 600 0.1 0.2 0.3 0.4 0.5 Testing step (t) ϵt Prediction Frequency GMMCϵ-GMMC ϵ-GMMC GMMC (a) (b) (c) Figure 3: Simulation result on ϵ-perturbed Gaussian Mixture Model Classifier ( ϵ-GMMC) and GMMC (perturbed-free). (a) Histogram of model predictions through time. A similar prediction frequency pattern is observed on CIFAR-10-C (Fig. 5a-left). (b) The probability density function of the two clusters after convergence versus the true data distribution. The initial two clusters of ϵ-GMMC collapsed into a single cluster with parameters stated in Lemma 2. In the perturbed-free, GMMC converges to the true data distribution. (c) Distance toward µ1 (|EPt [ˆµ0,t] − µ1|) and false- negative rate (ϵt) in simulation coincides with the result in Thm. 1 (with ϵt following Corollary 1). ϵ-GMMC merges the two initial clusters, resulting in a single one (Fig. 3b-left) with parameters that match Lemma 2. The distance from ˆµ0,t (initialized at µ0) towards µ1 converges (Fig. 3c-left, solid line), coincided with the analysis in Thm. 1 when ϵt is chosen following Corollary 1 (Fig. 3c, dashed line). GMMC (perturbed-free) stably produces accurate predictions (Fig. 3a-right) and approximates the true data distribution (Fig. 3b-right). The simulation empirically validates our analysis (Sec. 3.2), confirming the vulnerability of TTA models when the pseudo labels are inaccurately estimated. 5.2 Setup - Benchmark Datasets Datasets. We benchmark the performance on four TTA classification tasks. Specifically, CIFAR10 → CIFAR10-C, CIFAR100→ CIFAR100-C, and ImageNet → ImageNet-C [19] are three corrupted images classification tasks (corruption level 5, the most severe). Additionally, we incorporate DomainNet [44] with 126 categories from four domains for the task real → clipart, painting, sketch. Compared Methods. Besides PeTTA, the following algorithms are investigated: CoTTA [ 59], EATA [40], RMT [12], MECTA [22], RoTTA [61], ROID [37] and TRIBE [52]. Noteworthy, only RoTTA is specifically designed for the practical TTA setting while others fit the continual TTA setting in general. A parameter-free approach: LAME [ 7] and a reset-based approach (i.e., reverting the model to the source model after adapting to every 1, 000 images): RDumb [45] are also included. Recurring TTA. Following the practical TTA setup, multiple testing scenarios from each testing set will gradually change from one to another while the Dirichlet distribution (Dir(0.1) for CIFAR10- C, DomainNet, and ImageNet-C, and Dir(0.01) for CIFAR100-C) generates category temporally correlated batches of data. For all experiments, we set the number of revisits K = 20 (times) as this number is sufficient to fully observe the gradual degradation on existing TTA baselines. Implementation Details. We use PyTorch [43] for implementation. RobustBench [10] and torchvision [35] provide pre-trained source models. Hyper-parameter choices are kept as close as possible to the original selections of authors. Visit Sec. G for more implementation details. Unless otherwise noted, for all PeTTA experiments, the EMA update rate for robust batch normalization [61] and feature embedding statistics is set to 5e−2; α0 = 1e−3 and cosine similarity regularizer is used. On CIFAR10/100-C and ImageNet-C we use the self-training loss in [ 12] for LCLS and λ0 = 10 while the regular cross-entropy loss [ 13] and λ0 = 1 (severe domain shift requires prioritizing 7Table 1: Average classification error of the task CIFAR-10→ CIFAR-10-C in recurring TTA. The lowest error is in bold,(∗)average value across 5 runs (different random seeds) is reported for PeTTA. Recurring TTA visit− − − − − − − − − − − − − − − − − − − − − − − − − →Method1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 Avg Source 43.5 43.5 LAME [7] 31.1 31.1 CoTTA [59]82.2 85.6 87.2 87.8 88.2 88.5 88.7 88.7 88.9 88.9 88.9 89.2 89.2 89.2 89.1 89.2 89.2 89.1 89.3 89.388.3EATA [40]81.6 87.0 88.7 88.7 88.9 88.7 88.6 89.0 89.3 89.6 89.5 89.6 89.7 89.7 89.3 89.6 89.6 89.8 89.9 89.488.8RMT [12]77.5 76.9 76.5 75.8 75.5 75.5 75.4 75.4 75.5 75.3 75.5 75.6 75.5 75.5 75.7 75.6 75.7 75.6 75.7 75.875.8MECTA [22]72.2 82.0 85.2 86.3 87.0 87.3 87.3 87.5 88.1 88.8 88.9 88.9 88.6 89.1 88.7 88.8 88.5 88.6 88.3 88.886.9RoTTA [61]24.6 25.5 29.6 33.6 38.2 42.8 46.2 50.6 52.2 54.1 56.5 57.5 59.4 60.2 61.7 63.0 64.8 66.1 68.2 70.351.3RDumb [45]31.1 32.1 32.3 31.6 31.9 31.8 31.8 31.9 31.9 32.1 31.7 32.0 32.5 32.0 31.9 31.6 31.9 31.4 32.3 32.431.9ROID [37]72.7 72.6 73.1 72.4 72.7 72.8 72.7 72.7 72.9 72.8 72.9 72.9 72.8 72.5 73.0 72.8 72.5 72.5 72.7 72.772.7TRIBE [52]15.3 16.6 16.6 16.3 16.7 17.0 17.3 17.4 17.4 18.0 17.9 18.0 17.9 18.6 18.2 18.8 18.0 18.2 18.4 18.017.5PeTTA(ours)(∗) 24.323.022.622.422.422.522.322.522.822.822.622.722.722.922.622.722.622.822.923.022.8 adaptability) are applied in DomainNet experiments. In Appdx. F.5, we provide a sensitivity analysis on the choice of hyper-parameter λ0 in PeTTA. 5.3 Result - Benchmark Datasets Recurring TTA Performance. Fig. 1-right presents the testing error on CIFAR-10-C in recurring TTA setting. RoTTA [61] exhibits promising performance in the first several visits but soon raises and eventually exceeds the source model (no TTA). The classification error of compared methods on CIFAR-10→CIFAR-10-C, and ImageNet → ImageNet-C [19] tasks are shown in Tab. 1, and Tab. 2. Appdx. F.1 provides the results on the other two datasets. The observed performance degradation of CoTTA [59], EATA [40], RoTTA [61], and TRIBE [52] confirms the risk of error accumulation for an extensive period. While RMT [12], MECTA [22], and ROID [37] remain stable, they failed to adapt to the temporally correlated test stream at the beginning, with a higher error rate than the source model. LAME [7] (parameter-free TTA) and RDumb [45] (reset-based TTA) do not suffer from collapsing. However, their performance is lagging behind, and knowledge accumulation is limited in these approaches that could potentially favor a higher performance as achieved by PeTTA. Furthermore, LAME [7] is highly constrained by the source model, and selecting a precise reset frequency in RDumb [45] is challenging in practice (see Appdx. F.3 for a further discussion). 0 10 20 30 40 16 18 20 22 24 Recurring TTA Visit Classification Error PeTTA (ours) TRIBE [52] Figure 4: Classification error of TRIBE [ 52] and PeTTA (ours) of the task CIFAR-10→CIFAR10-C task in recurring TTA with 40 visits. In average, PeTTA outperforms almost every baseline approaches and persists across 20 vis- its over the three datasets. The only exception is at the case of TRIBE [ 52] on CIFAR-10- C. While this state-of-the-art model provides stronger adaptability, outweighing the PeTTA, and baseline RoTTA [61] in several recurrences, the risk of the model collapsing still presents in TRIBE [52]. This can be clearly observed when we increase the observation period to 40 recur- ring visits in Fig. 4. As the degree of freedom for adaptation in PeTTA is more constrained, it takes a bit longer for adaptation but remains sta- ble afterward. Fig. 5b-bottom exhibits the con- fusion matrix at the last visit with satisfactory accuracy. The same results are also observed when shuffling the order of domain shifts within each recurrence (Appdx. D.3), or extending the number of recurrences to 40 visits (Appdx. F.4). Continuously Changing Corruption (CCC) [45] Performance. Under CCC [45], Tab. 3 reveals the supreme performance of PeTTA over RoTTA [61] and RDumb [45]. Here, we report the average classification error between two consecutive adaptation step intervals. An adaptation step in this table corresponds to a mini-batch of data with 64 images. The model is adapted to 80, 000 steps in total with more than 5.1M images, significantly longer than 20 recurring TTA visits. Undoubtedly, PeTTA still achieves good performance where the corruptions are algorithmically generated, non-cyclic with two or more corruption types can happen simultaneously. This experiment also empirically justifies the construction of our recurring TTA as a diagnostic tool (Appdx. D.2) where similar observations are concluded on the two settings. Obviously, our recurring TTA is notably simpler than CCC [45]. 8Table 2: Average classification error of the task ImageNet → ImageNet-C in recurring TTA scenario. Recurring TTA visit− − − − − − − − − − − − − − − − − − − − − − − − − →Method1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 Avg Source 82.0 82.0 LAME [7] 80.9 80.9 CoTTA [59]98.6 99.1 99.4 99.4 99.5 99.5 99.5 99.5 99.6 99.7 99.6 99.6 99.6 99.6 99.6 99.6 99.6 99.6 99.7 99.799.5EATA [40]60.4 59.3 65.4 72.6 79.1 84.2 88.7 92.7 95.2 96.9 97.7 98.1 98.4 98.6 98.7 98.8 98.8 98.9 98.9 99.089.0RMT [12]72.3 71.0 69.9 69.1 68.8 68.5 68.4 68.3 70.0 70.2 70.1 70.2 72.8 76.8 75.6 75.1 75.1 75.2 74.8 74.771.8MECTA [22]77.2 82.8 86.1 87.9 88.9 89.4 89.8 89.9 90.0 90.4 90.6 90.7 90.7 90.8 90.8 90.9 90.8 90.8 90.7 90.889.0RoTTA [61]68.3 62.1 61.8 64.5 68.4 75.4 82.7 95.1 95.8 96.6 97.1 97.9 98.3 98.7 99.0 99.1 99.3 99.4 99.5 99.687.9RDumb [45]72.2 73.0 73.2 72.8 72.2 72.8 73.3 72.7 71.9 73.0 73.2 73.1 72.0 72.7 73.3 73.1 72.1 72.6 73.3 73.172.8ROID [37]62.7 62.3 62.3 62.3 62.5 62.3 62.4 62.4 62.3 62.6 62.5 62.3 62.5 62.4 62.5 62.4 62.4 62.5 62.4 62.562.4TRIBE [52]63.664.0 64.9 67.8 69.6 71.7 73.5 75.5 77.4 79.8 85.0 96.5 99.4 99.8 99.9 99.8 99.8 99.9 99.9 99.984.4PeTTA(ours)(∗) 65.361.759.859.159.459.659.859.359.460.060.361.060.760.460.660.760.860.760.460.260.5 Table 3: Average classification error on CCC [45] setting. Each column presents the average error within an adaptation interval (e.g., the second column provides the average error between the 6701 and 13400 adaptation steps). Each adaptation step here is performed on a mini-batch of 64 images. CCC [45] Adaptation Step− − − − − − − − − − − − − − − − − − − − − − − − − → Method6700 13400 20100 26800 33500 40200 46900 53600 60200 66800 73400 80000Avg Source 0.83 0.83 0.83 0.83 0.83 0.84 0.84 0.83 0.84 0.83 0.83 0.83 0.83 RoTTA [61]0.70 0.85 0.92 0.96 0.98 1.00 1.00 1.00 1.00 1.00 1.00 1.00 0.95 RDumb [45]0.78 0.74 0.75 0.77 0.75 0.72 0.75 0.77 0.75 0.74 0.75 0.75 0.75 PeTTA(ours) 0.67 0.63 0.62 0.65 0.65 0.64 0.64 0.68 0.63 0.63 0.65 0.65 0.64 0.46 0.44 0.4 0.43 0.46 0.47 0.44 0.43 0.48 0.4 0.43 0.43 0.41 airplane bird cat dog frog ship auto deer horse truck 0.13 0.34 0.44 0.32 0.14 0.44 0.51 0.46 0.46 0.34 airplane bird catdog frog ship auto deer horse truck Inter-category cosine similarity (source model)Misclassification rate of collapsed RoTTA 0: airplane 1: auto2: bird3: cat4: deer5: dog6: frog7: horse8: ship9: truck 0 1 2 3 4 5 6 7 8 9 0.69 0 0 0 0.08 0 0.04 0 0.13 0.05 0.34 0.17 0 0 0.1 0 0.03 0 0.23 0.13 0.24 0 0.1 0.03 0.44 0 0.11 0 0.07 0.01 0.21 0 0 0.11 0.32 0 0.21 0 0.14 0.01 0.14 0 0 0.01 0.71 0 0.06 0.01 0.06 0.01 0.21 0 0 0.07 0.44 0 0.16 0 0.1 0.01 0.05 0 0 0.08 0.51 0 0.25 0 0.1 0.01 0.17 0 0 0.03 0.46 0 0.04 0.21 0.06 0.04 0.46 0 0 0.01 0.06 0 0.03 0 0.41 0.03 0.34 0 0 0 0.12 0 0.03 0 0.18 0.32 0: airplane 1: auto2: bird3: cat4: deer5: dog6: frog7: horse8: ship9: truck 0 1 2 3 4 5 6 7 8 9 0.73 0.01 0.06 0.04 0.02 0 0.03 0.01 0.08 0.02 0.01 0.88 0.01 0.01 0 0 0.02 0 0.02 0.05 0.04 0 0.75 0.07 0.05 0.02 0.05 0.01 0.01 0 0.01 0 0.06 0.72 0.05 0.04 0.06 0.02 0.01 0.01 0.02 0 0.06 0.07 0.76 0.01 0.05 0.02 0.01 0 0 0 0.07 0.19 0.05 0.59 0.05 0.02 0.01 0.01 0 0 0.03 0.07 0.02 0.01 0.84 0 0.01 0.01 0.01 0 0.06 0.06 0.08 0.02 0.02 0.74 0 0.01 0.04 0.02 0.02 0.02 0.01 0 0.03 0 0.84 0.02 0.01 0.05 0.02 0.03 0.01 0 0.02 0.01 0.04 0.82 1 5 10 15 200 0.2 0.4 0.6 0.8 1 Visits 1 5 10 15 200 0.2 0.4 0.6 0.8 1 Visits RoTTA [61] PeTTA (ours) PeTTA (ours) - 20th visit RoTTA [61] - 20th visit Predicted label (a) (b)(c) True labelTrue label Prediction Frequency Figure 5: Recurring TTA (20 visits) on CIFAR-10 →CIFAR10-C task. (a) Histogram of model predictions (10 labels are color-coded). PeTTA achieves a persisting performance while RoTTA [61] degrades. (b) Confusion matrix at the last visit, RoTTA classifies all samples into a few categories (e.g., 0: airplane, 4: deer). (c) Force-directed graphs showing (left) the most prone to misclassification pairs (arrows indicating the portion and pointing from the true to the misclassified category); (right) similar categories tend to be easily collapsed. Edges denote the average cosine similarity of feature vectors (source model), only the highest similar pairs are shown. Best viewed in color. Collapsing Pattern. The rise in classification error (Fig. 1-right) can be reasoned by the prediction frequency of RoTTA [ 61] in an recurring TTA setting (Fig. 5a-left). Similar to ϵ-GMMC, the likelihood of receiving predictions on certain categories gradually increases and dominates the others. Further inspecting the confusion matrix of a collapsed model (Fig. 5b-top) reveals two major groups of categories are formed and a single category within each group represents all members, thereby becoming dominant. To see this, Fig. 5c-left simplifies the confusion matrix by only visualizing the 9Table 4: Average (across 20 visits) error of multiple variations of PeTTA: without (w/o) R(θ), LAL; LAL only; fixed regularization coefficient λ; adaptive coef- ficient λt, update rate αt; using anchor loss LAL. Method CF-10-CCF-100-CDN IN-C Baseline w/oR(θ),LAL 42.6 63.0 77.9 93.4 R(θ)fixedλ= 0.1λ0 43.3 65.0 80.0 92.5R(θ)fixedλ=λ0 42.0 64.6 66.6 92.9 LALonly 25.4 56.5 47.5 68.1 PeTTA -λt 27.1 55.0 59.7 92.7PeTTA -λt +αt 23.9 41.4 44.5 75.7PeTTA -λt +LAL 26.2 36.3 43.2 62.0 PeTTA -λt +αt +LAL 22.8 35.1 42.9 60.5 Table 5: Average (across 20 visits) error of PeTTA. PeTTA favors various choices of reg- ularizers R(θ): L2 and cosine similarity in conjunction with Fisher [27, 40] coefficient. Method CF-10-CCF-100-CDN IN-CR(θ) Fisher L2 ✗ 23.0 35.6 43.1 70.8✓ 22.7 36.0 43.9 70.0 Cosine ✗ 22.8 35.1 42.9 60.5✓ 22.6 35.9 43.3 63.8 CF: CIFAR, DN: DomainNet, IN: ImageNet top prone-to-misclassified pair of categories. Here, label deer is used for almost every living animal while airplane represents transport vehicles. The similarity between categories in the feature space of the source model (Fig. 5c-right) is correlated with the likelihood of being merged upon collapsing. As distance in feature space is analogous to |µ0 − µ1| (Thm. 1), closer clusters are at a higher risk of collapsing. This explains and showcases that the collapsing behavior is predictable up to some extent. 5.4 Ablation Study Effect of Each Component. Tab. 4 gives an ablation study on PeTTA, highlighting the use of a regularization term (R(θ)) with a fixed choice of λ, αnot only fails to mitigate model collapse but may also introduce a negative effect (rows 2-3). Trivially applying the anchor loss (LAL) alone is also incapable of eliminating the lifelong performance degradation in continual TTA (row 4). Within PeTTA, adopting the adaptiveλt scheme alone (row 5) or in conjunction with either αt or anchor loss LAL (rows 6-7) partially stabilizes the performance. Under the drastic domain shifts with a larger size of categories or model parameters (e.g., on CIFAR-100-C, DomainNet, ImageNet-C), restricting αt adjustment limits the ability of PeTTA to stop undesirable updates while a common regularization term without LAL is insufficient to guide the adaptation. Thus, leveraging all elements secures the persistence of PeTTA (row 8). Various Choices of Regularizers. The design of PeTTA is not coupled with any specific regu- larization term. Demonstrated in Tab. 5, PeTTA works well for the two common choices: L2 and cosine similarity. The conjunction use of Fisher coefficent [27, 40] for weighting the model parameter importance is also studied. While the benefit (in terms of improving accuracy) varies across datasets, PeTTA accommodates all choices, as the model collapse is not observed in any of the options. 6 Discussions and Conclusion On a Potential Risk of TTA in Practice. We provide empirical and theoretical evidence on the risk of deploying continual TTA algorithms. Existing studies fail to detect this issue with a single pass per test set. The recurring TTA could be conveniently adopted as astraightforward evaluation, where its challenging test stream magnifies the error accumulation that a model might encounter in practice. Limitations. PeTTA takes one step toward mitigating the gradual performance degradation of TTA. Nevertheless, a complete elimination of error accumulation cannot be guaranteed rigorously through regularization. Future research could delve deeper into expanding our efforts to develop an algorithm that achieves error accumulation-free by construction. Furthermore, as tackling the challenge of the temporally correlated testing stream is not the focus of PeTTA, using a small memory bank as in [61, 15] is necessary. It also assumes the features statistics from the source distribution are available (Appdx. E.3, E.4). These constraints potentially limit its scalability in real-world scenarios. Conclusion. Towards trustworthy and reliable TTA applications, we rigorously study theperformance degradation problem of TTA. The proposed recurring TTAsetting highlights the limitations of modern TTA methods, which struggle to prevent the error accumulation when continuously adapting to demanding test streams. Theoretically inspecting a failure case of ϵ−GMMC paves the road for designing PeTTA- a simple yet efficient solution that continuously assesses the model divergence for harmonizing the TTA process, balancing adaptation, and collapse prevention. 10Acknowledgements This work was supported by the Jump ARCHES Endowment through the Health Care Engineering Systems Center, JSPS/MEXT KAKENHI JP24K20830, ROIS NII Open Collaborative Research 2024-24S1201, in part by the National Institute of Health (NIH) under Grant R01 AI139401, and in part by the Vingroup Innovation Foundation under Grant VINIF.2021.DA00128. References [1] Kartik Ahuja, Ethan Caballero, Dinghuai Zhang, Jean-Christophe Gagnon-Audet, Yoshua Bengio, Ioannis Mitliagkas, and Irina Rish. Invariance principle meets information bottleneck for out-of-distribution gener- alization. In A. Beygelzimer, Y . Dauphin, P. Liang, and J. Wortman Vaughan, editors,Advances in Neural Information Processing Systems, 2021. URL https://openreview.net/forum?id=jlchsFOLfeF. [2] Rahaf Aljundi, Eugene Belilovsky, Tinne Tuytelaars, Laurent Charlin, Massimo Caccia, Min Lin, and Lucas Page-Caccia. Online continual learning with maximal interfered retrieval. In H. Wallach, H. Larochelle, A. Beygelzimer, F. d'Alché-Buc, E. Fox, and R. Garnett, editors,Advances in Neural Information Processing Systems, volume 32, 2019. URL https://proceedings.neurips.cc/paper_files/paper/2019/ file/15825aee15eb335cc13f9b559f166ee8-Paper.pdf. [3] Rahaf Aljundi, Min Lin, Baptiste Goujaud, and Yoshua Bengio. Gradient based sample se- lection for online continual learning. In Advances in Neural Information Processing Systems , volume 32, 2019. URL https://proceedings.neurips.cc/paper_files/paper/2019/file/ e562cd9c0768d5464b64cf61da7fc6bb-Paper.pdf. [4] Amitav Banerjee, U. B. Chitnis, S. L. Jadhav, J. S. Bhawalkar, and S. Chaudhury. Hypothesis testing, type I and type II errors. Industrial Psychiatry Journal, 18(2):127–131, 2009. ISSN 0972-6748. doi: 10.4103/0972-6748.62274. URL https://www.ncbi.nlm.nih.gov/pmc/articles/PMC2996198/. [5] Richard Bellman. Dynamic Programming. Princeton University Press, Princeton, NJ, USA, 1957. [6] Arno Blaas, Andrew Miller, Luca Zappella, Joern-Henrik Jacobsen, and Christina Heinze-Deml. Con- siderations for distribution shift robustness in health. In ICLR 2023 Workshop on Trustworthy Machine Learning for Healthcare, 2023. URL https://openreview.net/forum?id=y7XveyWYzIB. [7] Malik Boudiaf, Romain Mueller, Ismail Ben Ayed, and Luca Bertinetto. Parameter-free online test-time adaptation. In 2022 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pages 8334–8343, 2022. doi: 10.1109/CVPR52688.2022.00816. [8] Dian Chen, Dequan Wang, Trevor Darrell, and Sayna Ebrahimi. Contrastive test-time adaptation. In Proceedings of the IEEE International Conference on Computer Vision, 2022. [9] Thomas M. Cover and Joy A. Thomas.Elements of Information Theory (Wiley Series in Telecommunications and Signal Processing). Wiley-Interscience, USA, 2006. ISBN 0471241954. [10] Francesco Croce, Maksym Andriushchenko, Vikash Sehwag, Edoardo Debenedetti, Nicolas Flammarion, Mung Chiang, Prateek Mittal, and Matthias Hein. Robustbench: a standardized adversarial robustness benchmark. In Thirty-fifth Conference on Neural Information Processing Systems Datasets and Benchmarks Track, 2021. URL https://openreview.net/forum?id=SSKZPJCt7B. [11] Matthias De Lange, Rahaf Aljundi, Marc Masana, Sarah Parisot, Xu Jia, Ales Leonardis, Gregory Slabaugh, and Tinne Tuytelaars. A continual learning survey: Defying forgetting in classification tasks. IEEE Transactions on Pattern Analysis and Machine Intelligence, 44(7):3366–3385, 2022. doi: 10.1109/ TPAMI.2021.3057446. [12] Mario Döbler, Robert A. Marsden, and Bin Yang. Robust mean teacher for continual and gradual test-time adaptation. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pages 7704–7714, June 2022. [13] Yaroslav Ganin and Victor Lempitsky. Unsupervised domain adaptation by backpropagation. In Proceedings of the 32nd International Conference on Machine Learning , volume 37 of Proceedings of Machine Learning Research , pages 1180–1189, Lille, France, 07–09 Jul 2015. PMLR. URL https://proceedings.mlr.press/v37/ganin15.html. [14] Yaroslav Ganin, Evgeniya Ustinova, Hana Ajakan, Pascal Germain, Hugo Larochelle, François Laviolette, Mario Marchand, and Victor Lempitsky. Domain-Adversarial Training of Neural Networks, pages 189– 209. Springer International Publishing, 2017. doi: 10.1007/978-3-319-58347-1_10. URL https: //doi.org/10.1007/978-3-319-58347-1_10 . [15] Taesik Gong, Jongheon Jeong, Taewon Kim, Yewon Kim, Jinwoo Shin, and Sung-Ju Lee. NOTE: Robust continual test-time adaptation against temporal correlation. In Advances in Neural Information Processing Systems, 2022. 11[16] Yves Grandvalet and Yoshua Bengio. Semi-supervised learning by entropy minimization. In L. Saul, Y . Weiss, and L. Bottou, editors, Advances in Neural Information Processing Systems , volume 17, 2004. URL https://proceedings.neurips.cc/paper_files/paper/2004/file/ 96f2b50b5d3613adf9c27049b2a888c7-Paper.pdf. [17] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image recognition. arXiv preprint arXiv:1512.03385, 2015. [18] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Delving deep into rectifiers: Surpassing human-level performance on ImageNet classification. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR), pages 1026–1034, 2015. [19] Dan Hendrycks and Thomas Dietterich. Benchmarking neural network robustness to common corruptions and perturbations. Proceedings of the International Conference on Learning Representations, 2019. [20] Dan Hendrycks, Norman Mu, Ekin D. Cubuk, Barret Zoph, Justin Gilmer, and Balaji Lakshminarayanan. AugMix: A simple data processing method to improve robustness and uncertainty. Proceedings of the International Conference on Learning Representations (ICLR), 2020. [21] Dan Hendrycks, Steven Basart, Norman Mu, Saurav Kadavath, Frank Wang, Evan Dorundo, Rahul Desai, Tyler Zhu, Samyak Parajuli, Mike Guo, Dawn Song, Jacob Steinhardt, and Justin Gilmer. The many faces of robustness: A critical analysis of out-of-distribution generalization. In 2021 IEEE/CVF International Conference on Computer Vision (ICCV), pages 8320–8329, 2021. doi: 10.1109/ICCV48922.2021.00823. [22] Junyuan Hong, Lingjuan Lyu, Jiayu Zhou, and Michael Spranger. MECTA: Memory-economic continual test-time model adaptation. In The Eleventh International Conference on Learning Representations, 2023. URL https://openreview.net/forum?id=N92hjSf5NNh. [23] Fabian Isensee, Paul F. Jaeger, Simon A. A. Kohl, Jens Petersen, and Klaus H. Maier-Hein. nnU-Net: a self-configuring method for deep learning-based biomedical image segmentation. Nature Methods, 18(2):203–211, February 2021. ISSN 1548-7105. doi: 10.1038/s41592-020-01008-z. URL https: //www.nature.com/articles/s41592-020-01008-z . [24] Yusuke Iwasawa and Yutaka Matsuo. Test-time classifier adjustment module for model-agnostic domain generalization. In M. Ranzato, A. Beygelzimer, Y . Dauphin, P.S. Liang, and J. Wort- man Vaughan, editors, Advances in Neural Information Processing Systems , volume 34, pages 2427–2440, 2021. URL https://proceedings.neurips.cc/paper_files/paper/2021/file/ 1415fe9fea0fa1e45dddcff5682239a0-Paper.pdf. [25] Michael N. Katehakis and Arthur F. Veinott. The multi-armed bandit problem: Decomposition and compu- tation. Mathematics Operations Research, 12:262–268, 1987. URL https://api.semanticscholar. org/CorpusID:656323. [26] Diederik P. Kingma and Jimmy Ba. Adam: A method for stochastic optimization. In Yoshua Bengio and Yann LeCun, editors, 3rd International Conference on Learning Representations, ICLR 2015, San Diego, CA, USA, May 7-9, 2015, Conference Track Proceedings, 2015. URL http://arxiv.org/abs/1412. 6980. [27] James Kirkpatrick, Razvan Pascanu, Neil Rabinowitz, Joel Veness, Guillaume Desjardins, Andrei A. Rusu, Kieran Milan, John Quan, Tiago Ramalho, Agnieszka Grabska-Barwinska, Demis Hassabis, Claudia Clopath, Dharshan Kumaran, and Raia Hadsell. Overcoming catastrophic forgetting in neural networks.Pro- ceedings of the National Academy of Sciences, 114(13):3521–3526, 2017. doi: 10.1073/pnas.1611835114. URL https://www.pnas.org/doi/abs/10.1073/pnas.1611835114. [28] Dong-Hyun Lee. Pseudo-label : The simple and efficient semi-supervised learning method for deep neural networks. ICML 2013 Workshop : Challenges in Representation Learning (WREPL), 07 2013. [29] T. Lee, S. Chottananurak, T. Gong, and S. Lee. Aetta: Label-free accuracy estimation for test-time adaptation. In 2024 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pages 28643–28652, Los Alamitos, CA, USA, jun 2024. IEEE Computer Society. doi: 10.1109/CVPR52733. 2024.02706. URL https://doi.ieeecomputersociety.org/10.1109/CVPR52733.2024.02706. [30] Haoliang Li, Sinno Jialin Pan, Shiqi Wang, and Alex C. Kot. Domain generalization with adversarial feature learning. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR), June 2018. [31] Yanghao Li, Naiyan Wang, Jianping Shi, Jiaying Liu, and Xiaodi Hou. Revisiting batch normalization for practical domain adaptation. In International Conference on Learning Representations Workshop, 2017. URL https://openreview.net/forum?id=BJuysoFeg. [32] Zhizhong Li and Derek Hoiem. Learning without forgetting. IEEE Transactions on Pattern Analysis and Machine Intelligence, 40(12):2935–2947, 2018. doi: 10.1109/TPAMI.2017.2773081. [33] Jian Liang, Dapeng Hu, and Jiashi Feng. Do we really need to access the source data? Source hypothesis transfer for unsupervised domain adaptation. In International Conference on Machine Learning (ICML), pages 6028–6039, 2020. 12[34] Sen Lin, Peizhong Ju, Yingbin Liang, and Ness Shroff. Theory on forgetting and generalization of continual learning. In Proceedings of the 40th International Conference on Machine Learning, ICML’23, 2023. [35] TorchVision maintainers and contributors. Torchvision: Pytorch’s computer vision library. https: //github.com/pytorch/vision, 2016. [36] Robert A Marsden, Mario Döbler, and Bin Yang. Gradual test-time adaptation by self-training and style transfer. arXiv preprint arXiv:2208.07736, 2022. [37] Robert A Marsden, Mario Döbler, and Bin Yang. Universal test-time adaptation through weight ensem- bling, diversity weighting, and prior correction. In Proceedings of the IEEE/CVF Winter Conference on Applications of Computer Vision, pages 2555–2565, 2024. [38] Ben Mildenhall, Pratul P. Srinivasan, Matthew Tancik, Jonathan T. Barron, Ravi Ramamoorthi, and Ren Ng. NeRF: Representing scenes as neural radiance fields for view synthesis. In Proceedings of the European Conference on Computer Vision (ECCV), 2020. [39] A. Tuan Nguyen, Thanh Nguyen-Tang, Ser-Nam Lim, and Philip Torr. TIPI: Test time adaptation with transformation invariance. In Conference on Computer Vision and Pattern Recognition 2023, 2023. URL https://openreview.net/forum?id=NVh1cy37Ge. [40] Shuaicheng Niu, Jiaxiang Wu, Yifan Zhang, Yaofo Chen, Shijian Zheng, Peilin Zhao, and Mingkui Tan. Efficient test-time model adaptation without forgetting. In The Internetional Conference on Machine Learning, 2022. [41] Shuaicheng Niu, Jiaxiang Wu, Yifan Zhang, Zhiquan Wen, Yaofo Chen, Peilin Zhao, and Mingkui Tan. Towards stable test-time adaptation in dynamic wild world. In The Eleventh International Conference on Learning Representations, 2023. URL https://openreview.net/forum?id=g2YraF75Tj. [42] K. R. Parthasarathy. Introduction to Probability and Measure , volume 33 of Texts and Readings in Mathematics. Hindustan Book Agency, Gurgaon, 2005. ISBN 978-81-85931-55-5 978-93-86279-27-9. doi: 10.1007/978-93-86279-27-9. URL http://link.springer.com/10.1007/978-93-86279-27-9 . [43] Adam Paszke, Sam Gross, Francisco Massa, Adam Lerer, James Bradbury, Gregory Chanan, Trevor Killeen, Zeming Lin, Natalia Gimelshein, Luca Antiga, Alban Desmaison, Andreas Köpf, Edward Yang, Zach DeVito, Martin Raison, Alykhan Tejani, Sasank Chilamkurthy, Benoit Steiner, Lu Fang, Junjie Bai, and Soumith Chintala. Pytorch: An imperative style, high-performance deep learning library, 2019. [44] Xingchao Peng, Qinxun Bai, Xide Xia, Zijun Huang, Kate Saenko, and Bo Wang. Moment matching for multi-source domain adaptation. In Proceedings of the IEEE International Conference on Computer Vision, pages 1406–1415, 2019. [45] Ori Press, Steffen Schneider, Matthias Kuemmerer, and Matthias Bethge. RDumb: A simple approach that questions our progress in continual test-time adaptation. In Thirty-seventh Conference on Neural Information Processing Systems, 2023. URL https://openreview.net/forum?id=VfP6VTVsHc. [46] Joaquin Quionero-Candela, Masashi Sugiyama, Anton Schwaighofer, and Neil D. Lawrence. Dataset Shift in Machine Learning. The MIT Press, 2009. ISBN 0262170051. [47] Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, Gretchen Krueger, and Ilya Sutskever. Learning transferable visual models from natural language supervision. In Marina Meila and Tong Zhang, editors, Proceedings of the 38th International Conference on Machine Learning, volume 139 of Proceedings of Machine Learning Research, pages 8748–8763. PMLR, 18–24 Jul 2021. URL https://proceedings. mlr.press/v139/radford21a.html. [48] Benjamin Recht, Rebecca Roelofs, Ludwig Schmidt, and Vaishaal Shankar. Do ImageNet classifiers generalize to ImageNet? In Kamalika Chaudhuri and Ruslan Salakhutdinov, editors, Proceedings of the 36th International Conference on Machine Learning , volume 97 of Proceedings of Machine Learning Research, pages 5389–5400. PMLR, 09–15 Jun 2019. URL https://proceedings.mlr.press/v97/ recht19a.html. [49] Mooweon Rhee and Tohyun Kim. Exploration and Exploitation, pages 543–546. Palgrave Macmillan UK, London, 2018. ISBN 978-1-137-00772-8. doi: 10.1057/978-1-137-00772-8_388. URL https: //doi.org/10.1057/978-1-137-00772-8_388 . [50] Matthew Riemer, Ignacio Cases, Robert Ajemian, Miao Liu, Irina Rish, Yuhai Tu, , and Gerald Tesauro. Learning to learn without forgetting by maximizing transfer and minimizing interference. In Interna- tional Conference on Learning Representations, 2019. URL https://openreview.net/forum?id= B1gTShAct7. [51] Tanin Sirimongkolkasem and Reza Drikvandi. On Regularisation Methods for Analysis of High Di- mensional Data. Annals of Data Science , 6(4):737–763, December 2019. ISSN 2198-5812. doi: 10.1007/s40745-019-00209-4. URL https://doi.org/10.1007/s40745-019-00209-4 . 13[52] Yongyi Su, Xun Xu, and Kui Jia. Towards real-world test-time adaptation: Tri-net self-training with balanced normalization. Proceedings of the AAAI Conference on Artificial Intelligence, 38(13):15126– 15135, 2024. [53] Yu Sun, Xiaolong Wang, Zhuang Liu, John Miller, Alexei Efros, and Moritz Hardt. Test-time training with self-supervision for generalization under distribution shifts. In Hal Daumé III and Aarti Singh, editors, Proceedings of the 37th International Conference on Machine Learning, volume 119 of Proceedings of Machine Learning Research, pages 9229–9248. PMLR, 13–18 Jul 2020. URL https://proceedings. mlr.press/v119/sun20b.html. [54] Richard S. Sutton and Andrew G. Barto. Reinforcement Learning: An Introduction. MIT Press, Cambridge, MA, 2018. [55] Antti Tarvainen and Harri Valpola. Mean teachers are better role models: Weight-averaged consistency targets improve semi-supervised deep learning results. In Proceedings of the 31st International Conference on Neural Information Processing Systems, NIPS’17, page 1195–1204, 2017. ISBN 9781510860964. [56] Daniel Vela, Andrew Sharp, Richard Zhang, Trang Nguyen, An Hoang, and Oleg S. Pianykh. Temporal quality degradation in AI models. Scientific Reports, 12(1):11654, July 2022. ISSN 2045-2322. doi: 10.1038/s41598-022-15245-z. URL https://www.nature.com/articles/s41598-022-15245-z . [57] Dequan Wang, Evan Shelhamer, Shaoteng Liu, Bruno Olshausen, and Trevor Darrell. Tent: Fully test-time adaptation by entropy minimization. In International Conference on Learning Representations, 2021. URL https://openreview.net/forum?id=uXl3bZLkr3c. [58] Jindong Wang, Cuiling Lan, Chang Liu, Yidong Ouyang, and Tao Qin. Generalizing to unseen domains: A survey on domain generalization. In Zhi-Hua Zhou, editor, Proceedings of the Thirtieth International Joint Conference on Artificial Intelligence, IJCAI-21, pages 4627–4635. International Joint Conferences on Artificial Intelligence Organization, 8 2021. doi: 10.24963/ijcai.2021/628. URL https://doi.org/10. 24963/ijcai.2021/628. Survey Track. [59] Qin Wang, Olga Fink, Luc Van Gool, and Dengxin Dai. Continual test-time domain adaptation. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pages 7201–7211, June 2022. [60] Zachary Young and Robert Steele. Empirical evaluation of performance degradation of machine learning-based predictive models – a case study in healthcare information systems. International Journal of Information Management Data Insights , 2(1):100070, 2022. ISSN 2667-0968. doi: https: //doi.org/10.1016/j.jjimei.2022.100070. URL https://www.sciencedirect.com/science/article/ pii/S2667096822000143. [61] Longhui Yuan, Binhui Xie, and Shuang Li. Robust test-time adaptation in dynamic scenarios. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 15922– 15932, 2023. [62] Hao Zhao, Yuejiang Liu, Alexandre Alahi, and Tao Lin. On pitfalls of test-time adaptation. In ICLR 2023 Workshop on Pitfalls of limited data and computation for Trustworthy ML , 2023. URL https: //openreview.net/forum?id=0Go_RsG_dYn. 14Persistent Test-time Adaptation in Recurring Testing Scenarios Technical Appendices Table of Contents A Related Work 16 B Proof of Lemmas and Theorems 16 B.1 Proof of Lemma 1 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 17 B.2 Proof of Lemma 2. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 17 B.3 Proof of Theorem 1 and Corollary 1. . . . . . . . . . . . . . . . . . . . . . . . 18 C Further Justifications on Gaussian Mixture Model Classifier 19 D Further Justifications on the Recurring Testing Scenario 20 D.1 Recurring TTA Follows the Design of a Practical TTA Stream . . . . . . . . . . 20 D.2 Recurring TTA as a Diagnostic Tool . . . . . . . . . . . . . . . . . . . . . . . . 20 D.3 Recurring TTA with Random Orders . . . . . . . . . . . . . . . . . . . . . . . 20 E Further Justifications on Persistent TTA (PeTTA) 21 E.1 Pseudo Code . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 21 E.2 Anchor Loss . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 22 E.3 The Use of the Memory Bank . . . . . . . . . . . . . . . . . . . . . . . . . . . 23 E.4 Empirical Mean and Covariant Matrix of Feature Vectors on the Source Dataset . 23 E.5 Novelty of PeTTA . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 24 F Additional Experimental Results of PeTTA 24 F.1 Performance of PeTTA Versus Compared Methods . . . . . . . . . . . . . . . . 24 F.2 An Inspection of PeTTA . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 25 F.3 Does Model Reset Help? . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 25 F.4 PeTTA with 40 Recurring Visits . . . . . . . . . . . . . . . . . . . . . . . . . . 27 F.5 The Sensitivity of Hyper-parameter Choices in PeTTA . . . . . . . . . . . . . . 27 F.6 More Details on the Ablation Study . . . . . . . . . . . . . . . . . . . . . . . . 27 F.7 More Confusion Matrices in Recurring TTA Setting . . . . . . . . . . . . . . . 29 G Experimental Details 29 G.1 Computing Resources . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 29 G.2 Experiments on CCC Testing Stream . . . . . . . . . . . . . . . . . . . . . . . 29 G.3 Test-time Adaptation Methods . . . . . . . . . . . . . . . . . . . . . . . . . . . 29 G.4 The Use of Existing Assets . . . . . . . . . . . . . . . . . . . . . . . . . . . . 30 15A Related Work Towards Robust and Practical TTA. While forming the basis, early single-target TTA ap- proaches [53, 57, 39, 41, 33] is far from practice. Observing the dynamic of many testing envi- ronments, a continual TTA setting is proposed where an ML model continuously adapts to a sequence of multiple shifts [36, 59]. Meanwhile, recent studies [15, 7] point out that the category distribution realistic streams is highly temporally correlated. Towards real-world TTA setting, Yuanet al. [61] launch the practical TTA which considers the simultaneous occurrence of the two aforementioned challenges. For a robust and gradual adaptation, an update via the mean teacher [55] mechanism is exploited in many continual TTA algorithms [59, 61, 12, 22]. To moderate the temporally correlated test stream, common approaches utilize a small memory bank for saving a category-balanced subset of testing samples [15, 61], inspired by the replay methods [50, 2] to avoid forgetting in the task of continual learning [34, 3, 11]. Our study emphasizes another perspective: beyond a supreme performance, a desirable TTA should also sustain it for an extended duration. Temporal Performance Degradation.By studying the quality of various ML models across multiple industry applications [56, 60] the issue of AI “aging\" with the temporal model degradation progress, even with data coming from a stable process has been confirmed. In TTA, the continuous changes of model parameters through gradient descent aggravate the situation, as also recently noticed in [45]. Apart from observation, we attempt to investigate and provide theoretical insights towards the mechanism of this phenomenon. Accumulated Errors in TTA. In TTA, the issue of accumulated error has been briefly acknowledged. Previous works strive to avoid drastic changes to model parameters as a good practice. Up to some degree, it helps to avoid performance degradation. Nevertheless, it is still unclear whether their effectiveness truly eliminates the risk. To preserve in-distribution performance, regularization [27, 40] or replaying of training samples at test-time [ 12] have been used. Other studies explore reset (recovering the initial model parameters) strategies [59, 45], periodically or upon the running entropy loss approaches a threshold [ 41]. Unfortunately, knowledge accumulated in the preceding steps will vanish, and a bad heuristic choice of threshold or period leads to highly frequent model resets. Noteworthy, tuning those hyper-parameters is exceedingly difficult due to the unavailability of the validation set [62]. LAME [ 7] suggests a post-processing step for adaptation (without updating the parameters). This approach, however, still limits the knowledge accumulation. Our PeTTA is reset-free by achieving an adaptable continual test-time training. B Proof of Lemmas and Theorems In this section, we prove the theoretical results regarding the ϵ−perturbed Gaussian Mixture Model Classifier (ϵ−GMMC) introduced in Sec. 3.2. We first briefly summarize the definition of model collapse and the static data stream assumption: Definition 1 (Model Collapse). A model is said to be collapsed from step τ ∈ T, τ <∞ if there exists a non-empty subset of categories ˜Y ⊂ Ysuch that Pr{Yt ∈ ˜Y} > 0 but the marginal Pr{ˆYt ∈ ˜Y} converges to zero in probability: lim t→τ Pr{ˆYt ∈ ˜Y} = 0. Assumption 1 (Static Data Stream). The marginal distribution of the true label follows the same Bernoulli distribution Ber(p0): p0,t = p0, (p1,t = p1 = 1 − p0), ∀t ∈ T. Preliminary. Following the same set of notations introduced in the main text, recall that we denoted py,t ∆ = Pr{Yt = y}, ˆpy,t ∆ = Pr{ˆYt = y} (marginal distribution of the true label Yt and pseudo label ˆYt receiving label y, respectively) and ϵt = Pr{Yt = 1|ˆYt = 0} (the false negative rate (FNR) of 16ϵ−GMMC). At testing step t, we obtain the following relations: EPt h Xt|ˆYt = 0 i = (1 − ϵt)µ0 + ϵtµ1, (9) EPt h Xt|ˆYt = 1 i = µ1, (10) VarPt \u0010 Xt|ˆYt = 0 \u0011 = (1 − ϵt)σ2 0 + ϵtσ2 1 + ϵt(1 − ϵt)(µ0 − µ1)2, (11) VarPt \u0010 Xt|ˆYt = 1 \u0011 = σ2 1. (12) In addition, under Assumption 1, the marginal distribution Pt(x) (also referred as data distribution in our setup) is: Pt(x) = N(x; p0µ0 + p1µ1, p0σ2 0 + p1σ2 1 + p0p1(µ0 − µ1)2) ∀t ∈ T. (13) B.1 Proof of Lemma 1 Lemma 1 (Increasing FNR). Under Assumption 1, a binary ϵ-GMMC would collapsed (Def. 1) with lim t→τ ˆp1,t = 0 (or lim t→τ ˆp0,t = 1, equivalently) if and only if lim t→τ ϵt = p1. Proof. Under Assumption 1, we have EPt [Xt] = p0µ0 + (1 − p0)µ1. Also note that: EPt [Xt] = EPt h EPt h Xt|ˆYt ii = EPt h Xt|ˆYt = 0 i ˆp0,t + EPt h Xt|ˆYt = 1 i ˆp1,t (14) = [(1 − ϵt)µ0 + ϵtµ1] ˆp0,t + µ1(1 − ˆp0,t) = [(1 − ϵt)ˆp0,t] µ0 + [1 − ˆp0,t(1 − ϵt)] µ1 = p0µ0 + (1 − p0)µ1, where the second equality follows Eqs. 9-10. Therefore: ˆp0,t = p0 1 − ϵt . (15) Eq. 15 shows positive correlation between ˆp0,t and ϵt. Given lim t→τ ϵt = p1, taking the limit introduces: lim t→τ ˆp0,t = lim t→τ p0 1 − ϵt = p0 1 − p1 = 1. Similarly, having lim t→τ ˆp0,t = 1, the false negative rate ϵt when t → τ is: lim t→τ ϵt = 1 − p0 = p1. Since ˆp0,t + ˆp1,t = 1, lim t→τ ˆp1,t = 0, equivalently. Towards the collapsing point, the model tends to predict a single label (class 0 in the current setup). In addition, the FNR of the model ϵt also raises correspondingly. B.2 Proof of Lemma 2. Lemma 2 (ϵ-GMMC After Collapsing ). For a binary ϵ-GMMC model, with Assumption 1, if lim t→τ ˆp1,t = 0 (collapsing), the cluster 0 in GMMC converges in distribution to a single-cluster GMMC with parameters: N(ˆµ0,t, ˆσ2 0,t) d. → N(p0µ0 + p1µ1, p0σ2 0 + p1σ2 1 + p0p1(µ0 − µ1)2). Proof. From Eqs. 9-10, under the increasing type II collapse of ϵ−GMMC setting, the perturbation does not affect the approximation of µ1. Meanwhile, when ϵt increases, one can expect that ˆµ0,t 17moves further away from µ0 toward µ1. Frist, the mean teacher model of GMMC (Eq. 4, main text) gives: EPt h ˆµ0,t|ˆYt = 1 i = EPt−1 [ˆµ0,t−1] , EPt h ˆµ0,t|ˆYt = 0 i = (1 − α)EPt−1 h ˆµ0,t−1|ˆYt = 0 i + αEPt h Xt|ˆYt = 0 i = (1 − α)EPt−1 [ˆµ0,t−1] + α \u0010 EPt h Xi|ˆYt = 0 i\u0011 , EPt h ˆµ1,t|ˆYt = 1 i = (1 − α)EPt−1 h ˆµ1,t−1|ˆYt = 1 i + αEPt h Xt|ˆYt = 1 i = (1 − α)EPt−1 [ˆµ1,t−1] + α \u0010 EPt h Xi|ˆYt = 1 i\u0011 , EPt h ˆµ1,t|ˆYt = 0 i = EPt−1 [ˆµ1,t−1] . By defining uy,t = EPt [ˆµy,t], we obtain the following recurrence relation between u0,t and u0,t−1: u0,t = EPt h ˆµ0,t|ˆYt = 0 i ˆp0,t + EPt h ˆµ0,t|ˆYt = 1 i ˆp1,t = \u0010 (1 − α)u0,t−1 + αEPt h Xt|ˆYt = 0 i\u0011 ˆp0,t + u0,t−1 ˆp1,t = [(1 − α)ˆp0,t + ˆp1,t] u0,t−1 + αˆp0,tEPt h Xt|ˆYt = 0 i = (1 − αˆp0,t)u0,t−1 + αˆp0,tEPt h Xt|ˆYt = 0 i = (1 − αˆp0,t)u0,t−1 + αˆp0,t [(1 − ϵt)µ0 + ϵtµ1] . (16) Given lim t→τ ˆp0,t = 1, it follows that lim t→τ ϵ0,t = p1 by Lemma 1. From this point: u0,t = (1 − α)u0,t−1 + α (p0µ0 + p1µ1) ∀t > τ. Taking the limit t → ∞: lim t→∞ u0,t = lim t→∞ (1 − α)u0,t−1 + α (p0µ0 + p1µ1) = lim t→∞ (1 − α)t ˆµ0,0 + α tX i=1 (1 − α)i−1 (p0µ0 + p1µ1) = lim t→∞ (1 − α)t ˆµ0,0 + (1 − (1 − α)t)(p0µ0 + p1µ1) = p0µ0 + p1µ1. The second equation is obtained by solving the recurrence relation. When lim t→τ ˆp0,t = 1, {ˆµy,t}y∈{0,1} becomes a deterministic values. Hence, giving uy,t = EPt [ˆµy,t] = ˆµ0,t(∀t > τ) and lim t→∞ ˆµ0,t = lim t→∞ u0,t = p0µ0 + p1µ1. (17) Repeating the steps above with Eqs. 11-12 in place of Eqs. 9-10, we obtain a similar result for σ2 0,t: lim t→∞ ˆσ2 0,t = p0σ2 0 + p1σ2 1 + p0p1(µ0 − µ1)2. (18) By Lévy’s continuity theorem (p. 302, [ 42]), from Eqs. 17-18, when t → ∞, the estimated distribution of the first cluster N(x; ˆµ0,tˆσ2 0,t) converges to the whole data distribution Pt(x) (Eq. 13) when collapsing. B.3 Proof of Theorem 1 and Corollary 1. Theorem 1 (Convergence of ϵ−GMMC). For a binary ϵ-GMMC model, with Assumption 1, let the distance from ˆµ0,t toward µ1 is d0→1 t = |EPt [ˆµ0,t] − µ1|, then: d0→1 t − d0→1 t−1 ≤ α · p0 · \u0012 |µ0 − µ1| −d0→1 t−1 1 − ϵt \u0013 . 18Proof. Substituting Eq. 15 into ˆp0,t of Eq. 16 gives: u0,t = \u0012 1 − αp0 1 − ϵt \u0013 u0,t−1 + αp0 1 − ϵt [(1 − ϵt)µ0 + ϵtµ1] . Hence, we have the distance from u0,t toward µ1: |u0,t − µ1| = \f\f\f\f \u0012 1 − αp0 1 − ϵt \u0013 u0,t−1 + αp0µ0 + αp0ϵtµ1 1 − ϵt − µ1 \f\f\f\f = \f\f\f\f \u0012 1 − αp0 1 − ϵt \u0013 (u0,t−1 − µ1) + αp0µ0 + αp0ϵtµ1 1 − ϵt − αp0µ1 1 − ϵt \f\f\f\f = \f\f\f\f \u0012 1 − αp0 1 − ϵt \u0013 (u0,t−1 − µ1) + αp0µ0 − αp0µ1(1 − ϵt) 1 − ϵt \f\f\f\f = \f\f\f\f \u0012 1 − αp0 1 − ϵt \u0013 (u0,t−1 − µ1) + αp0(µ0 − µ1) \f\f\f\f ≤ \u0012 1 − αp0 1 − ϵt \u0013 |u0,t−1 − µ1| + αp0|µ0 − µ1|. The last inequality holds due to the triangle inequality. Equivalently, |u0,t − µ1| − |u0,t−1 − µ1| ≤α · p0 · \u0012 |µ0 − µ1| −|u0,t−1 − µ1| 1 − ϵt \u0013 . Let d0→1 t = |EPt [ˆµ0,t] − µ1|, we conclude that: d0→1 t − d0→1 t−1 ≤ α · p0 · \u0012 |µ0 − µ1| −d0→1 t−1 1 − ϵt \u0013 . Corollary 1 (A Condition forϵ−GMMC Collapse). With fixedp0, α, µ0, µ1, ϵ−GMMC is collapsed if there exists a sequence of {ϵt}τ τ−∆τ (τ ≥ ∆τ > 0) such that: p1 ≥ ϵt > 1 − d0→1 t−1 |µ0 − µ1|, t ∈ [τ − ∆τ , τ]. Proof. Initialized at µ0, ϵ-GMMC is collapsing when ˆµ0,t converges to the mid-point p0µ0 + p1µ1 (Lemma 2), i.e., moving closer to µ1. From Thm. 1, the distance towards µ1 d0→1 t < d0→1 t−1 if |µ0 − µ1| −|u0,t−1 − µ1| 1 − ϵt < 0 ⇔ |µ0 − µ1| < |u0,t−1 − µ1| 1 − ϵt ⇔ ϵt > 1 − |u0,t−1 − µ1| |µ0 − µ1| . When there exists this sequence{ϵt}τ τ−∆τ (τ ≥ ∆τ > 0) it follows that d0→1 t < d0→1 t−1 and ϵt > ϵt−1 is guaranteed ∀t ∈ [τ − ∆τ , τ]. Hence, lim t→τ ϵt = p1 (model collapsed, by Lemma 1). C Further Justifications on Gaussian Mixture Model Classifier One may notice that in ϵ-GMMC (Sec. 4.2), the classifier is defined ft(x) = argmaxy∈Y Pr(x|y; θt) (maximum likelihood estimation) while in general, ft(x) = argmaxy∈Y Pr(y|x; θt) (maximum a posterior estimation), parameterized by a neural network. In this case, since the equal prior (i.e., Pr(y; θt) = Pr(y′; θt), ∀y, y′ ∈ C) is enforced in ϵ-GMMC, the two definitions are equivalent. Proof. Having: argmaxy∈Y Pr(y|x; θt) = argmaxy∈Y Pr(x|y; θt) Pr(y; θt)P y′∈Y Pr(x|y′; θt) Pr(y′; θt) = argmaxy∈Y Pr(x|y; θt). We conclude that the two definitions are equivalent. In fact, it is well-known that maximum likelihood estimation is a special case of maximum a posterior estimation when the prior is uniform. 19D Further Justifications on the Recurring Testing Scenario D.1 Recurring TTA Follows the Design of a Practical TTA Stream Note that in recurring TTA, besides the recurrence of environments (or corruptions) as in [59, 40], the distribution of class labels is also temporally correlated (non-i.i.d.) as suggested by [15, 61] to reflect the practical testing stream better. In short, recurring TTA is formed by recurring the environments of practical TTA scenario introduced in [61] multiple times (readers are encouraged to visit the original paper for additional motivations on this scenario). D.2 Recurring TTA as a Diagnostic Tool Noticeably, CoTTA [59] also performed 10-round repetition across multiple domain shifts to simulate a lifelong TTA testing stream just like our recurring TTA. However, the key difference is CoTTA assumes the distribution of class labels is i.i.d., which does not hold in many real-life testing scenarios as argued in [ 15, 61]. Our recurring TTA lifts this assumption and allows temporally correlated (non-i.i.d.) label distribution (more challenging, more practical). This extension allows recurring TTA to spot the risk of model collapse on CoTTA [59] and other methods. The over-simplicity of the repeating scheme in CoTTA for spotting performance degradation is also suggested in [45]. Clearly, it seems not to be a problem at first glance in Tab. 5 of [59] (CoTTA’s 10-round repetition), but in fact, the risk in CoTTA remains, as explored in our scenario and also on CCC [45]. The construction of our recurring TTA is notably simple - a technical effort to extend the testing stream. However, this simplicity is on purpose, serving as a diagnostic tool for lifelong continual TTA. Counterintuitively, our experiments on four different tasks with the latest methods verify that even if the model is exposed to the same environment(the most basic case), their adaptability and performance are still consistently reduced (demonstrated visually in Fig. 1, quantitatively in Sec. 5.3). We believe that the extensive testing stream by recurrence in our setup is a simple yet sufficient scenario to demonstrate the vulnerability of existing continual TTA methods when facing the issue of model collapse (compared to CCC [45], a notably more complicated scenario than our recurring TTA). Indeed, recurring shifts are sufficient to show this failure mode and any lifelong TTA method should necessarily be able to handle recurring conditions. D.3 Recurring TTA with Random Orders Recall that in Sec. 3.1,recurring TTAis constructed by repeatingthe same sequence of D distributions K times. For example, a sequence with K = 2 could be P1 → P2 → ··· → PD → P1 → P2 → ··· → PD. For simplicity and consistency that promote reproducibility, the same order of image corruptions (following [61]) is used for all recurrences. This section presents supplementary experimental findings indicating that the order of image corruptions within each recurrence, indeed, does not affect the demonstration of TTA model collapse and the performance of our PeTTA. Experiment Setup. We refer to the setting same-order as using one order of image corruptions in [61] for all recurrences (specifically, on CIFAR-10/100-C and ImageNet-C:motion → snow → fog → shot → defocus → contrast → zoom → brightness → frost → elastic → glass → gaussian → pixelated → jpeg → impulse). Conversely, in random-order, the order of image corruptions is randomly shuffled at the beginning of each recurrence. Hence, the corruption orders across K recurrences are now entirely different. We redo the experiment of the second setting three times (with different random seeds = 0, 1, 2). Nevertheless, different TTA methods are ensured to be evaluated on the same testing stream, since it is fixed after generation. Without updating its parameters, the performance of the source model is trivially independent of the order of corruptions. Experimental Result. The experimental results are visualized in Fig. 6. The first column plots the experiments under the same-order, while the remaining three columns plot the experiments in the random-order setting, with varying random seeds. Note that the message conveyed by each sub-figure entirely matches that of Fig. 1-right. Discussions. Clearly, a similar collapsing pattern is observed in all three TTA tasks, with three combinations of 20 image corruption orders. This pattern also matches the easiest setting using the same order of image corruptions we promoted in recurring TTA. 201 5 10 15 20 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 1 5 10 15 20 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 1 5 10 15 20 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 1 5 10 15 20 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 Same-order Random-order (seed=0) Random-order (seed=1) Random-order (seed=2) Testing Error Recurring TTA visit Recurring TTA visit Recurring TTA visit Recurring TTA visit (a) CIFAR-10 → CIFAR-10-C task. 1 5 10 15 20 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1.0 1 5 10 15 20 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1.0 1 5 10 15 20 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1.0 1 5 10 15 20 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1.0 Same-order Random-order (seed=0) Random-order (seed=1) Random-order (seed=2) Testing Error Recurring TTA visit Recurring TTA visit Recurring TTA visit Recurring TTA visit (b) CIFAR-100 → CIFAR-100-C task. 1 5 10 15 20 0.5 0.6 0.7 0.8 0.9 1.0 1 5 10 15 20 0.5 0.6 0.7 0.8 0.9 1.0 1 5 10 15 20 0.5 0.6 0.7 0.8 0.9 1.0 1 5 10 15 20 0.5 0.6 0.7 0.8 0.9 1.0 Same-order Random-order (seed=0) Random-order (seed=1) Random-order (seed=2) Testing Error Recurring TTA visit Recurring TTA visit Recurring TTA visit Recurring TTA visit (c) ImageNet → ImageNet-C task. Figure 6: Recurring TTA with different order of corruptions. This figure plots the testing error of two TTA approaches: RoTTA - - [61], and, PeTTA- - (ours), and source model-×- as a reference performance under our recurring TTA (with 20 visits) across three TTA tasks. On the same-order experiments (column 1), the same order of image corruptions is applied for all 20 visits. Meanwhile, in random-order, this order is reshuffled at the beginning of each visit (columns 2-4). Random-order experiments are redone three times with different random seeds. Here, we empirically validate that using the same order of domain shifts (image corruptions) in our recurring TTA is sufficient to showcase the model collapse and evaluate the persistence of our PeTTA. Best viewed in color. E Further Justifications on Persistent TTA (PeTTA) E.1 Pseudo Code We summarize the key steps of our proposed PeTTA in Alg. 1, with the key part (lines 4-13) highlighted in blue. Our approach fits well in the general workflow of a TTA algorithm, enhancing the regular mean-teacher update step. Appdx. E.5 elaborates more on our contributions in PeTTA, distinguishing them from other components proposed in previous work. The notations and definitions of all components follow the main text (described in detail in Sec. 4). On line 8 of Alg. 1, as a 21Algorithm 1 Persistent TTA (PeTTA) Input: Classification model ft and its deep feature extractor ϕθt, both parameterized by θt ∈ Θ. Testing stream {Xt}T t=0, initial model parameter (θ0), initial update rate (α0), regularization term coefficient (λ0), empirical mean ({µy 0}y∈Y) and covariant matrix ({Σy 0}y∈Y) of feature vectors in the training set, ˆµy t EMA update rate (ν). 1 ˆµy 0 ← µy 0, ∀y ∈ Y; // Initialization 2 for t ∈ [1, ··· , T] do 3 ˆYt ← ft−1(Xt) ; // Obtaining pseudo-labels for all samples in Xt 4 // Persistent TTA (PeTTA) 5 ˆYt ← n ˆY (i) t |i = 1, ··· , Nt o ; // Set of (unique) pseudo-labels in Xt 6 ¯γt ← 0 ; 7 for y ∈ ˆYt do 8 γy t ← 1 − exp \u0010 −(ˆµy t − µy 0)T (Σy 0)−1 (ˆµy t − µy 0) \u0011 ; // Divergence sensing term on category y 9 ¯γt ← ¯γt + γy t | ˆYt| ; // Average divergence sensing term for step t 10 ˆµy t ← (1 − ν)ˆµy t−1 + νϕθt−1 (Xt|ˆYt = y) ; // EMA update of ˆµy t for samples with ˆYt = y 11 end 12 λt ← ¯γt · λ0 ; // Computing adaptive regularization term coefficient 13 αt ← (1 − ¯γt) · α0 ; // Computing adaptive update rate 14 // Regular Mean-teacher Update 15 θ′ t ← Optim θ′∈Θ EPt h LCLS \u0010 ˆYt, Xt; θ′ \u0011 + LAL (Xt; θ′) i + λtR(θ′) ; // Student model update 16 θt ← (1 − αt)θt−1 + αtθ′ t. ; // Teacher model update 17 // Final prediction 18 yeild ft(Xt) ; // Returning the final inference with updated model ft 19 end shorthand notation, ϕθt−1 (Xt|ˆYt = y) denotes the empirical mean of all feature vectors of X(i) t (extracted by ϕθt−1 \u0010 X(i) t \u0011 ) if ˆY (i) t = y, i= 1, ··· , Nt in the current testing batch. E.2 Anchor Loss KL Divergence Minimization-based Interpretation of Anchor Loss. In Sec. 4, we claimed that minimizing the anchor loss LAL is equivalent to minimizing the relative entropy (or KL divergence) between the output probability of two models parameterized by θ0 and θ. Proof. Having: DKL (Pr(y|Xt; θ0)||Pr(y|Xt; θ)) = X y∈Y Pr(y|Xt; θ0) log Pr(y|Xt; θ0) Pr(y|Xt; θ) = − X y∈Y Pr(y|Xt; θ0) log Pr(y|Xt; θ) | {z } LAL(Xt;θ) −H(Pr(y|Xt; θ0))| {z } constant . Hence, argmin θ∈Θ LAL(Xt; θ) = argmin θ∈Θ DKL (Pr(y|Xt; θ0)||Pr(y|Xt; θ)) . 22Intuitively, a desirable TTA solution should be able to adapt to novel testing distributions on the one hand, but it should not significantly diverge from the initial model. LAL fits this purpose, constraining the KL divergence between two models at each step. Connections between Anchor Loss and Regularizer Term. While supporting the same objective (collapse prevention by avoiding the model significantly diverging from the source model), the major difference between Anchor loss ( LAL) and the Regularizer term ( R(θ)) is that the anchor loss operates on the probability space of model prediction while the regularizer term works on the model parameter spaces. Tab. 4 (lines 1 and 5) summarizes the ablation study when each of them is eliminated. We see the role of the regularization term is crucial for avoiding model collapse, while the anchor loss guides the adaptation under the drastic domain shift. Nevertheless, fully utilizing all components is suggested for maintaining TTA persistence. E.3 The Use of the Memory Bank The size of Memory Bank. The size of the memory bank in PeTTA is relatively small, equal to the size of one mini-batch for update (64 images, specifically). The Use of the Memory Bank in PeTTA is Fair with Respect To the Compared Methods.Our directly comparable method - RoTTA [61] also takes this advantage (referred to as category-balanced sampling, Sec. 3.2 of [ 61]). Hence, the comparison between PeTTA and RoTTA is fair in terms of additional memory usage. Noteworthy, the use of a memory bank is a common practice in TTA literature (e.g., [15, 8, 61]), especially in situations where the class labels are temporally correlated or non-i.i.d. distributed (as we briefly summarized in Appdx. A - Related Work section). CoTTA [59], EATA [40] and MECTA [ 22] (compared method) assume labels are i.i.d. distributed. Hence, a memory bank is unnecessary, but their performance under temporally correlated label distribution has dropped significantly as a trade-off. The RMT [12] (compared method) does not require a memory bank but it needs to cache a portion of the source training set for replaying (Sec. 3.3 in [12]) which even requires more resources than the memory bank. Eliminating the Need for a Memory Bank. As addressing the challenge of temporally correlated label distribution on the testing stream is not the focus of PeTTA, we have conveniently adopted the use of the memory bank proposed in [61]. Since this small additional memory requirement is not universally applied in every real-world scenario, we believe that this is a reasonable assumption, and commonly adopted in TTA practices. Nevertheless, exploring alternative ways for reducing the memory size (e.g., storing the embedded features instead of the original image) would be an interesting future direction. E.4 Empirical Mean and Covariant Matrix of Feature Vectors on the Source Dataset Two Ways of Computing µy 0 and Σy 0 in Practice. One may notice that in PeTTA, computing γy t requires the pre-computed empirical mean (µy 0) and covariance (Σy 0) of the source dataset . This requirement may not be met in real-world situations where the source data is unavailable. In practice, the empirical mean and covariance matrix computed on the source distribution can be provided in the following two ways: 1. Most ideally, these values are computed directly by inference on the entire training set once the model is fully trained. They will be provided alongside the source-distribution pre-trained model as a pair for running TTA. 2. With only the source pre-trained model available, assume we can sample a set of unlabeled data from the source distribution. The (pseudo) labels for them are obtained by inferring from the source model. Since the source model is well-performed in this case, using pseudo is approximately as good as the true label. Accessing the Source Distribution Assumption in TTA. In fact, the second way is typically assumed to be possible in previous TTA methods such as EATA [40], and MECTA [22] (a compared method) to estimate a Fisher matrix (for anti-forgetting regularization purposes). Our work - PeTTA follows the same second setup as the previous approaches mentioned above. A variation of RMT [12] (a compared method) approach even requires having the fully labeled source data available at test-time for source replaying (Sec. 3.3 of [12]). This variation is used for comparison in our experiments. 23We believe that having the empirical mean and covariant matrix pre-computed on a portion of the source distribution in PeTTA is a reasonable assumption . Even in the ideal way, revealing the statistics might not severely violate the risk of data privacy leakage or require notable additional computing resources. Number of Samples Needed for Computation. To elaborate more on the feasibility of setting (2) mentioned above, we perform a small additional experiment on the performance of PeTTA while varying the number of samples used for computing the empirical mean and covariant matrix on the source distribution. In this setting, we use the test set of CIFAR-10, CIFAR-100, DomainNet validation set of ImageNet (original images, without corruption, or the real domain test set of DomainNet), representing samples from the source distribution. The total number of images is 10, 000 in CIFAR-10/A00, 50, 000 in ImageNet, and 69, 622 in DomainNet. We randomly sample 25%, 50%, 75%, and 100% of the images in this set to run PeTTA for 20 rounds of recurring. The result is provided in Tab. 6 below. Table 6: Average classification error of PeTTA (across 20 visits) with varying sizes of source samples used for computing feature empirical mean (µy 0) and covariant matrix (Σy 0). TTA Task 25% 50% 75% 100% CIFAR-10→CIFAR-10-C 22.96 22.99 23.03 22.75 CIFAR-100→CIFAR-100-C 35.01 35.11 35.09 35.15 DomainNet:real→clip→paint→sketch 43.18 43.12 43.15 42.89 ImageNet→ImageNet-C 61.37 59.68 61.05 60.46 The default choice of PeTTA is using 100% samples of the validation set of the source dataset. However, we showcase that it is possible to reduce the number of unlabeled samples from the source distribution to compute the empirical mean and covariant matrix for PeTTA, without significantly impacting its performance. E.5 Novelty of PeTTA PeTTA is composed of multiple components. Among them, the anchor loss is an existing idea (examples of previous work utilizing this idea are [ 32, 12]). Similarly, the mean-teacher update; and regularization are well-established techniques and very useful for the continual or gradual TTA scenario. Hence, we do not aim to improve or alternate these components. Nevertheless, the novelty of our contribution is the sensing of the divergence and adaptive model update, in which the importance of minimizing the loss (adaptation) and regularization (collapse prevention) is changed adaptively. In short, we propose a harmonic way of combining those elements adaptively to achieve a persistent TTA process. The design of PeTTA draws inspiration from a theoretical analysis (Sec. 3.2), empirically surpassing both the conventional reset-based approach [45] (Appdx. F.3) and other continual TTA approaches [61, 12, 59, 22, 7] on our proposed recurring TTA (Sec. 3.1, Appdx. F.1), as well as the previously established CCC [45] benchmark. F Additional Experimental Results of PeTTA F.1 Performance of PeTTA Versus Compared Methods Performance on CIFAR-100-C and Domainnet Datasets. Due to the length constraint, the classification errors on the tasks CIFAR-100→CIFAR-100-C, and real → clipart, painting, sketch of DomainNet are provided in Tab. 7 and Tab. 8. To prevent model collapse, the adaptability of PeTTA is more constrained. As a result, it requires more time for adaptation initially (e.g., in the first visit) but remains stable thereafter. Generally, consistent trends and observations are identified across all four TTA tasks. Standard Deviation of PeTTA Performance Across Multiple Runs. For PeTTA experiments marked with (*) in Tab. 1, Tab. 2, Tab. 7, and Tab. 8, the average performance across five independent runs with different random seeds is reported. Due to the space constraint, the corresponding standard deviation values are now reported in Tab. 9. Generally, the average standard deviation across runs 24Table 7: Average classification error of the task CIFAR-100 → CIFAR-100-C in recurring TTA scenario. The lowest error is highlighted in bold, (∗)average value across 5 runs (different random seeds) is reported for PeTTA. Recurring TTA visit− − − − − − − − − − − − − − − − − − − − − − − − − →Method1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 Avg Source 46.5 46.5 LAME [7] 40.5 40.5 CoTTA [59]53.4 58.4 63.4 67.6 71.4 74.9 78.2 81.1 84.0 86.7 88.8 90.7 92.3 93.5 94.7 95.6 96.3 97.0 97.3 97.683.1EATA [40]88.5 95.0 96.8 97.3 97.4 97.2 97.2 97.3 97.4 97.5 97.5 97.5 97.6 97.7 97.7 97.7 97.8 97.8 97.7 97.796.9RMT [12]50.5 48.6 47.9 47.4 47.3 47.1 46.9 46.9 46.6 46.8 46.7 46.5 46.5 46.6 46.5 46.5 46.5 46.5 46.5 46.547.1MECTA [22]44.8 44.3 44.6 43.1 44.8 44.2 44.4 43.8 43.8 43.9 44.6 43.8 44.4 44.6 43.9 44.2 43.8 44.4 44.9 44.244.2RoTTA [61]35.5 35.2 38.5 41.9 45.3 49.2 52.0 55.2 58.1 61.5 64.6 67.5 70.7 73.2 75.4 77.1 79.2 81.5 82.8 84.561.4RDumb [45]36.7 36.7 36.6 36.6 36.7 36.8 36.7 36.5 36.6 36.5 36.7 36.6 36.5 36.7 36.5 36.6 36.6 36.7 36.6 36.536.6ROID [37]76.4 76.4 76.2 76.2 76.3 76.1 75.9 76.1 76.3 76.3 76.6 76.3 76.8 76.7 76.6 76.3 76.2 76.0 75.9 76.076.3TRIBE [52]33.8 33.335.334.935.335.137.1 37.2 37.2 39.1 39.2 41.1 41.0 43.1 45.1 45.1 45.0 44.9 44.9 44.939.6PeTTA(ours)(∗) 35.834.434.735.035.135.135.235.335.335.335.235.335.235.235.135.235.235.235.235.235.1 Table 8: Average classification error of the task real → clipart → painting → sketch on DomainNet dataset in recurring TTA scenario. Episodic TTA visit− − − − − − − − − − − − − − − − − − − − − − − − − →Method1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 Avg Source 45.3 45.3 LAME [7] 45.6 45.6 CoTTA [59]96.2 97.1 97.4 97.8 98.1 98.2 98.4 98.4 98.4 98.5 98.6 98.6 98.6 98.6 98.6 98.7 98.7 98.7 98.7 98.798.3RMT [12]76.2 77.1 77.3 77.3 77.2 77.1 76.8 76.9 76.5 76.4 76.4 76.3 76.4 76.2 76.2 76.1 76.4 76.1 76.0 75.876.5MECTA [22]94.6 98.4 98.6 98.8 99.1 99.0 99.0 99.0 99.0 99.0 99.0 99.0 99.0 99.0 99.0 99.0 99.0 99.0 99.0 99.098.7RoTTA [61]44.3 43.8 44.7 46.7 48.7 50.8 52.7 55.0 57.1 59.7 62.7 65.1 68.0 70.3 72.7 75.2 77.2 79.6 82.6 85.362.1RDumb [45]44.3 44.4 44.3 44.5 44.2 44.2 44.3 44.5 44.4 44.2 44.3 44.3 44.3 44.3 44.5 44.3 44.2 44.3 44.4 44.344.3PeTTA(ours)(∗) 43.842.642.342.342.642.842.843.042.942.943.143.042.943.043.043.143.042.842.942.942.9 stays within ±0.1% for small datasets (CIFAR-10-C, CIFAR-100-C) and±0.5% for larger datasets (ImageNet-C, DomainNet). Table 9: Mean and standard deviation classification error of PeTTA on the four datasets: CIFAR-10-C (CF-10-C), CIFAR-100-C (CF-100-C), DomainNet (DN), and ImageNet-C (IN-C) with recurring TTA scenario. Each experiment is run 5 times with different random seeds. Recurring TTA visit− − − − − − − − − − − − − − − − − − − − − − − − − →Dataset1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 Avg CF-10-C24.3 23.0 22.6 22.4 22.4 22.5 22.3 22.5 22.8 22.8 22.6 22.7 22.7 22.9 22.6 22.7 22.6 22.8 22.9 23.022.8±0.4±0.3±0.4±0.3±0.3±0.3±0.4±0.2±0.3±0.4±0.4±0.2±0.1±0.3±0.5±0.2±0.2±0.3±0.4±0.5 ±0.1 CF-100-C35.8 34.4 34.7 35.0 35.1 35.1 35.2 35.3 35.3 35.3 35.2 35.3 35.2 35.2 35.1 35.2 35.2 35.2 35.2 35.235.1±0.4±0.4±0.2±0.2±0.1±0.1±0.2±0.2±0.1±0.2±0.1±0.2±0.2±0.1±0.1±0.1±0.1±0.1±0.2±0.2 ±0.1 DN43.8 42.6 42.3 42.3 42.6 42.8 42.8 43.0 42.9 42.9 43.1 43.0 42.9 43.0 43.0 43.1 43.0 42.8 42.9 42.942.9±0.1±0.1±0.2±0.2±0.3±0.3±0.3±0.4±0.4±0.4±0.4±0.4±0.4±0.3±0.3±0.2±0.4±0.3±0.3±0.3 ±0.3 IN-C65.3 61.7 59.8 59.1 59.4 59.6 59.8 59.3 59.4 60.0 60.3 61.0 60.7 60.4 60.6 60.7 60.8 60.7 60.4 60.260.5±0.6±0.5±0.5±0.5±1.4±1.1±1.0±0.5±0.8±0.9±0.4±0.8±0.9±0.8±0.9±0.8±1.0±0.6±0.6±0.7 ±0.5 F.2 An Inspection of PeTTA In Fig. 7, we showcase an inspection of our PeTTA on the task CIFAR-10→ CIFAR-10-C [19] in a typical recurring TTA with 20 visits. Specifically, the visualizations of PeTTA parameters ( ¯γt, λt, and αt), adaptation losses (LCLS, LAL) and regularization term (R(θ)) are provided. Here, we observe the values of adaptive parameters λt and αt continuously changing through time, as the testing scenarios evolve during recurring TTA. This proposed mechanismstabilizes the value of the loss functions, and regularization term, balancing between the two primary objectives: adaptation and preventing model collapse. Thus, the error rate persists as a result. A similar pattern is observed on other datasets (CIFAR-100-C [19] and DomainNet [44]). F.3 Does Model Reset Help? Experiment Setup. We use the term “model reset” to represent the action of “reverting the current TTA model to the source model” . This straightforward approach is named RDumb [ 45]. We thoroughly conducted experiments to compare the performance of RDumb with PeTTA. The implementation of RDumb in this setting is as follows. We employ RoTTA [61] as the base test-time adaptor due to the characteristics of the practical TTA [ 61] stream. The model (including model 25parameters, the optimizer state, and the memory bank) is reset after adapting itself to T images.1 For each dataset, three values of this hyper-parameter T are selected: • T = 1, 000: This is the value selected by the RDumb’s authors [ 45]. Unless specifically stated, we use this value when reporting the performance of RDumb [45] in all other tables. • T = 10, 000 (CIFAR-10/100-C), T = 5, 000 (ImageNet-C) and T = 24, 237 (Domain- Net).2 This value is equal to the number of samples in the test set of a single corruption type, i.e., the model is reset exactly after visiting each Pi’s (see Sec. 3.1 for notations). For DomainNet [44], since the number of images within each domain is unequal, the average number of images is used instead. • T = 150, 000 (CIFAR-10/100-C), T = 75, 000 (ImageNet-C) and T = 72, 712 (Domain- Net). This number is equal to the number of samples in one recurrence of our recurring TTA, i.e., the model is reset exactly after visitingP1 → ··· → PD. Here, D = 15 - types of corruptions [19] for CIFAR-10/100-C and ImageNet-C and D = 3 for DomainNet (clipart, painting, sketch). For example, the model is reset 20 times within a recurring TTA setting with 20 recurrences under this choice of T. The second and the last reset scheme could be interpreted as assuming the model has access to an oracle model with a capability of signaling the transitions between domains, or recurrences. Typically, this is an unrealistic capability in real-world scenarios, and a desirable continual TTA algorithm should be able to operate independently without knowing when the domain shift happening. Experimental Results. An empirical comparison between RDumb [45] and our PeTTA are reported in Tab. 10, Tab. 11, Tab. 12 and Tab. 13 for all four tasks. Table 10: Average classification error comparison between RDumb [45] (a reset-based approach) with different reset frequencies and our PeTTA on CIFAR-10→ CIFAR-10-C task. Recurring TTA visit− − − − − − − − − − − − − − − − − − − − − − − − − →Reset Every1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 Avg T= 100031.1 32.1 32.3 31.6 31.9 31.8 31.8 31.9 31.9 32.1 31.7 32.0 32.5 32.0 31.9 31.6 31.9 31.4 32.3 32.431.9T= 1000025.8 25.9 26.5 26.1 26.4 25.4 25.8 25.8 26.1 26.2 26.1 26.1 26.1 26.1 26.1 25.9 25.5 25.5 25.7 26.226.0T= 15000024.8 25.3 24.3 24.1 25.3 25.4 25.4 24.5 25.0 24.9 25.0 24.8 25.0 24.5 24.9 24.1 24.0 24.7 24.9 24.424.8 PeTTA(ours)(∗) 24.323.022.622.422.422.522.322.522.822.822.622.722.722.922.622.722.622.822.923.022.8 Table 11: Average classification error comparison between RDumb [45] (a reset-based approach) with different reset frequencies and our PeTTA on CIFAR-100-C dataset. Recurring TTA visit− − − − − − − − − − − − − − − − − − − − − − − − − →Reset Every1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 Avg T= 100036.7 36.7 36.6 36.6 36.7 36.8 36.7 36.5 36.6 36.5 36.7 36.6 36.5 36.7 36.5 36.6 36.6 36.7 36.6 36.536.6T= 1000043.5 43.6 43.7 43.7 43.4 43.5 43.6 43.4 43.5 43.6 43.8 43.5 43.5 43.6 43.4 43.6 43.5 43.8 43.7 43.643.6T= 15000035.435.4 35.4 35.3 35.4 35.4 35.5 35.6 35.4 35.4 35.535.3 35.235.435.135.835.135.6 35.3 35.835.4 PeTTA(ours)(∗) 35.834.434.735.035.135.135.235.335.335.335.235.335.235.235.135.235.235.235.235.235.1 Table 12: Average classification error comparison between RDumb [45] (a reset-based approach) with different reset frequencies and our PeTTA on DomainNet dataset. Recurring TTA visit− − − − − − − − − − − − − − − − − − − − − − − − − →Reset Every1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 Avg T= 100044.3 44.4 44.3 44.5 44.2 44.2 44.3 44.5 44.4 44.2 44.3 44.3 44.3 44.3 44.5 44.3 44.2 44.3 44.4 44.344.3T= 2423744.1 44.3 43.9 44.2 44.1 44.3 44.2 44.4 44.1 44.1 44.0 44.3 44.1 44.0 44.0 44.2 44.1 44.1 44.1 44.444.1T= 7271244.3 44.3 44.0 44.3 44.1 44.3 44.2 44.4 44.2 44.1 44.0 44.1 44.2 44.1 44.1 44.1 44.1 44.0 44.0 44.344.2 PeTTA(ours)(∗) 43.842.642.342.342.642.842.843.042.942.943.143.042.943.043.043.143.042.842.942.942.9 Discussions. Across datasets and reset frequencies, our PeTTA approach is always better than RDumb [45]. The supreme performance holds even when RDumb has access to the oracle information that can reset the model exactly at the transition between each domain shift or recurrence. Importantly, this oracle information is typically unavailable in practice. 1A slight abuse of notation. T here is the number of images between two consecutive resets, following the notation on Sec. 3 of [45], not the sample indices in our notations. 2A subset of 5, 000 samples from ImageNet-C are selected following RobustBench [10] for a consistent evaluation with other benchmarks. 26Table 13: Average classification error comparison between RDumb [45] (a reset-based approach) with different reset frequencies and our PeTTA on ImageNet-C dataset. Recurring TTA visit− − − − − − − − − − − − − − − − − − − − − − − − − →Reset Every1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 Avg T= 100072.2 73.0 73.2 72.8 72.2 72.8 73.3 72.7 71.9 73.0 73.2 73.1 72.0 72.7 73.3 73.1 72.1 72.6 73.3 73.172.8T= 500070.2 70.8 71.6 72.1 72.4 72.6 72.9 73.1 73.2 73.6 73.7 73.9 74.0 74.0 74.3 74.1 74.1 73.8 73.5 71.973.0T= 7500067.0 67.1 67.2 67.5 67.5 67.6 67.8 67.6 67.6 67.6 67.5 67.7 67.6 67.9 68.1 67.9 67.4 67.5 67.7 67.567.6 PeTTA(ours)(∗) 65.361.759.859.159.459.659.859.359.460.060.361.060.760.460.660.760.860.760.460.260.5 Noteworthy, it is clear that the performance of RDumb varies when changing the choice of the reset frequency. For a given choice of T, the better performance on one dataset does not guarantee the same performance on other datasets. For example, T = 1, 000 - the best empirical value found by RDumb authors [45] on CCC, does not give the best performance on our recurring TTA scenario; the second choice of T negatively impact the performance on many tasks; the third choice gives the best results, but knowing this exact recurrence frequency of the testing stream is unrealistic. The result highlights the challenge in practice when tuning this parameter (too slow/frequent), especially in the TTA setting where a validation set is unavailable. Our PeTTA, in contrast, is reset-free. F.4 PeTTA with 40 Recurring Visits To demonstrate the persistence of PeTTA over an even longer testing stream, in Tab. 14 and Fig. 8, we provide the evaluation results of PeTTA on recurring with 40 recurrences. F.5 The Sensitivity of Hyper-parameter Choices in PeTTA Table 15: Sensitivity of PeTTA with different choices ofλ0. Dataset λ0 = 1e0 λ0 = 5e0 λ0 = 1e1 λ0 = 5e1 λ0 = 1e2 CIFAR-10-C 22.9 22.7 22.8 23.2 24.1 CIFAR-100-C 35.7 35.3 35.1 35.6 36.1 ImageNet-C 61.2 61.0 60.5 61.3 62.4 There are two hyper-parameters in PeTTA: α0 and λ0. The initial learning rate of α0 = 1e−3 is used for all experiments. We do not tune this hyper-parameter, and the choice of α0 is universal across all datasets, following the previous works/compared methods (e.g., RoTTA [61], CoTTA [59]). Since λ0 is more specific to PeTTA, we included a sensitive analysis with different choices of λ0 on PeTTA, evaluated with images from CIFAR-10/100-C and ImageNet-C in Tab. 15. Overall, the choice of λ0 is not extremely sensitive, and while the best value is1e1 on most datasets, other choices such as 5e0 or 5e1 also produce roughly similar performance. Selecting λ0 is intuitive, the larger value of λ0 stronger prevents the model from collapsing but also limits its adaptability as a trade-off. In action, λ0 is an initial value and will be adaptively scaled with the sensing model divergence mechanism in PeTTA, meaning it does not require careful tuning. More generally, this hyper- parameter can be tuned similarly to the hyper-parameters of other TTA approaches, via an additional validation set, or some accuracy prediction algorithm [29] when labeled data is unavailable. F.6 More Details on the Ablation Study We provide the detailed classification error for each visit in the recurring TTA setting of each row entry in Tab. 4 (PeTTA Ablation Study): Tab. 16, Tab. 17, Tab. 18, Tab. 19; and Tab. 5 (PeTTA with various choices of regularizers): Tab. 20, Tab. 21, Tab. 22, Tab. 23. Fig. 9 presents an additional examination of the ablation study conducted on the task CIFAR-100 → CIFAR-100-C [19] for our PeTTA approach. We plot the classification error (top) and the value of ¯γt (bottom) for various PeTTA variations. As the model diverges from the initial state, the value of ¯γt increases. Unable to adjust αt or constraint the probability space via LAL limits the ability of PeTTA to prevent model collapse. In all variations with the model collapse in ablation studies, the rapid saturation of ¯γt is all observed. Therefore, incorporating all components in PeTTA is necessary. 27Table 16: Average classification error of multiple variations of PeTTA. Experiments on CIFAR10→ CIFAR10-C [19] task. Episodic TTA visit− − − − − − − − − − − − − − − − − − − − − − − − − →Method 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 Avg Baseline w/oR(θ) 23.5 24.0 27.4 29.9 33.4 35.6 38.0 40.7 43.1 45.0 46.0 48.6 50.0 49.7 50.8 51.5 52.3 53.3 54.3 55.542.6 R(θ)fixedλ= 0.1λ0 23.5 24.0 27.2 29.8 33.4 35.3 37.9 40.5 43.3 45.3 46.8 49.3 50.9 51.0 52.1 53.2 54.0 54.8 56.0 57.643.3R(θ)fixedλ=λ0 23.5 23.6 26.2 28.4 31.6 33.5 36.4 38.7 41.1 43.1 44.8 47.6 49.3 49.5 50.9 52.1 53.1 54.2 55.6 57.042.0 PeTTA-λt 24.9 25.3 26.0 26.4 27.2 26.5 27.2 27.1 27.4 27.7 27.8 28.0 27.5 28.0 27.7 27.4 27.0 27.6 27.8 27.827.1PeTTA-λt+αt 25.5 24.5 23.7 23.1 23.222.423.3 23.2 23.7 24.1 23.9 24.5 24.3 24.0 23.8 23.9 23.8 24.1 24.6 24.723.9PeTTA-λt+LAL 23.323.9 24.6 25.3 26.2 25.9 26.4 26.6 26.9 26.6 26.7 26.7 26.7 26.8 26.8 27.2 26.9 26.9 26.8 27.026.2 PeTTAαt+LAL 24.323.0 22.6 22.4 22.422.522.3 22.5 22.8 22.8 22.6 22.7 22.7 22.9 22.6 22.7 22.6 22.8 22.9 23.022.8 Table 17: Average classification error of multiple variations of PeTTA. Experiments on CIFAR-100 → CIFAR100-C [19] task. Episodic TTA visit− − − − − − − − − − − − − − − − − − − − − − − − − →Method 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 Avg Baseline w/oR(θ) 40.2 46.3 51.2 54.4 57.3 59.4 61.3 62.6 63.9 65.1 66.3 67.1 68.1 68.9 69.6 70.3 71.1 71.6 72.4 72.963.0 R(θ)fixedλ= 0.1λ0 40.5 46.1 51.5 55.1 58.2 60.5 62.6 64.2 65.7 67.3 68.6 69.5 70.6 71.6 72.5 73.4 74.2 74.9 75.8 76.565.0R(θ)fixedλ=λ0 41.8 47.6 52.6 56.1 58.9 60.7 62.5 63.9 65.0 66.2 67.1 68.3 69.5 70.3 71.4 72.4 73.4 74.1 75.0 75.664.6 PeTTA-λt 39.4 43.4 46.6 49.1 51.0 52.6 53.8 54.7 55.7 56.5 57.1 57.7 58.3 58.8 59.3 59.9 60.6 61.0 61.6 62.155.0PeTTA-λt+αt 39.4 40.1 40.8 40.7 41.2 41.5 41.4 41.6 41.5 41.5 41.7 41.6 41.8 41.7 41.8 42.0 41.9 41.9 42.0 41.841.4PeTTA-λt+LAL 36.2 35.6 35.7 36.1 36.2 36.4 36.4 36.5 36.2 36.2 36.6 36.5 36.5 36.6 36.5 36.6 36.5 36.5 36.3 36.536.3 PeTTAλt+αt+LAL 35.8 34.4 34.7 35.0 35.1 35.1 35.2 35.3 35.3 35.3 35.2 35.3 35.2 35.2 35.1 35.2 35.2 35.2 35.2 35.235.1 Table 18: Average classification error of multiple variations of PeTTA. Experiments onreal → clipart, painting, sketch task from DomainNet [44] task. Recurring TTA visit− − − − − − − − − − − − − − − − − − − − − − − − − →Method 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 Avg Baseline w/oR(θ) 52.3 69.0 68.6 68.6 69.4 70.5 71.8 73.4 75.6 77.6 78.8 81.0 82.8 84.3 85.9 87.4 88.5 89.9 90.8 92.177.9 R(θ)fixedλ= 0.1λ0 52.5 70.0 69.8 70.0 71.1 72.5 74.6 76.1 77.8 80.4 81.9 83.5 85.2 87.2 89.1 90.2 91.5 93.2 94.1 94.980.0R(θ)fixedλ=λ0 54.6 69.8 63.7 56.0 61.7 76.4 70.4 62.5 58.2 76.0 73.6 66.8 58.6 62.3 80.8 75.5 67.0 59.9 59.3 78.366.6 PeTTA-λt 49.2 64.5 62.4 60.9 59.6 58.6 57.7 57.8 57.6 57.7 58.0 58.5 59.0 59.5 59.8 61.1 62.0 62.6 63.6 64.959.7PeTTA-λt+αt 43.942.5 42.3 42.3 42.6 42.843.1 43.7 43.9 44.3 44.6 45.1 45.4 45.7 45.7 46.1 46.1 46.2 46.5 46.444.5PeTTA-λt+LAL 43.6 42.542.6 42.6 42.9 43.0 43.3 43.4 43.1 43.243.143.3 43.3 43.2 43.2 43.9 43.7 43.0 43.2 43.543.2 PeTTAλt+αt+LAL 43.8 42.642.3 42.3 42.6 42.8 42.8 43.0 42.9 42.9 43.1 43.0 42.9 43.0 43.0 43.1 43.0 42.8 42.9 42.942.9 Table 19: Average classification error of multiple variations of PeTTA. Experiments on ImageNet→ ImageNet-C [19] task. Recurring TTA visit− − − − − − − − − − − − − − − − − − − − − − − − − →Method 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 Avg Baseline w/oR(θ) 66.9 61.9 72.7 93.6 97.4 97.8 98.0 98.2 98.3 98.3 98.4 98.4 98.5 98.5 98.6 98.6 98.6 98.6 98.7 98.793.4 R(θ)fixedλ= 0.1λ0 65.5 70.9 79.1 85.2 90.3 92.6 95.8 95.8 95.4 97.3 96.9 97.7 97.9 98.2 98.0 98.7 98.6 98.4 98.4 98.792.5R(θ)fixedλ=λ0 66.5 62.1 73.0 93.5 97.0 97.2 97.5 97.5 97.6 97.5 97.7 97.7 97.7 97.8 97.9 97.9 98.0 98.0 98.0 97.992.9 PeTTA-λt 65.9 62.1 76.3 96.7 97.0 96.9 96.9 96.9 97.0 97.1 97.0 97.2 97.0 97.1 97.1 97.0 97.0 97.0 97.0 97.092.7PeTTA-λt+αt 64.870.5 74.6 75.8 75.5 75.8 76.1 76.2 76.2 76.5 76.7 77.0 76.9 77.4 77.1 77.3 77.2 77.4 77.6 77.475.7PeTTA-λt+LAL 64.8 61.160.0 59.8 60.4 60.4 61.2 61.2 61.8 61.9 62.1 62.2 62.1 62.9 62.1 62.8 62.7 62.1 62.8 66.662.0 PeTTA(ours)(∗) 65.3 61.7 59.8 59.1 59.4 59.6 59.8 59.3 59.4 60.0 60.3 61.0 60.7 60.4 60.6 60.7 60.8 60.7 60.4 60.260.5 Table 20: Average classification error of PeTTA with various choices of regularizers. Experiments on CIFAR-10 → CIFAR-10-C [19] task. Episodic TTA visit− − − − − − − − − − − − − − − − − − − − − − − − − →Method 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 Avg L2 25.6 24.8 23.8 23.1 23.2 22.7 23.0 22.7 22.7 22.7 22.8 22.7 22.8 22.7 22.522.3 22.2 22.4 22.7 22.823.0L2+Fisher25.2 23.7 22.5 21.8 22.3 21.5 22.3 22.1 22.5 22.8 22.6 22.622.622.8 22.6 22.9 22.6 22.9 23.0 23.322.7 Cosine 24.3 23.022.6 22.4 22.4 22.5 22.3 22.5 22.8 22.8 22.6 22.7 22.7 22.9 22.6 22.7 22.6 22.8 22.9 23.022.8Cosine+Fisher25.1 23.822.2 21.6 22.0 21.4 22.0 21.8 22.1 22.3 22.5 22.4 22.6 22.6 22.422.7 22.6 22.8 22.8 23.322.6 Table 21: Average classification error of PeTTA with various choices of regularizers. Experiments on CIFAR-100 → CIFAR-100-C [19] task. Recurring TTA visit− − − − − − − − − − − − − − − − − − − − − − − − − →Method 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 Avg L2 36.9 35.5 35.5 35.5 35.7 35.6 35.6 35.5 35.5 35.4 35.6 35.5 35.7 35.7 35.7 35.7 35.8 35.5 35.4 35.535.6L2+Fisher36.8 35.4 35.4 35.8 35.9 36.0 35.9 35.9 35.9 35.8 36.1 36.1 36.1 36.1 36.1 36.1 36.2 36.0 36.0 35.936.0 Cosine 35.8 34.4 34.7 35.0 35.1 35.1 35.2 35.3 35.3 35.3 35.2 35.3 35.2 35.2 35.1 35.2 35.2 35.2 35.2 35.235.1Cosine+Fisher36.7 35.2 35.5 35.6 35.9 35.9 36.1 36.0 36.0 35.9 36.0 36.0 36.0 36.1 36.0 36.0 35.9 35.9 35.9 36.035.9 28Table 22: Average classification error of PeTTA with various choices of regularizers. Experiments on real → clipart, painting, sketch task from DomainNet [44] dataset. Recurring TTA visit− − − − − − − − − − − − − − − − − − − − − − − − − →Method 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 Avg L2 43.8 42.7 42.5 42.4 42.8 42.9 43.0 43.1 43.1 43.2 43.4 43.3 43.2 43.3 43.2 43.2 43.4 43.0 43.1 43.143.1L2+Fisher43.9 42.8 42.7 43.0 43.2 43.4 43.6 43.8 43.9 44.1 44.0 44.2 44.2 44.2 44.4 44.4 44.5 44.5 44.5 44.543.9 Cosine 43.8 42.642.3 42.3 42.6 42.8 42.8 43.0 42.9 42.9 43.1 43.0 42.9 43.0 43.0 43.1 43.0 42.8 42.9 42.942.9Cosine+Fisher43.7 42.542.5 42.6 42.9 43.2 43.2 43.5 43.4 43.5 43.4 43.5 43.4 43.6 43.5 43.5 43.4 43.5 43.3 43.443.3 Table 23: Average classification error of PeTTA with various choices of regularizers. Experiments on ImageNet → ImageNet-C [19] task. Recurring TTA visit− − − − − − − − − − − − − − − − − − − − − − − − − →Method 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 Avg L2 70.8 72.2 71.5 69.8 72.3 69.3 70.3 70.5 70.0 70.8 70.2 72.1 71.4 70.8 70.9 70.9 69.7 71.0 71.1 70.470.8L2+Fisher70.5 70.0 69.5 69.4 69.6 69.9 69.2 69.3 72.2 70.4 71.0 70.5 71.7 71.5 71.3 68.4 68.6 68.8 68.7 68.770.0 Cosine 65.361.7 59.8 59.1 59.4 59.6 59.8 59.3 59.4 60.0 60.3 61.0 60.7 60.4 60.6 60.7 60.8 60.7 60.4 60.260.5Cosine+Fisher65.1 61.760.9 61.2 61.9 62.6 62.8 63.2 64.2 63.4 64.3 64.4 63.9 64.3 65.8 65.5 64.9 65.0 65.2 65.263.8 F.7 More Confusion Matrices in Recurring TTA Setting For the task CIFAR-10→ CIFAR-10-C [19] in recurring TTA setting (with 20 visits), we additionally showcase the confusion matrix of RoTTA [61] (Fig. 10) and our proposed PeTTA (Fig. 11) at each visit. Our PeTTA persistently achieves competitive performance across 20 visits while RoTTA [61] gradually degrades. G Experimental Details G.1 Computing Resources A computer cluster equipped with an Intel(R) Core(TM) 3.80GHz i7-10700K CPU, 64 GB RAM, and one NVIDIA GeForce RTX 3090 GPU (24 GB VRAM) is used for our experiments. G.2 Experiments on CCC Testing Stream In this section, we further evaluate the performance of our PeTTA on the testing data stream of Continuous Changing Corruption (CCC) [ 45] setting. Here we use the baseline accuracy 20%, transition speed 1000, and random seed 44.3 The compared methods are source model (ResNet 50), PeTTA, RoTTA [61], and RDumb [45]. Noteworthy, different from recurring TTA, the class labels here are i.i.d. distributed. The adaptation configuration of PeTTA follows the same settings as used on ImageNet-C, while the same setting introduced in Sec. F.3, with T = 1000 is used for RDumb [45]. G.3 Test-time Adaptation Methods Pre-trained Model on Source Distribution. Following previous studies [57, 61, 12, 59], only the batch norm layers are updated. As stated in Sec. 5.2, RobustBench [10] and torchvision [35] provide pre-trained models trained on source distributions. Specifically, for ImageNet-C and Do- mainNet experiments, a ResNet50 model [17] pre-trained on ImageNet V2 (specifically, checkpoint ResNet50_Weights.IMAGENET1K_V2 of torchvision) is used. From RobustBench, the model with checkpoint Standard and Hendrycks2020AugMix_ResNeXt [20] are adopted for CIFAR10-C and CIFAR-100-C experiments, respectively. Lastly, experiments on DomainNet dataset utilize the checkpoint (best_real_2020) provided in AdaContrast [8] study.4 Optimizer. Without specifically stated, Adam [26] optimizer with learning rate equal 1e−3, and β = (0.9, 0.999) is selected as a universal choice for all experiments. More Details on PeTTA. Since designing the batch normalization layers, and the memory bank is not the key focus of PeTTA, we conveniently adopt the implementation of the Robust Batch Norm layer and the Category-balanced Sampling strategy using a memory bank introduced in RoTTA [61]. 3https://github.com/oripress/CCC 4https://github.com/DianCh/AdaContrast 29G.4 The Use of Existing Assets Many components of PeTTA is utilized from the official repository of RoTTA [61] 5 and RMT [12]. 6 These two assets are released under MIT license. All the datasets, including CIFAR-10-C, CIFAR- 100-C and ImageNet-C [ 19] are publicly available online, released under Apache-2.0 license. 7 DomainNet dataset [44] (cleaned version) is also released for research purposes.8 5https://github.com/BIT-DA/RoTTA 6https://github.com/mariodoebler/test-time-adaptation 7https://github.com/hendrycks/robustness 8https://ai.bu.edu/M3SDA/ 300 10000 20000 30000 40000 Test-time adaptation step (t) 0.40 0.60 0.80 1.00 ¯γt 0 10000 20000 30000 40000 Test-time adaptation step (t) 4.00 6.00 8.00 10.00λt 0 10000 20000 30000 40000 Test-time adaptation step (t) 0.00 0.25 0.50 0.75 1.00αt 1e 3 0 10000 20000 30000 40000 Test-time adaptation step (t) 0.00 1.00 2.00 3.00 4.00LCLS 0 10000 20000 30000 40000 Test-time adaptation step (t) 0.00 2.50 5.00 7.50 10.00LAL 0 10000 20000 30000 40000 Test-time adaptation step (t) 0.00 0.50 1.00 1.50 2.00 R(θ) 0 10000 20000 30000 40000 Test-time adaptation step (t) 0.00 0.25 0.50 0.75 1.00Testing error Figure 7: An inspection of PeTTA on the task CIFAR-10 → CIFAR-10-C [19] in a recurring with 20 visits (visits are separated by the vertical dashed lines). Here, we visualize (rows 1-3) the dynamic of PeTTA adaptive parameters (¯γt, λt, αt), (rows 4-5) the value of the loss functions (LCLS, LAL) and (row 6) the value of the regularization term (R(θ)) and (row 7) the classification error rate at each step. The solid line in the foreground of each plot denotes the running mean. The plots show an adaptive change of λt, αt through time in PeTTA, which stabilizes TTA performance, making PeTTA achieve a persisting adaptation process in all observed values across 20 visits. 31Figure 8: Testing error of PeTTA with 40 recurring TTA visits. Total Visits CF-10-C CF-100-C IN-C 20 visits 22.8 35.1 60.5 40 visits 22.9 35.1 61.0 Table 14: Average testing error of PeTTA in recurring TTA with 20 and 40 visits. PeTTA demonstrates its persistence over an extended testing time horizon beyond the 20 th visit milestone (Fig. 8’s horizontal dashed line). 0 10000 20000 30000 40000 Test-time adaptation step (t) 0.20 0.30 0.40 0.50 0.60 0.70 0.80Testing Error PeTTA - λt Baseline w/o R(θ) PeTTA - λt + αt R(θ) fixed λ= 0.1λ0 PeTTA - λt + LAL R(θ) fixed λ= λ0 PeTTA - λt  + αt  + LAL 0 10000 20000 30000 40000 Test-time adaptation step (t) 0.20 0.40 0.60 0.80 1.00 ¯γt PeTTA - λt PeTTA - λt + αt PeTTA - λt + LAL PeTTA - λt  + αt  + LAL Figure 9: An inspection on the ablation study of multiple variations of PeTTA on the task CIFAR-100 → CIFAR-100-C [19] in an episodic TTA with 20 visits (visits are separated by the vertical dashed lines). (top): testing error of multiple variations of PeTTA. The performance of PeTTA without (w/o) R(θ), or fixed regularization coefficient ( λ = λ0/0.1λ0) degrades through time (the top 3 lines). The degradation of PeTTA -λt is still happening but at a slower rate (justification below). The performance of the other three variations persists through time with PeTTA -λt + αt + LAL achieves the best performance. (bottom): changes of ¯γt in multiple variations of PeTTA. When limiting the degree of freedom in adjusting αt or lacking of supervision from LAL (e.g., PeTTA -λt + αt, PeTTA -λt + LAL, and especially PeTTA -λt), the value of γt, unfortunately, escalates and eventually saturated. After this point, PeTTA has the same effect as using a fixed regularization coefficient. Therefore, fully utilizing all components is necessary to preserve the persistence of PeTTA. Best viewed in color. 320: airplane 1: auto2: bird3: cat4: deer5: dog6: frog7: horse8: ship9: truck 0 1 2 3 4 5 6 7 8 9 0.79 0.01 0.04 0.03 0.02 0.01 0.01 0.02 0.05 0.02 0.02 0.82 0.01 0.01 0 0.01 0.01 0.01 0.01 0.09 0.06 0 0.68 0.07 0.04 0.03 0.06 0.03 0.01 0.01 0.02 0.01 0.04 0.66 0.04 0.08 0.07 0.05 0.01 0.02 0.03 0 0.04 0.06 0.68 0.02 0.06 0.09 0.01 0.01 0.03 0 0.05 0.15 0.03 0.61 0.03 0.07 0.01 0.01 0.02 0.01 0.03 0.07 0.02 0.02 0.8 0.02 0 0.01 0.01 0 0.02 0.03 0.03 0.02 0.01 0.87 0 0.01 0.09 0.02 0.02 0.02 0.01 0 0.02 0.01 0.77 0.04 0.03 0.03 0.01 0.01 0 0 0.01 0.01 0.03 0.85 0: airplane 1: auto2: bird3: cat4: deer5: dog6: frog7: horse8: ship9: truck 0 1 2 3 4 5 6 7 8 9 0.76 0.01 0.03 0.03 0.01 0 0.03 0.02 0.07 0.03 0.02 0.76 0 0.01 0 0 0.03 0.01 0.02 0.16 0.07 0 0.63 0.08 0.06 0.02 0.08 0.04 0.01 0.01 0.02 0 0.04 0.7 0.04 0.04 0.09 0.05 0.01 0.02 0.03 0 0.03 0.05 0.73 0.01 0.06 0.08 0.01 0.01 0.01 0 0.03 0.23 0.04 0.53 0.06 0.08 0.01 0.01 0.02 0 0.02 0.1 0.02 0.01 0.81 0.01 0 0.01 0.01 0 0.01 0.05 0.03 0.01 0.01 0.87 0 0.01 0.08 0.01 0.01 0.02 0.01 0 0.02 0.01 0.8 0.04 0.03 0.02 0.01 0.02 0 0 0.02 0.01 0.02 0.87 0: airplane 1: auto2: bird3: cat4: deer5: dog6: frog7: horse8: ship9: truck 0 1 2 3 4 5 6 7 8 9 0.7 0.01 0.03 0.04 0.02 0 0.03 0.03 0.09 0.06 0.01 0.72 0 0.01 0 0 0.04 0 0.01 0.2 0.07 0 0.56 0.1 0.08 0.02 0.09 0.04 0.01 0.02 0.01 0 0.03 0.7 0.05 0.02 0.13 0.04 0 0.02 0.04 0 0.03 0.07 0.69 0 0.08 0.07 0.01 0.01 0.01 0 0.04 0.26 0.05 0.42 0.13 0.07 0 0.01 0.01 0 0.02 0.11 0.03 0 0.8 0.01 0 0.01 0.01 0 0.02 0.06 0.05 0.01 0.04 0.8 0 0.01 0.07 0.01 0.01 0.03 0.01 0 0.03 0.01 0.78 0.05 0.02 0.01 0.01 0.02 0.01 0 0.04 0.01 0.02 0.86 0: airplane 1: auto2: bird3: cat4: deer5: dog6: frog7: horse8: ship9: truck 0 1 2 3 4 5 6 7 8 9 0.62 0.01 0.03 0.06 0.03 0 0.05 0.04 0.09 0.08 0.01 0.66 0 0.02 0.01 0 0.04 0 0.02 0.25 0.07 0 0.48 0.13 0.1 0.02 0.13 0.03 0.01 0.02 0.01 0 0.02 0.68 0.05 0.02 0.17 0.03 0 0.02 0.03 0 0.02 0.07 0.67 0 0.12 0.07 0.01 0.01 0.01 0 0.02 0.29 0.07 0.39 0.14 0.06 0 0.01 0.01 0 0.01 0.11 0.04 0 0.8 0.01 0 0.01 0.01 0 0.02 0.08 0.06 0.01 0.06 0.75 0 0.01 0.05 0.01 0.01 0.04 0.02 0 0.05 0.01 0.74 0.07 0.01 0.01 0 0.03 0.01 0 0.05 0.01 0.02 0.86 True label 1st visit 2nd visit 3rd visit 4th visit 0: airplane 1: auto2: bird3: cat4: deer5: dog6: frog7: horse8: ship9: truck 0 1 2 3 4 5 6 7 8 9 0.56 0 0.03 0.07 0.04 0 0.07 0.04 0.1 0.1 0.01 0.61 0 0.01 0.01 0 0.07 0 0.02 0.26 0.08 0 0.42 0.13 0.13 0.02 0.15 0.03 0.01 0.02 0.02 0 0.01 0.62 0.06 0.02 0.21 0.03 0 0.02 0.03 0 0.02 0.06 0.66 0 0.16 0.06 0.01 0.01 0.01 0 0.02 0.3 0.08 0.34 0.17 0.06 0 0.02 0.01 0 0.01 0.12 0.07 0 0.76 0.01 0 0.02 0.01 0 0.02 0.1 0.08 0.01 0.08 0.69 0 0.02 0.05 0.01 0.01 0.05 0.02 0 0.09 0.01 0.68 0.09 0.01 0.01 0 0.03 0.02 0 0.09 0.01 0.02 0.83 0: airplane 1: auto2: bird3: cat4: deer5: dog6: frog7: horse8: ship9: truck 0 1 2 3 4 5 6 7 8 9 0.51 0 0.02 0.07 0.04 0 0.09 0.03 0.1 0.14 0.01 0.56 0 0.01 0.02 0 0.09 0 0.02 0.29 0.08 0 0.35 0.15 0.16 0.02 0.18 0.03 0.01 0.03 0.02 0 0.01 0.57 0.07 0.02 0.27 0.02 0 0.03 0.04 0 0.01 0.08 0.62 0 0.18 0.05 0.01 0.01 0.01 0 0.01 0.29 0.09 0.3 0.21 0.05 0 0.02 0.01 0 0.01 0.12 0.09 0 0.75 0 0 0.01 0.02 0 0.01 0.11 0.12 0.01 0.1 0.6 0 0.03 0.06 0.01 0 0.04 0.02 0 0.09 0 0.66 0.11 0.01 0.01 0 0.02 0.03 0 0.11 0 0.02 0.8 0: airplane 1: auto2: bird3: cat4: deer5: dog6: frog7: horse8: ship9: truck 0 1 2 3 4 5 6 7 8 9 0.48 0 0.02 0.08 0.04 0 0.11 0.03 0.11 0.13 0.01 0.54 0 0.01 0.02 0 0.11 0 0.02 0.28 0.09 0 0.3 0.16 0.16 0.02 0.21 0.02 0.01 0.03 0.02 0 0.01 0.51 0.08 0.01 0.33 0.01 0.01 0.02 0.03 0 0.01 0.05 0.65 0 0.21 0.03 0.01 0.01 0.02 0 0.01 0.27 0.11 0.25 0.28 0.03 0 0.02 0.01 0 0.01 0.12 0.1 0 0.75 0 0 0.01 0.02 0 0.01 0.11 0.13 0.01 0.13 0.56 0 0.03 0.06 0 0 0.06 0.03 0 0.13 0 0.6 0.11 0.02 0.01 0 0.03 0.04 0 0.15 0 0.02 0.73 0: airplane 1: auto2: bird3: cat4: deer5: dog6: frog7: horse8: ship9: truck 0 1 2 3 4 5 6 7 8 9 0.46 0 0.01 0.07 0.06 0 0.13 0.01 0.09 0.15 0.01 0.48 0 0.01 0.04 0 0.16 0 0.01 0.28 0.09 0 0.27 0.15 0.19 0.01 0.23 0.01 0.01 0.03 0.02 0 0.01 0.44 0.12 0.01 0.37 0.01 0.01 0.02 0.04 0 0.01 0.05 0.63 0 0.23 0.02 0.01 0.01 0.02 0 0.01 0.25 0.13 0.22 0.33 0.02 0 0.01 0.01 0 0 0.11 0.15 0 0.71 0 0 0.01 0.02 0 0.01 0.09 0.22 0 0.15 0.47 0 0.02 0.08 0 0 0.06 0.05 0 0.15 0 0.55 0.1 0.02 0.01 0 0.04 0.05 0 0.16 0 0.02 0.7 True label 5th visit 6th visit 7th visit 8th visit 0: airplane 1: auto2: bird3: cat4: deer5: dog6: frog7: horse8: ship9: truck 0 1 2 3 4 5 6 7 8 9 0.47 0 0.01 0.06 0.06 0 0.13 0.01 0.1 0.16 0.02 0.47 0 0.01 0.04 0 0.13 0 0.03 0.29 0.1 0 0.24 0.12 0.22 0.01 0.24 0.01 0.01 0.03 0.03 0 0 0.4 0.12 0.01 0.39 0 0.01 0.02 0.05 0 0.01 0.06 0.61 0 0.23 0.02 0.01 0.01 0.03 0 0.01 0.22 0.15 0.2 0.35 0.02 0 0.02 0.01 0 0 0.11 0.15 0 0.7 0 0.01 0.01 0.03 0 0.01 0.08 0.25 0 0.15 0.44 0.01 0.03 0.09 0 0 0.04 0.07 0 0.14 0 0.55 0.1 0.02 0.01 0 0.03 0.05 0 0.16 0 0.02 0.7 0: airplane 1: auto2: bird3: cat4: deer5: dog6: frog7: horse8: ship9: truck 0 1 2 3 4 5 6 7 8 9 0.46 0 0.01 0.05 0.07 0 0.14 0.01 0.1 0.16 0.04 0.43 0 0.02 0.07 0 0.14 0 0.03 0.27 0.11 0 0.22 0.11 0.23 0.01 0.26 0.01 0.02 0.03 0.04 0 0 0.33 0.16 0.01 0.43 0 0.01 0.02 0.05 0 0 0.03 0.66 0 0.23 0.01 0.01 0.01 0.04 0 0.01 0.22 0.15 0.18 0.37 0.01 0.01 0.02 0.01 0 0 0.1 0.19 0 0.68 0 0.01 0.01 0.03 0 0.01 0.08 0.28 0.01 0.16 0.41 0.01 0.03 0.11 0 0 0.04 0.05 0 0.14 0 0.56 0.09 0.04 0.01 0 0.02 0.08 0 0.18 0 0.02 0.65 0: airplane 1: auto2: bird3: cat4: deer5: dog6: frog7: horse8: ship9: truck 0 1 2 3 4 5 6 7 8 9 0.47 0 0.01 0.04 0.07 0 0.14 0 0.1 0.16 0.04 0.42 0 0.01 0.07 0 0.15 0 0.05 0.26 0.11 0 0.21 0.1 0.26 0.01 0.26 0 0.02 0.03 0.05 0 0 0.31 0.18 0.01 0.42 0 0.01 0.02 0.06 0 0 0.04 0.65 0 0.21 0.01 0.01 0.02 0.04 0 0.01 0.17 0.21 0.15 0.39 0.01 0.01 0.02 0.01 0 0 0.1 0.24 0 0.64 0 0.01 0.01 0.04 0 0.01 0.09 0.28 0 0.16 0.39 0 0.03 0.14 0 0 0.03 0.07 0 0.14 0 0.52 0.09 0.05 0.01 0 0.03 0.1 0 0.18 0 0.03 0.61 0: airplane 1: auto2: bird3: cat4: deer5: dog6: frog7: horse8: ship9: truck 0 1 2 3 4 5 6 7 8 9 0.49 0 0.01 0.03 0.06 0 0.14 0 0.11 0.17 0.07 0.4 0 0.01 0.07 0 0.12 0 0.07 0.27 0.13 0 0.19 0.08 0.27 0.01 0.25 0 0.02 0.03 0.07 0 0 0.27 0.19 0 0.43 0 0.02 0.03 0.07 0 0 0.02 0.64 0 0.23 0.01 0.01 0.01 0.06 0 0.01 0.19 0.18 0.13 0.39 0.01 0.01 0.02 0.02 0 0 0.09 0.22 0 0.65 0 0.01 0.01 0.05 0 0 0.07 0.32 0 0.15 0.36 0.01 0.04 0.17 0 0 0.03 0.07 0 0.12 0 0.53 0.08 0.06 0.01 0 0.01 0.13 0 0.17 0 0.03 0.59 True label 9th visit 10th visit 11th visit 12th visit 0: airplane 1: auto2: bird3: cat4: deer5: dog6: frog7: horse8: ship9: truck 0 1 2 3 4 5 6 7 8 9 0.5 0 0 0.02 0.08 0 0.13 0 0.1 0.15 0.09 0.37 0 0.01 0.11 0 0.11 0 0.08 0.24 0.15 0 0.18 0.07 0.31 0.01 0.24 0 0.03 0.02 0.09 0 0 0.24 0.17 0 0.44 0 0.02 0.03 0.08 0 0 0.02 0.66 0 0.19 0.01 0.02 0.02 0.08 0 0.01 0.15 0.23 0.11 0.38 0.01 0.01 0.02 0.02 0 0 0.08 0.31 0 0.55 0 0.02 0.01 0.05 0 0 0.05 0.37 0 0.14 0.34 0.01 0.04 0.2 0 0 0.03 0.06 0 0.12 0 0.52 0.08 0.08 0.01 0 0.01 0.11 0 0.15 0 0.04 0.59 0: airplane 1: auto2: bird3: cat4: deer5: dog6: frog7: horse8: ship9: truck 0 1 2 3 4 5 6 7 8 9 0.54 0 0 0.02 0.06 0 0.11 0 0.12 0.15 0.13 0.35 0 0.01 0.1 0 0.09 0 0.12 0.21 0.16 0 0.18 0.07 0.29 0.01 0.24 0 0.03 0.02 0.11 0 0 0.22 0.19 0 0.42 0 0.03 0.03 0.08 0 0 0.03 0.65 0 0.2 0.01 0.02 0.01 0.09 0 0.01 0.12 0.29 0.08 0.37 0 0.02 0.02 0.02 0 0 0.09 0.29 0 0.56 0 0.02 0.01 0.06 0 0 0.05 0.39 0 0.13 0.32 0.01 0.04 0.23 0 0 0.02 0.07 0 0.1 0 0.51 0.07 0.12 0.01 0 0.01 0.11 0 0.13 0 0.05 0.57 0: airplane 1: auto2: bird3: cat4: deer5: dog6: frog7: horse8: ship9: truck 0 1 2 3 4 5 6 7 8 9 0.56 0 0 0.02 0.08 0 0.1 0 0.12 0.12 0.18 0.32 0 0 0.11 0 0.08 0 0.13 0.19 0.18 0 0.15 0.05 0.34 0 0.2 0 0.04 0.02 0.12 0 0 0.19 0.27 0 0.36 0 0.04 0.02 0.09 0 0 0.02 0.69 0 0.15 0.01 0.02 0.02 0.11 0 0 0.1 0.33 0.07 0.33 0 0.03 0.01 0.03 0 0 0.09 0.35 0 0.5 0 0.02 0.01 0.08 0 0 0.04 0.43 0 0.1 0.29 0.01 0.04 0.26 0 0 0.02 0.08 0 0.08 0 0.51 0.06 0.15 0.01 0 0.01 0.12 0 0.1 0 0.07 0.55 0: airplane 1: auto2: bird3: cat4: deer5: dog6: frog7: horse8: ship9: truck 0 1 2 3 4 5 6 7 8 9 0.58 0 0 0.01 0.07 0 0.09 0 0.13 0.11 0.16 0.32 0 0 0.11 0 0.07 0 0.16 0.18 0.18 0 0.15 0.05 0.36 0 0.19 0 0.04 0.02 0.14 0 0 0.18 0.26 0 0.35 0 0.05 0.02 0.1 0 0 0.01 0.69 0 0.15 0.01 0.03 0.01 0.11 0 0 0.1 0.36 0.05 0.32 0 0.04 0.01 0.03 0 0 0.08 0.38 0 0.46 0 0.03 0.01 0.09 0 0 0.04 0.43 0 0.09 0.29 0.02 0.04 0.29 0 0 0.02 0.09 0 0.08 0 0.47 0.06 0.18 0.01 0 0.01 0.11 0 0.08 0 0.1 0.5 True label 13th visit 14th visit 15th visit 16th visit 0: airplane 1: auto2: bird3: cat4: deer5: dog6: frog7: horse8: ship9: truck 0 1 2 3 4 5 6 7 8 9 0.6 0 0 0.01 0.08 0 0.08 0 0.13 0.1 0.2 0.28 0 0 0.1 0 0.06 0 0.19 0.17 0.2 0 0.14 0.05 0.36 0 0.18 0 0.05 0.02 0.17 0 0 0.16 0.28 0 0.29 0 0.08 0.02 0.1 0 0 0.01 0.71 0 0.11 0.01 0.04 0.02 0.13 0 0 0.1 0.4 0.04 0.27 0 0.05 0.01 0.04 0 0 0.09 0.4 0 0.41 0 0.04 0.01 0.1 0 0 0.04 0.45 0 0.07 0.27 0.03 0.04 0.34 0 0 0.01 0.08 0 0.05 0 0.47 0.05 0.22 0.01 0 0.01 0.13 0 0.06 0 0.12 0.44 0: airplane 1: auto2: bird3: cat4: deer5: dog6: frog7: horse8: ship9: truck 0 1 2 3 4 5 6 7 8 9 0.62 0 0 0.01 0.09 0 0.08 0 0.13 0.08 0.24 0.26 0 0 0.1 0 0.05 0 0.19 0.15 0.2 0 0.13 0.04 0.41 0 0.16 0 0.05 0.02 0.16 0 0 0.14 0.3 0 0.29 0 0.09 0.02 0.11 0 0 0.01 0.7 0 0.1 0.01 0.05 0.02 0.14 0 0 0.09 0.42 0.02 0.27 0 0.05 0.01 0.03 0 0 0.09 0.44 0 0.39 0 0.04 0.01 0.12 0 0 0.04 0.43 0 0.06 0.28 0.03 0.04 0.35 0 0 0.01 0.07 0 0.06 0 0.46 0.04 0.26 0.01 0 0.01 0.13 0 0.06 0 0.13 0.41 0: airplane 1: auto2: bird3: cat4: deer5: dog6: frog7: horse8: ship9: truck 0 1 2 3 4 5 6 7 8 9 0.67 0 0 0 0.1 0 0.05 0 0.11 0.06 0.3 0.21 0 0 0.1 0 0.03 0 0.21 0.13 0.26 0 0.11 0.04 0.4 0 0.11 0 0.06 0.02 0.2 0 0 0.13 0.32 0 0.21 0 0.12 0.02 0.13 0 0 0.01 0.72 0 0.07 0.01 0.05 0.01 0.2 0 0 0.09 0.42 0.01 0.19 0 0.08 0.01 0.04 0 0 0.08 0.49 0 0.3 0 0.07 0.01 0.16 0 0 0.03 0.45 0 0.04 0.24 0.05 0.03 0.42 0 0 0.01 0.06 0 0.04 0 0.43 0.04 0.33 0.01 0 0 0.11 0 0.03 0 0.15 0.36 0: airplane 1: auto2: bird3: cat4: deer5: dog6: frog7: horse8: ship9: truck 0 1 2 3 4 5 6 7 8 9 0.69 0 0 0 0.08 0 0.04 0 0.13 0.05 0.34 0.17 0 0 0.1 0 0.03 0 0.23 0.13 0.24 0 0.1 0.03 0.44 0 0.11 0 0.07 0.01 0.21 0 0 0.11 0.32 0 0.21 0 0.14 0.01 0.14 0 0 0.01 0.71 0 0.06 0.01 0.06 0.01 0.21 0 0 0.07 0.44 0 0.16 0 0.1 0.01 0.05 0 0 0.08 0.51 0 0.25 0 0.1 0.01 0.17 0 0 0.03 0.46 0 0.04 0.21 0.06 0.04 0.46 0 0 0.01 0.06 0 0.03 0 0.41 0.03 0.34 0 0 0 0.12 0 0.03 0 0.18 0.32 Predicted label Predicted label Predicted label Predicted label True label 17th visit 18th visit 19th visit 20th visit Figure 10: The dynamic of the confusion matrix of RoTTA [61] in episodic TTA with 20 visits. 330: airplane 1: auto2: bird3: cat4: deer5: dog6: frog7: horse8: ship9: truck 0 1 2 3 4 5 6 7 8 9 0.77 0.01 0.04 0.03 0.03 0.01 0.02 0.02 0.05 0.02 0.02 0.84 0.01 0.02 0 0.01 0.02 0.01 0.02 0.06 0.04 0 0.69 0.07 0.05 0.05 0.05 0.02 0.01 0.01 0.04 0.01 0.05 0.62 0.05 0.1 0.06 0.04 0.01 0.02 0.03 0 0.06 0.07 0.68 0.05 0.04 0.05 0.01 0.01 0.01 0 0.04 0.14 0.03 0.7 0.03 0.04 0.01 0.01 0.01 0.01 0.04 0.06 0.03 0.03 0.78 0.01 0.01 0.01 0.03 0 0.03 0.04 0.04 0.04 0.01 0.79 0.01 0.01 0.08 0.02 0.02 0.02 0.01 0.01 0.02 0.01 0.8 0.03 0.03 0.05 0.02 0.02 0.01 0.01 0.01 0.01 0.03 0.82 0: airplane 1: auto2: bird3: cat4: deer5: dog6: frog7: horse8: ship9: truck 0 1 2 3 4 5 6 7 8 9 0.77 0.01 0.04 0.03 0.02 0.01 0.03 0.01 0.06 0.02 0.01 0.87 0.01 0.01 0 0.01 0.01 0 0.02 0.05 0.04 0 0.7 0.09 0.05 0.03 0.06 0.02 0.01 0.01 0.03 0.01 0.06 0.64 0.05 0.08 0.06 0.04 0.01 0.02 0.02 0 0.05 0.06 0.74 0.03 0.05 0.04 0.01 0.01 0.01 0 0.05 0.15 0.04 0.66 0.04 0.04 0.01 0.01 0.02 0.01 0.04 0.06 0.02 0.02 0.78 0.01 0.01 0.03 0.02 0 0.03 0.05 0.05 0.03 0.01 0.81 0 0.01 0.05 0.02 0.01 0.02 0.01 0 0.02 0.01 0.83 0.03 0.02 0.05 0.01 0.02 0.01 0.01 0.01 0.01 0.03 0.83 0: airplane 1: auto2: bird3: cat4: deer5: dog6: frog7: horse8: ship9: truck 0 1 2 3 4 5 6 7 8 9 0.74 0.01 0.05 0.03 0.02 0 0.03 0.01 0.06 0.02 0.02 0.87 0.01 0.02 0 0 0.01 0 0.02 0.05 0.05 0 0.7 0.07 0.05 0.03 0.06 0.02 0.01 0.01 0.02 0.01 0.05 0.68 0.05 0.07 0.07 0.03 0.01 0.02 0.02 0 0.05 0.06 0.77 0.02 0.04 0.03 0 0 0.01 0 0.07 0.15 0.04 0.65 0.04 0.03 0.01 0.01 0.01 0 0.03 0.07 0.03 0.02 0.83 0.01 0 0.01 0.01 0 0.03 0.04 0.04 0.02 0.01 0.82 0 0.01 0.06 0.02 0.01 0.02 0.01 0 0.02 0 0.85 0.02 0.02 0.05 0.01 0.02 0.01 0 0.01 0.01 0.03 0.85 0: airplane 1: auto2: bird3: cat4: deer5: dog6: frog7: horse8: ship9: truck 0 1 2 3 4 5 6 7 8 9 0.76 0.01 0.05 0.04 0.02 0 0.02 0.01 0.07 0.03 0.01 0.87 0.01 0.01 0 0 0.01 0 0.02 0.05 0.04 0 0.73 0.06 0.05 0.03 0.06 0.01 0.01 0.01 0.01 0.01 0.05 0.71 0.05 0.06 0.06 0.03 0.01 0.01 0.02 0 0.04 0.05 0.78 0.02 0.04 0.03 0.01 0 0.01 0 0.06 0.17 0.04 0.64 0.04 0.03 0.01 0.01 0.01 0 0.03 0.06 0.03 0.01 0.85 0.01 0 0.01 0.01 0 0.04 0.04 0.05 0.02 0.01 0.81 0.01 0.01 0.05 0.02 0.01 0.02 0.01 0 0.02 0 0.84 0.02 0.02 0.05 0.01 0.02 0.01 0 0.02 0.01 0.04 0.83 True label 1st visit 2nd visit 3rd visit 4th visit 0: airplane 1: auto2: bird3: cat4: deer5: dog6: frog7: horse8: ship9: truck 0 1 2 3 4 5 6 7 8 9 0.76 0.02 0.04 0.04 0.02 0 0.02 0.01 0.08 0.02 0.02 0.86 0.01 0.02 0 0 0.01 0 0.02 0.05 0.04 0 0.73 0.07 0.05 0.03 0.05 0.01 0.01 0.01 0.01 0.01 0.06 0.69 0.05 0.06 0.07 0.02 0.01 0.01 0.02 0 0.05 0.07 0.76 0.02 0.04 0.03 0.01 0 0.01 0 0.07 0.17 0.04 0.64 0.03 0.03 0.01 0.01 0.01 0 0.03 0.07 0.02 0.01 0.84 0.01 0.01 0.01 0.01 0 0.04 0.04 0.06 0.02 0.01 0.81 0.01 0.01 0.04 0.02 0.02 0.02 0.01 0 0.02 0 0.86 0.02 0.02 0.05 0.02 0.02 0.01 0 0.02 0.01 0.03 0.82 0: airplane 1: auto2: bird3: cat4: deer5: dog6: frog7: horse8: ship9: truck 0 1 2 3 4 5 6 7 8 9 0.74 0.01 0.05 0.04 0.02 0 0.03 0.01 0.07 0.02 0.01 0.88 0.01 0.01 0 0 0.01 0 0.02 0.05 0.05 0 0.74 0.07 0.05 0.02 0.05 0.01 0.01 0 0.01 0 0.05 0.7 0.06 0.06 0.07 0.02 0.01 0.01 0.01 0 0.04 0.06 0.79 0.02 0.04 0.02 0.01 0 0.01 0 0.06 0.17 0.04 0.65 0.04 0.03 0.01 0.01 0.01 0 0.03 0.07 0.02 0.01 0.85 0 0 0.01 0.01 0 0.04 0.04 0.06 0.02 0.01 0.8 0.01 0.01 0.04 0.02 0.01 0.02 0.01 0 0.02 0 0.87 0.02 0.02 0.05 0.02 0.02 0 0 0.02 0.01 0.04 0.83 0: airplane 1: auto2: bird3: cat4: deer5: dog6: frog7: horse8: ship9: truck 0 1 2 3 4 5 6 7 8 9 0.76 0.01 0.04 0.04 0.02 0 0.03 0.01 0.07 0.02 0.01 0.88 0.01 0.01 0 0 0.01 0 0.02 0.05 0.04 0 0.74 0.06 0.05 0.02 0.05 0.01 0.01 0.01 0.01 0.01 0.06 0.68 0.05 0.06 0.08 0.02 0.01 0.01 0.01 0 0.05 0.06 0.79 0.01 0.04 0.02 0 0 0.01 0 0.07 0.18 0.05 0.61 0.04 0.03 0.01 0.01 0 0 0.03 0.06 0.02 0.01 0.86 0 0 0 0.01 0 0.04 0.04 0.06 0.02 0.01 0.8 0 0.01 0.06 0.02 0.02 0.02 0.01 0 0.02 0 0.84 0.02 0.02 0.05 0.01 0.03 0.01 0 0.02 0.01 0.04 0.81 0: airplane 1: auto2: bird3: cat4: deer5: dog6: frog7: horse8: ship9: truck 0 1 2 3 4 5 6 7 8 9 0.75 0.01 0.04 0.04 0.02 0 0.03 0.01 0.08 0.02 0.01 0.87 0.01 0.01 0 0 0.02 0 0.02 0.06 0.04 0 0.73 0.08 0.05 0.02 0.05 0.01 0.01 0.01 0.01 0 0.05 0.73 0.05 0.05 0.07 0.02 0.01 0.01 0.01 0 0.05 0.06 0.79 0.01 0.05 0.02 0.01 0 0.01 0 0.06 0.18 0.04 0.63 0.04 0.02 0.01 0.01 0 0 0.03 0.08 0.02 0.01 0.83 0 0 0 0.01 0 0.04 0.05 0.07 0.02 0.01 0.79 0 0.01 0.04 0.02 0.01 0.02 0.01 0 0.02 0 0.86 0.02 0.02 0.04 0.01 0.03 0.01 0 0.03 0.01 0.04 0.81 True label 5th visit 6th visit 7th visit 8th visit 0: airplane 1: auto2: bird3: cat4: deer5: dog6: frog7: horse8: ship9: truck 0 1 2 3 4 5 6 7 8 9 0.74 0.01 0.05 0.04 0.02 0 0.03 0.01 0.08 0.02 0.01 0.88 0.01 0.01 0 0 0.02 0 0.02 0.05 0.04 0 0.74 0.07 0.05 0.02 0.06 0.01 0.01 0.01 0.01 0 0.06 0.71 0.05 0.05 0.07 0.02 0.01 0.01 0.01 0 0.04 0.07 0.79 0.01 0.04 0.02 0.01 0 0.01 0 0.07 0.19 0.05 0.62 0.04 0.02 0.01 0 0 0 0.03 0.07 0.02 0.01 0.84 0 0 0.01 0.01 0 0.05 0.05 0.08 0.02 0.02 0.77 0 0.01 0.04 0.02 0.02 0.02 0.01 0 0.02 0 0.85 0.02 0.02 0.05 0.02 0.02 0.01 0 0.02 0.01 0.04 0.83 0: airplane 1: auto2: bird3: cat4: deer5: dog6: frog7: horse8: ship9: truck 0 1 2 3 4 5 6 7 8 9 0.74 0.01 0.05 0.04 0.02 0 0.03 0.01 0.08 0.02 0.01 0.88 0.01 0.01 0 0 0.02 0 0.02 0.06 0.04 0 0.73 0.07 0.05 0.02 0.05 0.01 0.01 0.01 0.01 0.01 0.05 0.7 0.06 0.05 0.08 0.02 0.02 0.02 0.02 0 0.05 0.07 0.79 0.01 0.04 0.02 0.01 0 0.01 0 0.07 0.19 0.05 0.6 0.04 0.03 0.01 0.01 0 0 0.04 0.07 0.02 0.01 0.84 0 0 0.01 0.01 0 0.04 0.05 0.08 0.02 0.01 0.78 0 0 0.04 0.02 0.02 0.02 0.01 0 0.02 0 0.85 0.02 0.02 0.05 0.02 0.02 0.01 0 0.02 0.01 0.04 0.81 0: airplane 1: auto2: bird3: cat4: deer5: dog6: frog7: horse8: ship9: truck 0 1 2 3 4 5 6 7 8 9 0.73 0.02 0.06 0.05 0.02 0 0.04 0.01 0.07 0.02 0.01 0.87 0.01 0.01 0 0 0.02 0 0.02 0.06 0.04 0 0.74 0.08 0.05 0.02 0.05 0.01 0.01 0.01 0.01 0 0.06 0.73 0.05 0.05 0.07 0.01 0.01 0.01 0.02 0 0.06 0.07 0.76 0.01 0.04 0.02 0.01 0 0 0 0.06 0.19 0.05 0.61 0.05 0.02 0.01 0 0 0 0.03 0.07 0.02 0.01 0.86 0 0 0 0.01 0 0.04 0.05 0.08 0.02 0.01 0.77 0 0.01 0.04 0.02 0.02 0.02 0.01 0 0.03 0 0.84 0.02 0.01 0.04 0.02 0.03 0.01 0 0.02 0 0.03 0.83 0: airplane 1: auto2: bird3: cat4: deer5: dog6: frog7: horse8: ship9: truck 0 1 2 3 4 5 6 7 8 9 0.72 0.01 0.05 0.04 0.02 0 0.04 0.01 0.08 0.02 0.01 0.87 0.01 0.01 0 0 0.02 0 0.02 0.04 0.05 0 0.72 0.08 0.05 0.02 0.05 0.01 0.01 0 0.01 0 0.06 0.73 0.05 0.04 0.06 0.02 0.01 0.01 0.02 0 0.05 0.06 0.79 0.01 0.04 0.02 0.01 0 0.01 0 0.06 0.19 0.05 0.61 0.04 0.03 0.01 0 0 0 0.03 0.09 0.02 0.01 0.83 0 0 0 0.01 0 0.05 0.05 0.07 0.02 0.01 0.78 0 0.01 0.03 0.02 0.02 0.02 0.01 0 0.03 0 0.85 0.02 0.01 0.05 0.01 0.03 0.01 0 0.02 0 0.04 0.83 True label 9th visit 10th visit 11th visit 12th visit 0: airplane 1: auto2: bird3: cat4: deer5: dog6: frog7: horse8: ship9: truck 0 1 2 3 4 5 6 7 8 9 0.73 0.01 0.05 0.04 0.02 0 0.03 0.01 0.09 0.02 0.01 0.86 0.01 0.01 0 0 0.02 0 0.02 0.06 0.04 0 0.73 0.08 0.05 0.02 0.05 0.01 0.01 0 0.02 0 0.06 0.73 0.05 0.04 0.06 0.02 0.01 0.01 0.01 0 0.05 0.06 0.8 0.01 0.04 0.02 0.01 0 0.01 0 0.07 0.19 0.05 0.6 0.04 0.02 0.01 0.01 0 0 0.03 0.07 0.02 0.01 0.86 0 0 0 0.01 0 0.05 0.05 0.07 0.02 0.01 0.77 0 0 0.03 0.02 0.02 0.02 0.01 0 0.04 0 0.83 0.02 0.01 0.05 0.02 0.02 0.01 0 0.02 0 0.04 0.82 0: airplane 1: auto2: bird3: cat4: deer5: dog6: frog7: horse8: ship9: truck 0 1 2 3 4 5 6 7 8 9 0.75 0.01 0.05 0.04 0.02 0 0.03 0.01 0.08 0.02 0.01 0.87 0.01 0.02 0 0 0.02 0 0.02 0.05 0.05 0 0.72 0.08 0.05 0.02 0.05 0.01 0.01 0 0.01 0.01 0.05 0.73 0.05 0.05 0.07 0.01 0.01 0.01 0.01 0 0.05 0.06 0.79 0.01 0.05 0.02 0.01 0 0.01 0 0.07 0.21 0.05 0.57 0.05 0.02 0.01 0 0 0 0.03 0.07 0.02 0.01 0.86 0 0 0 0.01 0 0.05 0.05 0.08 0.02 0.02 0.76 0 0.01 0.04 0.02 0.02 0.02 0.01 0 0.02 0 0.85 0.02 0.02 0.05 0.02 0.03 0.01 0 0.02 0 0.04 0.81 0: airplane 1: auto2: bird3: cat4: deer5: dog6: frog7: horse8: ship9: truck 0 1 2 3 4 5 6 7 8 9 0.72 0.01 0.05 0.05 0.02 0 0.03 0.01 0.08 0.02 0.01 0.88 0.01 0.01 0 0 0.02 0 0.02 0.05 0.04 0 0.73 0.08 0.05 0.02 0.05 0.01 0.01 0 0.01 0 0.06 0.72 0.05 0.04 0.07 0.01 0.01 0.01 0.02 0 0.04 0.06 0.79 0.01 0.04 0.02 0.01 0 0.01 0 0.07 0.2 0.05 0.6 0.04 0.02 0.01 0.01 0 0 0.04 0.07 0.02 0.01 0.85 0 0 0 0.01 0 0.05 0.05 0.08 0.02 0.01 0.78 0 0.01 0.04 0.02 0.02 0.02 0.01 0 0.02 0 0.85 0.02 0.02 0.05 0.02 0.02 0.01 0 0.02 0 0.04 0.82 0: airplane 1: auto2: bird3: cat4: deer5: dog6: frog7: horse8: ship9: truck 0 1 2 3 4 5 6 7 8 9 0.75 0.01 0.05 0.04 0.02 0 0.02 0.01 0.09 0.02 0.01 0.86 0.01 0.02 0 0 0.02 0 0.02 0.05 0.04 0 0.74 0.07 0.05 0.02 0.05 0.01 0.01 0.01 0.02 0 0.06 0.73 0.05 0.04 0.07 0.01 0.01 0.01 0.02 0 0.05 0.06 0.78 0.01 0.05 0.02 0.01 0 0.01 0 0.07 0.19 0.05 0.6 0.05 0.02 0.01 0.01 0 0 0.03 0.07 0.02 0.01 0.85 0 0 0.01 0.01 0 0.04 0.06 0.08 0.02 0.01 0.77 0 0.01 0.04 0.02 0.03 0.03 0.01 0 0.04 0 0.8 0.02 0.01 0.05 0.02 0.02 0.01 0 0.02 0 0.04 0.83 True label 13th visit 14th visit 15th visit 16th visit 0: airplane 1: auto2: bird3: cat4: deer5: dog6: frog7: horse8: ship9: truck 0 1 2 3 4 5 6 7 8 9 0.73 0.01 0.06 0.04 0.02 0 0.04 0.01 0.07 0.02 0.01 0.88 0.01 0.01 0 0 0.02 0 0.02 0.05 0.04 0 0.75 0.07 0.05 0.02 0.05 0.01 0.01 0 0.01 0 0.06 0.74 0.05 0.05 0.06 0.01 0.01 0.01 0.01 0 0.05 0.06 0.8 0.01 0.04 0.02 0 0 0.01 0 0.07 0.2 0.05 0.59 0.06 0.02 0.01 0.01 0 0 0.04 0.08 0.02 0.01 0.84 0 0 0 0.01 0 0.05 0.05 0.08 0.02 0.02 0.76 0 0.01 0.05 0.01 0.01 0.02 0.01 0 0.02 0 0.85 0.02 0.02 0.05 0.02 0.03 0.01 0 0.03 0 0.03 0.81 0: airplane 1: auto2: bird3: cat4: deer5: dog6: frog7: horse8: ship9: truck 0 1 2 3 4 5 6 7 8 9 0.72 0.01 0.05 0.04 0.02 0 0.03 0.01 0.08 0.02 0.01 0.88 0.01 0.01 0 0 0.02 0 0.02 0.04 0.04 0 0.73 0.07 0.06 0.02 0.06 0.01 0.01 0 0.01 0 0.06 0.73 0.05 0.04 0.07 0.01 0.01 0.01 0.01 0 0.06 0.06 0.79 0.01 0.05 0.02 0.01 0 0.01 0 0.07 0.21 0.05 0.59 0.04 0.02 0.01 0 0 0 0.04 0.07 0.02 0.01 0.86 0 0 0 0.01 0 0.05 0.05 0.08 0.02 0.01 0.76 0.01 0.01 0.05 0.02 0.02 0.03 0.01 0 0.02 0 0.85 0.01 0.02 0.05 0.02 0.03 0.01 0 0.03 0.01 0.04 0.8 0: airplane 1: auto2: bird3: cat4: deer5: dog6: frog7: horse8: ship9: truck 0 1 2 3 4 5 6 7 8 9 0.73 0.01 0.06 0.04 0.02 0 0.03 0.01 0.08 0.02 0.01 0.88 0.01 0.01 0 0 0.02 0 0.02 0.05 0.04 0 0.73 0.06 0.05 0.02 0.06 0.01 0.01 0.01 0.01 0 0.06 0.73 0.05 0.05 0.07 0.01 0.01 0.01 0.01 0 0.05 0.06 0.78 0.01 0.05 0.02 0.01 0 0.01 0 0.07 0.21 0.05 0.58 0.05 0.02 0.01 0.01 0 0 0.03 0.07 0.02 0.01 0.85 0 0.01 0.01 0.01 0 0.06 0.05 0.08 0.02 0.02 0.75 0 0.01 0.03 0.02 0.02 0.02 0.01 0 0.03 0 0.85 0.02 0.02 0.05 0.02 0.02 0.01 0 0.02 0 0.04 0.83 0: airplane 1: auto2: bird3: cat4: deer5: dog6: frog7: horse8: ship9: truck 0 1 2 3 4 5 6 7 8 9 0.73 0.01 0.06 0.04 0.02 0 0.03 0.01 0.08 0.02 0.01 0.88 0.01 0.01 0 0 0.02 0 0.02 0.05 0.04 0 0.75 0.07 0.05 0.02 0.05 0.01 0.01 0 0.01 0 0.06 0.72 0.05 0.04 0.06 0.02 0.01 0.01 0.02 0 0.06 0.07 0.76 0.01 0.05 0.02 0.01 0 0 0 0.07 0.19 0.05 0.59 0.05 0.02 0.01 0.01 0 0 0.03 0.07 0.02 0.01 0.84 0 0.01 0.01 0.01 0 0.06 0.06 0.08 0.02 0.02 0.74 0 0.01 0.04 0.02 0.02 0.02 0.01 0 0.03 0 0.84 0.02 0.01 0.05 0.02 0.03 0.01 0 0.02 0.01 0.04 0.82 Predicted label Predicted label Predicted label Predicted label True label 17th visit 18th visit 19th visit 20th visit Figure 11: The dynamic of the confusion matrix of PeTTA (ours) in episodic TTA with 20 visits. 34NeurIPS Paper Checklist 1. Claims Question: Do the main claims made in the abstract and introduction accurately reflect the paper’s contributions and scope? Answer: [Yes] Justification: We have highlighted the three main claims and contributions of our work in both the abstract (highlighted in bold font) and the introduction section (listed as bullet points). Guidelines: • The answer NA means that the abstract and introduction do not include the claims made in the paper. • The abstract and/or introduction should clearly state the claims made, including the contributions made in the paper and important assumptions and limitations. A No or NA answer to this question will not be perceived well by the reviewers. • The claims made should match theoretical and experimental results, and reflect how much the results can be expected to generalize to other settings. • It is fine to include aspirational goals as motivation as long as it is clear that these goals are not attained by the paper. 2. Limitations Question: Does the paper discuss the limitations of the work performed by the authors? Answer: [Yes] Justification: We have discussed the limitations and potential future work of our study in Sec. 6. Specifically, three main limitations are included: (1) Collapse prevention can not be guaranteed through regularization, PeTTA requires (2) the use of a relatively small memory bank is available and (3) the empirical mean and covariant matrix of feature vectors on the source dataset is computable. We also include discussions in Appdx. E.3 and Appdx. E.4 to further elaborate (2), and (3) respectively. Guidelines: • The answer NA means that the paper has no limitation while the answer No means that the paper has limitations, but those are not discussed in the paper. • The authors are encouraged to create a separate \"Limitations\" section in their paper. • The paper should point out any strong assumptions and how robust the results are to violations of these assumptions (e.g., independence assumptions, noiseless settings, model well-specification, asymptotic approximations only holding locally). The authors should reflect on how these assumptions might be violated in practice and what the implications would be. • The authors should reflect on the scope of the claims made, e.g., if the approach was only tested on a few datasets or with a few runs. In general, empirical results often depend on implicit assumptions, which should be articulated. • The authors should reflect on the factors that influence the performance of the approach. For example, a facial recognition algorithm may perform poorly when image resolution is low or images are taken in low lighting. Or a speech-to-text system might not be used reliably to provide closed captions for online lectures because it fails to handle technical jargon. • The authors should discuss the computational efficiency of the proposed algorithms and how they scale with dataset size. • If applicable, the authors should discuss possible limitations of their approach to address problems of privacy and fairness. • While the authors might fear that complete honesty about limitations might be used by reviewers as grounds for rejection, a worse outcome might be that reviewers discover limitations that aren’t acknowledged in the paper. The authors should use their best judgment and recognize that individual actions in favor of transparency play an impor- tant role in developing norms that preserve the integrity of the community. Reviewers will be specifically instructed to not penalize honesty concerning limitations. 353. Theory Assumptions and Proofs Question: For each theoretical result, does the paper provide the full set of assumptions and a complete (and correct) proof? Answer: [Yes] Justification: We have provided the full proof of all lemmas and theorem in Appdx. B. Guidelines: • The answer NA means that the paper does not include theoretical results. • All the theorems, formulas, and proofs in the paper should be numbered and cross- referenced. • All assumptions should be clearly stated or referenced in the statement of any theorems. • The proofs can either appear in the main paper or the supplemental material, but if they appear in the supplemental material, the authors are encouraged to provide a short proof sketch to provide intuition. • Inversely, any informal proof provided in the core of the paper should be complemented by formal proofs provided in appendix or supplemental material. • Theorems and Lemmas that the proof relies upon should be properly referenced. 4. Experimental Result Reproducibility Question: Does the paper fully disclose all the information needed to reproduce the main ex- perimental results of the paper to the extent that it affects the main claims and/or conclusions of the paper (regardless of whether the code and data are provided or not)? Answer: [Yes] Justification: This study propose a new TTA approach - PeTTA. A full description of this approach is given in Sec. 4 with its pseudo-code provided in Appdx. E.1. The implementation of PeTTA in Python is also attached as supplemental material. Additionally, Sec. 5.2 and Appdx. G are dedicated to providing further implementation details for reproducing the main experimental results. Lastly, the construction of recurring TTA is notably simple, and can be easily extended to other TTA streams. Its configuration on each tasks is described in the Recurring TTA paragraph of Sec. 5.2. Guidelines: • The answer NA means that the paper does not include experiments. • If the paper includes experiments, a No answer to this question will not be perceived well by the reviewers: Making the paper reproducible is important, regardless of whether the code and data are provided or not. • If the contribution is a dataset and/or model, the authors should describe the steps taken to make their results reproducible or verifiable. • Depending on the contribution, reproducibility can be accomplished in various ways. For example, if the contribution is a novel architecture, describing the architecture fully might suffice, or if the contribution is a specific model and empirical evaluation, it may be necessary to either make it possible for others to replicate the model with the same dataset, or provide access to the model. In general. releasing code and data is often one good way to accomplish this, but reproducibility can also be provided via detailed instructions for how to replicate the results, access to a hosted model (e.g., in the case of a large language model), releasing of a model checkpoint, or other means that are appropriate to the research performed. • While NeurIPS does not require releasing code, the conference does require all submis- sions to provide some reasonable avenue for reproducibility, which may depend on the nature of the contribution. For example (a) If the contribution is primarily a new algorithm, the paper should make it clear how to reproduce that algorithm. (b) If the contribution is primarily a new model architecture, the paper should describe the architecture clearly and fully. (c) If the contribution is a new model (e.g., a large language model), then there should either be a way to access this model for reproducing the results or a way to reproduce the model (e.g., with an open-source dataset or instructions for how to construct the dataset). 36(d) We recognize that reproducibility may be tricky in some cases, in which case authors are welcome to describe the particular way they provide for reproducibility. In the case of closed-source models, it may be that access to the model is limited in some way (e.g., to registered users), but it should be possible for other researchers to have some path to reproducing or verifying the results. 5. Open access to data and code Question: Does the paper provide open access to the data and code, with sufficient instruc- tions to faithfully reproduce the main experimental results, as described in supplemental material? Answer: [Yes] Justification: This study does not involve any private datasets. All datasets used in our exper- iments are publicly available online from previous works (more information in Appdx. G.4). The source code of PeTTA is also attached as supplemental material. Guidelines: • The answer NA means that paper does not include experiments requiring code. • Please see the NeurIPS code and data submission guidelines ( https://nips.cc/ public/guides/CodeSubmissionPolicy) for more details. • While we encourage the release of code and data, we understand that this might not be possible, so “No” is an acceptable answer. Papers cannot be rejected simply for not including code, unless this is central to the contribution (e.g., for a new open-source benchmark). • The instructions should contain the exact command and environment needed to run to reproduce the results. See the NeurIPS code and data submission guidelines (https: //nips.cc/public/guides/CodeSubmissionPolicy) for more details. • The authors should provide instructions on data access and preparation, including how to access the raw data, preprocessed data, intermediate data, and generated data, etc. • The authors should provide scripts to reproduce all experimental results for the new proposed method and baselines. If only a subset of experiments are reproducible, they should state which ones are omitted from the script and why. • At submission time, to preserve anonymity, the authors should release anonymized versions (if applicable). • Providing as much information as possible in supplemental material (appended to the paper) is recommended, but including URLs to data and code is permitted. 6. Experimental Setting/Details Question: Does the paper specify all the training and test details (e.g., data splits, hyper- parameters, how they were chosen, type of optimizer, etc.) necessary to understand the results? Answer: [Yes] Justification: The experimental settings of the key results in the paper have been provided in Sec. 5.1 (Simulation Setup) and Sec. 5.2 (Setup - Benchmark Datasets). In the supplementary material, any additional experimental results beyond the main paper, such as those in Appdx. D.3, and Appdx. F.3, are consistently preceded by a subsection titledExperiment Setup summarizing the experimental details before presenting the results. Guidelines: • The answer NA means that the paper does not include experiments. • The experimental setting should be presented in the core of the paper to a level of detail that is necessary to appreciate the results and make sense of them. • The full details can be provided either with the code, in appendix, or as supplemental material. 7. Experiment Statistical Significance Question: Does the paper report error bars suitably and correctly defined or other appropriate information about the statistical significance of the experiments? 37Answer: [Yes] Justification: Due to the limited computing resources, we only extensively evaluate the performance of our proposed method (PeTTA) across 5 independent runs, with different random seeds. Specifically, the mean values in 5 runs are reported in Tab. 1, Tab. 2, Tab. 7, and Tab. 8. The corresponding standard deviation values are provided in Appdx. F.1. Guidelines: • The answer NA means that the paper does not include experiments. • The authors should answer \"Yes\" if the results are accompanied by error bars, confi- dence intervals, or statistical significance tests, at least for the experiments that support the main claims of the paper. • The factors of variability that the error bars are capturing should be clearly stated (for example, train/test split, initialization, random drawing of some parameter, or overall run with given experimental conditions). • The method for calculating the error bars should be explained (closed form formula, call to a library function, bootstrap, etc.) • The assumptions made should be given (e.g., Normally distributed errors). • It should be clear whether the error bar is the standard deviation or the standard error of the mean. • It is OK to report 1-sigma error bars, but one should state it. The authors should preferably report a 2-sigma error bar than state that they have a 96% CI, if the hypothesis of Normality of errors is not verified. • For asymmetric distributions, the authors should be careful not to show in tables or figures symmetric error bars that would yield results that are out of range (e.g. negative error rates). • If error bars are reported in tables or plots, The authors should explain in the text how they were calculated and reference the corresponding figures or tables in the text. 8. Experiments Compute Resources Question: For each experiment, does the paper provide sufficient information on the com- puter resources (type of compute workers, memory, time of execution) needed to reproduce the experiments? Answer: [Yes] Justification: We have provided the information on the computing resources used in our experiments in Appdx. G.1. Guidelines: • The answer NA means that the paper does not include experiments. • The paper should indicate the type of compute workers CPU or GPU, internal cluster, or cloud provider, including relevant memory and storage. • The paper should provide the amount of compute required for each of the individual experimental runs as well as estimate the total compute. • The paper should disclose whether the full research project required more compute than the experiments reported in the paper (e.g., preliminary or failed experiments that didn’t make it into the paper). 9. Code Of Ethics Question: Does the research conducted in the paper conform, in every respect, with the NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines? Answer: [Yes] Justification: The authors have reviewed and to the best of our judgment, this study has conformed to the NeurIPS Code of Ethics. Guidelines: • The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics. • If the authors answer No, they should explain the special circumstances that require a deviation from the Code of Ethics. 38• The authors should make sure to preserve anonymity (e.g., if there is a special consid- eration due to laws or regulations in their jurisdiction). 10. Broader Impacts Question: Does the paper discuss both potential positive societal impacts and negative societal impacts of the work performed? Answer: [No] Justification: This study advances the research in test-time adaptation area in general, and not tied to particular applications. Hence, there are no significant potential societal consequences of our work which we feel must be specifically highlighted here. Guidelines: • The answer NA means that there is no societal impact of the work performed. • If the authors answer NA or No, they should explain why their work has no societal impact or why the paper does not address societal impact. • Examples of negative societal impacts include potential malicious or unintended uses (e.g., disinformation, generating fake profiles, surveillance), fairness considerations (e.g., deployment of technologies that could make decisions that unfairly impact specific groups), privacy considerations, and security considerations. • The conference expects that many papers will be foundational research and not tied to particular applications, let alone deployments. However, if there is a direct path to any negative applications, the authors should point it out. For example, it is legitimate to point out that an improvement in the quality of generative models could be used to generate deepfakes for disinformation. On the other hand, it is not needed to point out that a generic algorithm for optimizing neural networks could enable people to train models that generate Deepfakes faster. • The authors should consider possible harms that could arise when the technology is being used as intended and functioning correctly, harms that could arise when the technology is being used as intended but gives incorrect results, and harms following from (intentional or unintentional) misuse of the technology. • If there are negative societal impacts, the authors could also discuss possible mitigation strategies (e.g., gated release of models, providing defenses in addition to attacks, mechanisms for monitoring misuse, mechanisms to monitor how a system learns from feedback over time, improving the efficiency and accessibility of ML). 11. Safeguards Question: Does the paper describe safeguards that have been put in place for responsible release of data or models that have a high risk for misuse (e.g., pretrained language models, image generators, or scraped datasets)? Answer: [NA] Justification: To the best of our judgment, this study poses no risks for misuse. Guidelines: • The answer NA means that the paper poses no such risks. • Released models that have a high risk for misuse or dual-use should be released with necessary safeguards to allow for controlled use of the model, for example by requiring that users adhere to usage guidelines or restrictions to access the model or implementing safety filters. • Datasets that have been scraped from the Internet could pose safety risks. The authors should describe how they avoided releasing unsafe images. • We recognize that providing effective safeguards is challenging, and many papers do not require this, but we encourage authors to take this into account and make a best faith effort. 12. Licenses for existing assets Question: Are the creators or original owners of assets (e.g., code, data, models), used in the paper, properly credited and are the license and terms of use explicitly mentioned and properly respected? 39Answer: [Yes] Justification: The original papers that produced the code package or dataset have been properly cited throughout the paper. Further information on the licenses of used assets are provided in Appdx. G.4. Guidelines: • The answer NA means that the paper does not use existing assets. • The authors should cite the original paper that produced the code package or dataset. • The authors should state which version of the asset is used and, if possible, include a URL. • The name of the license (e.g., CC-BY 4.0) should be included for each asset. • For scraped data from a particular source (e.g., website), the copyright and terms of service of that source should be provided. • If assets are released, the license, copyright information, and terms of use in the package should be provided. For popular datasets, paperswithcode.com/datasets has curated licenses for some datasets. Their licensing guide can help determine the license of a dataset. • For existing datasets that are re-packaged, both the original license and the license of the derived asset (if it has changed) should be provided. • If this information is not available online, the authors are encouraged to reach out to the asset’s creators. 13. New Assets Question: Are new assets introduced in the paper well documented and is the documentation provided alongside the assets? Answer: [NA] Justification: This study does not release new assets. Guidelines: • The answer NA means that the paper does not release new assets. • Researchers should communicate the details of the dataset/code/model as part of their submissions via structured templates. This includes details about training, license, limitations, etc. • The paper should discuss whether and how consent was obtained from people whose asset is used. • At submission time, remember to anonymize your assets (if applicable). You can either create an anonymized URL or include an anonymized zip file. 14. Crowdsourcing and Research with Human Subjects Question: For crowdsourcing experiments and research with human subjects, does the paper include the full text of instructions given to participants and screenshots, if applicable, as well as details about compensation (if any)? Answer: [NA] Justification: This study does not involve crowdsourcing nor research with human subjects. Guidelines: • The answer NA means that the paper does not involve crowdsourcing nor research with human subjects. • Including this information in the supplemental material is fine, but if the main contribu- tion of the paper involves human subjects, then as much detail as possible should be included in the main paper. • According to the NeurIPS Code of Ethics, workers involved in data collection, curation, or other labor should be paid at least the minimum wage in the country of the data collector. 15. Institutional Review Board (IRB) Approvals or Equivalent for Research with Human Subjects 40Question: Does the paper describe potential risks incurred by study participants, whether such risks were disclosed to the subjects, and whether Institutional Review Board (IRB) approvals (or an equivalent approval/review based on the requirements of your country or institution) were obtained? Answer: [NA] Justification: This study does not involve crowdsourcing nor research with human subjects. Guidelines: • The answer NA means that the paper does not involve crowdsourcing nor research with human subjects. • Depending on the country in which research is conducted, IRB approval (or equivalent) may be required for any human subjects research. If you obtained IRB approval, you should clearly state this in the paper. • We recognize that the procedures for this may vary significantly between institutions and locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the guidelines for their institution. • For initial submissions, do not include any information that would break anonymity (if applicable), such as the institution conducting the review. 41",
      "meta_data": {
        "arxiv_id": "2311.18193v4",
        "authors": [
          "Trung-Hieu Hoang",
          "Duc Minh Vo",
          "Minh N. Do"
        ],
        "published_date": "2023-11-30T02:24:44Z",
        "pdf_url": "https://arxiv.org/pdf/2311.18193v4.pdf"
      },
      "llm_extracted_info": {
        "main_contributions": "The paper addresses the overlooked problem of gradual performance degradation (model collapse) in Test-Time Adaptation (TTA) over prolonged periods, especially in recurring testing scenarios where environments reappear. It introduces \"recurring TTA\" as a novel diagnostic setting. It formally defines TTA model collapse and provides a theoretical analysis using an ϵ-perturbed Gaussian Mixture Model Classifier (ϵ-GMMC) to identify dataset- and algorithm-dependent factors causing error accumulation. Finally, it proposes Persistent TTA (PeTTA), an adaptation scheme that senses model divergence and adaptively adjusts strategy to balance adaptation and collapse prevention, demonstrating superior and persistent performance over existing baselines.",
        "methodology": "The paper first establishes a theoretical foundation using an ϵ-perturbed Gaussian Mixture Model Classifier (ϵ-GMMC) to simulate and analyze model collapse, identifying data- and algorithm-dependent factors. Based on these insights, it proposes Persistent TTA (PeTTA). PeTTA employs a divergence sensing mechanism that measures the Mahalanobis distance of feature embedding vectors from the source model's statistics (γy t). This sensed divergence (¯γt) is then used to adaptively adjust the regularization coefficient (λt) and the exponential moving average (EMA) update rate (αt) of the mean-teacher model, balancing adaptation and collapse prevention. It also incorporates an anchor loss (LAL) to minimize KL divergence between the current and source model's output probabilities, along with a category-balanced memory bank and robust batch normalization from prior work.",
        "experimental_setup": "Experiments were conducted on four TTA classification tasks: CIFAR-10 → CIFAR-10-C, CIFAR-100 → CIFAR-100-C, ImageNet → ImageNet-C (corruption level 5), and DomainNet (real → clipart, painting, sketch). A synthesized ϵ-perturbed Gaussian Mixture Model Classifier (ϵ-GMMC) dataset was used for theoretical validation. Performance was benchmarked in a novel \"recurring TTA\" setting (K=20 revisits with temporally correlated batches) and on the \"Continuously Changing Corruption (CCC)\" benchmark. Compared methods include CoTTA, EATA, RMT, MECTA, RoTTA, ROID, TRIBE, LAME, and RDumb. Implementation utilized PyTorch, pre-trained ResNet50 models from RobustBench and torchvision, Adam optimizer, and specific hyperparameters for PeTTA's adaptive components. Results for PeTTA are averaged over 5 runs with different random seeds, reporting mean and standard deviation.",
        "limitations": "The paper acknowledges that complete elimination of error accumulation cannot be rigorously guaranteed solely through regularization. PeTTA's current design necessitates the use of a small memory bank to handle temporally correlated testing streams, and it assumes the availability of feature statistics (empirical mean and covariant matrix) from the source distribution. These requirements could potentially limit its scalability and applicability in certain real-world scenarios where source data is strictly inaccessible or memory is severely constrained.",
        "future_research_directions": "Future work could focus on developing algorithms that inherently prevent error accumulation by construction, moving beyond regularization-based mitigation. Additionally, exploring alternative strategies for reducing the memory footprint, such as storing only embedded features instead of raw images in the memory bank, is suggested to improve scalability."
      }
    },
    {
      "title": "Test-Time Training with Self-Supervision for Generalization under Distribution Shifts",
      "abstract": "In this paper, we propose Test-Time Training, a general approach for\nimproving the performance of predictive models when training and test data come\nfrom different distributions. We turn a single unlabeled test sample into a\nself-supervised learning problem, on which we update the model parameters\nbefore making a prediction. This also extends naturally to data in an online\nstream. Our simple approach leads to improvements on diverse image\nclassification benchmarks aimed at evaluating robustness to distribution\nshifts.",
      "full_text": "Test-Time Training with Self-Supervision for Generalization under Distribution Shifts Yu Sun1 Xiaolong Wang1 2 Zhuang Liu1 John Miller1 Alexei A. Efros1 Moritz Hardt1 Abstract In this paper, we propose Test-Time Training, a general approach for improving the performance of predictive models when training and test data come from different distributions. We turn a sin- gle unlabeled test sample into a self-supervised learning problem, on which we update the model parameters before making a prediction. This also extends naturally to data in an online stream. Our simple approach leads to improvements on di- verse image classiﬁcation benchmarks aimed at evaluating robustness to distribution shifts. 1. Introduction Supervised learning remains notoriously weak at generaliza- tion under distribution shifts. Unless training and test data are drawn from the same distribution, even seemingly minor differences turn out to defeat state-of-the-art models (Recht et al., 2018). Adversarial robustness and domain adapta- tion are but a few existing paradigms that try to anticipate differences between the training and test distribution with either topological structure or data from the test distribution available during training. We explore a new take on gener- alization that does not anticipate the distribution shifts, but instead learns from them at test time. We start from a simple observation. The unlabeled test sample xpresented at test time gives us a hint about the distribution from which it was drawn. We propose to take advantage of this hint on the test distribution by allowing the model parameters θto depend on the test sample x, but not its unknown label y. The concept of a variable decision boundary θ(x) is powerful in theory since it breaks away from the limitation of ﬁxed model capacity (see additional discussion in Section A1), but the design of a feedback mechanism from xto θ(x) raises new challenges in practice that we only begin to address here. 1University of California, Berkeley 2University of California, San Diego. Correspondence to: Yu Sun <yusun@berkeley.edu>. Proceedings of the 37 th International Conference on Machine Learning, Vienna, Austria, PMLR 119, 2020. Copyright 2020 by the author(s). Our proposed test-time training method creates a self- supervised learning problem based on this single test sample x, updating θat test time before making a prediction. Self- supervised learning uses an auxiliary task that automatically creates labels from unlabeled inputs. In our experiments, we use the task of rotating each input image by a multiple of 90 degrees and predicting its angle (Gidaris et al., 2018). This approach can also be easily modiﬁed to work outside the standard supervised learning setting. If several test samples arrive in a batch, we can use the entire batch for test-time training. If samples arrive in an online stream, we obtain further improvements by keeping the state of the parameters. After all, prediction is rarely a single event. The online version can be the natural mode of deployment under the additional assumption that test samples are produced by the same or smoothly changing distribution shifts. We experimentally validate our method in the context of object recognition on several standard benchmarks. These include images with diverse types of corruption at various levels (Hendrycks & Dietterich, 2019), video frames of moving objects (Shankar et al., 2019), and a new test set of unknown shifts collected by (Recht et al., 2018). Our algorithm makes substantial improvements under distribu- tion shifts, while maintaining the same performance on the original distribution. In our experiments, we compare with a strong baseline (labeled joint training) that uses both supervised and self- supervised learning at training-time, but keeps the model ﬁxed at test time. Recent work shows that training-time self- supervision improves robustness (Hendrycks et al., 2019a); our joint training baseline corresponds to an improved imple- mentation of this work. A comprehensive review of related work follows in Section 5. We complement the empirical results with theoretical inves- tigations in Section 4, and establish an intuitive sufﬁcient condition on a convex model of when Test-Time Training helps; this condition, roughly speaking, is to have correlated gradients between the loss functions of the two tasks. Project website: https://test-time-training.github.io/. arXiv:1909.13231v3  [cs.LG]  1 Jul 2020Test-Time Training with Self-Supervision for Generalization under Distribution Shifts 2. Method This section describes the algorithmic details of our method. To set up notation, consider a standard K-layer neural net- work with parameters θk for layer k. The stacked parameter vector θ = ( θ1,...,θ K) speciﬁes the entire model for a classiﬁcation task with loss function lm(x,y; θ) on the test sample (x,y). We call this the main task, as indicated by the subscript of the loss function. We assume to have training data (x1,y1),..., (xn,yn) drawn i.i.d. from a distribution P. Standard empirical risk minimization solves the optimization problem: min θ 1 n n∑ i=1 lm(xi,yi; θ). (1) Our method requires a self-supervised auxiliary task with loss function ls(x). In this paper, we choose the rotation prediction task (Gidaris et al., 2018), which has been demon- strated to be simple and effective at feature learning for convolutional neural networks. The task simply rotates x in the image plane by one of 0, 90, 180 and 270 degrees and have the model predict the angle of rotation as a four- way classiﬁcation problem. Other self-supervised tasks in Section 5 might also be used for our method. The auxiliary task shares some of the model parameters θe = ( θ1,...,θ κ) up to a certain κ ∈ {1,...,K }. We designate those κlayers as a shared feature extractor. The auxiliary task uses its own task-speciﬁc parameters θs = (θ′ κ+1,...,θ ′ K). We call the unshared parameters θs the self-supervised task branch, and θm = (θκ+1,...,θ K) the main task branch . Pictorially, the joint architecture is a Y-structure with a shared bottom and two branches. For our experiments, the self-supervised task branch has the same architecture as the main branch, except for the output dimensionality of the last layer due to the different number of classes in the two tasks. Training is done in the fashion of multi-task learning (Caru- ana, 1997); the model is trained on both tasks on the same data drawn fromP. Losses for both tasks are added together, and gradients are taken for the collection of all parameters. The joint training problem is therefore min θe,θm,θs 1 n n∑ i=1 lm(xi,yi; θm,θe) + ls(xi; θs,θe). (2) Now we describe the standard version of Test-Time Training on a single test sample x. Simply put, Test-Time Training ﬁne-tunes the shared feature extractor θe by minimizing the auxiliary task loss on x. This can be formulated as min θe ls(x; θs,θe). (3) Denote θ∗ e the (approximate) minimizer of Equation 3. The model then makes a prediction using the updated parameters θ(x) = (θ∗ e,θm). Empirically, the difference is negligible between minimizing Equation 3 over θe versus over both θe and θs. Theoretically, the difference exists only when optimization is done with more than one gradient step. Test-Time Training naturally beneﬁts from standard data augmentation techniques. On each test sample x, we per- form the exact same set of random transformations as for data augmentation during training, to form a batch only con- taining these augmented copies of xfor Test-Time Training. Online Test-Time Training. In the standard version of our method, the optimization problem in Equation 3 is al- ways initialized with parameters θ= (θe,θs) obtained by minimizing Equation 2. After making a prediction on x, θ∗ e is discarded. Outside of the standard supervised learning setting, when the test samples arrive online sequentially, the online version solves the same optimization problem as in Equation 3 to update the shared feature extractor θe. How- ever, on test sample xt, θis instead initialized with θ(xt−1) updated on the previous sample xt−1. This allows θ(xt) to take advantage of the distributional information available in x1,...,x t−1 as well as xt. 3. Empirical Results We experiment with both versions of our method (standard and online) on three kinds of benchmarks for distribution shifts, presented here in the order of visually low to high- level. Our code is available at the project website. Network details. Our architecture and hyper-parameters are consistent across all experiments. We use ResNets (He et al., 2016b), which are constructed differently for CIFAR-10 (Krizhevsky & Hinton, 2009) (26-layer) and Ima- geNet (Russakovsky et al., 2015) (18-layer). The CIFAR-10 dataset contains 50K images for training, and 10K images for testing. The ImageNet contains 1.2M images for train- ing and the 50K validation images are used as the test set. ResNets on CIFAR-10 have three groups, each containing convolutional layers with the same number of channels and size of feature maps; our splitting point is the end of the second group. ResNets on ImageNet have four groups; our splitting point is the end of the third group. We use Group Normalization (GN) instead of Batch Nor- malization (BN) in our architecture, since BN has been shown to be ineffective when training with small batches, for which the estimated batch statistics are not accurate (Ioffe & Szegedy, 2015). This technicality hurts Test-Time Training since each batch only contains (augmented) copies of a single image. Different from BN, GN is not dependent on batch size and achieves similar results on our baselines.Test-Time Training with Self-Supervision for Generalization under Distribution Shifts originalgauss shot impulsedefocus glass motion zoom snow frost fog bright contrastelasticpixelate jpeg 0 10 20 30 40 50Error (%) Object recognition task only Joint training (Hendrycks et al. 2019) TTT TTT-Online Figure 1.Test error (%) on CIFAR-10-C with level 5 corruptions.We compare our approaches, Test-Time Training (TTT) and its online version (TTT-Online), with two baselines: object recognition without self-supervision, and joint training with self-supervision but keeping the model ﬁxed at test time. TTT improves over the baselines and TTT-Online improves even further. We report results with BN in Section A4 of the appendix for completeness. We directly compare our architecture to that of Hendrycks et al. (2018) in subsection A4.5. Optimization details. For joint training (Equation 2), we use stochastic gradient descent with standard hyper- parameters as (Huang et al., 2016; He et al., 2016a). For Test-Time Training (Equation 3), we use stochastic gradient descent with the learning rate set to that of the last epoch during training, which is 0.001 in all our experiments. We set weight decay and momentum to zero during Test-Time Training, inspired by practice in (He et al., 2018; Liu et al., 2018). For the standard version of Test-Time Training, we take ten gradient steps, using batches independently gener- ated by the same image. For online version of Test-Time Training, we take only one gradient step given each new im- age. We use random crop and random horizontal ﬂip for data augmentation. See Section A2 of the appendix for computa- tional aspects of our method. In all the tables and ﬁgures, object recognition task onlyrefers to the plain ResNet model (using GN, unless otherwise speciﬁed); joint training refers to the model jointly trained on both the main task and the self-supervised task, ﬁxed at test time; this has been pro- posed as the method in Hendrycks et al. (2019a); Test-Time Training (TTT) refers to the standard version described sec- tion 2; and online Test-Time Training (TTT-Online)refers to the online version that does not discardθ(xt) for xt arriving sequentially from the same distribution. Performance for TTT-Online is calculated as the average over the entire test set; we always shufﬂe the test set before TTT-Online to avoid ordering artifacts. 3.1. Object Recognition on Corrupted Images Hendrycks & Dietterich (2019) propose to benchmark ro- bustness of object recognition with 15 types of corruptions from four broad categories: noise, blur, weather and digital. Each corruption type comes in ﬁve levels of severity, with level 5 the most severe (details and sample images in the ap- pendix). The corruptions are simulated to mimic real-world corruptions as much as possible on copies of the test set for both CIFAR-10 and ImageNet. The new test sets are named as CIFAR-10-C and ImageNet-C, respectively. In the pro- posed benchmark, training should be done on the original training set, and the diversity of corruption types should make it difﬁcult for any methods to work well across the board if it relies too much on corruption speciﬁc knowledge. For online Test-Time Training, we take the entire test set as a stream of incoming images, and update and test on each image in an online manner as it arrives. CIFAR-10-C. Our results on the level 5 corruptions (most severe) are shown in Figure 1. The results on levels 1-4 are shown in Section A4 in appendix. Across all ﬁve levels and 15 corruption types, both standard and online versions of Test-Time Training improve over the object recognition task only baseline by a large margin. The standard version always improves over joint training, and the online version often improves signiﬁcantly (>10%) over joint training and never hurts by more than 0.2%. Speciﬁcally, TTT-Online contributes >24% on the three noise types and 38% on pix- elation. For a learning problem with the seemingly unstable setup that abuses a single image, this kind of consistency is rather surprising. The baseline ResNet-26 with object recognition task only has error 8.9% on the original test set of CIFAR-10. The joint training baseline actually improves performance on the original to 8.1%. More surprisingly, unlike many other methods that trade off original performance for robustness, Test-Time Training further improves on the original test set by 0.2% consistently over multiple independent trials. This suggests that our method does not choose between speciﬁcity and generality.Test-Time Training with Self-Supervision for Generalization under Distribution Shifts originalgauss shot impulsedefocus glass motion zoom snow frost fog bright contrastelasticpixelate jpeg 0 20 40 60Accuracy (%) Object recognition task only Joint training (Hendrycks et al. 2019) TTT TTT-Online 0 20000 40000 Number of samples 60 62 64 66 68 70 72 74 76Accuracy (%) Original Sliding window average 0 20000 40000 Number of samples 12 15 18 21 24 27 30 33Accuracy (%) Gaussian Noise Sliding window average 0 20000 40000 Number of samples 16 18 20 22 24 26 28 30 32Accuracy (%) Defocus Blur Sliding window average 0 20000 40000 Number of samples 28 30 32 34 36 38Accuracy (%) Zoom Blur Sliding window average 0 20000 40000 Number of samples 33 36 39 42 45 48 51 54Accuracy (%) Fog Sliding window average 0 20000 40000 Number of samples 30 33 36 39 42 45 48 51Accuracy (%) Elastic Transform Sliding window average Figure 2.Test accuracy (%) on ImageNet-C with level 5 corruptions.Upper panel: Our approaches, TTT and TTT-Online, show signiﬁcant improvements in all corruption types over the two baselines. Lower panel: We show the accuracy of TTT-Online as the average over a sliding window of 100 samples; TTT-Online generalizes better as more samples are evaluated (x-axis), without hurting on the original distribution. We use accuracy instead of error here because the baseline performance is very low for most corruptions. Separate from our method, it is interesting to note that joint training consistently improves over the single-task baseline, as discovered by Hendrycks et al. (2019a). Hendrycks & Dietterich (2019) have also experimented with various other training methods on this benchmark, and point to Adversar- ial Logit Pairing (ALP) (Kannan et al., 2018) as the most effective approach. Results of this additional baseline on all levels of CIFAR-10-C are shown in the appendix, along with its implementation details. While surprisingly robust under some of the most severe corruptions (especially the three noise types), ALP incurs a much larger error (by a factor of two) on the original distribution and some corruptions (e.g. all levels of contrast and fog), and hurts performance signiﬁcantly when the corruptions are not as severe (espe- cially on levels 1-3); this kind of tradeoff is to be expected for methods based on adversarial training. ImageNet-C. Our results on the level 5 corruptions (most severe) are shown in Figure 2. We use accuracy instead of error for this dataset because the baseline performance is very low for most corruptions. The general trend is roughly the same as on CIFAR-10-C. The standard version of TTT always improves over the baseline and joint training, while the online version only hurts on the original by 0.1% over the baseline, but signiﬁcantly improves (by a factor of more than three) on many of the corruption types. In the lower panel of Figure 2, we visualize how the accu- racy (averaged over a sliding window) of the online version changes as more images are tested. Due to space constraints, we show this plot on the original test set, as well as every third corruption type, following the same order as in the original paper. On the original test set, there is no visible trend in performance change after updating on the 50,000 samples. With corruptions, accuracy has already risen sig- niﬁcantly after 10,000 samples, but is still rising towards the end of the 50,000 samples, indicating room for additional improvements if more samples were available. Without seeing a single label, TTT-Online behaves as if we were training on the test set from the appearance of the plots.Test-Time Training with Self-Supervision for Generalization under Distribution Shifts orig gauss shot impul defoc glass motn zoom snow frost fog brit contr elast pixel jpeg TTT-Online 8.2 25.8 22.6 30.6 14.6 34.4 18.3 17.1 20.0 18.0 16.9 11.2 15.6 21.6 18.1 21.2 UDA-SS 9.0 28.2 26.5 20.8 15.6 43.7 24.5 23.8 25.0 24.9 17.2 12.7 11.6 22.1 20.3 22.6 Table 1.Test error (%) on CIFAR-10-C with level 5 corruption.Comparison between online Test-Time Training (TTT-Online) and unsupervised domain adaptation by self-supervision (UDA-SS) (Sun et al., 2019) with access to the entire (unlabeled) test set during training. We highlight the lower error in bold. We have abbreviated the names of the corruptions, in order: original test set, Gaussian noise, shot noise, impulse noise, defocus blur, glass blue, motion blur, zoom blur, snow, frost, fog, brightness, contrast, elastic transformation, pixelation, and JPEG compression. The reported numbers for TTT-Online are the same as in Figure 1. See complete table in Table A2. 0 2000 4000 6000 8000 Number of samples 12 16 20 24 28 32 36 40 44 48Error (%) Gaussian Noise Joint training TTT TTT-Online UDA-SS 0 2000 4000 6000 8000 Number of samples 9 12 15 18 21 24 27 30 33 36Error (%) Shot Noise Joint training TTT TTT-Online UDA-SS 0 2000 4000 6000 8000 Number of samples 15 20 25 30 35 40 45 50Error (%) Impulse Noise Joint training TTT TTT-Online UDA-SS Figure 3.Test error (%) on CIFAR-10-C, for the three noise types, with gradually changing distribution.The distribution shifts are created by increasing the standard deviation of each noise type from small to large, the further we go on the x-axis. As the samples get noisier, all methods suffer greater errors the more we evaluate into the test set, but online Test-Time Training (TTT-Online) achieves gentler slopes than joint training. For the ﬁrst two noise types, TTT-Online also achieves better results over unsupervised domain adaptation by self-supervision (UDA-SS) (Sun et al., 2019). Comparison with unsupervised domain adaptation. Table 1 empirically compares online Test-Time Training (TTT-Online) with unsupervised domain adaptation through self-supervision (UDA-SS) (Sun et al., 2019), which is sim- ilar to our method in spirit but is designed for the setting of unsupervised domain adaptation (Section 5 provides a sur- vey of other related work in this setting). Given labeled data from the training distribution and unlabeled data from the test distribution, UDA-SS hopes to ﬁnd an invariant repre- sentation that extracts useful features for both distributions by learning to perform a self-supervised task, speciﬁcally rotation prediction, simultaneously on data from both. It then learns a labeling function on top of the invariant rep- resentation using the labeled data. In our experiments, the unlabeled data given to UDA-SS is the entire test set itself without the labels. Because TTT-Online can only learn from the unlabeled test samples that have already been evaluated on, it is given less information than UDA-SS at all times. In this sense, UDA- SS should be regarded as an oracle rather than a baseline. Surprisingly, TTT-Online outperforms UDA-SS on 13 out of the 15 corruptions as well as the original distribution. Our explanation is that UDA-SS has to ﬁnd an invariant representation for both distributions, while TTT-Online only adapts the representation to be good for the current test distribution. That is, TTT-Online has the ﬂexibility to forget the training distribution representation, which is no longer relevant. This suggests that in our setting, forgetting is not harmful and perhaps should even be taken advantage of. Gradually changing distribution shifts.In our previous experiments, we have been evaluating the online version under the assumption that the test inputs xt for t= 1...nare all sampled from the same test distribution Q, which can be different from the training distribution P. This assumption is indeed satisﬁed for i.i.d. samples from a shufﬂed test set. But here we show that this assumption can in fact be relaxed to allow xt ∼Qt, where Qt is close to Qt+1 (in the sense of distributional distance). We call this the assumption of gradually changing distribution shifts. We perform experiments by simulating such distribution shifts on the three noise types of CIFAR-10-C. For each noise type, xt is corrupted with standard deviation σt, and σ1,...,σ n interpolate between the standard deviation of level 1 and level 5. So xt is more severely corrupted as we evaluate further into the test set and t grows larger. As shown in Figure 3, TTT-Online still improves upon joint training (and our standard version) with this relaxed assumption, and even upon UDA-SS for the ﬁrst two noise types.Test-Time Training with Self-Supervision for Generalization under Distribution Shifts Accuracy (%) Airplane Bird Car Dog Cat Horse Ship Average Object recognition task only 67.9 35.8 42.6 14.7 52.0 42.0 66.7 41.4 Joint training (Hendrycks et al., 2019a) 70.2 36.7 42.6 15.5 52.0 44.0 66.7 42.4 TTT (standard version) 70.2 39.2 42.6 21.6 54.7 46.0 77.8 45.2 TTT-Online 70.2 39.2 42.6 22.4 54.7 46.0 77.8 45.4 Table 2.Class-wise and average classiﬁcation accuracy (%) on CIFAR classes in VID-Robust, adapted from (Shankar et al., 2019). Test-Time Training (TTT) and online Test-Time Training (TTT-Online) improve over the two baselines on average, and by a large margin on “ship” and “dog” classes where the rotation task is more meaningful than in classes like “airplane” (sample images in Figure A7). 3.2. Object Recognition on Video Frames The Robust ImageNet Video Classiﬁcation (VID-Robust) dataset was developed by Shankar et al. (2019) from the Ima- geNet Video detection dataset (Russakovsky et al., 2015), to demonstrate how deep models for object recognition trained on ImageNet (still images) fail to adapt well to video frames. The VID-Robust dataset contains 1109 sets of video frames in 30 classes; each set is a short video clip of frames that are similar to an anchor frame. Our results are reported on the anchor frames. To map the 1000 ImageNet classes to the 30 VID-Robust classes, we use the max-conversion function in Shankar et al. (2019). Without any modiﬁcations for videos, we apply our method to VID-Robust on top of the same ImageNet model as in the previous subsection. Our classiﬁcation accuracy is reported in Table 3. In addition, we take the seven classes in VID-Robust that overlap with CIFAR-10, and re-scale those video frames to the size of CIFAR-10 images, as a new test set for the model trained on CIFAR-10 in the previous subsection. Again, we apply our method to this dataset without any modiﬁcations. Our results are shown in Table 2, with a breakdown for each class. Noticing that Test-Time Training does not improve on the airplane class, we inspect some airplane samples (Figure A7), and observe black margins on two sides of most images, which provide a trivial hint for rotation prediction. In addition, given an image of airplanes in the sky, it is often impossible even for humans to tell if it is rotated. This shows that our method requires the self-supervised task to be both well deﬁned and non-trivial. 3.3. CIFAR-10.1: Unknown Distribution Shifts CIFAR-10.1 (Recht et al., 2018) is a new test set of size 2000 modeled after CIFAR-10, with the exact same classes and image dimensionality, following the dataset creation process documented by the original CIFAR-10 paper as closely as possible. The purpose is to investigate the distribution shifts present between the two test sets, and the effect on object recognition. All models tested by the authors suffer a large performance drop on CIFAR-10.1 comparing to CIFAR-10, even though there is no human noticeable difference, and Method Accuracy (%) Object recognition task only 62.7 Joint training (Hendrycks et al., 2019a) 63.5 TTT (standard version) 63.8 TTT-Online 64.3 Table 3.Test accuracy (%) on VID-Robust dataset (Shankar et al., 2019). TTT and TTT-Online improve over the baselines. Method Error (%) Object recognition task only 17.4 Joint training (Hendrycks et al., 2019a) 16.7 TTT (standard version) 15.9 Table 4.Test error (%) on CIFAR-10.1 (Recht et al., 2018). TTT is the ﬁrst method to improve the performance of an existing model on this new test set. both have the same human accuracy. This demonstrates how insidious and ubiquitous distribution shifts are, even when researchers strive to minimize them. The distribution shifts from CIFAR-10 to CIFAR-10.1 pose an extremely difﬁcult problem, and no prior work has been able to improve the performance of an existing model on this new test set, probably because: 1) researchers cannot even identify the distribution shifts, let alone describe them mathematically; 2) the samples in CIFAR-10.1 are only revealed at test time; and even if they were revealed during training, the distribution shifts are too subtle, and the sample size is too small, for domain adaptation (Recht et al., 2018). On the original CIFAR-10 test set, the baseline with only object recognition has error 8.9%, and with joint training has 8.1%; comparing to the ﬁrst two rows of Table 4, both suffer the typical performance drop (by a factor of two). TTT yields an improvement of 0.8% (relative improvement of 4.8%) over joint training. We recognize that this improve- ment is small relative to the performance drop, but see it as an encouraging ﬁrst step for this very difﬁcult problem.Test-Time Training with Self-Supervision for Generalization under Distribution Shifts 0 10 20 30 40 50 60 Gradient inner product 0 1 2 3 4 5Improvement (%) Level 5 Level 4 Level 3 Level 2 Level 1 0 10 20 30 40 50 60 Gradient inner product 0 5 10 15 20 25 30 35Improvement (%) Level 5 Level 4 Level 3 Level 2 Level 1 Figure 4.Scatter plot of the inner product between the gradients (on the shared feature extractor θe) of the main task lm and the self- supervised task le, and the improvement in test error (%) from Test-Time Training, for the standard (left) and online (right) version. Each point is the average over a test set, and each scatter plot has 75 test sets, from all 15 types of corruptions over ﬁve levels as described in subsection 3.1. The blue lines and bands are the best linear ﬁts and the 99% conﬁdence intervals. The linear correlation coefﬁcients are 0.93 and 0.89 respectively, indicating strong positive correlation between the two quantities, as suggested by Theorem 1. 4. Theoretical Results This section contains our preliminary study of when and why Test-Time Training is expected to work. For convex models, we prove that positive gradient correlation between the loss functions leads to better performance on the main task after Test-Time Training. Equipped with this insight, we then empirically demonstrate that gradient correlation governs the success of Test-Time Training on the deep learning model discussed in Section 3. Before stating our main theoretical result, we ﬁrst illustrate the general intuition with a toy model. Consider a regression problem where x∈Rd denotes the input, y1 ∈R denotes the label, and the objective is the square loss (ˆy−y1)2/2 for a prediction ˆy. Consider a two layer linear network parametrized by A∈Rh×d and v ∈Rh (where hstands for the hidden dimension). The prediction according to this model is ˆy= v⊤Ax, and the main task loss is lm(x,y1; A,v) = 1 2 ( y1 −v⊤Ax )2 . (4) In addition, consider a self-supervised regression task that also uses the square loss and automatically generates a label ys for x. Let the self-supervised head be parametrized by w∈Rh. Then the self-supervised task loss is ls(x,y2; A,w) = 1 2 ( y2 −w⊤Ax )2 . (5) Now we apply Test-Time Training to update the shared feature extractor Aby one step of gradient descent on ls, which we can compute with y2 known. This gives us A′←A−η ( y2 −w⊤Ax )( −wx⊤) , (6) where A′is the updated matrix and ηis the learning rate. If we set η= η∗where η∗= y1 −v⊤Ax (y2 −w⊤Ax) v⊤wx⊤x, (7) then with some simple algebra, it is easy to see that the main task loss lm(x,y1; A′,v) = 0. Concretely, Test-Time Training drives the main task loss down to zero with a single gradient step for a carefully chosen learning rate. In prac- tice, this learning rate is unknown since it depends on the unknown y1. However, since our model is convex, as long as η∗is positive, it sufﬁces to set η to be a small positive constant (see details in the appendix). If x̸= 0, one sufﬁ- cient condition for η∗to be positive (when neither loss is zero) is to have sign ( y1 −v⊤Ax ) = sign ( y2 −w⊤Ax ) (8) and v⊤w>0 . (9) For our toy model, both parts of the condition above have an intuition interpretation. The ﬁrst part says that the mistakes should be correlated, in the sense that predictions from both tasks are mistaken in the same direction. The second part, v⊤w>0, says that the decision boundaries on the feature space should be correlated. In fact, these two parts hold iff. ⟨∇lm(A),∇ls(A)⟩>0 (see a simple proof of this fact in the appendix). To summarize, if the gradients have positive correlation, Test-Time Training is guaranteed to reduce the main task loss. Our main theoretical result extends this to general smooth and convex loss functions.Test-Time Training with Self-Supervision for Generalization under Distribution Shifts Theorem 1. Let lm(x,y; θ) denote the main task loss on test instance x,y with parameters θ, and ls(x; θ) the self- supervised task loss that only depends onx. Assume that for all x,y, lm(x,y; θ) is differentiable, convex andβ-smooth in θ, and both ∥∇lm(x,y; θ)∥,∥∇ls(x,θ)∥≤ Gfor all θ. With a ﬁxed learning rate η= ϵ βG2 , for every x,y such that ⟨∇lm(x,y; θ),∇ls(x; θ)⟩>ϵ, (10) we have lm(x,y; θ) >lm(x,y; θ(x)), (11) where θ(x) = θ−η∇ls(x; θ) i.e. Test-Time Training with one step of gradient descent. The proof uses standard techniques in optimization, and is left for the appendix. Theorem 1 reveals gradient correlation as a determining factor of the success of Test-Time Training in the smooth and convex case. In Figure 4, we empirically show that our insight also holds for non-convex loss func- tions, on the deep learning model and across the diverse set of corruptions considered in Section 3; stronger gradient cor- relation clearly indicates more performance improvement over the baseline. 5. Related Work Learning on test instances. Shocher et al. (2018) pro- vide a key inspiration for our work by showing that image super-resolution could be learned at test time simply by try- ing to upsample a downsampled version of the input image. More recently, Bau et al. (2019) improve photo manipula- tion by adapting a pre-trained GAN to the statistics of the input image. One of the earlier examples of this idea comes from Jain & Learned-Miller (2011), who improve Viola- Jones face detection (Viola et al., 2001) by bootstrapping the more difﬁcult faces in an image from the more easily detected faces in that same image. The online version of our algorithm is inspired by the work of Mullapudi et al. (2018), which makes video segmentation more efﬁcient by using a student model that learns online from a teacher model. The idea of online updates has also been used in Kalal et al. (2011) for tracking and detection. A recent work in echocardiography (Zhu et al., 2019) improves the deep learning model that tracks myocardial motion and cardiac blood ﬂow with sequential updates. Lastly, we share the philosophy of transductive learning (Vapnik, 2013; Gam- merman et al., 1998), but have little in common with their classical algorithms; recent work by Tripuraneni & Mackey (2019) theoretically explores this for linear prediction, in the context of debiasing the LASSO estimator. Self-supervised learning studies how to create labels from the data, by designing various pretext tasks that can learn semantic information without human annotations, such as context prediction (Doersch et al., 2015), solving jig- saw puzzles (Noroozi & Favaro, 2016), colorization (Lars- son et al., 2017; Zhang et al., 2016), noise prediction (Bo- janowski & Joulin, 2017), feature clustering (Caron et al., 2018). Our paper uses rotation prediction (Gidaris et al., 2018). Asano et al. (2019) show that self-supervised learn- ing on only a single image, surprisingly, can produce low- level features that generalize well. Closely related to our work, Hendrycks et al. (2019a) propose that jointly training a main task and a self-supervised task (our joint training baseline in Section 3) can improve robustness on the main task. The same idea is used in few-shot learning (Su et al., 2019), domain generalization (Carlucci et al., 2019), and unsupervised domain adaptation (Sun et al., 2019). Adversarial robustness studies the robust risk RP,∆(θ) = Ex,y∼P maxδ∈∆ l(x + δ,y; θ), where l is some loss function, and ∆ is the set of perturbations; ∆ is often chosen as the Lp ball, for p ∈{1,2,∞}. Many popular algorithms formulate and solve this as a robust optimization problem (Goodfellow et al., 2014; Madry et al., 2017; Sinha et al., 2017; Raghunathan et al., 2018; Wong & Kolter, 2017; Croce et al., 2018), and the most well known technique is adversarial training. Another line of work is based on randomized smoothing (Cohen et al., 2019; Salman et al., 2019), while some other approaches, such as input transformations (Guo et al., 2017; Song et al., 2017), are shown to be less effective (Athalye et al., 2018). There are two main problems with the approaches above. First, all of them can be seen as smoothing the decision boundary. This establishes a theoretical tradeoff between accuracy and robustness (Tsipras et al., 2018; Zhang et al., 2019), which we also observe empirically with our adversarial training baseline in Section 3. Intuitively, the more diverse ∆ is, the less effective this one-boundary-ﬁts-all approach can be for a particular element of ∆. Second, adversarial methods rely heavily on the mathematical structure of ∆, which might not accurately model perturbations in the real world. Therefore, generalization remains hard outside of the ∆ we know in advance or can mathematically model, especially for non-adversarial distribution shifts. Empirically, Kang et al. (2019) shows that robustness for one ∆ might not transfer to another, and training on the L∞ball actually hurts robustness on the L1 ball. Non-adversarial robustness studies the effect of corrup- tions, perturbations, out-of-distribution examples, and real- world distribution shifts (Hendrycks et al., 2019b;a; 2018; Hendrycks & Gimpel, 2016). Geirhos et al. (2018) show that training on images corrupted by Gaussian noise makes deep learning models robust to this particular noise type, but does not improve performance on images corrupted by another noise type e.g. salt-and-pepper noise.Test-Time Training with Self-Supervision for Generalization under Distribution Shifts Unsupervised domain adaptation (a.k.a. transfer learn- ing) studies the problem of distribution shifts, when an unlabeled dataset from the test distribution (target domain) is available at training time, in addition to a labeled dataset from the training distribution (source domain) (Chen et al., 2011; Gong et al., 2012; Long et al., 2015; Ganin et al., 2016; Long et al., 2016; Tzeng et al., 2017; Hoffman et al., 2017; Csurka, 2017; Chen et al., 2018). The limitation of the problem setting, however, is that generalization might only be improved for this speciﬁc test distribution, which can be difﬁcult to anticipate in advance. Prior work try to anticipate broader distributions by using multiple and evolv- ing domains (Hoffman et al., 2018; 2012; 2014). Test-Time Training does not anticipate any test distribution, by chang- ing the setting of unsupervised domain adaptation, while taking inspiration from its algorithms. Our paper is a follow- up to Sun et al. (2019), which we explain and empirically compare with in Section 3. Our update rule can be viewed as performing one-sample unsupervised domain adaptation on the ﬂy, with the caveat that standard domain adaptation techniques might become ill-deﬁned when there is only one sample from the target domain. Domain generalization studies the setting where a meta distribution generates multiple environment distributions, some of which are available during training (source), while others are used for testing (target) (Li et al., 2018; Shankar et al., 2018; Muandet et al., 2013; Balaji et al., 2018; Ghifary et al., 2015; Motiian et al., 2017; Li et al., 2017a; Gan et al., 2016). With only a few environments, information on the meta distribution is often too scarce to be helpful, and with many environments, we are back to the i.i.d. setting where each environment can be seen as a sample, and a strong baseline is to simply train on all the environments (Li et al., 2019). The setting of domain generalization is limited by the inherent tradeoff between speciﬁcity and generality of a ﬁxed decision boundary, and the fact that generalization is again elusive outside of the meta distribution i.e. the actual P learned by the algorithm. One (few)-shot learning studies how to learn a new task or a new classiﬁcation category using only one (or a few) sample(s), on top of a general representation that has been learned on diverse samples (Snell et al., 2017; Vinyals et al., 2016; Fei-Fei et al., 2006; Ravi & Larochelle, 2016; Li et al., 2017b; Finn et al., 2017; Gidaris & Komodakis, 2018). Our update rule can be viewed as performing one-shot self- supervised learning and can potentially be improved by progress in one-shot learning. Continual learning (a.k.a. learning without forgetting) studies the setting where a model is made to learn a sequence of tasks, and not forget about the earlier ones while training for the later (Li & Hoiem, 2017; Lopez-Paz & Ranzato, 2017; Kirkpatrick et al., 2017; Santoro et al., 2016). In contrast, with Test-Time Training, we are not concerned about forgetting the past test samples since they have already been evaluated on; and if a past sample comes up by any chance, it would go through Test-Time Training again. In addition, the impact of forgetting the training set is minimal, because both tasks have already been jointly trained. Online learning (a.k.a. online optimization) is a well- studied area of learning theory (Shalev-Shwartz et al., 2012; Hazan et al., 2016). The basic setting repeats the following: receive xt, predict ˆyt, receive yt from a worst-case oracle, and learn. Final performance is evaluated using the regret, which colloquially translates to how much worse the online learning algorithm performs in comparison to the best ﬁxed model in hindsight. In contrast, our setting never reveals any yt during testing even for the online version, so we do not need to invoke the concept of the worst-case oracle or the regret. Also, due to the lack of feedback from the envi- ronment after predicting, our algorithm is motivated to learn (with self-supervision) before predicting ˆyt instead of after. Note that some of the previously covered papers (Hoffman et al., 2014; Jain & Learned-Miller, 2011; Mullapudi et al., 2018) use the term “online learning” outside of the learning theory setting, so the term can be overloaded. 6. Discussion The idea of test-time training also makes sense for other tasks, such as segmentation and detection, and in other ﬁelds, such as speech recognition and natural language process- ing. For machine learning practitioners with prior domain knowledge in their respective ﬁelds, their expertise can be leveraged to design better special-purpose self-supervised tasks for test-time training. Researchers for general-purpose self-supervised tasks can also use test-time training as an evaluation benchmark, in addition to the currently prevalent benchmark of pre-training and ﬁne-tuning. More generally, we hope this paper can encourage re- searchers to abandon the self-imposed constraint of a ﬁxed decision boundary for testing, or even the artiﬁcial division between training and testing altogether. Our work is but a small step toward a new paradigm where much of the learning happens after a model is deployed. Acknowledgements. This work is supported by NSF grant 1764033, DARPA and Berkeley DeepDrive. This paper took a long time to develop, and beneﬁted from con- versations with many of our colleagues, including Ben Recht and his students Ludwig Schmidt, Vaishaal Shanker and Becca Roelofs; Ravi Teja Mullapudi, Achal Dave and Deva Ramanan; and Armin Askari, Allan Jabri, Ashish Kumar, Angjoo Kanazawa and Jitendra Malik.Test-Time Training with Self-Supervision for Generalization under Distribution Shifts References Asano, Y . M., Rupprecht, C., and Vedaldi, A. Surprising effectiveness of few-image unsupervised feature learning. arXiv preprint arXiv:1904.13132, 2019. Athalye, A., Carlini, N., and Wagner, D. Obfuscated gradients give a false sense of security: Circumvent- ing defenses to adversarial examples. arXiv preprint arXiv:1802.00420, 2018. Balaji, Y ., Sankaranarayanan, S., and Chellappa, R. Metareg: Towards domain generalization using meta-regularization. In Advances in Neural Information Processing Systems, pp. 998–1008, 2018. Bau, D., Strobelt, H., Peebles, W., Wulff, J., Zhou, B., Zhu, J.-Y ., and Torralba, A. Semantic photo manipulation with a generative image prior. ACM Transactions on Graphics (TOG), 38(4):59, 2019. Bojanowski, P. and Joulin, A. Unsupervised learning by predicting noise. In Proceedings of the 34th International Conference on Machine Learning-Volume 70, pp. 517– 526. JMLR. org, 2017. Carlucci, F. M., D’Innocente, A., Bucci, S., Caputo, B., and Tommasi, T. Domain generalization by solving jigsaw puzzles. In Proceedings of the IEEE Conference on Com- puter Vision and Pattern Recognition , pp. 2229–2238, 2019. Caron, M., Bojanowski, P., Joulin, A., and Douze, M. Deep clustering for unsupervised learning of visual features. In Proceedings of the European Conference on Computer Vision (ECCV), pp. 132–149, 2018. Caruana, R. Multitask learning. Machine learning, 28(1): 41–75, 1997. Chen, M., Weinberger, K. Q., and Blitzer, J. Co-training for domain adaptation. In Advances in neural information processing systems, pp. 2456–2464, 2011. Chen, X., Sun, Y ., Athiwaratkun, B., Cardie, C., and Wein- berger, K. Adversarial deep averaging networks for cross- lingual sentiment classiﬁcation. Transactions of the Asso- ciation for Computational Linguistics, 6:557–570, 2018. Cohen, J. M., Rosenfeld, E., and Kolter, J. Z. Certiﬁed adversarial robustness via randomized smoothing. arXiv preprint arXiv:1902.02918, 2019. Croce, F., Andriushchenko, M., and Hein, M. Provable robustness of relu networks via maximization of linear regions. arXiv preprint arXiv:1810.07481, 2018. Csurka, G. Domain adaptation for visual applications: A comprehensive survey. arXiv preprint arXiv:1702.05374, 2017. Ding, G. W., Wang, L., and Jin, X. AdverTorch v0.1: An adversarial robustness toolbox based on pytorch. arXiv preprint arXiv:1902.07623, 2019. Doersch, C., Gupta, A., and Efros, A. A. Unsupervised visual representation learning by context prediction. In Proceedings of the IEEE International Conference on Computer Vision, pp. 1422–1430, 2015. Fei-Fei, L., Fergus, R., and Perona, P. One-shot learning of object categories. IEEE transactions on pattern analysis and machine intelligence, 28(4):594–611, 2006. Finn, C., Abbeel, P., and Levine, S. Model-agnostic meta- learning for fast adaptation of deep networks. In Proceed- ings of the 34th International Conference on Machine Learning-Volume 70, pp. 1126–1135. JMLR. org, 2017. Gammerman, A., V ovk, V ., and Vapnik, V . Learning by transduction. In Proceedings of the Fourteenth conference on Uncertainty in artiﬁcial intelligence , pp. 148–155. Morgan Kaufmann Publishers Inc., 1998. Gan, C., Yang, T., and Gong, B. Learning attributes equals multi-source domain generalization. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pp. 87–97, 2016. Ganin, Y ., Ustinova, E., Ajakan, H., Germain, P., Larochelle, H., Laviolette, F., Marchand, M., and Lempitsky, V . Domain-adversarial training of neural networks. The Journal of Machine Learning Research, 17(1):2096–2030, 2016. Geirhos, R., Temme, C. R., Rauber, J., Sch¨utt, H. H., Bethge, M., and Wichmann, F. A. Generalisation in humans and deep neural networks. In Advances in Neural Information Processing Systems, pp. 7538–7550, 2018. Ghifary, M., Bastiaan Kleijn, W., Zhang, M., and Balduzzi, D. Domain generalization for object recognition with multi-task autoencoders. In Proceedings of the IEEE international conference on computer vision, pp. 2551– 2559, 2015. Gidaris, S. and Komodakis, N. Dynamic few-shot visual learning without forgetting. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pp. 4367–4375, 2018. Gidaris, S., Singh, P., and Komodakis, N. Unsupervised rep- resentation learning by predicting image rotations. arXiv preprint arXiv:1803.07728, 2018. Gong, B., Shi, Y ., Sha, F., and Grauman, K. Geodesic ﬂow kernel for unsupervised domain adaptation. In2012 IEEE Conference on Computer Vision and Pattern Recognition, pp. 2066–2073. IEEE, 2012.Test-Time Training with Self-Supervision for Generalization under Distribution Shifts Goodfellow, I. J., Shlens, J., and Szegedy, C. Explain- ing and harnessing adversarial examples. arXiv preprint arXiv:1412.6572, 2014. Guo, C., Rana, M., Cisse, M., and van der Maaten, L. Coun- tering adversarial images using input transformations. arXiv preprint arXiv:1711.00117, 2017. Hazan, E. et al. Introduction to online convex optimization. Foundations and Trends® in Optimization, 2(3-4):157– 325, 2016. He, K., Zhang, X., Ren, S., and Sun, J. Deep residual learn- ing for image recognition. In Proceedings of the IEEE conference on computer vision and pattern recognition, pp. 770–778, 2016a. He, K., Zhang, X., Ren, S., and Sun, J. Identity mappings in deep residual networks. In European conference on computer vision, pp. 630–645. Springer, 2016b. He, K., Girshick, R., and Doll ´ar, P. Rethinking imagenet pre-training. arXiv preprint arXiv:1811.08883, 2018. Hendrycks, D. and Dietterich, T. Benchmarking neural network robustness to common corruptions and perturba- tions. arXiv preprint arXiv:1903.12261, 2019. Hendrycks, D. and Gimpel, K. A baseline for detecting misclassiﬁed and out-of-distribution examples in neural networks. arXiv preprint arXiv:1610.02136, 2016. Hendrycks, D., Mazeika, M., Wilson, D., and Gimpel, K. Using trusted data to train deep networks on labels cor- rupted by severe noise. InAdvances in neural information processing systems, pp. 10456–10465, 2018. Hendrycks, D., Lee, K., and Mazeika, M. Using pre-training can improve model robustness and uncertainty. arXiv preprint arXiv:1901.09960, 2019a. Hendrycks, D., Mazeika, M., Kadavath, S., and Song, D. Improving model robustness and uncertainty estimates with self-supervised learning. arXiv preprint, 2019b. Hoffman, J., Kulis, B., Darrell, T., and Saenko, K. Discover- ing latent domains for multisource domain adaptation. In European Conference on Computer Vision, pp. 702–715. Springer, 2012. Hoffman, J., Darrell, T., and Saenko, K. Continuous man- ifold based adaptation for evolving visual domains. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pp. 867–874, 2014. Hoffman, J., Tzeng, E., Park, T., Zhu, J.-Y ., Isola, P., Saenko, K., Efros, A. A., and Darrell, T. Cycada: Cycle- consistent adversarial domain adaptation. arXiv preprint arXiv:1711.03213, 2017. Hoffman, J., Mohri, M., and Zhang, N. Algorithms and theory for multiple-source adaptation. In Advances in Neural Information Processing Systems, pp. 8246–8256, 2018. Huang, G., Sun, Y ., Liu, Z., Sedra, D., and Weinberger, K. Q. Deep networks with stochastic depth. In European conference on computer vision, pp. 646–661. Springer, 2016. Ioffe, S. and Szegedy, C. Batch normalization: Accelerating deep network training by reducing internal covariate shift. arXiv preprint arXiv:1502.03167, 2015. Jain, V . and Learned-Miller, E. Online domain adaptation of a pre-trained cascade of classiﬁers. In CVPR 2011, pp. 577–584. IEEE, 2011. Kalal, Z., Mikolajczyk, K., and Matas, J. Tracking-learning- detection. IEEE transactions on pattern analysis and machine intelligence, 34(7):1409–1422, 2011. Kang, D., Sun, Y ., Brown, T., Hendrycks, D., and Steinhardt, J. Transfer of adversarial robustness between perturbation types. arXiv preprint arXiv:1905.01034, 2019. Kannan, H., Kurakin, A., and Goodfellow, I. Adversarial logit pairing. arXiv preprint arXiv:1803.06373, 2018. Kirkpatrick, J., Pascanu, R., Rabinowitz, N., Veness, J., Des- jardins, G., Rusu, A. A., Milan, K., Quan, J., Ramalho, T., Grabska-Barwinska, A., et al. Overcoming catastrophic forgetting in neural networks. Proceedings of the national academy of sciences, 114(13):3521–3526, 2017. Krizhevsky, A. and Hinton, G. Learning multiple layers of features from tiny images. Technical report, Citeseer, 2009. Larsson, G., Maire, M., and Shakhnarovich, G. Colorization as a proxy task for visual understanding. In CVPR, 2017. Li, D., Yang, Y ., Song, Y .-Z., and Hospedales, T. M. Deeper, broader and artier domain generalization. In Proceed- ings of the IEEE International Conference on Computer Vision, pp. 5542–5550, 2017a. Li, D., Zhang, J., Yang, Y ., Liu, C., Song, Y .-Z., and Hospedales, T. M. Episodic training for domain gen- eralization. arXiv preprint arXiv:1902.00113, 2019. Li, Y ., Tian, X., Gong, M., Liu, Y ., Liu, T., Zhang, K., and Tao, D. Deep domain generalization via conditional invariant adversarial networks. In Proceedings of the European Conference on Computer Vision (ECCV), pp. 624–639, 2018.Test-Time Training with Self-Supervision for Generalization under Distribution Shifts Li, Z. and Hoiem, D. Learning without forgetting. IEEE transactions on pattern analysis and machine intelligence, 40(12):2935–2947, 2017. Li, Z., Zhou, F., Chen, F., and Li, H. Meta-sgd: Learning to learn quickly for few-shot learning. arXiv preprint arXiv:1707.09835, 2017b. Liu, Z., Sun, M., Zhou, T., Huang, G., and Darrell, T. Re- thinking the value of network pruning. arXiv preprint arXiv:1810.05270, 2018. Long, M., Cao, Y ., Wang, J., and Jordan, M. I. Learn- ing transferable features with deep adaptation networks. arXiv preprint arXiv:1502.02791, 2015. Long, M., Zhu, H., Wang, J., and Jordan, M. I. Unsupervised domain adaptation with residual transfer networks. In Advances in Neural Information Processing Systems, pp. 136–144, 2016. Lopez-Paz, D. and Ranzato, M. Gradient episodic memory for continual learning. In Advances in Neural Information Processing Systems, pp. 6467–6476, 2017. Madry, A., Makelov, A., Schmidt, L., Tsipras, D., and Vladu, A. Towards deep learning models resistant to adversarial attacks. arXiv preprint arXiv:1706.06083 , 2017. Motiian, S., Piccirilli, M., Adjeroh, D. A., and Doretto, G. Uniﬁed deep supervised domain adaptation and gen- eralization. In Proceedings of the IEEE International Conference on Computer Vision, pp. 5715–5725, 2017. Muandet, K., Balduzzi, D., and Sch ¨olkopf, B. Domain generalization via invariant feature representation. In International Conference on Machine Learning, pp. 10– 18, 2013. Mullapudi, R. T., Chen, S., Zhang, K., Ramanan, D., and Fatahalian, K. Online model distillation for efﬁcient video inference. arXiv preprint arXiv:1812.02699, 2018. Noroozi, M. and Favaro, P. Unsupervised learning of visual representations by solving jigsaw puzzles. In European Conference on Computer Vision , pp. 69–84. Springer, 2016. Raghunathan, A., Steinhardt, J., and Liang, P. Certiﬁed defenses against adversarial examples. arXiv preprint arXiv:1801.09344, 2018. Ravi, S. and Larochelle, H. Optimization as a model for few-shot learning. IEEE transactions on pattern analysis and machine intelligence, 2016. Recht, B., Roelofs, R., Schmidt, L., and Shankar, V . Do cifar-10 classiﬁers generalize to cifar-10? arXiv preprint arXiv:1806.00451, 2018. Russakovsky, O., Deng, J., Su, H., Krause, J., Satheesh, S., Ma, S., Huang, Z., Karpathy, A., Khosla, A., Bernstein, M., Berg, A. C., and Fei-Fei, L. ImageNet Large Scale Visual Recognition Challenge. International Journal of Computer Vision (IJCV) , 115(3):211–252, 2015. doi: 10.1007/s11263-015-0816-y. Salman, H., Yang, G., Li, J., Zhang, P., Zhang, H., Razen- shteyn, I., and Bubeck, S. Provably robust deep learn- ing via adversarially trained smoothed classiﬁers. arXiv preprint arXiv:1906.04584, 2019. Santoro, A., Bartunov, S., Botvinick, M., Wierstra, D., and Lillicrap, T. Meta-learning with memory-augmented neu- ral networks. In International conference on machine learning, pp. 1842–1850, 2016. Shalev-Shwartz, S. et al. Online learning and online con- vex optimization. Foundations and Trends® in Machine Learning, 4(2):107–194, 2012. Shankar, S., Piratla, V ., Chakrabarti, S., Chaudhuri, S., Jyothi, P., and Sarawagi, S. Generalizing across domains via cross-gradient training. arXiv preprint arXiv:1804.10745, 2018. Shankar, V ., Dave, A., Roelofs, R., Ramanan, D., Recht, B., and Schmidt, L. Do image classiﬁers generalize across time? arXiv, 2019. Shocher, A., Cohen, N., and Irani, M. zero-shot super- resolution using deep internal learning. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pp. 3118–3126, 2018. Sinha, A., Namkoong, H., and Duchi, J. Certifying some dis- tributional robustness with principled adversarial training. arXiv preprint arXiv:1710.10571, 2017. Snell, J., Swersky, K., and Zemel, R. Prototypical networks for few-shot learning. In Advances in Neural Information Processing Systems, pp. 4077–4087, 2017. Song, Y ., Kim, T., Nowozin, S., Ermon, S., and Kushman, N. Pixeldefend: Leveraging generative models to understand and defend against adversarial examples. arXiv preprint arXiv:1710.10766, 2017. Su, J.-C., Maji, S., and Hariharan, B. Boosting supervi- sion with self-supervision for few-shot learning. arXiv preprint arXiv:1906.07079, 2019. Sun, Y ., Tzeng, E., Darrell, T., and Efros, A. A. Unsuper- vised domain adaptation through self-supervision. arXiv preprint, 2019.Test-Time Training with Self-Supervision for Generalization under Distribution Shifts Tripuraneni, N. and Mackey, L. Debiasing linear prediction. arXiv preprint arXiv:1908.02341, 2019. Tsipras, D., Santurkar, S., Engstrom, L., Turner, A., and Madry, A. Robustness may be at odds with accuracy. arXiv preprint arXiv:1805.12152, 2018. Tzeng, E., Hoffman, J., Saenko, K., and Darrell, T. Adver- sarial discriminative domain adaptation. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pp. 7167–7176, 2017. Vapnik, V .The nature of statistical learning theory. Springer science & business media, 2013. Vinyals, O., Blundell, C., Lillicrap, T., Wierstra, D., et al. Matching networks for one shot learning. In Advances in neural information processing systems, pp. 3630–3638, 2016. Viola, P., Jones, M., et al. Rapid object detection using a boosted cascade of simple features. CVPR (1), 1(511- 518):3, 2001. Wong, E. and Kolter, J. Z. Provable defenses against adver- sarial examples via the convex outer adversarial polytope. arXiv preprint arXiv:1711.00851, 2017. Zhang, H., Yu, Y ., Jiao, J., Xing, E. P., Ghaoui, L. E., and Jor- dan, M. I. Theoretically principled trade-off between ro- bustness and accuracy. arXiv preprint arXiv:1901.08573, 2019. Zhang, R., Isola, P., and Efros, A. A. Colorful image col- orization. In European conference on computer vision, pp. 649–666. Springer, 2016. Zhu, W., Huang, Y ., Vannan, M. A., Liu, S., Xu, D., Fan, W., Qian, Z., and Xie, X. Neural multi-scale self-supervised registration for echocardiogram dense tracking. arXiv preprint arXiv:1906.07357, 2019.Appendix: Test-Time Training with Self-Supervision for Generalization under Distribution Shifts A1. Informal Discussion on Our Variable Decision Boundary In the introduction, we claim that in traditional supervised learning θgives a ﬁxed decision boundary, while ourθgives a variable decision boundary. Here we informally discuss this claim. Denote the input space Xand output space Y. A decision boundary is simply a mapping f : X →Y. Let Θ be a model class e.g Rd. Now consider a family of parametrized functions gθ : X→Y , where θ∈Θ. In the context of deep learning, gis the neural network architecture and θcontains the parameters. We say that f is a ﬁxed decision boundary w.r.t. g and Θ if there exists θ ∈Θ s.t. f(x) = gθ(x) for every x ∈X , and a variable decision boundary if for every x∈X, there exists θ∈Θ s.t. f(x) = gθ(x). Note how selection of θcan depend on xfor a variable decision boundary, and cannot for a ﬁxed one. It is then trivial to verify that our claim is true under those deﬁnitions. A critical reader might say that with an arbitrarily large model class, can’t every decision boundary be ﬁxed? Yes, but this is not the end of the story. Let d = dim( X) × dim(Y), and consider the enormous model class Θ′= Rd which is capable of representing all possible mappings be- tween Xand Y. Let g′ θ′ simply be the mapping represented by θ′ ∈Θ′. A variable decision boundary w.r.t. g and Θ then indeed must be a ﬁxed decision boundary w.r.t. g′and Θ′, but we would like to note two things. First, without any prior knowledge, generalization in Θ′is impossible with any ﬁnite amount of training data; reasoning about g′and Θ′is most likely not productive from an algorithmic point of view, and the concept of a variable decision boundary is to avoid such reasoning. Second, selecting θbased on xfor a variable decision boundary can be thought of as “training” on all points x ∈Rd; however, “training” only happens when necessary, for the xthat it actually encounters. Altogether, the concept of a variable decision boundary is different from what can be described by traditional learning theory. A formal discussion is beyond the scope of this paper and might be of interest to future work. A2. Computational Aspects of Our Method At test time, our method is 2 × batch size × number of iterations times slower than regular test- ing, which only performs a single forward pass for each sample. As the ﬁrst work on Test-Time Training, this paper is not as concerned about computational efﬁciency as improving robustness, but here we provide two poten- tial solutions that might be useful, but have not been thor- oughly veriﬁed. The ﬁrst is to use the thresholding trick on ls, introduced as a solution for the small batches prob- lem in the method section. For the models considered in our experiments, roughly 80% of the test instances fall below the threshold, so Test-Time Training can only be performed on the other 20% without much effect on per- formance, because those 20% contain most of the sam- ples with wrong predictions. The second is to reduce the number of iterations of test-time updates. For the online version, the number of iterations is al- ready 1, so there is nothing to do. For the standard ver- sion, we have done some preliminary experiments setting number of iterations to 1 (instead of 10) and learn- ing rate to 0.01 (instead of 0.001), and observing results almost as good as the standard hyper-parameter setting. A more in depth discussion on efﬁciency is left for future works, which might, during training, explicitly make the model amenable to fast updates. A3. Proofs Here we prove the theoretical results in the main paper. A3.1. The Toy Problem The following setting applies to the two lemmas; this is simply the setting of our toy problem, reproduced here for ease of reference.Test-Time Training with Self-Supervision for Generalization under Distribution Shifts Consider a two layer linear network parametrized by A∈ Rh×d (shared) and v,w ∈Rh (ﬁxed) for the two heads, respectively. Denote x∈Rd the input and y1,y2 ∈R the labels for the two tasks, respectively. For the main task loss lm(A; v) = 1 2 ( y1 −v⊤Ax )2 , (12) and the self-supervised task loss ls(A; w) = 1 2 ( y2 −w⊤Ax )2 , (13) Test-Time Training yields an updated matrix A′←A−η ( y2 −w⊤Ax )( −wx⊤) , (14) where ηis the learning rate. Lemma 1. Following the exposition of the main paper, let η∗= (y1 −v⊤Ax) (y2 −w⊤Ax)v⊤wx⊤x. (15) Assume η∗∈[ϵ,∞) for some ϵ> 0. Then for any η∈(0,ϵ], we are guaranteed an improvement on the main loss i.e. lm(A′) <lm(A). Proof. From the exposition of the main paper, we know that lm(A−η∗∇lsA)) = 0, which can also be derived from simple algebra. Then by convexity, we have lm(A−η∇ls(A)) (16) = lm (( 1 − η η∗ ) A+ η η∗(A−η∗∇ls(A)) ) (17) ≤ ( 1 − η η∗ ) lm(A) + 0 (18) ≤ ( 1 −η ϵ ) lm(A) (19) <lm(A), (20) where the last inequality uses the assumption that lm(A) > 0, which holds because η∗>0. Lemma 2. Deﬁne ⟨U,V⟩= vec (U)⊤vec (V) i.e. the Frobenious inner product, then sign (η∗) = sign (⟨∇lm(A),∇ls(A)⟩) . (21) Proof. By simple algebra, ⟨∇lm(A),∇ls(A)⟩ = ⟨ ( y1 −v⊤Ax )( −vx⊤) , ( y2 −w⊤Ax )( −wx⊤) ⟩ = ( y1 −v⊤Ax )( y2 −w⊤Ax ) Tr ( xv⊤wx⊤) = ( y1 −v⊤Ax )( y2 −w⊤Ax ) v⊤wx⊤x, which has the same sign as η∗. A3.2. Proof of Theorem 1 For any η, by smoothness and convexity, lm(x,y; θ(x)) = lm(x,y; θ−η∇ls(x; θ)) ≤lm(x,y; θ) + η⟨∇lm(x,y; θ),∇ls(x,θ)⟩ + η2β 2 ∥∇ls(x; θ)∥2 . Denote η∗= ⟨∇lm(x,y; θ),∇ls(x,θ)⟩ β∥∇ls(x; θ)∥2 . Then Equation 22 becomes lm(x,y; θ−η∗∇ls(x; θ)) (22) ≤lm(x,y; θ) −⟨∇lm(x,y; θ),∇ls(x,θ)⟩2 2β∥∇ls(x; θ)∥2 . (23) And by our assumptions on the gradient norm and gradient inner product, lm(x,y; θ) −lm(x,y; θ−η∗∇ls(x; θ)) ≥ ϵ2 2βG2 . (24) Because we cannot observe η∗in practice, we instead use a ﬁxed learning rate η = ϵ βG2 , as stated in Theorem 1. Now we argue that this ﬁxed learning rate still improves performance on the main task. By our assumptions, η∗ ≥ ϵ βG2 , so η ∈(0,η∗]. Denote g= ∇ls(x; θ), then by convexity of lm, lm(x,y; θ(x)) = lm(x,y; θ−ηg) (25) = lm ( x,y; ( 1 − η η∗ ) θ+ η η∗(θ−η∗g) ) (26) ≤ ( 1 − η η∗ ) lm(x,y; θ) + η η∗lm(x,y; θ−η∗g) (27) Combining with Equation 24, we have lm(x,y; θ(x)) ≤ ( 1 − η η∗ ) lm(x,y; θ) + η η∗ ( lm(x,y; θ) − ϵ2 2βG2 ) = lm(x,y; θ) − η η∗ ϵ2 2βG2 Since η/η∗>0, we have shown that lm(x,y; θ) −lm(x,y; θ(x)) >0. (28)Test-Time Training with Self-Supervision for Generalization under Distribution Shifts A4. Additional Results on the Common Corruptions Dataset For table aethetics, we use the following abbreviations: B for baseline, JT for joint training, TTT for Test-Time Train- ing standard version, and TTT-Online for online Test-Time Training i.e. the online version. We have abbreviated the names of the corruptions, in order: original test set, Gaussian noise, shot noise, impulse noise, defocus blur, glass blue, motion blur, zoom blur, snow, frost, fog, brightness, contrast, elastic transformation, pixelation, and JPEG compression. A4.1. Results Using Batch Normalization As discussed in the results section, Batch Normalization (BN) is ineffective for small batches, which are the inputs for Test-Time Training (both standard and online version) since there is only one sample available when forming each batch; therefore, our main results are based on a ResNet using Group Normalization (GN). Figure A2 and Table A1 show results of our method on CIFAR-10-C level 5, with a ResNet using Batch Normalization (BN). These results are only meant to be a point of reference for the curious readers. In the early stage of this project, we have experimented with two potential solutions to the small batches problem with BN. The naive solution is to ﬁx the BN layers during Test-Time Training. but this diminishes the performance gains since there are fewer shared parameters. The better solution, adopted for the results below, is hard example mining: instead of updating on all inputs, we only update on inputs that incur large self-supervised task loss ls, where the large improvements might counter the negative effects of inaccurate statistics. Test-Time Training (standard version) is still very effective with BN. In fact, some of the improvements are quite dra- matic, such as on contrast (34%), defocus blue (18%) and Gaussian noise (22% comparing to joint-training, and 16% comparing to the baseline). Performance on the original distribution is still almost the same, and the original error with BN is in fact slightly lower than with GN, and takes half as many epochs to converge. We did not further experiment with BN because of two rea- sons: 1) The online version does not work with BN, because the problem with inaccurate batch statistics is exacerbated when training online for many (e.g. 10000) steps. 2) The baseline error for almost every corruption type is signiﬁ- cantly higher with BN than with GN. Although unrelated to the main idea of our paper, we make the interesting note that GN signiﬁcantly improves model robustness. A4.2. Additional Baseline: Adversarial Logit Pairing As discussed in the results section, Hendrycks & Dietterich (2019) point to Adversarial Logit Pairing (ALP) (Kannan et al., 2018) as an effective method for improving model robustness to corruptions and perturbations, even though it was designed to defend against adversarial attacks. We take ALP as an additional baseline on all benchmarks based on CIFAR-10 (using GN), following the training proce- dure in Kannan et al. (2018) and their recommended hyper- parameters. The implementation of the adversarial attack comes from the codebase of Ding et al. (2019). We did not run ALP on ImageNet because the two papers we reference for this method, Kannan et al. (2018) and Hendrycks & Di- etterich (2019), did not run on ImageNet or make any claim or recommendation. A4.3. Results on CIFAR-10-C and ImageNet-C, Level 5 Table A2 and Table A3 correspond to the bar plots in the results section. Two rows of Table A2 have been presented as Table 1 in the main text. A4.4. Results on CIFAR-10-C, Levels 1-4 The following bar plots and tables are on levels 1-4 of CIFAR-10-C. The original distribution is the same for all levels, so are our results on the original distribution. A4.5. Direct Comparison with Hendrycks et al. (2019a) The following comparison has been requested by an anony- mous reviewer for our ﬁnal version. Our joint training baseline is based on Hendrycks et al. (2019a), but also incor- porates some architectural changes (see below). We found these changes improved the robustness of our method, and felt that it was important to give the baseline the same ben- eﬁt. Note that our joint training baseline overall performs better than Hendrycks: Compare Table S2 to Figure 3 of Hendrycks et al. (2019a) (provided by the authors), our baseline has average error of 22.8% across all corruptions and levels, while their average error is 28.6%. Summary of architectural changes: 1) Group Normalization (GN) instead of Batch Normalization (BN). For complete- ness, the results with BN are provided in Table S1; c.f. GN results in Table S2 which signiﬁcantly improves robustness, with or without self-supervision. 2) We split after the sec- ond residual group, while they split after the third residual group right before the linear layer. This consistently gives about 0.5% - 1% improvement. 3) We use a ResNet-26, while they use a 40-2 Wide ResNet. But our baseline still performs better than their method even though our network is 4x smaller, due to the two tricks above.Test-Time Training with Self-Supervision for Generalization under Distribution Shifts Gaussian Noise  Shot Noise  Impulse Noise  Defocus Blur  Frosted Glass Blur Motion Blur  Zoom Blur  Snow  Frost  Fog Brightness  Contrast  Elastic  Pixelate  JPEG Figure A1.Sample images from the Common Corruptions Benchmark, taken from the original paper by Hendrycks & Dietterich (2019). originalgauss shot impulsedefocus glass motion zoom snow frost fog bright contrastelasticpixelate jpeg 0 20 40 60Error (%) Object recognition task only Joint training (Hendrycks et al. 2019) TTT Figure A2.Test error (%) on CIFAR-10-C, level 5, ResNet-26 with Batch Normalization. orig gauss shot impul defoc glass motn zoom snow frost fog brit contr elast pixel jpeg B 7.9 63.9 58.8 64.3 46.3 54.6 41.6 45.9 31.9 44.0 37.5 13.0 69.2 33.8 61.4 31.7 JT 7.5 70.7 65.6 67.2 43.1 55.4 40.9 42.7 30.3 44.5 42.5 12.7 58.6 30.7 62.6 31.9 TTT 7.9 47.9 45.2 54.8 27.6 50.4 31.5 30.9 28.7 34.3 26.9 12.6 35.2 30.6 51.2 31.3 Table A1.Test error (%) on CIFAR-10-C, level 5, ResNet-26 with Batch Normalization. orig gauss shot impul defoc glass motn zoom snow frost fog brit contr elast pixel jpeg B 8.9 50.5 47.2 56.1 23.7 51.7 24.3 26.3 25.6 34.4 28.1 13.5 25.0 27.4 55.8 29.8 JT 8.1 49.4 45.3 53.4 24.2 48.5 24.8 26.4 25.0 32.5 27.5 12.6 25.3 24.0 51.6 28.7 TTT 7.9 45.6 41.8 50.0 21.8 46.1 23.0 23.9 23.9 30.0 25.1 12.2 23.9 22.6 47.2 27.2 TTT-Online 8.2 25.8 22.6 30.6 14.6 34.4 18.3 17.1 20.0 18.0 16.9 11.2 15.6 21.6 18.1 21.2 UDA-SS 9.0 28.2 26.5 20.8 15.6 43.7 24.5 23.8 25.0 24.9 17.2 12.7 11.6 22.1 20.3 22.6 ALP 16.5 22.7 22.9 28.3 25.0 25.6 27.4 23.1 25.2 27.2 64.8 21.7 73.6 23.0 20.2 18.9 Table A2.Test error (%) on CIFAR-10-C, level 5, ResNet-26. orig gauss shot impul defoc glass motn zoom snow frost fog brit contr elast pixel jpeg B 68.9 1.3 2.0 1.3 7.5 6.6 11.8 16.2 15.7 14.9 15.3 43.9 9.7 16.5 15.3 23.4 JT 69.1 2.1 3.1 2.1 8.7 6.7 12.3 16.0 15.3 15.8 17.0 45.3 11.0 18.4 19.7 22.9 TTT 69.0 3.1 4.5 3.5 10.1 6.8 13.5 18.5 17.1 17.9 20.0 47.0 14.4 20.9 22.8 25.3 TTT-Online 68.8 26.3 28.6 26.9 23.7 6.6 28.7 33.4 35.6 18.7 47.6 58.3 35.3 44.3 47.8 44.3 Table A3.Test accuracy (%) on ImageNet-C, level 5, ResNet-18.Test-Time Training with Self-Supervision for Generalization under Distribution Shifts originalgauss shot impulsedefocus glass motion zoom snow frost fog bright contrastelasticpixelate jpeg 0 10 20 30 40 50Error (%) Object recognition task only Joint training (Hendrycks et al. 2019) TTT TTT-Online Figure A3.Test error (%) on CIFAR-10-C, level 4. See the results section for details. orig gauss shot impul defoc glass motn zoom snow frost fog brit contr elast pixel jpeg B 8.9 46.4 39.2 44.8 15.3 52.5 19.1 20.5 21.3 26.9 13.3 10.5 13.7 20.8 35.3 26.9 JT 8.1 45.0 38.3 42.2 16.4 50.2 20.7 20.5 21.1 25.4 14.1 10.0 14.7 19.0 33.2 25.1 TTT 7.9 41.5 35.4 39.8 15.0 47.8 19.1 18.4 20.1 24.0 13.5 10.0 14.1 17.7 29.4 24.5 TTT-Online 8.2 22.9 20.0 23.9 11.2 35.1 15.6 13.8 18.6 15.9 12.3 9.7 11.9 16.7 13.6 19.8 ALP 16.5 21.3 20.5 24.5 20.7 25.9 23.7 21.4 24.2 23.9 42.2 17.5 53.7 22.1 19.1 18.5 Table A4.Test error (%) on CIFAR-10-C, level 4, ResNet-26. originalgauss shot impulsedefocus glass motion zoom snow frost fog bright contrastelasticpixelate jpeg 0 10 20 30 40Error (%) Object recognition task only Joint training (Hendrycks et al. 2019) TTT TTT-Online Figure A4.Test error (%) on CIFAR-10-C, level 3. See the results section for details. orig gauss shot impul defoc glass motn zoom snow frost fog brit contr elast pixel jpeg B 8.9 42.2 35.1 30.7 12.2 41.7 18.6 17.5 19.0 25.3 10.8 9.7 11.6 15.3 21.7 24.6 JT 8.1 40.2 34.4 29.9 12.2 37.9 20.8 17.3 18.4 25.0 11.4 9.2 12.0 15.2 20.8 22.8 TTT 7.9 37.2 31.6 28.6 11.5 35.8 19.1 15.8 17.8 23.3 11.0 9.1 11.6 14.3 18.9 22.3 TTT-Online 8.2 21.3 17.7 17.9 9.0 23.4 15.3 12.5 16.4 15.8 10.9 9.0 10.7 12.8 12.2 18.7 ALP 16.5 20.0 19.3 20.5 19.2 21.2 24.0 20.5 20.9 24.2 30.1 16.6 39.6 20.9 17.8 18.0 Table A5.Test error (%) on CIFAR-10-C, level 3, ResNet-26.Test-Time Training with Self-Supervision for Generalization under Distribution Shifts originalgauss shot impulsedefocus glass motion zoom snow frost fog bright contrastelasticpixelate jpeg 0 10 20 30 40Error (%) Object recognition task only Joint training (Hendrycks et al. 2019) TTT TTT-Online Figure A5.Test error (%) on CIFAR-10-C, level 2. See the results section for details. orig gauss shot impul defoc glass motn zoom snow frost fog brit contr elast pixel jpeg B 8.9 31.7 22.6 24.3 9.9 42.6 14.9 14.7 21.7 18.4 9.8 9.1 10.0 13.1 17.1 22.4 JT 8.1 31.0 22.6 23.4 9.1 39.2 16.4 14.2 21.2 17.5 9.4 8.3 10.6 12.8 15.9 20.5 TTT 7.9 28.8 20.7 23.0 9.0 36.6 15.4 13.1 20.2 16.9 9.2 8.3 10.2 12.5 14.8 19.7 TTT-Online 8.2 16.8 13.8 15.5 8.5 23.4 13.3 11.5 16.8 12.7 9.4 8.4 9.7 12.4 11.5 17.0 ALP 16.5 18.0 17.2 19.0 17.8 20.7 21.2 19.3 19.0 20.1 22.4 16.3 29.2 20.3 17.4 17.8 Table A6.Test error (%) on CIFAR-10-C, level 2, ResNet-26. originalgauss shot impulsedefocus glass motion zoom snow frost fog bright contrastelasticpixelate jpeg 0 10 20 30 40Error (%) Object recognition task only Joint training (Hendrycks et al. 2019) TTT TTT-Online Figure A6.Test error (%) on CIFAR-10-C, level 1. See the results section for details. orig gauss shot impul defoc glass motn zoom snow frost fog brit contr elast pixel jpeg B 8.9 21.7 17.1 17.0 9.0 44.0 12.1 13.9 14.3 13.4 9.2 8.9 9.0 13.2 12.0 17.3 JT 8.1 20.4 16.6 16.9 8.2 40.5 12.2 13.0 13.1 12.3 8.4 8.1 8.5 12.9 11.3 15.9 TTT 7.9 19.1 15.8 16.5 8.0 37.9 11.7 12.2 12.8 11.9 8.2 8.0 8.3 12.6 11.1 15.5 TTT-Online 8.2 13.8 11.9 12.2 8.5 24.4 10.5 11.5 12.4 10.7 8.5 8.3 8.6 12.4 10.7 14.4 ALP 17.0 16.8 17.6 16.8 20.9 18.7 19.0 17.3 17.5 17.4 16.1 18.4 20.4 17.0 17.2 17.5 Table A7.Test error (%) on CIFAR-10-C, level 1, ResNet-26.Test-Time Training with Self-Supervision for Generalization under Distribution Shifts Figure A7.Sample Images from the VID-Robust dataset (Shankar et al., 2019) in the results section adapted to CIFAR-10. Each row shows eight sample images from one class. The seven classes shown are, in order: airplane, bird, car, dog, cat, horse, ship.",
      "meta_data": {
        "arxiv_id": "1909.13231v3",
        "authors": [
          "Yu Sun",
          "Xiaolong Wang",
          "Zhuang Liu",
          "John Miller",
          "Alexei A. Efros",
          "Moritz Hardt"
        ],
        "published_date": "2019-09-29T08:09:15Z",
        "pdf_url": "https://arxiv.org/pdf/1909.13231v3.pdf"
      },
      "llm_extracted_info": {
        "main_contributions": "The paper introduces Test-Time Training (TTT), a novel approach to improve the generalization of predictive models under distribution shifts. TTT converts a single unlabeled test sample into a self-supervised learning problem, on which model parameters are updated before making a prediction. The method extends to online data streams and demonstrates significant performance improvements on diverse image classification benchmarks evaluating robustness to distribution shifts, including CIFAR-10-C, ImageNet-C, VID-Robust, and CIFAR-10.1. A key finding is that TTT-Online often outperforms unsupervised domain adaptation methods (UDA-SS) which have access to the entire unlabeled test set. Theoretically, the paper establishes that positive gradient correlation between the main task loss and the self-supervised task loss is a sufficient condition for TTT to be beneficial in convex models, a finding empirically supported for deep learning models.",
        "methodology": "Test-Time Training (TTT) begins with a standard K-layer neural network with a shared feature extractor (θe) and two task-specific branches: a main task branch (θm) and a self-supervised task branch (θs), forming a Y-structure. The model is first pre-trained using multi-task learning on both the main classification task (loss lm) and an auxiliary self-supervised task (loss ls) on the original training data. For the self-supervised task, rotation prediction is used, where the model predicts one of four rotation angles (0, 90, 180, 270 degrees) applied to the input image. At test time, for each unlabeled test sample, the shared feature extractor θe is fine-tuned by minimizing the self-supervised task loss ls on that specific sample. The prediction for the main task is then made using the updated parameters. The standard TTT version re-initializes parameters for each test sample, while the online version (TTT-Online) retains the updated parameters from the previous test sample, allowing adaptation to smoothly changing distributions. Optimization is performed using Stochastic Gradient Descent, with a learning rate of 0.001 and zero weight decay and momentum during test-time training. Standard TTT takes ten gradient steps per sample, while TTT-Online takes one. Data augmentation techniques like random crop and horizontal flip are also applied to the test sample during this fine-tuning step.",
        "experimental_setup": "The method was evaluated using ResNet architectures: ResNet-26 for CIFAR-10-based experiments and ResNet-18 for ImageNet-based experiments. Group Normalization (GN) was used instead of Batch Normalization (BN) due to the small batch size (single image) at test time, which makes BN ineffective. Experiments were conducted on three categories of distribution shifts: 1. Object recognition on corrupted images using CIFAR-10-C and ImageNet-C datasets, which feature 15 types of corruptions across five severity levels. 2. Object recognition on video frames using the VID-Robust dataset, which includes video frames from ImageNet Video detection, and a re-scaled version for CIFAR-10 models. 3. Unknown distribution shifts using CIFAR-10.1, a new test set for CIFAR-10 where shifts are subtle and unidentified. Baselines included a plain ResNet model ('Object recognition task only'), a model jointly trained on main and self-supervised tasks but fixed at test time ('Joint training'), Unsupervised Domain Adaptation by Self-Supervision (UDA-SS) as an oracle, and Adversarial Logit Pairing (ALP) for CIFAR-10 benchmarks. Performance was measured using test error (%) and accuracy (%), with online version results calculated as an average over a sliding window. The theoretical findings on gradient correlation were also empirically validated on the deep learning models across various corruptions.",
        "limitations": "A primary limitation is the computational overhead, as Test-Time Training is significantly slower (2x batch size x number of iterations) than standard inference, performing multiple forward/backward passes per test sample. While the paper prioritizes robustness over efficiency, this is a practical constraint. The effectiveness of the method depends on the quality of the self-supervised task; it needs to be well-defined and non-trivial for the given data, as observed with the 'airplane' class where rotation prediction was ineffective due to trivial visual cues or inherent ambiguity. The theoretical analysis supporting TTT's success is limited to convex models, although empirical results suggest its applicability to non-convex deep learning models. Furthermore, the method is incompatible with Batch Normalization (BN) layers due to the single-image batch size at test time, necessitating the use of Group Normalization. The online version of TTT also assumes gradually changing distribution shifts, where sequential test samples come from smoothly evolving distributions.",
        "future_research_directions": "Future work could focus on improving the computational efficiency of Test-Time Training, potentially by designing models amenable to faster updates during training or exploring techniques like thresholding to apply updates only on challenging samples. Developing better and more general-purpose self-supervised tasks, leveraging domain knowledge from various fields (e.g., segmentation, detection, speech recognition, NLP), is another key direction. The paper also proposes using TTT as a new evaluation benchmark for self-supervised tasks. Further theoretical work is suggested to formalize the concept of variable decision boundaries, which differs from traditional learning theory. Exploring connections with and potentially improving TTT through advancements in one-shot learning is also mentioned. More broadly, the authors encourage a paradigm shift towards learning that occurs continuously after model deployment, abandoning the self-imposed constraints of fixed decision boundaries and the strict division between training and testing."
      }
    },
    {
      "title": "Repurposing Pretrained Models for Robust Out-of-domain Few-Shot Learning",
      "abstract": "Model-agnostic meta-learning (MAML) is a popular method for few-shot learning\nbut assumes that we have access to the meta-training set. In practice, training\non the meta-training set may not always be an option due to data privacy\nconcerns, intellectual property issues, or merely lack of computing resources.\nIn this paper, we consider the novel problem of repurposing pretrained MAML\ncheckpoints to solve new few-shot classification tasks. Because of the\npotential distribution mismatch, the original MAML steps may no longer be\noptimal. Therefore we propose an alternative meta-testing procedure and combine\nMAML gradient steps with adversarial training and uncertainty-based stepsize\nadaptation. Our method outperforms \"vanilla\" MAML on same-domain and\ncross-domains benchmarks using both SGD and Adam optimizers and shows improved\nrobustness to the choice of base stepsize.",
      "full_text": "Published as a conference paper at ICLR 2021 REPURPOSING PRETRAINED MODELS FOR ROBUST OUT-OF-DOMAIN FEW-SHOT LEARNING Namyeong Kwon, Hwidong Na∗ Samsung Advanced Institute of Technology (SAIT), South Korea {ny.kwon,hwidong.na}@samsung.com Gabriel Huang Mila, Universit´e de Montr´eal Simon Lacoste-Julien† Mila, Universit´e de Montr´eal SAIT AI Lab, Montreal ABSTRACT Model-agnostic meta-learning (MAML) is a popular method for few-shot learning but assumes that we have access to the meta-training set. In practice, training on the meta-training set may not always be an option due to data privacy concerns, intellectual property issues, or merely lack of computing resources. In this paper, we consider the novel problem of repurposing pretrained MAML checkpoints to solve new few-shot classiﬁcation tasks. Because of the potential distribution mis- match, the original MAML steps may no longer be optimal. Therefore we propose an alternative meta-testing procedure and combine MAML gradient steps with ad- versarial training and uncertainty-based stepsize adaptation. Our method outper- forms “vanilla” MAML on same-domain and cross-domains benchmarks using both SGD and Adam optimizers and shows improved robustness to the choice of base stepsize. 1 I NTRODUCTION Deep learning approaches have shown improvements based on massive datasets and enormous com- puting resources. Despite their success, it is still challenging to apply state-of-the-art methods in the real world. For example, in semiconductor manufacturing (Nishi & Doering, 2000), collecting each new data point is costly and time consuming because it requires setting up a new manufacturing pro- cess accordingly. Moreover, in the case of a “destructive inspection”, the cost is very high because the wafer must be destroyed for measurement. Therefore, learning from small amounts of data is important for practical purposes. Meta-learning (learning-to-learn) approaches have been proposed for learning under limited data constraints. A meta-learning model optimizes its parameters for the best performance on the distri- bution of tasks. In particular, few-shot learning (FSL) formulates “learning from limited data” as an n-way k-shot problem, where nis the number of classes and kis the number of labeled samples per class. For each task in FSL, a support set is provided for training, while a query set is provided for evaluation. Ideally, a meta-learning model trained over a set of tasks (meta-training) will generalize well to new tasks (meta-testing). Model-agnostic meta-learning (MAML) (Finn et al., 2017) is a general end-to-end approach for solving few-shot learning tasks. MAML is trained on the meta-training tasks to learn a model initialization (also known as checkpoint) such that a few gradient steps on the support set will yield the best predictions on the query set. However, in practice it may not always be possible to retrain or ﬁnetune on the meta-training set. This situation may arise when the meta-training data is conﬁdential, subject to restrictive licences, contains private user information, or protected intellectual property such as semiconductor manu- ∗Work done as visiting researchers at SAIT AI Lab, Montreal. †Canada CIFAR AI Chair 1 arXiv:2103.09027v1  [cs.LG]  16 Mar 2021Published as a conference paper at ICLR 2021 facturing know-how. Another reason is that one may not have the computing resources necessary for running large-scale meta-training. In this paper, we consider the novel problem of repurposing MAML checkpoints to solve new few- shot classiﬁcation tasks, without the option of (re)training on the meta-training set. Since the meta- testing set (new tasks) may differ in distribution from the meta-training set, the implicit assumption –in end-to-end learning– of identically distributed tasks may not hold, so there is no reason why the meta-testing gradient steps should match the meta-training. Therefore, we investigate various improvements over the default MAML gradient steps for test time adaptation. Conceptually, our approach consists of collecting information while training the model on a new support set and then proposing ways to use this information to improve the adaptation. In this paper, we consider the variance of model parameters during ensemble training as a source of information to use. We propose algorithms that uses this information both to adapt the stepsizes for MAML as well as to generate “task-speciﬁc” adversarial examples to help robust adaptation to the new task. Our main contributions are the following: • We motivate the novel problem of repurposing MAML checkpoints to solve cross-domain few- shot classiﬁcation tasks, in the case where the meta-training set is inaccessible, and propose a method based on uncertainty-based stepsize adaptation and adversarial data augmentation, which has the particularity that meta-testing differs from meta-training steps. • Compared to “vanilla” MAML, our method shows improved accuracy and robustness to the choice of base stepsizes on popular cross-domain and same-domain benchmarks, using both the SGD and Adam (Kingma & Ba, 2014) optimizers. Our results also indicate that adversarial training (AT) is helpful in improving the model performance during meta-testing. To the best of our knowledge, our work is the ﬁrst few-shot learning method to combine the use of ensemble methods for stepsize computation and generating adversarial examples from the meta- testing support set for improved robustness. Moreover, our empirical observation of improving over the default meta-testing procedure of MAML motivates further research on alternative ways to leverage published model checkpoints. 2 R ELATED WORK 2.1 M ETA-LEARNING AND MODEL -AGNOSTIC META -LEARNING FSL approaches deal with an extremely small amount of training data and can be classiﬁed into three categories. First, metric-based approaches solve few-shot tasks by training a feature extractor that maximizes inter-class similarity and intra-class dissimilarity (Vinyals et al., 2016; Snell et al., 2017; Sung et al., 2018). Second, memory-based approaches utilize previous tasks for new tasks with external memories (Santoro et al., 2016; Mishra et al., 2018). Third, optimization-based approaches search for good initial parameters during training and adapt the pretrained model for new tasks (Finn et al., 2017; Lee & Choi, 2018; Grant et al., 2018). We focus on the optimization-based approaches and suggest better training especially for the general family of MAML methods. 2.2 U NCERTAINTY FOR MODEL TRAINING Uncertainty is an important criterion for measuring the robustness of a neural network (NN). Bayesian neural networks (BNN) (Blundell et al., 2015) obtain model uncertainty by placing prior distributions over the weights p(ω). This uncertainty has been used to adapt the stepsizes during continual learning in Uncertainty-guided Continual BNNs (UCB) (Ebrahimi et al., 2020). For each parameter, UCB scales its stepsize inversely proportional to the uncertainty of the parameter in the BNN to reduce changes in important parameters while allowing less important parameters to be modiﬁed faster in favor of learning new tasks. Our approach also decreases the stepsizes for “uncer- tain” parameters, but using a different notion of uncertainty, and instead in the context of FSL with a pretrained MAML checkpoint. Unlike BNNs, deep ensembles (Lakshminarayanan et al., 2017) estimate uncertainty of NNs us- ing ensembles over randomly initialized parameters combined with adversarial training (AT). Deep ensembles have been successfully used to boost predictive performance and AT has been used to im- 2Published as a conference paper at ICLR 2021 prove robustness to adversarial examples. Lakshminarayanan et al. (2017) showed that they produce predictive uncertainty estimates comparable in quality to BNNs. Deep ensembles, however, are not directly applicable for FSL tasks, as training randomly initialized parameters from scratch with a limited amount of training data yields poor performance. Our approach is partly inspired from deep ensembles, adapting it to the FSL setting by using instead a parameter perturbations of the MAML checkpoint model rather than a random initialization. We use in particular a multiplicative Gaussian perturbation that rescales the parameters, as the information content of the weights is said to be invariant to their scale (Wen et al., 2018). 2.3 A DVERSARIAL TRAINING FOR ATTACK AND DEFENSE Deep NNs are sensitive to adversarial attacks (Goodfellow et al., 2015; Madry et al., 2018; Moosavi- Dezfooli et al., 2016). These methods generate an adversarial sample to fool a trained model, where the generated image looks identical to the original one for humans. Goodfellow et al. (2015) pro- posed the fast gradient sign method (FGSM) which generates adversarial example using sign of input gradient. While stronger attacks have been proposed (such as using projected gradient de- scent (Madry et al., 2018)), we will focus on the FGSM approach in this paper for simplicity. In FSL, adversarial attack and defense have also been studied. ADversarialMeta-Learner (Yin et al., 2018) utilized one-step adversarial attack for generating adversarial samples during meta-training. There is little consideration, however, for the degradation of accuracy in the original sample in ad- versarial defense approaches. Xie et al. (2020) reported improvement using generated examples for adversarial attack on the large scale training. Motiian et al. (2017) propose adversarial domain adaptation for FSL. Despite similar keywords, their work is distinct from ours and relies on using generative adversarial networks discriminators to confuse domains, whereas we rely on adversarial examples for improved robustness. To our best knowledge, there is no prior FSL work which uses adversarial examples to enhance the model at test-time. 2.4 D OMAIN ADAPTATION Domain adaptation methods (DA) (Ben-David et al., 2010) attempt to alleviate the distribution mismatch between source and target domains. Most of the recently proposed DA approaches are based on generative adversarial networks (GANs) (Goodfellow et al., 2014). GAN-based DA ap- proaches require having access to the large amount of unlabeled data from both the source and target datasets (Tzeng et al., 2017; Zhang et al., 2019; Wilson & Cook, 2020). DA methods cannot be di- rectly applied in the FSL scenario due to limited number of target domain samples ( shots). Some domain-adaptive FSL (DA-FSL) methods have been proposed in the case where only a very few samples from the target domain are available (Motiian et al., 2017; Zhao et al., 2020). DA and DA-FSL methods cannot be directly applied to our setting because we assume that the meta-training dataset is inaccessible — mirroring real world situations in which access to the meta- training set is restricted by privacy and conﬁdentiality concerns. Our approach differs from DA and DA-FSL by not requiring access to the meta-training dataset (source domain). 3 P ROPOSED METHOD At meta-testing time, MAML normally uses the support set to compute ﬁxed gradient steps, which were “calibrated” using end-to-end learning during meta-training. That is, the learned initialization is such that a ﬁxed combination of stepsize and loss result in the desired result. However, those stepsizes and losses may be suboptimal on the new task, especially if the new task is out-of-domain with respect to the meta-training tasks. Our method is based on the assumption that the support set can be used to improve the meta-testing procedure itself, beyond merely serving as training examples. We start by leveraging the support set to estimate task-speciﬁc uncertainties over the model parameters. Then, we propose two im- provements over the “vanilla” MAML gradient steps : we scale the gradient steps using layer-wise stepsizes computed from the support set and we train using task-speciﬁc adversarial examples. 3Published as a conference paper at ICLR 2021 3.1 M OTIVATING HYPOTHESES At meta-testing time, we start by assuming that we can estimate task-speciﬁc uncertainties over the model parameters. One possibility, which we adopt in Section 3.2, is to train deep ensembles (Lak- shminarayanan et al., 2017) on the support set and use the ensemble to estimate variances over model parameters. Each model learns slightly different parameters, and yields slightly different gradients. We regard the variance over the parameters and input gradients as task-speciﬁc uncertainties. Given the uncertainty estimates, we propose two modiﬁcations to the original MAML gradient steps. Proposal 1 (Task-speciﬁc stepsizes) Use lower stepsizes for model parameters with higher variance. In our case, the variance could be further ampliﬁed if high-variance components were to be moved with large stepsizes. Therefore, we carefully move high-variance components with a lower stepsize with the hope is that we can limit the variance over the model parameters. This approach can be related with the fact that lower stepsizes should be taken for SGD when the gradients are very noisy (Dieuleveut et al., 2020). Proposal 2 (Task-speciﬁc adversarial examples) Use adversarial examples with higher ad- versarial perturbation on input components with higher variance over the input gradient. The intuition is that if only slightly perturbed models (from the ensemble) disagree on parts of the input gradient, then it means that they disagree on what to learn and therefore those parts of the input are more vulnerable to attack. Therefore, we propose to use AT with stronger adver- sarial perturbation in the weak parts of the input, with the hope to incur improved robustness on those parts of the input. We regard adversarial training as a form ofdata augmentation or regularization at meta-testing time, which we hope allows the model to overcome the limited size of the support set. 3.2 U NCERTAINTY -BASED GRADIENT STEPS AT TEST -TIME We improve over the default MAML gradient steps by implementing the ideas presented in the previ- ous section. The resulting approach is detailed in Algorithm 1 and is a combination of uncertainty- based stepsize adaption (USA, based on Proposal 1) and uncertainty-based fast gradient sign method (UFGSM), adversarial training (AT), and generating additional adversarial examples from deep en- sembles (EnAug) which are based on Proposal 2. Denote L(D,θ) = 1/|D|∑ (x,y)∈Dlθ(x,y) the cross-entropy loss for model θ over the labeled dataset D = {(x,y) ... }, Aθ(x,y) = x+ ϵsign(∇xlθ(x,y)) the FGSM adversarial example computed from (x,y) and LAT(D,θ) = 1/|D|∑ (x,y)∈Dlθ(Aθ(x,y),y) the resulting adversarial cross-entropy, which we refer to as AT. For easy reference, all notations are summarized in Ap- pendix A.1. Starting from the pretrained MAML checkpoint θ0, we perturb the model parameters with multi- plicative Gaussian noise to create a deep ensemble (θm 0 )1≤m≤M (lines 2-4). Then, we repeat the following for T steps. At time t, run gradient descent on each model θm t of the ensemble (line 7), where the loss is a combination of the usual cross-entropy Land AT loss LAT as in (Lakshmi- narayanan et al., 2017) but on the support set, and the stepsizesαadap are updated using USA (details below). Also, we generate adversarial examples using FGSM and UFGSM (details below) and store them into DAug (lines 8 and 11). Finally, we run T gradient steps on the original checkpoint, where the loss is a combination of cross-entropy on the support set DSpt, AT loss on the support set, and cross-entropy on the ensemble-augmented support set DAug (lines 13-16). Note that we recover the original MAML steps for ( λAT = λAug = 0,λα = 1), while we refer to the case (λAT = λAug = 1,λα = 0) as our full method. 4Published as a conference paper at ICLR 2021 Algorithm 1: Uncertainty-based Gradient Steps at Test-time Data: New task support set DSpt = {(x1,y1),..., (xn×k,yn×k)}with nways and kshots. Require: Base stepsize α, pretrained weights θ0, Gaussian variance σ, AT coefﬁcient ϵ, size of ensemble M, number of gradient steps T, selection coefﬁcients in {0,1}for: base stepsize λα, adversarial loss λAT, and augmented cross-entropy λAug . 1 DAug ←∅, αadap ←α 2 for m= 1to M do 3 θm 0 ←θ0(1 +N(0,σ)) ⊿Initialize deep ensemble 4 end 5 for t= 1to T do 6 for m= 1to M do 7 θm t ←θm t−1 −αadap ⊙∇θm t−1 [L(DSpt,θm t−1) +LAT(DSpt,θm t−1)] 8 DAug ←DAug ∪{(Aθm t−1 (x,y),y) |(x,y) ∈DSpt} ⊿EnAug 9 end 10 αadap ←USA(α,θ1:M t ) ⊿Algorithm 2 11 DAug ←DAug ∪{UFGSM(θ0,θ1:M t ,x,y ) |(x,y) ∈DSpt} ⊿Algorithm 3 12 end 13 α←λαα+ (1−λα)αadap 14 for t= 1to T do 15 θt ←θt−1 −α⊙∇θt−1 [L(DSpt,θt−1) +λATLAT(DSpt,θt−1) +λAugL(DAug,θt−1)] 16 end USA. We propose uncertainty-based stepsize adaptation (USA), which assigns lower stepsizes to layers1 with higher uncertainty ( Algorithm 2) – we call this loosely an “inverse-relationship” below. Let αdenote the default (scalar) stepsize and θ1:M the parameters of the ensemble models, where M is the size of the ensemble and the number of layers in model is L. To compute the adapted layer-wise stepsizes αadap, we compute u, the parameter-wise standard deviation of the model parameters over the ensemble ( line 2), apply an inverse-relationship transformation which ﬂips the max with the min (line 3), average over each layer (line 4) and L1-normalize the result (line 5). The design choices for USA is explained in Appendix A.2. The resulting stepsizes αadap have an inverse-relationship with the variance of each layer. We give an example of applying USA to the 4-ConvNet architecture on miniImageNet (Figure 1). On the left, we plot the layer-wise standard deviations of the parameters and on the right the corresponding USA stepsizes, which follow an inverse relationship with the standard deviations. Algorithm 2: USA 1 Function USA(α, θ1:M t ) 2 u= Std(θ1:M t ) 3 c= Max(u) −u+ Min(u) 4 µl ←average of cover each layer l 5 αadap = αµl/(1/L∑L l=1 µl) 6 return αadap 7 end Algorithm 3: UFGSM Deﬁne: MinMaxNorm(x) = x−Min(x) Max(x)−Min(x) 1 Function UFGSM(θ, θ1:M t , x, y) 2 u= Std({∇xlθ′(x,y) |θ′∈θ1:M t }) 3 u= MinMaxNorm(u) 4 x′= x+ ϵu⊙sign(∇xlθ(x,y)) 5 return x′ 6 end UFGSM. We also propose uncertainty-based FGSM (UFGSM) to generate adversarial examples, with higher adversarial perturbation on input components with higher variance over the input gradi- ent (Algorithm 3). Starting from an image xand label y, we compute the input gradient for each model from the ensemble and calculate the standard deviation uover the ensemble (line 2). Then, we linearly map ubetween 0 and 1 ( line 3), compute the FGSM adversarial example for the pre- 1In this study, we choose to use layer-wise stepsizes, but it is also possible to use kernel-wise or parameter- wise stepsizes. The motivation for using layer-wise stepsizes is because features from the same layer tend to have the same level of abstraction (Zeiler & Fergus, 2014). 5Published as a conference paper at ICLR 2021 0 0.02 0.04 0.06 0.08 0.1 0.12 Uncertainty (Standard deviation) Layer (a) Uncertainty of layers 0 0.002 0.004 0.006 0.008 0.01 0.012 Stepsize Layer ■Convolution layer weight ■Convolution layer bias ■Batch normalization layer gamma ■Batch normalization layer beta ■Linear layer weight and bias (b) Uncertainty-based stepsize adaption Figure 1: USA converts uncertainty into layer-wise adapted stepsize. Hereα= 0.01. (a) is standard deviation of trained ensemble models’ weights. Each layer has a difference uncertainty. (b) is adapted stepsize by USA. Each layer has a different stepsize based on the uncertainty. x + ϵ× u ⊙ sign(∇xlθ(x, y)) = x′(UFGSM) vs. x′(FGSM) Figure 2: Applying UFGSM to 4-ConvNet on miniImageNet with ϵ= 0.05. Starting from the clean image x, we add the signed gradient sign(∇xlθ(x,y)) after rescaling it by the uncertainty over the input gradient u, to generate the adversarial example x′(UFGSM). Note how UFGSM generated a more natural image than FGSM (rightmost, u= 1). trained model θ0 and rescale it using u, so that areas of higher variance get more perturbation. We give an example of applying UFGSM to miniImageNet in Figure 2. 4 E XPERIMENTAL EVALUATION We train MAML onminiImageNet (Vinyals et al., 2016) training split; we then apply our method on the resulting checkpoint. We evaluate our model on the test split of miniImageNet – for the same- domain setting – as well as CUB-200-2011 (Welinder et al., 2010), Trafﬁc Sign (Houben et al., 2013) and VGG Flower (Nilsback & Zisserman, 2008) – for the cross-domain setting. These datasets are denoted as Mini, Birds, Signs and Flowers respectively. A desirable feature for an optimizer is to maintain good performance in a broad range of stepsizes (Asi & Duchi, 2019). Therefore, we evaluate our approach not only at the optimal stepsize, but also over a broad range of base stepsizes2 from 10−4 to 1. We evaluate the performance with three metrics: All, Top-1 and Top-40%.3 If two methods have comparable Top-1 performance, but one has better Top-40% performance, then that method is more robust to the choice of base stepsize. Detailed experimental setup and pretrained model selection are included inAppendix A.3. Our code is available athttps://github.com/ NamyeongK/USA_UFGSM/. 4.1 M AIN RESULTS Our main results are in Table 1. We compare the default MAML steps, which we take as our base- line (denoted as SGD), with our full method (denoted as SGD+All), which consists of combining 2We determine the ranges of stepsize based on training performance onminiImageNet and keep it the same for other datasets. We selected the minimum and maximum stepsize where performance decreased drastically. 3All is the average accuracy over all stepsizes. Top-1 is the accuracy of the best performing stepsize. Top- 40% is the average of the top 40% accuracies among all the stepsizes 6Published as a conference paper at ICLR 2021 Table 1: Main results. We compare the default MAML steps (SGD) with our method (SGD+All) on same-domain and cross-domain benchmarks. Metric Dataset 5-way 1-shot 5-way 5-shot 10-way 1-shot SGD SGD+All SGD SGD+All SGD SGD+All All Mini 37.21±0.22 36.24±0.22 38.14±0.39 43.48±0.38 21.31±0.37 22.07±0.4 Birds 30.07±0.52 29.83±0.46 33.29±0.29 38.07±0.33 17.54±0.06 17.81±0.12 Flowers 40.54 ±0.48 40.86±0.45 44.55±0.43 53.10±0.55 26.79±0.41 28.07±0.41 Signs 34.19 ±0.76 34.76±0.61 45.39±0.45 51.02±0.5 23.77±0.13 24.88±0.13 Avg. 35.50±0.49 35.42±0.44 40.34±0.39 46.42±0.44 22.35±0.24 23.21±0.26 Top-1 Mini 46.71±0.81 46.62±0.58 62.46±0.68 62.16±0.66 31.10±0.66 31.42±0.82 Birds 36.03 ±0.48 36.43±0.58 50.71±0.54 51.82±0.64 23.16±0.21 23.61±0.39 Flowers 51.97 ±0.79 54.36±1.04 70.63±0.76 74.52±0.69 38.61±0.49 40.68±0.58 Signs 43.44 ±0.99 44.10±1.03 72.52±0.84 74.13±0.90 35.10±0.21 35.11±0.26 Avg. 44.54 ±0.77 45.37±0.81 64.08±0.71 65.66±0.72 31.99±0.39 32.70±0.51 Top-40% Mini 45.07 ±0.52 45.79±0.44 55.38±0.46 60.70±0.91 28.65±0.64 30.56±0.77 Birds 34.88 ±0.55 35.77±0.55 45.50±0.49 51.03±0.63 21.78±0.07 22.87±0.30 Flowers 49.98 ±0.69 53.16±0.73 65.16±0.73 73.55±0.86 36.14±0.53 39.75±0.61 Trafﬁc 41.81 ±1.02 43.39±1.01 67.34±0.43 71.45±0.90 33.56±0.11 34.27±0.17 Avg. 42.94 ±0.70 44.52±0.68 58.34±0.53 64.18±0.82 30.03±0.34 31.86±0.46 USA, UFSGM, EnAug, and AT ( λAT = λAug = 1,λα = 0 in Algorithm 1). In terms of abso- lute performance (Top-1 row), our method outperforms the baseline on cross-domain tasks (Birds, Flowers, Signs), while the performance is comparable on same-domain tasks (Mini). In terms of robustness to the base stepsize (All and Top-40% rows), our method outperforms the baseline over a large range of stepsizes for 5-way 5-shot and 10-way 1-shot tasks, while for 5-way 1-shot the results are either comparable (All) or better (Top-40%) depending on the metric considered. More results can be found in Appendix A.6. 4.2 D ISCUSSION Ablation Study. We perform an ablation study for the 5-way 1-shot case in Table 2 and plot the accuracy at different base stepsizes for the Flowers dataset in Figure 3. Comparing SGD to SGD+AT in Table 2, we observe that adversarial training is beneﬁcial over the baseline (Top1 and Top-40%), except for large stepsizes with SGD, which is reﬂected in the All metric and in the dip in performance in Figure 3a. Notice also how the use of AT ﬂattens the accuracy curve near the optimal stepsize. Comparing SGD to SGD+USA, we observe a very small but consistent improvement over the baseline in the vicinity of the optimal stepsize (Top-1 and Top-40%). Comparing SGD+USA to SGD+USA+UFGSM, we observe improvement over a wide range of stepsizes, which is reﬂected in the curves and in All and Top-40% metrics. Comparing SGD+USA+UFGSM+EnAug+AT to SGD+USA+UFGSM+EnAug shows that AT+EnAug is beneﬁcial in terms of absolute performance (Top-1) as well as creating robustness to the choice of stepsize (Top-40%). The drop for the All metric is explained by the dip in the curve for the largest stepsizes. Overall, the best absolute performance (Top-1) is always obtained through some use of adversarial training, while using the full method consistently results in increased robustness to the choice of stepsize (best Top-40% performance). Appendix A.5 shows various AT results. UFGSM vs. FGSM. We compare UFGSM to FGSM (Goodfellow et al., 2015) in Table 3. The results show that our uncertainty-based approach consistently outperforms FGSM, which suggests that the uncertainty information extracted from the support set was useful. SGD vs. Adam. Our method can be used with SGD and Adam, however, we have mainly fo- cused on SGD throughout the paper. SGD tends to yield the best results (Top-1 and Top-40%), as shown in Table 4 for the baseline and full method. More results for Adam can be found in Sections A.6 and A.8 of the appendix. Best checkpoint vs. Overﬁtted checkpoint. We also consider the problem of repurposing an overﬁtted checkpoint. We ﬁnd that our method improves both absolute performance and robustness 7Published as a conference paper at ICLR 2021 Table 2: Ablation Study for 5-way 1-shot classiﬁcation with SGD optimizer. SGD+AT corresponds to using ( λAT = λα = 1,λAug = 0) in Algorithm 1, SGD+USA to ( λAT = λAug = λα = 0), SGD+USA+UFGSM to ( λAT = λα = 0,λAug = 1) and SGD+USA+UFGSM+EnAug+AT to (λAT = λAug = 1,λα = 0). Metric Dataset 5-way 1-shot SGD SGD+AT SGD+USA SGD+USA +UFGSM SGD+USA +UFGSM +EnAug+AT All Mini 37.21 ±0.22 34.60±0.18 37.53±0.27 38.71±0.28 36.24±0.22 Birds 30.07 ±0.52 28.81±0.45 30.39±0.53 31.07±0.50 29.83±0.46 Flowers 40.54 ±0.48 38.80±0.44 41.09±0.49 42.33±0.60 40.86±0.45 Signs 34.19 ±0.76 33.42±0.59 35.02±0.82 36.10±0.48 34.76±0.61 Avg. 35.50 ±0.49 33.91±0.41 36.01±0.36 37.05±0.27 35.42±0.44 Top-1 Mini 46.71 ±0.81 46.58±0.65 46.72±0.71 46.66±0.77 46.62±0.58 Birds 36.03 ±0.48 36.40±0.48 36.08±0.49 36.02±0.48 36.43±0.58 Flowers 51.97 ±0.79 54.55±0.84 52.01±0.77 52.30±0.77 54.36±1.04 Signs 43.43 ±0.99 43.98±0.98 43.42±1.06 43.96±0.64 44.10±1.03 Avg. 44.54 ±0.77 45.38±0.74 44.56±0.44 44.73±0.41 45.37±0.81 Top-40% Mini 45.07 ±0.52 45.53±0.50 45.23±0.59 45.76±0.62 45.79±0.44 Birds 34.88 ±0.55 35.57±0.60 35.17±0.55 35.30±0.48 35.77±0.55 Flowers 49.98 ±0.69 52.92±0.68 50.39±0.71 51.07±0.67 53.16±0.73 Signs 41.83 ±1.02 43.19±0.96 42.08±1.06 43.18±0.73 43.39±1.01 Avg. 42.94 ±0.70 44.30±0.68 43.22±0.43 43.83±0.39 44.52±0.68 0.2 0.24 0.28 0.32 0.36 0.4 0.44 0.48 0.52 0.56 0.0001 0.0003 0.0005 0.0007 0.0009 0.002 0.004 0.006 0.008 0.01 0.03 0.05 0.07 0.09 0.2 0.4 0.6 0.8 1 Accuracy Stepsize SGD USA USA+UFGSM USA+UFGSM+EnAug USA+UFGSM+EnAug+AT (a) With SGD optimizer 0.24 0.27 0.3 0.33 0.36 0.39 0.42 0.45 0.48 0.51 0.54 0.57 0.0001 0.0003 0.0005 0.0007 0.0009 0.002 0.004 0.006 0.008 0.01 0.03 0.05 0.07 0.09 0.2 0.4 0.6 0.8 1 Accuracy Stepsize Adam USA USA+UFGSM USA+UFGSM+EnAug USA+UFGSM+EnAug+AT (b) With Adam optimizer Figure 3: Ablation study for 5-way 1-shot classiﬁcation on Flowers dataset. 8Published as a conference paper at ICLR 2021 Table 3: Comparing UFGSM against FGSM (Goodfellow et al., 2015) on 5-way 1-shot tasks. For all metrics and datasets, the proposed uncertainty- based method is better. Metric Dataset SGD+USA +UFGSM +FGSM All Mini 38.71±0.28 36.97±0.54 Birds 31.07±0.50 30.05±0.55 Flowers 42.33±0.60 41.17±0.55 Signs 36.10±0.48 34.78±0.50 Avg. 37.05±0.27 35.74±0.27 Top-1 Mini 46.66±0.77 43.93±0.85 Birds 36.02±0.48 34.52±0.74 Flowers 52.30±0.77 50.63±0.84 Signs 43.96±0.64 42.62±1.00 Avg. 44.73±0.41 42.93±0.47 Top-40% Mini 45.76±0.62 43.18±0.87 Birds 35.30±0.48 33.88±0.75 Flowers 51.07±0.67 49.47±0.69 Signs 43.18±0.73 41.92±0.90 Avg. 43.83±0.39 42.11±0.46 Table 4: Comparing SGD vs. Adam for default MAML steps (baseline) and our method (baseline+all) on 5-way 1-shot classiﬁcation. Metric Dataset Baseline Baseline+All SGD Adam SGD Adam All Mini 37.21±0.22 31.87±0.20 36.24±0.22 33.57±0.29 Birds 30.07±0.52 28.02±0.39 29.83±0.46 29.51±0.46 Flowers40.54±0.48 40.33±0.57 40.86±0.45 44.58±0.70 Signs 34.19±0.76 33.98±0.43 34.76±0.61 35.10±0.43 Avg. 35.50±0.49 33.55±0.22 35.42±0.44 35.69±0.29 Top-1 Mini 46.71±0.81 43.08±0.10 46.62±0.58 44.72±0.35 Birds 36.03±0.48 34.00±0.63 36.43±0.58 35.45±0.67 Flowers51.97±0.79 51.28±0.47 54.36±1.04 56.04±1.14 Signs 43.43±0.99 42.53±0.79 44.10±1.03 43.01±0.85 Avg. 44.54±0.77 42.72±0.32 45.37±0.81 44.80±0.54 Top-40% Mini 45.07±0.52 40.18±0.14 45.79±0.44 42.92±0.35 Birds 34.88±0.55 32.38±0.61 35.77±0.55 34.54±0.66 Flowers49.98±0.69 48.18±0.44 53.16±0.73 54.08±0.76 Signs 41.83±1.02 40.90±0.70 43.39±1.01 42.26±0.77 Avg. 42.94±0.70 40.41±0.29 44.52±0.68 43.45±0.40 to stepsize. In fact, the gains are more substantial on the overﬁtted (worse) checkpoint than the best checkpoint (Appendix A.8). Validating Proposal 1 and 2. Our method is built on the hypotheses that we should take small stepsizes on high uncertainty parameters and add more adversarial perturbation on high uncertainty input gradients. To validate those choices, we have tried taking large stepsizes on high uncertainty parameters, and using less adversarial perturbation on high uncertainty input gradients, which re- sulted in lower accuracy and robustness to the stepsize (Appendix A.7). Freezing most uncertain layers. We observed that the batch normalization (BN) layers have the most uncertainty (Figure 1). In Proposal 1, we argued that the higher uncertainty component ampli- ﬁes the error when we use bigger stepsize. As suggested by an anonymous reviewer, this motivates an additional experiment that we perform where we “freeze” the BN layer during meta-testing (i.e. we manually set the stepsize for the BN scale and shift parameters to zero). We performed the BN layer freezing experiment on SGD+All (see detailed results inTable 10and Figure 12 in Appendix A.9). It turns out that SGD+All (w/ freezing BN) outperforms SGD+All (w/o freezing BN) on the All metric, with most improvement in the higher stepsize range ( >0.1). This is consistent with our intuition that updating high-uncertainty layers (such as BN) with large stepsizes can be harmful. 5 C ONCLUSION In this paper we considered the novel problem of repurposing pretrained MAML checkpoints for out-of-domain few-shot learning. Our method uses deep ensembles to estimate model parameter and input gradient uncertainties over the support set, and builds upon the default MAML gradient steps through the addition of uncertainty-based adversarial training and adaptive stepsizes. Our experiments over popular few-shot benchmarks show that our method yields increased accuracy and robustness to the choice of base stepsize. More generally, our results motivate the use of adversarial learning as a data augmentation scheme for improving few-shot generalization. In the future, it would be interesting to apply our method to related settings such as domain adaption and transfer learning. ACKNOWLEDGMENTS We thank Hugo Larochelle and Reza Babanezhad for insightful discussions, Damien Scieur and Emmanuel Bengio for helpful feedback on the manuscript. We also thank the anonymous reviewers for their comments and suggestions. 9Published as a conference paper at ICLR 2021 This research was partially supported by the Canada CIFAR AI Chair Program, the NSERC Dis- covery Grant RGPIN-2017-06936 and a Google Focused Research award. Simon Lacoste-Julien is a CIFAR Associate Fellow in the Learning in Machines & Brains program. REFERENCES Hilal Asi and John C. Duchi. The importance of better models in stochastic optimization. InNational Academy of Sciences, 2019. Shai Ben-David, John Blitzer, Koby Crammer, Alex Kulesza, Fernando Pereira, and Jennifer Wort- man Vaughan. A theory of learning from different domains. In Machine Learning, 2010. Charles Blundell, Julien Cornebise, Koray Kavukcuoglu, and Daan Wierstra. Weight uncertainty in neural network. In International Conference on Machine Learning, 2015. Aymeric Dieuleveut, Alain Durmus, Francis Bach, et al. Bridging the gap between constant step size stochastic gradient descent and markov chains. In Annals of Statistics. Institute of Mathematical Statistics, 2020. Sayna Ebrahimi, Mohamed Elhoseiny, Trevor Darrell, and Marcus Rohrbach. Uncertainty-guided continual learning with bayesian neural networks. In International Conference on Learning Rep- resentations, 2020. Chelsea Finn, Pieter Abbeel, and Sergey Levine. Model-agnostic meta-learning for fast adaptation of deep networks. In International Conference on Machine Learning, 2017. Ian J. Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing Xu, David Warde-Farley, Sherjil Ozair, Aaron Courville, and Yoshua Bengio. Generative adversarial nets. In Advances in Neural Infor- mation Processing Systems, 2014. Ian J. Goodfellow, Jonathon Shlens, and Christian Szegedy. Explaining and harnessing adversarial examples. In International Conference on Learning Representations, 2015. Erin Grant, Chelsea Finn, Sergey Levine, Trevor Darrell, and Thomas Grifﬁths. Recasting gradient- based meta-learning as hierarchical bayes. In International Conference on Learning Representa- tions, 2018. Sebastian Houben, Johannes Stallkamp, Jan Salmen, Marc Schlipsing, and Christian Igel. Detection of trafﬁc signs in real-world images: The german trafﬁc sign detection benchmark. In Interna- tional Joint Conference on Neural Networks, 2013. Diederik P Kingma and Jimmy Ba. Adam: A method for stochastic optimization. In arXiv preprint arXiv:1412.6980, 2014. Balaji Lakshminarayanan, Alexander Pritzel, and Charles Blundell. Simple and scalable predictive uncertainty estimation using deep ensembles. In Advances in Neural Information Processing Systems, 2017. Yoonho Lee and Seungjin Choi. Gradient-based meta-learning with learned layerwise metric and subspace. In International Conference on Machine Learning, 2018. Aleksander Madry, Aleksandar Makelov, Ludwig Schmidt, Dimitris Tsipras, and Adrian Vladu. Towards deep learning models resistant to adversarial attacks. In International Conference on Learning Representations, 2018. Nikhil Mishra, Mostafa Rohaninejad, Xi Chen, and Pieter Abbeel. A simple neural attentive meta- learner. In International Conference on Learning Representations, 2018. Seyed Mohsen Moosavi-Dezfooli, Alhussein Fawzi, and Pascal Frossard. Deepfool: A simple and accurate method to fool deep neural networks. InComputer Vision and Pattern Recognition, 2016. Saeid Motiian, Quinn Jones, Seyed Iranmanesh, and Gianfranco Doretto. Few-shot adversarial domain adaptation. In Advances in Neural Information Processing Systems, 2017. 10Published as a conference paper at ICLR 2021 Maria Elena Nilsback and Andrew Zisserman. Automated ﬂower classiﬁcation over a large number of classes. In Indian Conference on Computer Vision, Graphics and Image Processing, 2008. Yoshio Nishi and Robert Doering. Handbook of semiconductor manufacturing technology. CRC press, 2000. Sachin Ravi and Hugo Larochelle. Optimization as a model for few-shot learning. In International Conference on Learning Representations, 2017. Adam Santoro, Sergey Bartunov, Matthew Botvinick, Daan Wierstra, and Timothy Lillicrap. Meta- learning with memory-augmented neural networks. In International Conference on Machine Learning, 2016. Jake Snell, Kevin Swersky, and Richard Zemel. Prototypical networks for few-shot learning. In Advances in Neural Information Processing Systems, 2017. Flood Sung, Yongxin Yang, Li Zhang, Tao Xiang, Philip H.S. Torr, and Timothy M. Hospedales. Learning to compare: Relation network for few-shot learning. In Computer Vision and Pattern Recognition, 2018. Eleni Triantaﬁllou, Tyler Zhu, Vincent Dumoulin, Pascal Lamblin, Utku Evci, Kelvin Xu, Ross Goroshin, Carles Gelada, Kevin Swersky, Pierre-Antoine Manzagol, and Hugo Larochelle. Meta- dataset: A dataset of datasets for learning to learn from few examples. InInternational Conference on Learning Representations, 2020. Hung-Yu Tseng, Hsin-Ying Lee, Jia-Bin Huang, and Ming-Hsuan Yang. Cross-domain few-shot classiﬁcation via learned feature-wise transformation. In International Conference on Learning Representations, 2020. Eric Tzeng, Judy Hoffman, Kate Saenko, and Trevor Darrell. Adversarial discriminative domain adaptation. In Conference on Computer Vision and Pattern Recognition, 2017. Oriol Vinyals, Charles Blundell, Timothy Lillicrap, Koray Kavukcuoglu, and Daan Wierstra. Match- ing networks for one shot learning. InAdvances in Neural Information Processing Systems, 2016. Peter Welinder, Steve Branson, Takeshi Mita, Catherine Wah, Florian Schroff, Serge Belongie, and Pietro Perona. Caltech-ucsd birds 200. 2010. Yeming Wen, Paul Vicol, Jimmy Ba, Dustin Tran, and Roger Grosse. Flipout: Efﬁcient pseudo- independent weight perturbations on mini-batches. In International Conference on Learning Representations, 2018. Garrett Wilson and Diane J. Cook. A survey of unsupervised deep domain adaptation. In ACM Transactions on Intelligent Systems and Technology, 2020. Cihang Xie, Mingxing Tan, Boqing Gong, Jiang Wang, Alan L. Yuille, and Quoc V . Le. Adversarial examples improve image recognition. In Computer Vision and Pattern Recognition, 2020. Chengxiang Yin, Jian Tang, Zhiyuan Xu, and Yanzhi Wang. Adversarial meta-learning. In arXiv preprint arXiv:1806.03316, 2018. Matthew D Zeiler and Rob Fergus. Visualizing and understanding convolutional networks. In European conference on computer vision, 2014. Yabin Zhang, Hui Tang, Kui Jia, and Mingkui Tan. Domain-symmetric networks for adversarial domain adaptation. In Computer Vision and Pattern Recognition, 2019. An Zhao, Mingyu Ding, Zhiwu Lu, Tao Xiang, Yulei Niu, Jiechao Guan, Ji Rong Wen, and Ping Luo. Domain-adaptive few-shot learning. In arXiv, 2020. 11Published as a conference paper at ICLR 2021 A A PPENDIX A.1 N OTATIONS AND ACRONYMS Notations and acronyms used repeatedly throughout the paper are summarized below. Notation Meaning USA Uncertainty-based stepsize adaption I/USA Inverse USA FGSM Fast gradient sign method optimization (Goodfellow et al., 2015) UFGSM Uncertainty-based FGSM I/UFGSM Inverse UFGSM EnAug Ensemble augmentation T Number of test-time gradient steps M Size of deep ensemble θ0 Pretrained MAML checkpoint to repurpose θm t Ensemble element mat time t θT Model parameters after meta-testing α Base stepsize αadap Uncertainty-based stepsize (parameter-wise) DSpt Support set (test-time) DAug Adversarial examples based on DSpt A.2 D ESIGN CHOICES FOR USA Here, we introduce the design choices of the Algorithm 2. Line 3: We calculate the maximum Max(u) and minimum Min(u) values among the estimated stan- dard deviations ufor each parameter. As proposal 1, in order to assign to low stepsize to high uncer- tainty, the standard deviation of each parameter is subtracted from the maximum value Max(u) −u for inverse-relationship. At this time, the largest value among parameter values becomes 0. To pre- vent this, we add the smallest Max(u) −u+ Min(u). Note that all values of uare non-negative. We also tried alternatives ways like non-inverse-relationship and relative standard deviation, but it did not seem to work better. To keep the same scale of the uncertainty before and after the transforma- tion, we chose the inverse-relationship using Max(u) −u+ Min(u). Line 4: In this study, we use layer-wise adapted stepsize. In order to assign the same stepsize for each layer, we calculate the average of the parameters of each layer and the values of the parameters of the corresponding layer are replaced with the average value. It repeats all layers and performs the corresponding operation. Line 5: To more easily compare the effectiveness of USA with the baseline, we set the average stepsize equals to the base stepsize, i.e. 1/|αadap|∑αadap = α. If USA is applied without this rescaling, than the average stepsize can change, which makes it difﬁcult to distinguish the effect of using an overall different stepsize (for constant SGD e.g.) vs. our adaptation of stepsizes. A.3 E XPERIMENTAL SETUP AND PRETRAINED MODEL SELECTION Our baseline model was trained using the same hyperparameters with miniImageNet training of MAML except the inner loop stepsize. The inner loop stepsize was set to 0.1 for reproducing the 5-way 1-shot accuracy reported in the original MAML paper. We trained the model for 150,000 iter- ations. A pretrained model was selected with the validation accuracy among miniImageNet training checkpoints. We used the checkpoint with the highest validation accuracy as the pretrained model θ0. The highest test accuracy we achieved was 49.24% during meta-training. For the checkpoint that we chose with the highest validation performance, the test accuracy was 47.58%. Note that the cross-domain performance highly depended on which checkpoint we used. See Fig.4 in Appendix A.4. 12Published as a conference paper at ICLR 2021 The size of ensemble is M = 5 which is the same as the deep ensembles (Lakshminarayanan et al., 2017). The parameter for the FGSM is ϵ = 0.05. The scale value a of Gaussian random perturbation for ensemble model training is σ = 0.05. The gradient step is T = 10which is same with miniImageNet test of MAML. We do not split those datasets exceptminiImageNet, because we do not use the datasets in the meta- training. We used the same splits as Ravi & Larochelle (2017) forminiImageNet. Tseng et al. (2020) used a randomly split manner for the cross-domain test and Triantaﬁllou et al. (2020) used all trafﬁc signs dataset for the test. A.4 C ROSS -DOMAIN ACCURACY WHILE META -TRAINING MAML ON MINI IMAGE NET The performance of the cross-domain signiﬁcantly differed depending on the selected checkpoint. Figure 4 shows the performance for every 200 iterations. We selected the checkpoint based on miniImageNet validation performance, which is the highest at the 57,200 iteration. Figure 4: Cross-domain performance while meta-training on miniImageNet. X-axis is training iter- ation on meta-training on miniImageNet and y-axis is classiﬁcation accuracy. A.5 A DDITIONAL RESULTS FOR AT-ONLY SETTING . To evaluate the effect of AT, we only applied AT in MAML (λα = λAT = 1,λAug = 0). Table 5, Figure 5 and 6 show AT results on 5-way 1-shot with SGD and Adam. Adam is worse than SGD in the results. However, we found that AT and Adam are good combination for a meta-test training. A.6 A DDITIONAL RESULTS FOR 5-WAY 5-SHOT AND 10- WAY 1-SHOT CLASSIFICATION . We present some additional results for 5-way 5-shot and 10-way 1-shot classiﬁcation in this section. Table 6 shows the 5-way 5-shot and 10-way 1-shot classiﬁcation results. Especially in the case of the Top-40% Avg our method signiﬁcantly increased the performance about 5.84% and 1.83% in 5- way 5-shot and 10-way 1-shot respectively. It means that it can increase the probability to select of 13Published as a conference paper at ICLR 2021 Table 5: 5-way 1-shot classiﬁcation results. In order to verify the effectiveness of AT, we applied only AT among the proposed method and the all proposed method. Metric Dataset Baseline Ours (SGD) Baseline Ours (Adam) SGD AT Only All Adam AT Only All All Mini 37.21±0.22 34.60±0.18 36.24±0.22 31.87±0.20 33.40±0.33 33.57±0.29 Birds 30.07±0.52 28.81±0.45 29.83±0.46 28.02±0.39 29.37±0.51 29.51±0.46 Flowers 40.54±0.48 38.80±0.44 40.86±0.45 40.33±0.57 44.53±0.68 44.58±0.70 Signs 34.18±0.76 33.42±0.59 34.76±0.61 33.98±0.43 34.96±0.47 35.10±0.43 Avg. 35.50±0.49 33.91±0.41 35.42±0.44 33.55±0.22 35.57±0.29 35.69±0.29 Top-1 Mini 46.71±0.81 46.58±0.65 46.62±0.58 43.08±0.10 44.80±0.40 44.72±0.35 Birds 36.03±0.48 36.40±0.48 36.43±0.58 34.00±0.63 35.28±0.66 35.45±0.67 Flowers 51.97±0.79 54.55±0.84 54.36±1.04 51.28±0.47 56.10±1.06 56.04±1.14 Signs 43.44±0.99 43.98±0.98 44.10±1.03 42.53±0.79 43.13±0.83 43.01±0.85 Avg. 44.54±0.77 45.38±0.74 45.37±0.81 42.72±0.32 44.83±0.51 44.80±0.54 Top-40% Mini 45.07±0.52 45.53±0.50 45.79±0.44 40.18±0.14 42.59±0.43 42.92±0.35 Birds 34.88±0.55 35.57±0.60 35.77±0.55 32.38±0.61 34.26±0.73 34.54±0.66 Flowers 49.98±0.69 52.92±0.68 53.16±0.73 48.18±0.44 53.88±0.72 54.08±0.76 Signs 41.81±1.02 43.19±0.96 43.39±1.01 40.90±0.70 42.06±0.74 42.26±0.77 Avg. 42.94±0.70 44.30±0.68 44.52±0.68 40.41±0.29 43.20±0.42 43.45±0.40 0.2 0.22 0.24 0.26 0.28 0.3 0.32 0.34 0.36 0.38 0.4 0.42 0.44 0.46 0.48 0.0001 0.0004 0.0007 0.001 0.004 0.007 0.01 0.04 0.07 0.1 0.4 0.7 1 (a) Mini 0.2 0.22 0.24 0.26 0.28 0.3 0.32 0.34 0.36 0.38 0.0001 0.0004 0.0007 0.001 0.004 0.007 0.01 0.04 0.07 0.1 0.4 0.7 1 Accuracy Stepsize SGD AT Only ALL (b) Birds 0.2 0.24 0.28 0.32 0.36 0.4 0.44 0.48 0.52 0.56 0.0001 0.0004 0.0007 0.001 0.004 0.007 0.01 0.04 0.07 0.1 0.4 0.7 1 (c) Flowers 0.2 0.22 0.24 0.26 0.28 0.3 0.32 0.34 0.36 0.38 0.4 0.42 0.44 0.46 0.0001 0.0004 0.0007 0.001 0.004 0.007 0.01 0.04 0.07 0.1 0.4 0.7 1 (d) Signs Figure 5: 5-way 1-shot classiﬁcation results with AT on SGD. optimal stepsize for a new task. As shown in Figure 7, there are more ﬂatten curve near the highest accuracy in both tasks. A.7 V ALIDATING PROPOSALS 1 AND 2 WITH “INVERSE ” COUNTERPARTS . To validate proposal 1, we evaluate the inverse strategy of USA, denoted I/USA, which assigns higher stepsizes to layers with higher uncertainty. Speciﬁcally, we ﬂip the stepsizes by replacingline 3 of Algorithm 2 withc←u. To measure the effect of USA, we only applied USA in MAML (λα = λAT = λAug = 0). The results for 5-way 1-shot classiﬁcation are in Table 7. USA outperforms the baselines over broad ranges of stepsizes compared to Adam and SGD. I/USA signiﬁcantly decreased the performance of all metrics. Therefore, USA reﬂected useful knowledge well into the stepsize for a new task. See accuracy by stepsizes in Fig 8. Figure 8 shows the results for verifying of Proposal 1. USA shows more ﬂatten curve in Top-1 accuracy ranges than SGD and I/USA. I/USA degraded the performance through all ranges of stepsizes. For verifying the proposal 2, we evaluated three methods UFGSM , I/UFGSM and FGSM on USA. I/UFGSM is inversely implemented method of UFGSM. FGSM uses examples generated by FGSM instead of UFGSM. To measure the effect of UFGSM, we only applied UFGSM in MAML with USA (λα = λAT = 0,λAug = 1). Table 7 shows 5-way 1-shot classiﬁcation results. UFGSM outperforms all the other methods for every metric. In spite of I/UFGSM had stronger adversar- ial perturbation than UFGSM, the performance was worse than UFGSM. Note that I/UFGSM and FGSM showed almost similar performances. The reason is that most of the input pixels have an un- certainty close to 0; therefore, when scaling after inverse, most pixels have a value of 1. Therefore, the generated examples are almost similar to the adversarial example in which FGSM is applied. See accuracy by stepsizes in Fig 9. Figure 9 shows the result for verifying of proposal 2. UFGSM 14Published as a conference paper at ICLR 2021 0.22 0.24 0.26 0.28 0.3 0.32 0.34 0.36 0.38 0.4 0.42 0.44 0.46 0.0001 0.0004 0.0007 0.001 0.004 0.007 0.01 0.04 0.07 0.1 0.4 0.7 1 (a) Mini 0.22 0.24 0.26 0.28 0.3 0.32 0.34 0.36 0.0001 0.0004 0.0007 0.001 0.004 0.007 0.01 0.04 0.07 0.1 0.4 0.7 1 Accuracy Stepsize Adam AT Only ALL (b) Birds 0.24 0.27 0.3 0.33 0.36 0.39 0.42 0.45 0.48 0.51 0.54 0.57 0.0001 0.0004 0.0007 0.001 0.004 0.007 0.01 0.04 0.07 0.1 0.4 0.7 1 (c) Flowers 0.22 0.24 0.26 0.28 0.3 0.32 0.34 0.36 0.38 0.4 0.42 0.44 0.0001 0.0004 0.0007 0.001 0.004 0.007 0.01 0.04 0.07 0.1 0.4 0.7 1 (d) Signs Figure 6: 5-way 1-shot classiﬁcation results with AT on Adam. Table 6: 5-way 5-shot and 10-way 1-shot classiﬁcation results using SGD. Our method outperformed both tasks. Especially 5-way 5-shot classiﬁcation performance signiﬁcantly improved than baseline. Metric Dataset 5-way 5-shot 10-way 1-shot Baseline Ours (SGD) Baseline Ours (SGD) SGD SGD+AT SGD+All SGD SGD+AT SGD+All All Mini 38.14±0.39 41.06±0.35 43.48±0.38 21.31±0.37 20.97±0.37 22.07±0.40 Birds 33.29±0.29 36.04±0.28 38.07±0.33 17.54±0.06 17.08±0.07 17.81±0.12 Flowers 44.55±0.43 48.94±0.48 53.10±0.55 26.79±0.41 26.50±0.42 28.07±0.41 Signs 45.39±0.45 47.98±0.55 51.02±0.50 23.77±0.13 23.99±0.11 24.88±0.13 Avg. 40.34±0.39 43.51±0.41 46.42±0.44 22.35±0.24 22.14±0.24 23.21±0.26 Top-1 Mini 62.46±0.68 62.07±0.61 62.16±0.66 31.10±0.66 31.18±0.81 31.42±0.82 Birds 50.71±0.54 51.93±0.62 51.82±0.64 23.16±0.21 23.42±0.37 23.61±0.39 Flowers 70.63±0.76 74.14±0.72 74.52±0.69 38.61±0.49 40.84±0.58 40.68±0.58 Signs 72.52±0.84 73.66±0.98 74.13±0.90 35.10±0.21 34.76±0.17 35.11±0.26 Avg. 64.08±0.71 65.45±0.73 65.66±0.72 31.99±0.39 32.55±0.48 32.70±0.51 Top-40% Mini 55.38±0.46 59.97±0.69 60.70±0.91 28.65±0.64 30.22±0.74 30.56±0.77 Birds 45.50±0.49 50.48±0.53 51.03±0.63 21.78±0.07 22.61±0.22 22.87±0.30 Flowers 65.16±0.73 72.61±0.78 73.55±0.86 36.14±0.53 39.40±0.66 39.75±0.61 Trafﬁc 67.34±0.43 70.56±0.87 71.45±0.90 33.56±0.11 34.21±0.13 34.27±0.17 Avg. 58.34±0.53 63.40±0.72 64.18±0.82 30.03±0.34 31.61±0.44 31.86±0.46 shows ﬂatter curve near the best performing accuracy than SGD, I/UFGSM and FGSM. FGSM and I/UFGSM show very similar trends. We explained the reason why the two methods gave similar results. As can be seen in Fig 10, almost all of the input gradient uncertainty is near zero. When we inverse the value, almost all the value is 1 (See Fig 11). UFGSM improves performance despite less adversarial perturbation than I/UFGSM and FGSM. Through this, even a small change can help model learning if correct (useful) information is reﬂected. A.8 5- WAY 5-SHOT CLASSIFICATION RESULTS ON OVERFITTED CHECKPOINT As can seen in Appendix A.4, overﬁtted models on miniImageNet degraded performance over all datasets. We investigated the 5-way 5-shot classiﬁcation results on the checkpoint after meta- training with MAML for 150K iterations. Table 8 and 9 show the results with SGD and Adam. The number in the parentheses is the difference from the baseline for each checkpoint (e.g. SGD or Adam). The results using the (Last) checkpoint are worse than (Validation) checkpoint due to the overﬁtting. However the performance boost from using our method is more substantial on the over- ﬁtted (Last) checkpoint than the best (Validation) checkpoint, both in terms of absolute performance (Top-1) and robustness to choice of base stepsize (other metrics). A.9 F REEZING MOST UNCERTAIN LAYERS . Since some of the BatchNorm parameters have the highest uncertainty (see Figure 1a), we have experimented freezing the BatchNorm parameters during meta-testing (i.e. setting their stepsize to zero). It turns out that SGD+All (w/ freezing BN) outperforms SGD+All (w/o freezing BN) on the All metric (Table 10), with most improvement in the higher stepsize range (>0.1) (Figure 12). We 15Published as a conference paper at ICLR 2021 0.2 0.24 0.28 0.32 0.36 0.4 0.44 0.48 0.52 0.56 0.6 0.64 0.0001 0.0004 0.0007 0.001 0.004 0.007 0.01 0.04 0.07 0.1 0.4 0.7 1 0.2 0.24 0.28 0.32 0.36 0.4 0.44 0.48 0.52 0.0001 0.0005 0.0009 0.004 0.008 0.03 0.07 0.2 0.6 1 Accuracy Stepsize SGD AT Only ALL 0.2 0.25 0.3 0.35 0.4 0.45 0.5 0.55 0.6 0.65 0.7 0.75 0.0001 0.0004 0.0007 0.001 0.004 0.007 0.01 0.04 0.07 0.1 0.4 0.7 1 0.2 0.24 0.28 0.32 0.36 0.4 0.44 0.48 0.52 0.56 0.6 0.64 0.68 0.72 0.76 0.0001 0.0004 0.0007 0.001 0.004 0.007 0.01 0.04 0.07 0.1 0.4 0.7 1 0.1 0.12 0.14 0.16 0.18 0.2 0.22 0.24 0.26 0.28 0.3 0.32 0.0001 0.0004 0.0007 0.001 0.004 0.007 0.01 0.04 0.07 0.1 0.4 0.7 1 (a) Mini 0.1 0.12 0.14 0.16 0.18 0.2 0.22 0.24 0.0001 0.0004 0.0007 0.001 0.004 0.007 0.01 0.04 0.07 0.1 0.4 0.7 1 Accuracy Stepsize SGD AT Only ALL (b) Birds 0.1 0.14 0.18 0.22 0.26 0.3 0.34 0.38 0.42 0.0001 0.0004 0.0007 0.001 0.004 0.007 0.01 0.04 0.07 0.1 0.4 0.7 1 (c) Flowers 0.1 0.12 0.14 0.16 0.18 0.2 0.22 0.24 0.26 0.28 0.3 0.32 0.34 0.36 0.0001 0.0004 0.0007 0.001 0.004 0.007 0.01 0.04 0.07 0.1 0.4 0.7 1 (d) Signs Figure 7: 5-way 5-shot (Top) and 10-way 1-shot (Bottom) classiﬁcation with MAML(SGD) and our proposed method. In terms of robustness, our method outperforms MAML(SGD) more effectively. In addition, we show that our method outperforms Top-1 accuracy for Birds, Flowers and Signs. Also our method shows ﬂatter curve near highest accuracy sections. This increases the probability of optimal stepsize selection. 0.2 0.24 0.28 0.32 0.36 0.4 0.44 0.48 0.0001 0.0004 0.0007 0.001 0.004 0.007 0.01 0.04 0.07 0.1 0.4 0.7 1 (a) Mini 0.2 0.22 0.24 0.26 0.28 0.3 0.32 0.34 0.36 0.38 0.0001 0.0005 0.0009 0.004 0.008 0.03 0.07 0.2 0.6 1 Accuracy Stepsize SGD USA I/USA (b) Birds 0.2 0.24 0.28 0.32 0.36 0.4 0.44 0.48 0.52 0.56 0.0001 0.0004 0.0007 0.001 0.004 0.007 0.01 0.04 0.07 0.1 0.4 0.7 1 (c) Flowers 0.2 0.23 0.26 0.29 0.32 0.35 0.38 0.41 0.44 0.47 0.0001 0.0004 0.0007 0.001 0.004 0.007 0.01 0.04 0.07 0.1 0.4 0.7 1 (d) Signs Figure 8: 5-way 1-shot classiﬁcation for verifying proposal 1. We compared to USA and I/USA. notice a similar trend for 5-way 5-shot SGD, where SGD (w/ freezing BN) also outperforms SGD (w/o freezing BN) on the All metric, with most improvement in the higher stepsize range. 16Published as a conference paper at ICLR 2021 Table 7: 5-way 1-shot classiﬁcation results for verifying proposal 1 and 2. I/USA and I/UFGSM degraded performance than USA and UFGSM respectively. Our approach which is applied USA and UFGSM outperformed SGD on almost of the metrics. Metric Dataset Baseline SGD SGD+USA SGD Adam +USA +I/USA +UFGSM +I/UFGSM +FGSM All Mini 37.21 31.85 37.53 28.73 38.71 37.25 36.97 Birds 30.07 28.17 30.39 24.98 31.07 30.27 30.05 Flowers 40.54 40.49 41.09 30.52 42.33 41.44 41.17 Signs 34.21 34.28 35.02 27.90 36.10 34.98 34.78 Avg. 35.50 33.68 36.01 28.03 37.05 35.98 35.74 Top-1 Mini 46.71 43.07 46.72 44.13 46.66 44.31 43.93 Birds 36.03 34.30 36.08 34.20 36.02 34.77 34.52 Flowers 51.97 51.38 52.01 49.34 52.30 50.90 50.63 Signs 43.44 43.00 43.42 41.03 43.96 42.82 42.62 Avg. 44.54 42.93 44.56 42.17 44.73 43.20 42.93 Top-40% Mini 45.07 40.16 45.23 38.33 45.76 43.60 43.18 Birds 34.88 32.64 35.17 30.28 35.30 34.20 33.88 Flowers 49.98 48.28 50.39 42.19 51.07 49.79 49.47 Signs 41.81 41.32 42.08 35.78 43.18 42.17 41.92 Avg. 42.94 40.60 43.22 36.64 43.83 42.44 42.11 0.2 0.24 0.28 0.32 0.36 0.4 0.44 0.48 0.0001 0.0004 0.0007 0.001 0.004 0.007 0.01 0.04 0.07 0.1 0.4 0.7 1 (a) Mini 0.2 0.22 0.24 0.26 0.28 0.3 0.32 0.34 0.36 0.38 0.0001 0.0005 0.0009 0.004 0.008 0.03 0.07 0.2 0.6 1 Accuracy Stepsize SGD +UFGSM +I/UFGSM +FGSM (b) Birds 0.2 0.24 0.28 0.32 0.36 0.4 0.44 0.48 0.52 0.56 0.0001 0.0004 0.0007 0.001 0.004 0.007 0.01 0.04 0.07 0.1 0.4 0.7 1 (c) Flowers 0.2 0.23 0.26 0.29 0.32 0.35 0.38 0.41 0.44 0.47 0.0001 0.0004 0.0007 0.001 0.004 0.007 0.01 0.04 0.07 0.1 0.4 0.7 1 (d) Signs Figure 9: 5-way 1-shot classiﬁcation results to verify proposal 2. We compared to UFGSM, I/UFGSM and FGSM. (a) Uncertainty map (u)  (b) Flatten uncertainty  (c) Histogram of uncertainty Figure 10: (a) shows the re-scaled input gradient uncertainty to generate the UFGSM example. (b) is a ﬂatten plot of the re-scaled input gradient uncertainty. (c) shows a histogram of the uncertainty. 17Published as a conference paper at ICLR 2021 (a) Uncertainty map (u)  (b) Flatten uncertainty  (c) Histogram of uncertainty Figure 11: (a) shows the re-scaled inverse input gradient uncertainty to generate the UFGSM exam- ple. (b) is a ﬂatten plot of the re-scaled inverse input gradient uncertainty. (c) shows a histogram of the uncertainty. Table 8: 5-way 5-shot classiﬁcation results between selected checkpoint with SGD Metric Dataset Checkpoint (Validation) Checkpoint (Last) Baseline Ours (SGD) Baseline Ours (SGD) SGD AT Only All SGD AT Only All All Mini 38.14±0.39 41.06±0.35 43.48±0.38 34.98±0.54 38.66±0.52 41.89±0.61 (-) (2.92) (5.34) (-) (3.68) (6.91) Birds 33.29±0.29 36.04±0.28 38.07±0.33 31.33±0.51 34.34±0.49 36.90±0.5 (-) (2.75) (4.78) (-) (3.01) (5.57) Flowers 44.55±0.43 48.94±0.48 53.10±0.55 40.96±0.67 45.6±0.6 50.58±0.67 (-) (4.39) (8.55) (-) (4.64) (9.62) Signs 45.39±0.45 47.98±0.55 51.02±0.50 42.05±0.35 46.01±0.36 49.88±0.35 (-) (2.59) (5.63) (-) (3.96) (7.83) Avg. 40.34±0.39 43.51±0.41 46.42±0.44 37.33±0.52 41.15±0.49 44.81±0.53 (-) (3.17) (6.08) (-) (3.82) (7.48) Top-1 Mini 62.46±0.68 62.07±0.61 62.16±0.66 58.49±0.72 58.44±0.93 58.72±1.08 (-) (-0.39) (-0.3) (-) (-0.05) (0.23) Birds 50.71±0.54 51.93±0.62 51.82±0.64 46.77±0.53 48.33±0.72 48.58±0.7 (-) (1.22) (1.11) (-) (1.56) (1.81) Flowers 70.63±0.76 74.14±0.72 74.52±0.69 66.31±1.02 68.99±0.81 69.73±0.71 (-) (3.51) (3.89) (-) (2.68) (3.42) Signs 72.52±0.84 73.66±0.98 74.13±0.90 67.75±0.57 70.71±0.48 71.08±0.53 (-) (1.14) (1.61) (-) (2.96) (3.33) Avg. 64.08±0.71 65.45±0.73 65.66±0.72 59.83±0.71 61.62±0.73 62.03±0.75 (-) (1.37) (1.58) (-) (1.79) (2.2) Top-40% Mini 55.38±0.46 59.97±0.69 60.70±0.91 50.56±0.72 56.44±0.87 57.22±1 (-) (4.59) (5.32) (-) (5.88) (6.66) Birds 45.50±0.49 50.48±0.53 51.03±0.63 42.13±0.55 47.22±0.73 47.98±0.73 (-) (4.98) (5.53) (-) (5.09) (5.85) Flowers 65.16±0.73 72.61±0.78 73.55±0.86 59.8±1.02 67.37±0.83 68.93±0.79 (-) (7.45) (8.39) (-) (7.57) (9.13) Signs 67.34±0.43 70.56±0.87 71.45±0.90 61.95±0.66 68.03±0.64 69.41±0.5 (-) (3.22) (4.11) (-) (6.08) (7.46) Avg. 58.34±0.53 63.40±0.72 64.18±0.82 53.61±0.74 59.77±0.77 60.89±0.76 (-) (5.06) (5.84) (-) (6.16) (7.28) 18Published as a conference paper at ICLR 2021 Table 9: 5-way 5-shot classiﬁcation results of selected checkpoint with Adam Metric Dataset Checkpoint (Validation) Checkpoint (Last) Baseline Ours (Adam) Baseline Ours (Adam) Adam AT Only All Adam AT Only All All Mini 40.08±1.33 41.38±1.07 41.64±0.97 37.25±0.64 39.47±0.56 40.36±0.62 (-) (1.3) (1.56) (-) (2.21) (3.1) Birds 36.68±1.05 38.79±1 38.78±0.91 35.39±0.67 37.86±0.72 38.15±0.69 (-) (2.11) (2.10) (-) (2.47) (2.76) Flowers 58.00±1.55 60.92±1.24 60.36±1.14 54.36±0.79 59.09±0.63 59.32±0.69 (-) (2.92) (2.36) (-) (4.73) (4.96) Signs 52.65±0.57 52.37±0.6 52.53±0.64 50.48±0.6 51.13±0.45 51.95±0.49 (-) (-0.28) (-0.12) (-) (0.65) (1.47) Avg. 46.85±1.12 48.37±0.98 48.33±0.91 44.37±0.67 46.89±0.59 47.45±0.62 (-) (1.52) (1.48) (-) (2.52) (3.08) Top-1 Mini 58.89±0.73 60.35±0.83 60.08±0.83 55.25±1.02 57.79±0.81 57.86±0.92 (-) (1.46) (1.19) (-) (2.54) (2.61) Birds 47.92±0.75 50.88±0.66 50.84±0.75 46.02±0.85 49.89±1 49.78±0.97 (-) (2.96) (2.92) (-) (3.87) (3.76) Flowers 73.21±1.19 77.51±0.94 77.45±0.89 69.09±0.77 76.61±0.51 76.51±0.71 (-) (4.3) (4.24) (-) (7.52) (7.42) Signs 70.98±1.07 71.30±1.52 71.72±1.52 67.51±0.72 69.75±0.83 70.30±0.79 (-) (0.32) (0.74) (-) (2.24) (2.79) Avg. 62.75±0.93 65.01±0.99 65.02±1 59.47±0.84 63.51±0.79 63.61±0.85 (-) (2.26) (2.27) (-) (4.04) (4.14) Top-40% Mini 52.17±1.89 54.96±1.18 55.75±1.07 46.95±0.82 51.81±0.71 53.29±0.76 (-) (2.79) (3.58) (-) (4.86) (6.34) Birds 43.88±1.06 47.93±0.91 48.26±0.87 41.41±0.8 46.71±0.96 47.11±0.88 (-) (4.05) (4.38) (-) (5.3) (5.7) Flowers 68.43±1.76 73.86±1.02 74.03±1.02 63.44±0.77 72.41±0.79 72.56±0.64 (-) (5.43) (5.60) (-) (8.97) (9.12) Signs 66.85±0.78 67.36±1.3 67.89±1.39 62.95±0.81 65.43±0.68 66.37±0.71 (-) (0.51) (1.04) (-) (2.48) (3.42) Avg. 57.83±1.37 61.03±1.1 61.48±1.09 53.69±0.8 59.09±0.78 59.83±0.75 (-) (3.2) (3.65) (-) (5.4) (6.14) 0.2 0.22 0.24 0.26 0.28 0.3 0.32 0.34 0.36 0.38 0.4 0.42 0.44 0.46 0.48 0.5 0.0001 0.0004 0.0007 0.001 0.004 0.007 0.01 0.04 0.07 0.1 0.4 0.7 1 0.2 0.22 0.24 0.26 0.28 0.3 0.32 0.34 0.36 0.38 0.0001 0.0004 0.0007 0.001 0.004 0.007 0.01 0.04 0.07 0.1 0.4 0.7 1 Accuracy Stepsize w/o BN freezing w/ BN freezing 0.2 0.24 0.28 0.32 0.36 0.4 0.44 0.48 0.52 0.56 0.0001 0.0004 0.0007 0.001 0.004 0.007 0.01 0.04 0.07 0.1 0.4 0.7 1 0.2 0.22 0.24 0.26 0.28 0.3 0.32 0.34 0.36 0.38 0.4 0.42 0.44 0.46 0.48 0.0001 0.0004 0.0007 0.001 0.004 0.007 0.01 0.04 0.07 0.1 0.4 0.7 1 0.2 0.24 0.28 0.32 0.36 0.4 0.44 0.48 0.52 0.56 0.6 0.64 0.68 0.0001 0.0004 0.0007 0.001 0.004 0.007 0.01 0.04 0.07 0.1 0.4 0.7 1 (a) Mini 0.2 0.24 0.28 0.32 0.36 0.4 0.44 0.48 0.52 0.56 0.0001 0.0004 0.0007 0.001 0.004 0.007 0.01 0.04 0.07 0.1 0.4 0.7 1 Accuracy Stepsize w/o BN freezing w/ BN freezing (b) Birds 0.2 0.24 0.28 0.32 0.36 0.4 0.44 0.48 0.52 0.56 0.6 0.64 0.68 0.72 0.76 0.8 0.0001 0.0004 0.0007 0.001 0.004 0.007 0.01 0.04 0.07 0.1 0.4 0.7 1 (c) Flowers 0.2 0.24 0.28 0.32 0.36 0.4 0.44 0.48 0.52 0.56 0.6 0.64 0.68 0.72 0.76 0.8 0.0001 0.0004 0.0007 0.001 0.004 0.007 0.01 0.04 0.07 0.1 0.4 0.7 1 (d) Signs Figure 12: 5-way 1-shot (Top) and 5-way 5-shot (Bottom) classiﬁcation results of freezing BN layers. We tested freezing BN on SGD+All. SGD+All (w/ BN freezing) outperformed SGD+All (w/o BN freezing) in the higher stepsize range (>0.1). 19Published as a conference paper at ICLR 2021 Table 10: 5-way 1-shot and 5-way 5-shot classiﬁcation results of freezing BN layers. We tested freezing BN on our SGD+All. SGD+All (w/ freezing BN) outperformed in All metric on 5-way 1-shot and 5-way 5-shot. Metric Dataset 5-way 1-shot (SGD+All) 5-way 5-shot (SGD+All) w/ freezing BN w/o freezing BN w/ freezing BN w/o freezing BN All Mini 37.13±0.24 36.24±0.22 45.42±0.42 43.48±0.38 Birds 30.74±0.50 29.83±0.46 40.29±0.47 38.07±0.33 Flowers 43.88±0.56 40.86±0.45 59.41±0.68 53.10±0.55 Signs 36.37±0.67 34.76±0.61 54.65±0.56 51.02±0.50 Avg. 37.03±0.49 35.42±0.44 49.94±0.54 46.42±0.44 Top-1 Mini 46.48 ±0.54 46.62±0.58 62.18±0.66 62.16±0.66 Birds 36.41 ±0.59 36.43±0.58 51.90±0.73 51.82±0.64 Flowers 54.16 ±0.80 54.36±1.04 74.59±0.79 74.52±0.69 Signs 44.18 ±1.09 44.10±1.03 73.98±0.88 74.13±0.90 Avg. 45.31 ±0.76 45.37±0.81 65.66±0.76 65.66±0.72 Top-40% Mini 45.44 ±0.48 45.79±0.44 60.62±0.66 60.70±0.91 Birds 35.67 ±0.54 35.77±0.55 51.04±0.73 51.03±0.63 Flowers 52.97 ±0.71 53.16±0.73 73.56±0.73 73.55±0.86 Trafﬁc 43.38 ±0.99 43.39±1.01 71.21±0.79 71.45±0.90 Avg. 44.37 ±0.68 44.52±0.68 64.11±0.73 64.18±0.82 20",
      "meta_data": {
        "arxiv_id": "2103.09027v1",
        "authors": [
          "Namyeong Kwon",
          "Hwidong Na",
          "Gabriel Huang",
          "Simon Lacoste-Julien"
        ],
        "published_date": "2021-03-16T12:53:09Z",
        "pdf_url": "https://arxiv.org/pdf/2103.09027v1.pdf"
      },
      "llm_extracted_info": {
        "main_contributions": "The paper addresses the novel problem of repurposing pretrained Model-Agnostic Meta-Learning (MAML) checkpoints for new few-shot classification tasks, particularly in out-of-domain scenarios, without access to the original meta-training set. It proposes an alternative meta-testing procedure that combines MAML gradient steps with uncertainty-based stepsize adaptation (USA) and adversarial training using uncertainty-based Fast Gradient Sign Method (UFGSM). The method achieves improved accuracy and robustness to base stepsize choice on same-domain and cross-domain benchmarks compared to 'vanilla' MAML, using both SGD and Adam optimizers. It is noted as the first few-shot learning method to combine ensemble methods for stepsize computation and generating adversarial examples from the meta-testing support set for enhanced robustness.",
        "methodology": "The proposed method starts by taking a pretrained MAML checkpoint (θ0) and creating a deep ensemble by perturbing its parameters with multiplicative Gaussian noise. During T meta-testing steps, each ensemble model undergoes gradient descent using a combined cross-entropy and adversarial training (AT) loss on the support set. Layer-wise stepsizes (αadap) are dynamically adapted using Uncertainty-based Stepsize Adaptation (USA), which assigns lower stepsizes to layers with higher parameter uncertainty (standard deviation across the ensemble). Task-specific adversarial examples are generated using Uncertainty-based FGSM (UFGSM), which applies higher perturbation to input components exhibiting higher variance over the input gradient across the ensemble, and these examples augment the support set (EnAug). Finally, the original checkpoint is updated for T steps using a loss that combines cross-entropy on the support set, AT loss on the support set, and cross-entropy on the ensemble-augmented support set.",
        "experimental_setup": "MAML was meta-trained on the miniImageNet training split, and the proposed method was evaluated on the resulting checkpoint. Evaluation datasets included miniImageNet test split (same-domain, denoted Mini) and cross-domain datasets: CUB-200-2011 (Birds), Traffic Sign (Signs), and VGG Flower (Flowers). Experiments were conducted for 5-way 1-shot, 5-way 5-shot, and 10-way 1-shot classification tasks, using SGD and Adam optimizers. Key hyperparameters included an ensemble size of 5, an FGSM coefficient ϵ=0.05, a Gaussian perturbation scale σ=0.05, and 10 test-time gradient steps. Performance was measured using three metrics: 'All' (average accuracy over all stepsizes), 'Top-1' (accuracy of the best stepsize), and 'Top-40%' (average of the top 40% accuracies, indicating robustness to stepsize choice). Ablation studies were performed to analyze the contribution of each component (USA, UFGSM, EnAug, AT). Comparisons were also made between UFGSM and standard FGSM, SGD and Adam, and the method's performance on best vs. overfitted checkpoints, and the effect of freezing uncertain BatchNorm layers.",
        "limitations": "Not explicitly mentioned for the proposed method itself, but deep ensembles inherently add computational overhead during meta-testing compared to a single model. The problem addressed by the paper (inaccessibility of meta-training data for MAML) is itself a limitation of standard meta-learning approaches.",
        "future_research_directions": "Future work includes applying the proposed method to related settings such as domain adaptation and transfer learning. More generally, the empirical success of the method motivates further research into alternative ways to leverage published model checkpoints for improved generalization."
      }
    }
  ]
}